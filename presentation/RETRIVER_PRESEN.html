<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Krishna Suratwala">
<meta name="dcterms.date" content="2025-04-09">

<title>Retrieval-Augmented Generation (RAG) Pipeline for Machine Learning Q&amp;A: Theoretical Overview</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="RETRIVER_PRESEN_files/libs/clipboard/clipboard.min.js"></script>
<script src="RETRIVER_PRESEN_files/libs/quarto-html/quarto.js"></script>
<script src="RETRIVER_PRESEN_files/libs/quarto-html/popper.min.js"></script>
<script src="RETRIVER_PRESEN_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="RETRIVER_PRESEN_files/libs/quarto-html/anchor.min.js"></script>
<link href="RETRIVER_PRESEN_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="RETRIVER_PRESEN_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="RETRIVER_PRESEN_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="RETRIVER_PRESEN_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="RETRIVER_PRESEN_files/libs/bootstrap/bootstrap-83e1525847d6f4a6e75d0779fc47d721.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="RETRIVER_PRESEN_files/libs/quarto-diagram/mermaid.min.js"></script>
<script src="RETRIVER_PRESEN_files/libs/quarto-diagram/mermaid-init.js"></script>
<link href="RETRIVER_PRESEN_files/libs/quarto-diagram/mermaid.css" rel="stylesheet">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#objectives" id="toc-objectives" class="nav-link" data-scroll-target="#objectives">Objectives</a></li>
  <li><a href="#architectural-diagram" id="toc-architectural-diagram" class="nav-link" data-scroll-target="#architectural-diagram">Architectural Diagram</a>
  <ul class="collapse">
  <li><a href="#components" id="toc-components" class="nav-link" data-scroll-target="#components">Components</a></li>
  </ul></li>
  <li><a href="#flowchart" id="toc-flowchart" class="nav-link" data-scroll-target="#flowchart">Flowchart</a>
  <ul class="collapse">
  <li><a href="#workflow-steps" id="toc-workflow-steps" class="nav-link" data-scroll-target="#workflow-steps">Workflow Steps</a></li>
  </ul></li>
  <li><a href="#knowledge-base-creation-theoretical-framework" id="toc-knowledge-base-creation-theoretical-framework" class="nav-link" data-scroll-target="#knowledge-base-creation-theoretical-framework">Knowledge Base Creation: Theoretical Framework</a>
  <ul class="collapse">
  <li><a href="#process-description" id="toc-process-description" class="nav-link" data-scroll-target="#process-description">Process Description</a></li>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  </ul></li>
  <li><a href="#rag-pipeline-theoretical-framework" id="toc-rag-pipeline-theoretical-framework" class="nav-link" data-scroll-target="#rag-pipeline-theoretical-framework">RAG Pipeline: Theoretical Framework</a>
  <ul class="collapse">
  <li><a href="#retriever" id="toc-retriever" class="nav-link" data-scroll-target="#retriever">Retriever</a></li>
  <li><a href="#generator" id="toc-generator" class="nav-link" data-scroll-target="#generator">Generator</a></li>
  <li><a href="#workflow" id="toc-workflow" class="nav-link" data-scroll-target="#workflow">Workflow</a></li>
  </ul></li>
  <li><a href="#embeddings-and-indexing-deep-dive" id="toc-embeddings-and-indexing-deep-dive" class="nav-link" data-scroll-target="#embeddings-and-indexing-deep-dive">Embeddings and Indexing: Deep Dive</a>
  <ul class="collapse">
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings">Embeddings</a></li>
  <li><a href="#indexing" id="toc-indexing" class="nav-link" data-scroll-target="#indexing">Indexing</a></li>
  <li><a href="#integration" id="toc-integration" class="nav-link" data-scroll-target="#integration">Integration</a></li>
  </ul></li>
  <li><a href="#full-example" id="toc-full-example" class="nav-link" data-scroll-target="#full-example">Full Example</a>
  <ul class="collapse">
  <li><a href="#input" id="toc-input" class="nav-link" data-scroll-target="#input">Input</a></li>
  <li><a href="#knowledge-base-chunks" id="toc-knowledge-base-chunks" class="nav-link" data-scroll-target="#knowledge-base-chunks">Knowledge Base Chunks</a></li>
  <li><a href="#embedding-process" id="toc-embedding-process" class="nav-link" data-scroll-target="#embedding-process">Embedding Process</a></li>
  <li><a href="#indexing-and-retrieval" id="toc-indexing-and-retrieval" class="nav-link" data-scroll-target="#indexing-and-retrieval">Indexing and Retrieval</a></li>
  <li><a href="#generation-process" id="toc-generation-process" class="nav-link" data-scroll-target="#generation-process">Generation Process</a></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output">Output</a></li>
  </ul></li>
  <li><a href="#technical-notes" id="toc-technical-notes" class="nav-link" data-scroll-target="#technical-notes">Technical Notes</a></li>
  <li><a href="#status-and-future-directions" id="toc-status-and-future-directions" class="nav-link" data-scroll-target="#status-and-future-directions">Status and Future Directions</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Retrieval-Augmented Generation (RAG) Pipeline for Machine Learning Q&amp;A: Theoretical Overview</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Krishna Suratwala </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 9, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>This document provides a theoretical overview of a Retrieval-Augmented Generation (RAG) system designed to answer questions about classification algorithms. The system integrates a custom knowledge base derived from Wikipedia with advanced natural language processing techniques. It employs a retriever (using Sentence-BERT and FAISS) to fetch relevant information and a generator (TinyLLaMA) to synthesize concise answers. Below, we explore the architecture, workflows, and key concepts such as embeddings and indexing, supported by detailed diagrams and examples.</p>
<hr>
</section>
<section id="objectives" class="level2">
<h2 class="anchored" data-anchor-id="objectives">Objectives</h2>
<ul>
<li>Develop a structured knowledge base covering 108 machine learning topics from Wikipedia.</li>
<li>Design a RAG pipeline to retrieve contextually relevant content and generate accurate responses.</li>
<li>Explain critical technical components (embeddings and indexing) with theoretical depth and practical examples.</li>
</ul>
<hr>
</section>
<section id="architectural-diagram" class="level2">
<h2 class="anchored" data-anchor-id="architectural-diagram">Architectural Diagram</h2>
<p>Here’s a detailed text-based representation of the system architecture:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[User Query&lt;br&gt;e.g., 'What is logistic regression?'] --&gt;|Input| B[RAG Pipeline]
    
    subgraph B[RAG Pipeline]
        direction LR
        C[Retriever] --&gt;|Retrieves| E[Knowledge Base]
        D[Generator] --&gt;|Generates| F[Generated Answer]
        C --&gt;|Provides Context| D
    end
    
    subgraph C[Retriever]
        C1[Sentence-BERT&lt;br&gt;Semantic Embeddings] --&gt; C2[FAISS Index&lt;br&gt;L2 Similarity Search]
    end
    
    subgraph D[Generator]
        D1[TinyLLaMA&lt;br&gt;1.1B Parameters&lt;br&gt;Text Generation]
    end
    
    subgraph E[Knowledge Base]
        E1[108 Topics&lt;br&gt;Wikipedia]
        E2[Preprocessed Chunks]
        E3[JSON Format]
    end
    
    subgraph F[Generated Answer]
        F1[e.g., 'Logistic regression is a statistical...']
    end
    
    B --&gt;|Output| F
    
    style A fill:#dfd,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style C fill:#bbf,stroke:#333
    style D fill:#bbf,stroke:#333
    style E fill:#bbf,stroke:#333
    style F fill:#dfd,stroke:#333,stroke-width:2px
    style C1 fill:#e6f,stroke:#333
    style C2 fill:#e6f,stroke:#333
    style D1 fill:#e6f,stroke:#333
    style E1 fill:#e6f,stroke:#333
    style E2 fill:#e6f,stroke:#333
    style E3 fill:#e6f,stroke:#333
    style F1 fill:#e6f,stroke:#333
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<section id="components" class="level3">
<h3 class="anchored" data-anchor-id="components">Components</h3>
<ul>
<li><strong>User Query</strong>: The input question posed by the user, seeking concise information about machine learning concepts.</li>
<li><strong>Retriever</strong>:
<ul>
<li><strong>Sentence-BERT</strong>: Transforms text into 384-dimensional semantic embeddings for meaning-based comparison.</li>
<li><strong>FAISS</strong>: Indexes embeddings and performs similarity searches using Euclidean (L2) distance to retrieve top-K relevant chunks.</li>
</ul></li>
<li><strong>Knowledge Base</strong>: A preprocessed collection of 108 Wikipedia topics, stored as text chunks in a JSON structure, including metadata and mathematical content.</li>
<li><strong>Generator</strong>: TinyLLaMA, a lightweight small language model (SLM) with 1.1 billion parameters, generates human-readable answers from retrieved context.</li>
<li><strong>Output</strong>: The final answer, accompanied by retrieved chunks for transparency and validation.</li>
</ul>
<hr>
</section>
</section>
<section id="flowchart" class="level2">
<h2 class="anchored" data-anchor-id="flowchart">Flowchart</h2>
<p>Here’s a detailed text-based flowchart of the RAG process:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[Start] --&gt; B[Load Knowledge Base]
    B --&gt;|108 Topics from Wikipedia&lt;br&gt;Preprocessed into Chunks&lt;br&gt;Stored as JSON| C[Build Retriever]
    C --&gt;|Sentence-BERT: Embed Chunks into 384D Vectors&lt;br&gt;FAISS: Index Vectors with Flat L2| D[User Inputs Question]
    D --&gt;|e.g., 'What is logistic regression?'| E[Embed Question]
    E --&gt;|Sentence-BERT: Convert to 384D Vector&lt;br&gt;Captures Semantic Meaning| F[Retrieve Top-K Chunks]
    F --&gt;|FAISS: Compute L2 Distances&lt;br&gt;Return K=3 Closest Chunks| G[Load Small Language Model]
    G --&gt;|TinyLLaMA: 1.1B Parameters&lt;br&gt;Configured for Text Generation| H[Generate Answer]
    H --&gt;|Combine Question + Retrieved Chunks&lt;br&gt;Synthesize Concise Response| I[Output Answer + Retrieved Chunks]
    I --&gt;|Answer: Short, Accurate Response&lt;br&gt;Chunks: Transparency| J[End]

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style J fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333
    style C fill:#bbf,stroke:#333
    style D fill:#dfd,stroke:#333
    style E fill:#bbf,stroke:#333
    style F fill:#bbf,stroke:#333
    style G fill:#bbf,stroke:#333
    style H fill:#bbf,stroke:#333
    style I fill:#dfd,stroke:#333
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<section id="workflow-steps" class="level3">
<h3 class="anchored" data-anchor-id="workflow-steps">Workflow Steps</h3>
<ol type="1">
<li><strong>Load Knowledge Base</strong>: Accesses a preprocessed dataset of 108 Wikipedia topics, segmented into chunks for retrieval.</li>
<li><strong>Build Retriever</strong>: Converts chunks into semantic embeddings and indexes them for efficient search.</li>
<li><strong>Embed Question</strong>: Transforms the user’s question into a vector representation.</li>
<li><strong>Retrieve Top-K Chunks</strong>: Identifies the K most relevant chunks based on vector similarity.</li>
<li><strong>Generate Answer</strong>: Uses the retrieved context to produce a concise answer via the SLM.</li>
<li><strong>Output</strong>: Delivers the answer and supporting chunks to the user.</li>
</ol>
<hr>
</section>
</section>
<section id="knowledge-base-creation-theoretical-framework" class="level2">
<h2 class="anchored" data-anchor-id="knowledge-base-creation-theoretical-framework">Knowledge Base Creation: Theoretical Framework</h2>
<section id="process-description" class="level3">
<h3 class="anchored" data-anchor-id="process-description">Process Description</h3>
<ul>
<li><strong>Source</strong>: Content is sourced from 108 Wikipedia pages covering machine learning topics (e.g., Logistic Regression, Random Forest, SVM).</li>
<li><strong>Preprocessing</strong>:
<ul>
<li><strong>Text Cleaning</strong>: Removes extraneous characters, retaining basic punctuation and mathematical formulas (e.g., LaTeX like <code>\( w^T x + b = 0 \)</code>).</li>
<li><strong>Segmentation</strong>: Splits text into sentences and further into chunks of 200–300 words to optimize retrieval granularity.</li>
<li><strong>Mathematical Preservation</strong>: Identifies and preserves formulas (e.g., <code>\[ \min \frac{1}{2} ||w||^2 \]</code>) and key terms (e.g., <code>w</code>, <code>b</code>, <code>γ</code>) for technical accuracy.</li>
</ul></li>
<li><strong>Storage</strong>: Organizes data into a JSON structure with:
<ul>
<li><strong>Metadata</strong>: Includes topic title, URL, timestamp, and counts (e.g., sentences, formulas).</li>
<li><strong>Content</strong>: Stores cleaned text, sentence lists, chunked segments, and mathematical elements.</li>
</ul></li>
</ul>
</section>
<section id="purpose" class="level3">
<h3 class="anchored" data-anchor-id="purpose">Purpose</h3>
<ul>
<li>Provides a reliable, structured foundation of machine learning knowledge.</li>
<li>Ensures compatibility with retrieval systems by segmenting text into manageable units.</li>
<li>Maintains technical integrity by preserving mathematical content.</li>
</ul>
<hr>
</section>
</section>
<section id="rag-pipeline-theoretical-framework" class="level2">
<h2 class="anchored" data-anchor-id="rag-pipeline-theoretical-framework">RAG Pipeline: Theoretical Framework</h2>
<section id="retriever" class="level3">
<h3 class="anchored" data-anchor-id="retriever">Retriever</h3>
<ul>
<li><strong>Semantic Embeddings</strong>:
<ul>
<li><strong>Concept</strong>: Text is converted into 384-dimensional vectors that encode meaning, allowing semantic similarity comparisons.</li>
<li><strong>Mechanism</strong>: A transformer-based model processes text, leveraging contextual understanding to position similar meanings closer in vector space.</li>
</ul></li>
<li><strong>Similarity Search</strong>:
<ul>
<li><strong>Concept</strong>: An index organizes embeddings to enable rapid identification of the most relevant content.</li>
<li><strong>Mechanism</strong>: Uses Euclidean distance to measure vector proximity, retrieving the top-K closest matches.</li>
</ul></li>
</ul>
</section>
<section id="generator" class="level3">
<h3 class="anchored" data-anchor-id="generator">Generator</h3>
<ul>
<li><strong>Concept</strong>: A small language model synthesizes a natural language response from retrieved context.</li>
<li><strong>Mechanism</strong>: Combines the question and retrieved chunks into a prompt, then generates a concise answer using probabilistic text generation techniques.</li>
</ul>
</section>
<section id="workflow" class="level3">
<h3 class="anchored" data-anchor-id="workflow">Workflow</h3>
<ol type="1">
<li><strong>Initialization</strong>: Loads and indexes the knowledge base.</li>
<li><strong>Retrieval</strong>: Embeds the question and retrieves relevant chunks.</li>
<li><strong>Generation</strong>: Produces an answer based on the retrieved context.</li>
<li><strong>Delivery</strong>: Presents the answer with supporting evidence.</li>
</ol>
<hr>
</section>
</section>
<section id="embeddings-and-indexing-deep-dive" class="level2">
<h2 class="anchored" data-anchor-id="embeddings-and-indexing-deep-dive">Embeddings and Indexing: Deep Dive</h2>
<section id="embeddings" class="level3">
<h3 class="anchored" data-anchor-id="embeddings">Embeddings</h3>
<ul>
<li><strong>Definition</strong>: Embeddings are high-dimensional vectors (384D in this case) that represent text in a way that captures its semantic meaning.</li>
<li><strong>How They Work</strong>:
<ul>
<li>Text is processed through a transformer model with 6 layers, which uses self-attention to understand word relationships.</li>
<li>Output: A fixed-size vector (e.g., <code>[0.12, -0.34, ..., 0.89]</code>) where proximity in vector space indicates semantic similarity.</li>
<li>Example: “Logistic regression” and “binary classification” yield vectors closer together than “logistic regression” and “car engine.”</li>
</ul></li>
<li><strong>Purpose</strong>:
<ul>
<li>Enables machines to compare text based on meaning rather than exact word matches.</li>
<li>Forms the basis for retrieval by providing a numerical representation of content.</li>
</ul></li>
</ul>
</section>
<section id="indexing" class="level3">
<h3 class="anchored" data-anchor-id="indexing">Indexing</h3>
<ul>
<li><strong>Definition</strong>: Indexing is the organization of embeddings into a searchable structure for efficient similarity retrieval.</li>
<li><strong>How It Works</strong>:
<ul>
<li>Embeddings are stored in a matrix, and an index computes distances between a query vector and all stored vectors.</li>
<li>Uses <strong>L2 Distance</strong> (Euclidean): [ d(, ) = ] where smaller distances indicate higher similarity.</li>
<li>Example: A Flat L2 index performs exact searches by calculating distances for all vectors, returning the top-K nearest.</li>
</ul></li>
<li><strong>Purpose</strong>:
<ul>
<li>Reduces retrieval time from linear (O(n)) to near-constant time for small datasets, scalable with approximate methods for larger ones.</li>
<li>Ensures relevant content is quickly identified.</li>
</ul></li>
</ul>
</section>
<section id="integration" class="level3">
<h3 class="anchored" data-anchor-id="integration">Integration</h3>
<ul>
<li><strong>Embedding → Indexing</strong>: Text is embedded into vectors, then indexed for fast search.</li>
<li><strong>Search Process</strong>: A question’s embedding is compared against the index to retrieve the most semantically similar chunks.</li>
</ul>
<hr>
</section>
</section>
<section id="full-example" class="level2">
<h2 class="anchored" data-anchor-id="full-example">Full Example</h2>
<section id="input" class="level3">
<h3 class="anchored" data-anchor-id="input">Input</h3>
<ul>
<li><strong>Question</strong>: “What is logistic regression, answer in short?”</li>
</ul>
</section>
<section id="knowledge-base-chunks" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-base-chunks">Knowledge Base Chunks</h3>
<ol type="1">
<li><strong>Chunk 1</strong> (From Logistic_regression): “Logistic regression is a statistical method for binary classification. It uses the logistic function to model probabilities.”</li>
<li><strong>Chunk 2</strong> (From Linear_regression): “Linear regression predicts continuous outcomes using a linear equation, unlike logistic regression.”</li>
<li><strong>Chunk 3</strong> (From Machine_learning): “Machine learning includes methods like logistic regression and SVM for predictive modeling.”</li>
</ol>
</section>
<section id="embedding-process" class="level3">
<h3 class="anchored" data-anchor-id="embedding-process">Embedding Process</h3>
<ul>
<li><strong>Conceptual Vectors</strong> (Simplified from 384D to 3D for illustration):
<ul>
<li>Chunk 1: ( = [0.12, -0.34, 0.89] )</li>
<li>Chunk 2: ( = [0.45, 0.23, -0.67] )</li>
<li>Chunk 3: ( = [0.78, -0.12, 0.34] )</li>
<li>Question: ( = [0.15, -0.30, 0.85] )</li>
</ul></li>
</ul>
</section>
<section id="indexing-and-retrieval" class="level3">
<h3 class="anchored" data-anchor-id="indexing-and-retrieval">Indexing and Retrieval</h3>
<ul>
<li><strong>Index Structure</strong> (Simplified):</li>
</ul>
<p>[ [0.12, -0.34, 0.89], // Chunk 1 [0.45, 0.23, -0.67], // Chunk 2 [0.78, -0.12, 0.34] // Chunk 3]</p>
<ul>
<li><strong>L2 Distance Calculation</strong>:</li>
<li>( d(, ) = )</li>
<li>( d(, ) )</li>
<li>( d(, ) )</li>
<li><strong>Top-2 Results</strong>: Chunks 1 and 3 (smallest distances).</li>
</ul>
</section>
<section id="generation-process" class="level3">
<h3 class="anchored" data-anchor-id="generation-process">Generation Process</h3>
<ul>
<li><strong>Context Combined</strong>:</li>
<li>“From Logistic_regression: Logistic regression is a statistical method for binary classification…”</li>
<li>“From Machine_learning: Machine learning includes methods like logistic regression and SVM…”</li>
<li><strong>Generated Answer</strong>: “Logistic regression is a statistical method for binary classification using the logistic function.”</li>
</ul>
</section>
<section id="output" class="level3">
<h3 class="anchored" data-anchor-id="output">Output</h3>
<ul>
<li><strong>Answer</strong>: “Logistic regression is a statistical method for binary classification using the logistic function.”</li>
<li><strong>Retrieved Chunks</strong>:</li>
</ul>
<ol type="1">
<li>“Logistic regression is a statistical method for binary classification…”</li>
<li>“Machine learning includes methods like logistic regression and SVM…”</li>
</ol>
<hr>
</section>
</section>
<section id="technical-notes" class="level2">
<h2 class="anchored" data-anchor-id="technical-notes">Technical Notes</h2>
<ul>
<li><strong>Embedding Size</strong>: 384D vectors provide a balance between semantic richness and computational efficiency.</li>
<li><strong>Indexing Choice</strong>: Flat L2 index ensures exact matches, suitable for 108 topics; scalable with approximate nearest neighbors (ANN) for larger datasets.</li>
<li><strong>Generation Model</strong>: TinyLLaMA’s 1.1 billion parameters offer lightweight yet effective text synthesis.</li>
<li><strong>Advantages</strong>:</li>
<li><strong>Scalability</strong>: Efficient retrieval even with growing knowledge bases.</li>
<li><strong>Accuracy</strong>: Semantic embeddings capture meaning beyond keyword matching.</li>
<li><strong>Transparency</strong>: Retrieved chunks allow validation of answers.</li>
<li><strong>Challenges</strong>:</li>
<li>Limited to the scope of 108 topics; niche details may be missed.</li>
<li>Mathematical formulas are preserved but not rendered in answers.</li>
<li>SLM may lack the reasoning depth of larger models.</li>
</ul>
<hr>
</section>
<section id="status-and-future-directions" class="level2">
<h2 class="anchored" data-anchor-id="status-and-future-directions">Status and Future Directions</h2>
<ul>
<li><strong>Current State</strong>: Knowledge base covers 108 topics; RAG pipeline is fully functional for short-answer queries.</li>
<li><strong>Future Work</strong>:</li>
<li>Expand the knowledge base with additional topics or external sources.</li>
<li>Enhance SLM capabilities for deeper reasoning.</li>
<li>Integrate LaTeX rendering for mathematical formulas in answers.</li>
</ul>
<hr>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>#This RAG system combines a robust knowledge base with state-of-the-art retrieval and generation techniques to deliver accurate, context-aware answers about machine learning. The use of embeddings and indexing ensures semantic relevance and efficiency, making it a scalable solution for educational and technical applications.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>