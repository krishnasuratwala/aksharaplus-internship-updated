{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b00b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#google colab link:-\n",
    "#https://colab.research.google.com/drive/1RP1tFr_FQJIPz5qzUljN-i6IsGo2-yQD#scrollTo=wlErfRoKPUOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975bf1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted.\")\n",
    "\n",
    "# Define your base project directory within Google Drive\n",
    "# IMPORTANT: Replace 'my_finetune_project' with your actual project folder name.\n",
    "# It's recommended to create this folder in your Drive first (e.g., in \"My Drive/Colab Notebooks/my_finetune_project\")\n",
    "BASE_PROJECT_DIR = \"/content/drive/My Drive/Colab Notebooks/finetuned_+_rag_project\"\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(BASE_PROJECT_DIR, exist_ok=True)\n",
    "print(f\"Base project directory set to: {BASE_PROJECT_DIR}\")\n",
    "\n",
    "# Optional: Change current working directory to your project folder\n",
    "# This makes it easier to use relative paths later, but absolute paths are safer.\n",
    "# %cd {BASE_PROJECT_DIR}\n",
    "# print(f\"Changed current working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "import os\n",
    "import shutil\n",
    "import faiss # Make sure faiss is imported here too for consistency with path definition\n",
    "\n",
    "global GDRIVE_KNOWLEDGE_BASE_PATH, GDRIVE_FINETUNED_MODEL_PATH, GDRIVE_FAISS_INDEX_PATH, KNOWLEDGE_BASE_PATH_TO_USE\n",
    "\n",
    "GDRIVE_KNOWLEDGE_BASE_PATH = os.path.join(BASE_PROJECT_DIR, \"rephrased_output.json\")\n",
    "GDRIVE_FINETUNED_MODEL_PATH = os.path.join(BASE_PROJECT_DIR, \"finetuned_qwen\")\n",
    "GDRIVE_FAISS_INDEX_PATH = os.path.join(BASE_PROJECT_DIR, \"my_faiss_index.bin\")\n",
    "\n",
    "LOCAL_TEMP_DIR = \"/content/temp_rag_data\"\n",
    "os.makedirs(LOCAL_TEMP_DIR, exist_ok=True)\n",
    "\n",
    "LOCAL_KNOWLEDGE_BASE_PATH = os.path.join(LOCAL_TEMP_DIR, \"rephrased_output.json\")\n",
    "\n",
    "if not os.path.exists(LOCAL_KNOWLEDGE_BASE_PATH) and os.path.exists(GDRIVE_KNOWLEDGE_BASE_PATH):\n",
    "    print(f\"Copying knowledge base from Google Drive to local: {GDRIVE_KNOWLEDGE_BASE_PATH} -> {LOCAL_KNOWLEDGE_BASE_PATH}\")\n",
    "    shutil.copy(GDRIVE_KNOWLEDGE_BASE_PATH, LOCAL_KNOWLEDGE_BASE_PATH)\n",
    "    print(\"Knowledge base copied to local storage.\")\n",
    "elif not os.path.exists(GDRIVE_KNOWLEDGE_BASE_PATH):\n",
    "    print(f\"Error: Knowledge base file not found in Google Drive at {GDRIVE_KNOWLEDGE_BASE_PATH}. Please ensure it's uploaded.\")\n",
    "else:\n",
    "    print(f\"Knowledge base already exists locally at {LOCAL_KNOWLEDGE_BASE_PATH}. Skipping copy.\")\n",
    "\n",
    "KNOWLEDGE_BASE_PATH_TO_USE = LOCAL_KNOWLEDGE_BASE_PATH\n",
    "\n",
    "print(f\"Knowledge base path for use: {KNOWLEDGE_BASE_PATH_TO_USE}\")\n",
    "print(f\"Fine-tuned model path: {GDRIVE_FINETUNED_MODEL_PATH}\")\n",
    "print(f\"FAISS index path (Drive): {GDRIVE_FAISS_INDEX_PATH}\")\n",
    "\n",
    "if not os.path.exists(GDRIVE_FINETUNED_MODEL_PATH):\n",
    "    print(f\"Warning: Fine-tuned model directory not found at {GDRIVE_FINETUNED_MODEL_PATH}.\")\n",
    "    print(\"Please ensure your fine-tuning script has completed successfully and saved the model to this location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting RAG & Gradio library installation process...\")\n",
    "!pip install sentence-transformers -q\n",
    "print(\"Installed sentence-transformers.\")\n",
    "import torch # Important for checking GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU detected. Installing faiss-gpu...\")\n",
    "    !pip install faiss-gpu -q\n",
    "else:\n",
    "    print(\"No GPU detected. Installing faiss-cpu...\")\n",
    "    !pip install faiss-cpu -q\n",
    "print(\"Installed FAISS.\")\n",
    "!pip install gradio -q\n",
    "print(\"Installed Gradio.\")\n",
    "!pip install -U bitsandbytes accelerate -q # Crucial for 8-bit quantization\n",
    "print(\"Installed/Upgraded bitsandbytes and accelerate.\")\n",
    "!pip install numpy huggingface_hub -q\n",
    "print(\"Installed numpy and huggingface_hub.\")\n",
    "print(\"\\n--- ALL REQUIRED LIBRARY INSTALLATIONS COMPLETE ---\")\n",
    "print(\"Important: Please RESTART RUNTIME (Runtime -> Restart runtime) now,\")\n",
    "print(\"then re-run cells from the top (starting with Drive Mount)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0422f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Question Generation Pipeline Functions\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from huggingface_hub import login # Keep if still using for model access\n",
    "import os\n",
    "\n",
    "# Load the knowledge base with nested \"Introduction\" structure\n",
    "def load_knowledge_base(knowledge_base_path):\n",
    "    print(f\"Attempting to load knowledge base from: {knowledge_base_path}\")\n",
    "    try:\n",
    "        with open(knowledge_base_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                f.seek(0)\n",
    "                data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "        chunks = []\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]\n",
    "        for item in data:\n",
    "            for topic_name, sections in item.items():\n",
    "                for section_name, levels in sections.items():\n",
    "                    for i in range(1, 4):\n",
    "                        level_key = f\"chunks_level{i}\"\n",
    "                        if level_key in levels and isinstance(levels[level_key], list):\n",
    "                            for chunk in levels[level_key]:\n",
    "                                if isinstance(chunk, dict) and \"text\" in chunk:\n",
    "                                    chunks.append({\n",
    "                                        \"text\": chunk[\"text\"],\n",
    "                                        \"title\": topic_name,\n",
    "                                        \"section\": section_name,\n",
    "                                        \"level\": i\n",
    "                                    })\n",
    "                                else:\n",
    "                                    print(f\"Warning: Skipping malformed chunk in {level_key}: {chunk}\")\n",
    "                        elif level_key in levels:\n",
    "                            print(f\"Warning: Expected list for {level_key}, found {type(levels[level_key])}. Skipping.\")\n",
    "        if not chunks:\n",
    "            raise ValueError(\"No valid chunks found in knowledge base after parsing.\")\n",
    "        print(f\"Loaded {len(chunks)} chunks from knowledge base.\")\n",
    "        return chunks\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Knowledge base not found at {knowledge_base_path}. Please check the path and ensure it's in your Google Drive or copied locally.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON/JSONL format in {knowledge_base_path}: {e}. Inspect the file for malformed lines.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading knowledge base: {e}\")\n",
    "\n",
    "# NEW: Retrieve chunks based on Topic, Subtopic, and Difficulty Level\n",
    "def retrieve_chunks_by_criteria(all_chunks, topic, subtopic, level, num_chunks=3):\n",
    "    # print(f\"Retrieving chunks for Topic: '{topic}', Subtopic: '{subtopic}', Level: '{level}'\") # Muted for cleaner console\n",
    "\n",
    "    filtered_chunks = []\n",
    "    for chunk in all_chunks:\n",
    "        topic_match = chunk['title'].lower() == topic.lower()\n",
    "        subtopic_match = chunk['section'].lower() == subtopic.lower()\n",
    "        level_match = chunk['level'] == level\n",
    "\n",
    "        if topic_match and subtopic_match and level_match:\n",
    "            filtered_chunks.append(chunk)\n",
    "\n",
    "    if not filtered_chunks:\n",
    "        # print(f\"Warning: No chunks found for Topic: '{topic}', Subtopic: '{subtopic}', Level: '{level}'. Returning empty list.\") # Muted for cleaner console\n",
    "        return []\n",
    "\n",
    "    selected_chunks = random.sample(filtered_chunks, min(num_chunks, len(filtered_chunks)))\n",
    "    # print(f\"Retrieved {len(selected_chunks)} chunks based on criteria.\") # Muted for cleaner console\n",
    "    return selected_chunks\n",
    "\n",
    "# Load fine-tuned Qwen model (max_new_tokens updated)\n",
    "# (Keeping this function as is)\n",
    "def load_qwen_local(finetuned_model_path):\n",
    "    print(f\"Loading fine-tuned Qwen model from: {finetuned_model_path}\")\n",
    "    try:\n",
    "        base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path, trust_remote_code=True)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Base Qwen model loaded.\")\n",
    "\n",
    "        model = PeftModel.from_pretrained(base_model, finetuned_model_path, trust_remote_code=True)\n",
    "        model.eval()\n",
    "\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=250, # Increased max_new_tokens slightly for potentially longer answers\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        print(\"Text generation pipeline created.\")\n",
    "        return generator\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load fine-tuned Qwen model: {e}\")\n",
    "\n",
    "# NEW: Generate different types of questions with refined prompts and parsing\n",
    "def generate_question(topic, subtopic, level, question_type, retrieved_chunks, generator):\n",
    "    context = \"\\n\".join([f\"From {chunk['title']} - {chunk['section']} (Level {chunk['level']}):\\n{chunk['text']}\" for chunk in retrieved_chunks])\n",
    "\n",
    "    if not context:\n",
    "        return \"Not enough relevant context to generate a question.\", \"\", \"\"\n",
    "\n",
    "    question_output = \"\"\n",
    "    blank_val = \"\"\n",
    "    correct_ans = \"\"\n",
    "\n",
    "    # --- Prompt Engineering for Structured Output ---\n",
    "    # Using explicit start/end markers and very clear instructions\n",
    "    # Increased max_new_tokens in generator call for more flexibility\n",
    "\n",
    "    if question_type == \"General Question\":\n",
    "        instruction = \"Generate a concise and clear general question based on the provided context. The question should immediately follow the 'QUESTION:' tag.\"\n",
    "\n",
    "        prompt_template = f\"\"\"You are an AI tutor helping a student learn machine learning.\n",
    "\n",
    "Context from the textbook:\n",
    "{context}\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "QUESTION: \"\"\" # Added a space after the colon to encourage content generation\n",
    "\n",
    "    elif question_type == \"Fill in the Blanks\":\n",
    "        instruction = \"Generate a fill-in-the-blank question from the provided context. Choose a key term or concept to be the blank. Provide the blanked sentence. Then, on a new line, provide ONLY the correct answer for the blank. The answer must be a single word. Ensure the question immediately follows 'FIB_QUESTION:' and the answer immediately follows 'FIB_ANSWER:'\"\n",
    "\n",
    "        prompt_template = f\"\"\"You are an AI tutor helping a student learn machine learning.\n",
    "\n",
    "Context from the textbook:\n",
    "{context}\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "FIB_QUESTION: \"\"\" # Added a space after the colon\n",
    "        # The prompt also needs to guide the model to put the answer immediately after FIB_ANSWER:\n",
    "        # This will be handled in how we strip the generated text\n",
    "        # prompt_template += \"\\nFIB_ANSWER: \" # This line is handled by the model completing the pattern\n",
    "\n",
    "    elif question_type == \"Multiple Choice Question\":\n",
    "        instruction = \"Generate a multiple-choice question (MCQ) from the provided context. The question should have one correct answer and three plausible incorrect options. Clearly label the options (A, B, C, D). Then, on a new line, provide ONLY the correct option letter (e.g., 'A', 'B', 'C', or 'D'). Ensure the question immediately follows 'MCQ_QUESTION:' and the answer immediately follows 'MCQ_ANSWER:'\"\n",
    "\n",
    "        prompt_template = f\"\"\"You are an AI tutor helping a student learn machine learning.\n",
    "\n",
    "Context from the textbook:\n",
    "{context}\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "MCQ_QUESTION: \"\"\" # Added a space after the colon\n",
    "        # prompt_template += \"\\nA) \\nB) \\nC) \\nD) \\nMCQ_ANSWER: \" # Model should generate these\n",
    "\n",
    "    else:\n",
    "        return \"Invalid question type selected.\", \"\", \"\"\n",
    "\n",
    "    # Generate response\n",
    "    response = generator(prompt_template, max_new_tokens=400, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "    generated_text = response[0][\"generated_text\"]\n",
    "\n",
    "    # --- Robust Parsing Logic ---\n",
    "    # Look for the specific markers to extract the exact parts\n",
    "\n",
    "    if question_type == \"General Question\":\n",
    "        # Expected: \"QUESTION: [Your generated question text]\"\n",
    "        if \"QUESTION:\" in generated_text:\n",
    "            # We want everything *after* \"QUESTION: \" until the next instruction or end of text\n",
    "            start_index = generated_text.find(\"QUESTION:\") + len(\"QUESTION:\")\n",
    "            question_output = generated_text[start_index:].strip()\n",
    "            # Clean up any trailing instructions or conversational filler generated by the model\n",
    "            question_output = question_output.split(\"Instruction:\")[0].split(\"Context from the textbook:\")[0].strip()\n",
    "            question_output = question_output.split(\"Question:\")[0].strip() # Catch variations\n",
    "        else:\n",
    "            question_output = \"N/A (Failed to parse General Question)\"\n",
    "\n",
    "    elif question_type == \"Fill in the Blanks\":\n",
    "        # Expected: \"FIB_QUESTION: [Blanked sentence]\\nFIB_ANSWER: [Single-word answer]\"\n",
    "        fib_q_marker = \"FIB_QUESTION:\"\n",
    "        fib_a_marker = \"FIB_ANSWER:\"\n",
    "\n",
    "        if fib_q_marker in generated_text and fib_a_marker in generated_text:\n",
    "            q_start = generated_text.find(fib_q_marker) + len(fib_q_marker)\n",
    "            a_start = generated_text.find(fib_a_marker)\n",
    "\n",
    "            question_output = generated_text[q_start:a_start].strip() # Get text between Q and A markers\n",
    "            correct_ans = generated_text[a_start + len(fib_a_marker):].strip().split('\\n')[0].strip() # Get only the first line/word after FIB_ANSWER:\n",
    "\n",
    "            # Attempt to extract blank_val if '_____' is present and answer is single word\n",
    "            if '_____' in question_output:\n",
    "                blank_val = correct_ans # The single word answer is the blank value\n",
    "            else:\n",
    "                blank_val = \"N/A (No blank found)\"\n",
    "        else:\n",
    "            question_output = \"N/A (Failed to parse FIB Question)\"\n",
    "            correct_ans = \"N/A (Failed to parse FIB Answer)\"\n",
    "            blank_val = \"N/A\"\n",
    "\n",
    "    elif question_type == \"Multiple Choice Question\":\n",
    "        # Expected: \"MCQ_QUESTION: [Question]\\nA) [Option A]\\n...\\nMCQ_ANSWER: [Letter]\"\n",
    "        mcq_q_marker = \"MCQ_QUESTION:\"\n",
    "        mcq_a_marker = \"MCQ_ANSWER:\"\n",
    "\n",
    "        if mcq_q_marker in generated_text and mcq_a_marker in generated_text:\n",
    "            q_start = generated_text.find(mcq_q_marker) + len(mcq_q_marker)\n",
    "            a_start = generated_text.find(mcq_a_marker)\n",
    "\n",
    "            question_output = generated_text[q_start:a_start].strip() # Get text between Q and A markers\n",
    "            correct_ans = generated_text[a_start + len(mcq_a_marker):].strip().split('\\n')[0].strip() # Get only the first line/letter after MCQ_ANSWER:\n",
    "        else:\n",
    "            question_output = \"N/A (Failed to parse MCQ Question)\"\n",
    "            correct_ans = \"N/A (Failed to parse MCQ Answer)\"\n",
    "\n",
    "    return question_output, blank_val, correct_ans\n",
    "\n",
    "# Global variables for single loading (optimization)\n",
    "_chunks = None\n",
    "_generator = None\n",
    "\n",
    "# Main question generation pipeline function\n",
    "def question_generation_pipeline(topic, subtopic, level, question_type):\n",
    "    print(f\"\\n--- Running question generation pipeline for: '{topic}' - '{subtopic}' - Level {level}, Type: '{question_type}' ---\")\n",
    "    try:\n",
    "        global _chunks, _generator\n",
    "\n",
    "        if _generator is None:\n",
    "            print(\"Initializing Qwen model and loading knowledge base for the first time...\")\n",
    "            # Use the global variables (defined in Cell 2) to pass as arguments\n",
    "            # Ensure KNOWLEDGE_BASE_PATH_TO_USE and GDRIVE_FINETUNED_MODEL_PATH are defined globally\n",
    "            # For demonstration, I'll add placeholder definitions here for local testing if not in Colab\n",
    "            # In a real Colab setup, these would be defined in an earlier cell.\n",
    "            if 'KNOWLEDGE_BASE_PATH_TO_USE' not in globals():\n",
    "                print(\"WARNING: KNOWLEDGE_BASE_PATH_TO_USE not defined. Using dummy path for testing.\")\n",
    "                KNOWLEDGE_BASE_PATH_TO_USE = \"dummy_knowledge_base.json\" # Replace with your actual path\n",
    "            if 'GDRIVE_FINETUNED_MODEL_PATH' not in globals():\n",
    "                print(\"WARNING: GDRIVE_FINETUNED_MODEL_PATH not defined. Using dummy path for testing.\")\n",
    "                GDRIVE_FINETUNED_MODEL_PATH = \"dummy_model_path\" # Replace with your actual path\n",
    "\n",
    "            _chunks = load_knowledge_base(KNOWLEDGE_BASE_PATH_TO_USE)\n",
    "            _generator = load_qwen_local(GDRIVE_FINETUNED_MODEL_PATH)\n",
    "            print(\"Qwen model and knowledge base initialized.\")\n",
    "\n",
    "        # Retrieve chunks based on user criteria\n",
    "        retrieved_chunks = retrieve_chunks_by_criteria(_chunks, topic, subtopic, level, num_chunks=3)\n",
    "\n",
    "        if not retrieved_chunks:\n",
    "            print(f\"Warning: No relevant chunks found for Topic: '{topic}', Subtopic: '{subtopic}', Level: '{level}'. Cannot generate question.\")\n",
    "            return \"No relevant context found to generate a question for your selection.\", \"\", \"\"\n",
    "\n",
    "        # Generate question using the new function\n",
    "        question_output, blank_val, correct_ans = generate_question(topic, subtopic, level, question_type, retrieved_chunks, _generator)\n",
    "\n",
    "        # --- Console Output Control ---\n",
    "        print(\"\\nüîç Retrieved Chunks:\")\n",
    "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "            print(f\"**{i}.** From *{chunk['title']} - {chunk['section']}* (Level {chunk['level']}):\")\n",
    "            print(f\"{chunk['text'][:300]}...\\n\") # Display first 300 chars of retrieved chunk text\n",
    "\n",
    "        print(f\"\\nüìå Generated {question_type}:\")\n",
    "        print(f\"Question: {question_output}\")\n",
    "        if blank_val and blank_val != \"N/A\": # Only print if meaningful\n",
    "            print(f\"Blank Value: {blank_val}\")\n",
    "        if correct_ans and correct_ans != \"N/A (Failed to parse FIB Answer)\": # Only print if meaningful\n",
    "            print(f\"Correct Answer/Option: {correct_ans}\")\n",
    "\n",
    "\n",
    "        return question_output, blank_val, correct_ans\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in question generation pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for debugging\n",
    "        return f\"Error: {e}\", \"\", \"\" # Return error to Gradio\n",
    "\n",
    "# Example usage (for testing in a non-Gradio context)\n",
    "if __name__ == \"__main__\":\n",
    "    # Define dummy paths for local testing IF you don't have Cell 2 values set\n",
    "    # In a real Colab notebook, ensure these are set in an earlier cell.\n",
    "    global KNOWLEDGE_BASE_PATH_TO_USE, GDRIVE_FINETUNED_MODEL_PATH\n",
    "    KNOWLEDGE_BASE_PATH_TO_USE = \"dummy_knowledge_base.json\" # Replace with your actual path\n",
    "    GDRIVE_FINETUNED_MODEL_PATH = \"dummy_model_path\" # Replace with your actual path\n",
    "\n",
    "    # Create a dummy knowledge base file for testing `load_knowledge_base`\n",
    "    # This is crucial for local testing if the file doesn't exist\n",
    "    dummy_kb_content = {\n",
    "        \"Introduction\": {\n",
    "            \"Linear Regression\": {\n",
    "                \"chunks_level1\": [{\"text\": \"Linear regression is a basic type of machine learning.\", \"id\": \"lr_intro_e1\"}],\n",
    "                \"chunks_level2\": [{\"text\": \"Linear regression models the relationship between a dependent variable and one or more independent variables using a linear equation.\", \"id\": \"lr_intro_m1\"}],\n",
    "                \"chunks_level3\": [{\"text\": \"Understanding the assumptions of linear regression, such as linearity, independence, homoscedasticity, and normality of residuals, is crucial for its valid application.\", \"id\": \"lr_intro_h1\"}]\n",
    "            },\n",
    "            \"Decision Trees\": {\n",
    "                \"chunks_level1\": [{\"text\": \"Decision trees are flow-chart like structures.\", \"id\": \"dt_intro_e1\"}],\n",
    "                \"chunks_level2\": [{\"text\": \"A decision tree is a non-parametric supervised learning method used for classification and regression.\", \"id\": \"dt_intro_m1\"}],\n",
    "                \"chunks_level3\": [{\"text\": \"The core algorithm for building decision trees is often based on concepts like Gini impurity or information gain for splitting nodes.\", \"id\": \"dt_intro_h1\"}]\n",
    "            }\n",
    "        },\n",
    "        \"Logistic Regression\": {\n",
    "            \"Introduction\": {\n",
    "                \"chunks_level1\": [{\"text\": \"Logistic regression is used for binary classification problems. It predicts a probability.\", \"id\": \"logreg_intro_e1\"}],\n",
    "                \"chunks_level2\": [{\"text\": \"Probabilistic classification algorithms like Logistic Regression use statistics to determine the most likely category for an input, providing a probability rather than just a direct classification.\", \"id\": \"logreg_intro_m1\"}],\n",
    "                \"chunks_level3\": [{\"text\": \"The logistic function (sigmoid) transforms arbitrary real-valued input into a value between 0 and 1, representing a probability. The logit function is its inverse.\", \"id\": \"logreg_intro_h1\"}]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    with open(KNOWLEDGE_BASE_PATH_TO_USE, \"w\") as f:\n",
    "        json.dump(dummy_kb_content, f)\n",
    "    print(f\"Created dummy knowledge base at {KNOWLEDGE_BASE_PATH_TO_USE}\")\n",
    "\n",
    "    # You would typically load your actual model here.\n",
    "    # For a local test without a real model, this will fail unless you mock `AutoModelForCausalLM` and `AutoTokenizer`.\n",
    "    # To run this `if __name__ == \"__main__\":` block fully without a GPU and real model,\n",
    "    # you'd need to mock the `transformers` and `peft` parts.\n",
    "    # For now, let's assume `load_qwen_local` will attempt to load, and if it fails,\n",
    "    # the error handling will catch it.\n",
    "\n",
    "    test_cases = [\n",
    "\n",
    "        {\"topic\": \"Logistic Regression\", \"subtopic\": \"Introduction\", \"level\": 2, \"type\": \"Fill in the Blanks\"},\n",
    "\n",
    "    ]\n",
    "\n",
    "    for case in test_cases:\n",
    "        # Note: If you don't have the Qwen model locally, these calls will fail unless mocked.\n",
    "        # This is for demonstrating prompt/parsing logic changes, not full execution without model.\n",
    "        question_output, blank_val, correct_ans = question_generation_pipeline(\n",
    "            case[\"topic\"], case[\"subtopic\"], case[\"level\"], case[\"type\"]\n",
    "        )\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\") # Larger separator for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f760726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Gradio Interface\n",
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Assume KNOWLEDGE_BASE_PATH_TO_USE is defined in Cell 2\n",
    "# And question_generation_pipeline, load_knowledge_base, etc. are defined in Cell 4\n",
    "\n",
    "# --- Helper to dynamically get topics, subtopics, and levels from the knowledge base ---\n",
    "def get_knowledge_base_structure(knowledge_base_path):\n",
    "    topics = set()\n",
    "    subtopics_by_topic = {}\n",
    "    levels = set()\n",
    "\n",
    "    try:\n",
    "        with open(knowledge_base_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]\n",
    "\n",
    "        for item in data:\n",
    "            for topic_name, sections in item.items():\n",
    "                topics.add(topic_name)\n",
    "                if topic_name not in subtopics_by_topic:\n",
    "                    subtopics_by_topic[topic_name] = set()\n",
    "                for section_name, levels_data in sections.items():\n",
    "                    subtopics_by_topic[topic_name].add(section_name)\n",
    "                    for i in range(1, 4): # Assuming levels 1, 2, 3 based on your JSON structure\n",
    "                        level_key = f\"chunks_level{i}\"\n",
    "                        if level_key in levels_data and isinstance(levels_data[level_key], list) and levels_data[level_key]:\n",
    "                             levels.add(i)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Knowledge base not found at {knowledge_base_path}. Cannot populate dropdowns.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from {knowledge_base_path}: {e}. Cannot populate dropdowns.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while parsing knowledge base: {e}\")\n",
    "\n",
    "    sorted_topics = sorted(list(topics))\n",
    "    sorted_subtopics_by_topic = {\n",
    "        topic: sorted(list(s_topics)) for topic, s_topics in subtopics_by_topic.items()\n",
    "    }\n",
    "    sorted_levels = sorted(list(levels))\n",
    "\n",
    "    return sorted_topics, sorted_subtopics_by_topic, sorted_levels\n",
    "\n",
    "# Get initial structure\n",
    "TOPICS, SUBTOPICS_BY_TOPIC, LEVELS = get_knowledge_base_structure(KNOWLEDGE_BASE_PATH_TO_USE)\n",
    "\n",
    "# --- Gradio Interface Logic ---\n",
    "\n",
    "def update_subtopics(selected_topic):\n",
    "    if selected_topic in SUBTOPICS_BY_TOPIC:\n",
    "        # Return a Gradio update object to change choices and potentially value\n",
    "        return gr.Dropdown(choices=SUBTOPICS_BY_TOPIC[selected_topic], value=SUBTOPICS_BY_TOPIC[selected_topic][0] if SUBTOPICS_BY_TOPIC[selected_topic] else None, interactive=True)\n",
    "    else:\n",
    "        return gr.Dropdown(choices=[], value=None, interactive=True)\n",
    "\n",
    "def gradio_generate_question(topic, subtopic, level, question_type, progress=gr.Progress()):\n",
    "    progress(0, desc=\"Starting generation...\")\n",
    "\n",
    "    # Validate inputs\n",
    "    if not topic or not subtopic or not level or not question_type:\n",
    "        progress(1, desc=\"Error: Missing selections.\")\n",
    "        return gr.Markdown(\"### Error: Please select a Topic, Subtopic, Difficulty Level, and Question Type.\"), \"\", \"\"\n",
    "\n",
    "    try:\n",
    "        level_int = int(level)\n",
    "    except ValueError:\n",
    "        progress(1, desc=\"Error: Invalid difficulty level.\")\n",
    "        return gr.Markdown(\"### Error: Invalid difficulty level selected. Please choose a valid number.\"), \"\", \"\"\n",
    "\n",
    "    progress(0.1, desc=\"Loading Qwen model and knowledge base (first time only)...\")\n",
    "    # Call your question generation pipeline\n",
    "    # The pipeline internally handles _generator and _chunks caching\n",
    "    question_output, blank_val, correct_ans = question_generation_pipeline(topic, subtopic, level_int, question_type)\n",
    "\n",
    "    progress(0.8, desc=\"Formatting output...\")\n",
    "\n",
    "    # Prepare markdown output for the main display\n",
    "    # Only include question_output here\n",
    "    main_display_content = f\"### Generated {question_type}\\n\"\n",
    "    if question_output:\n",
    "        main_display_content += f\"**Question:** {question_output}\\n\\n\"\n",
    "    else:\n",
    "        main_display_content += \"**Error or No Question Generated.** Please check your selections and try again.\\n\\n\"\n",
    "\n",
    "    # These will go into the dedicated Textbox outputs\n",
    "    # If the question type is General Question, these will be \"N/A\"\n",
    "    blank_val_output = blank_val if blank_val else \"N/A\"\n",
    "    correct_ans_output = correct_ans if correct_ans else \"N/A\" # This will contain the direct answer for FIB/MCQ option\n",
    "\n",
    "    progress(1, desc=\"Generation complete!\")\n",
    "\n",
    "    # We now return 3 outputs: main_display_content, blank_val_output, correct_ans_output\n",
    "    return gr.Markdown(main_display_content), gr.Textbox(value=blank_val_output), gr.Textbox(value=correct_ans_output)\n",
    "\n",
    "# --- Gradio Interface Definition ---\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# AI Tutor: Question Generation\")\n",
    "    gr.Markdown(\"Select a topic, subtopic, difficulty, and question type to generate educational content.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            topic_dropdown = gr.Dropdown(\n",
    "                label=\"Select Topic\",\n",
    "                choices=TOPICS,\n",
    "                value=TOPICS[0] if TOPICS else None,\n",
    "                interactive=True,\n",
    "                scale=1\n",
    "            )\n",
    "            subtopic_dropdown = gr.Dropdown(\n",
    "                label=\"Select Subtopic\",\n",
    "                choices=SUBTOPICS_BY_TOPIC.get(TOPICS[0], []) if TOPICS else [],\n",
    "                value=SUBTOPICS_BY_TOPIC.get(TOPICS[0], [])[0] if TOPICS and SUBTOPICS_BY_TOPIC.get(TOPICS[0]) else None,\n",
    "                interactive=True,\n",
    "                scale=1\n",
    "            )\n",
    "        with gr.Column():\n",
    "            level_dropdown = gr.Dropdown(\n",
    "                label=\"Select Difficulty Level\",\n",
    "                choices=LEVELS,\n",
    "                value=LEVELS[0] if LEVELS else None,\n",
    "                interactive=True,\n",
    "                scale=1\n",
    "            )\n",
    "            question_type_radio = gr.Radio(\n",
    "                label=\"Select Question Type\",\n",
    "                choices=[\"General Question\", \"Fill in the Blanks\", \"Multiple Choice Question\"],\n",
    "                value=\"General Question\",\n",
    "                interactive=True,\n",
    "                scale=1\n",
    "            )\n",
    "            generate_button = gr.Button(\"Generate Question\", variant=\"primary\", scale=2)\n",
    "\n",
    "    # Output area - Simplified\n",
    "    with gr.Column():\n",
    "        # Main question display (includes question and options for MCQ)\n",
    "        output_question = gr.Markdown(label=\"Generated Question\")\n",
    "\n",
    "        # Display for the correct answer (will be N/A for General Questions)\n",
    "        output_answer = gr.Textbox(label=\"Correct Answer / Blank Value\", interactive=False)\n",
    "        # We can combine blank_val_output and correct_ans_output into one text box if desired.\n",
    "        # If we separate them, it might be more explicit. For simplicity, I'm combining them.\n",
    "        # If blank_val_output is used, correct_ans_output will be blank, and vice-versa, or N/A.\n",
    "\n",
    "    # Define interactions\n",
    "    topic_dropdown.change(\n",
    "        fn=update_subtopics,\n",
    "        inputs=topic_dropdown,\n",
    "        outputs=subtopic_dropdown\n",
    "    )\n",
    "\n",
    "    generate_button.click(\n",
    "        fn=gradio_generate_question,\n",
    "        inputs=[topic_dropdown, subtopic_dropdown, level_dropdown, question_type_radio],\n",
    "        outputs=[output_question, output_answer] # Only pass two outputs now\n",
    "    )\n",
    "\n",
    "# Launch the Gradio interface\n",
    "print(\"Launching Gradio interface...\")\n",
    "demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
