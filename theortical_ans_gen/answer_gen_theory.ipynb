{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d88da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in .\\aksharaplus\\lib\\site-packages (4.49.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sentence-transformers in .\\aksharaplus\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: faiss-cpu in .\\aksharaplus\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: filelock in .\\aksharaplus\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in .\\aksharaplus\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\aksharaplus\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\aksharaplus\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\aksharaplus\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\aksharaplus\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in .\\aksharaplus\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in .\\aksharaplus\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in .\\aksharaplus\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\aksharaplus\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in .\\aksharaplus\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in .\\aksharaplus\\lib\\site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in .\\aksharaplus\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in .\\aksharaplus\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in .\\aksharaplus\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\aksharaplus\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\aksharaplus\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in .\\aksharaplus\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in .\\aksharaplus\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in .\\aksharaplus\\lib\\site-packages (from torch>=2.0.0->accelerate) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in .\\aksharaplus\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\aksharaplus\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in .\\aksharaplus\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\aksharaplus\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\aksharaplus\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\aksharaplus\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\aksharaplus\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in .\\aksharaplus\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in .\\aksharaplus\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\aksharaplus\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-win_amd64.whl (66.5 MB)\n",
      "   ---------------------------------------- 0.0/66.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.6/66.5 MB 8.3 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 5.2/66.5 MB 13.3 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 9.2/66.5 MB 15.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 13.1/66.5 MB 16.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 17.0/66.5 MB 16.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 21.2/66.5 MB 17.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 25.2/66.5 MB 17.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 29.4/66.5 MB 17.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 33.6/66.5 MB 17.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 37.7/66.5 MB 17.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 41.7/66.5 MB 17.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 45.9/66.5 MB 18.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 49.8/66.5 MB 18.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 53.7/66.5 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 57.7/66.5 MB 18.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 61.6/66.5 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.5/66.5 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  66.3/66.5 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 66.5/66.5 MB 16.8 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes, accelerate\n",
      "Successfully installed accelerate-1.7.0 bitsandbytes-0.46.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\aksharaplus\\aksharaplus\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted.\")\n",
    "\n",
    "# Define your base project directory within Google Drive\n",
    "# IMPORTANT: Replace 'my_finetune_project' with your actual project folder name.\n",
    "# It's recommended to create this folder in your Drive first (e.g., in \"My Drive/Colab Notebooks/my_finetune_project\")\n",
    "BASE_PROJECT_DIR = \"/content/drive/My Drive/Colab Notebooks/finetuned_+_rag_project\"\n",
    "\n",
    "# Create the project directory if it doesn't exist\n",
    "os.makedirs(BASE_PROJECT_DIR, exist_ok=True)\n",
    "print(f\"Base project directory set to: {BASE_PROJECT_DIR}\")\n",
    "\n",
    "# Optional: Change current working directory to your project folder\n",
    "# This makes it easier to use relative paths later, but absolute paths are safer.\n",
    "# %cd {BASE_PROJECT_DIR}\n",
    "# print(f\"Changed current working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "import os\n",
    "import shutil\n",
    "import faiss # Make sure faiss is imported here too for consistency with path definition\n",
    "\n",
    "global GDRIVE_KNOWLEDGE_BASE_PATH, GDRIVE_FINETUNED_MODEL_PATH, GDRIVE_FAISS_INDEX_PATH, KNOWLEDGE_BASE_PATH_TO_USE\n",
    "\n",
    "GDRIVE_KNOWLEDGE_BASE_PATH = os.path.join(BASE_PROJECT_DIR, \"rephrased_output.json\")\n",
    "GDRIVE_FINETUNED_MODEL_PATH = os.path.join(BASE_PROJECT_DIR, \"finetuned_qwen\")\n",
    "GDRIVE_FAISS_INDEX_PATH = os.path.join(BASE_PROJECT_DIR, \"my_faiss_index.bin\")\n",
    "\n",
    "LOCAL_TEMP_DIR = \"/content/temp_rag_data\"\n",
    "os.makedirs(LOCAL_TEMP_DIR, exist_ok=True)\n",
    "\n",
    "LOCAL_KNOWLEDGE_BASE_PATH = os.path.join(LOCAL_TEMP_DIR, \"rephrased_output.json\")\n",
    "\n",
    "if not os.path.exists(LOCAL_KNOWLEDGE_BASE_PATH) and os.path.exists(GDRIVE_KNOWLEDGE_BASE_PATH):\n",
    "    print(f\"Copying knowledge base from Google Drive to local: {GDRIVE_KNOWLEDGE_BASE_PATH} -> {LOCAL_KNOWLEDGE_BASE_PATH}\")\n",
    "    shutil.copy(GDRIVE_KNOWLEDGE_BASE_PATH, LOCAL_KNOWLEDGE_BASE_PATH)\n",
    "    print(\"Knowledge base copied to local storage.\")\n",
    "elif not os.path.exists(GDRIVE_KNOWLEDGE_BASE_PATH):\n",
    "    print(f\"Error: Knowledge base file not found in Google Drive at {GDRIVE_KNOWLEDGE_BASE_PATH}. Please ensure it's uploaded.\")\n",
    "else:\n",
    "    print(f\"Knowledge base already exists locally at {LOCAL_KNOWLEDGE_BASE_PATH}. Skipping copy.\")\n",
    "\n",
    "KNOWLEDGE_BASE_PATH_TO_USE = LOCAL_KNOWLEDGE_BASE_PATH\n",
    "\n",
    "print(f\"Knowledge base path for use: {KNOWLEDGE_BASE_PATH_TO_USE}\")\n",
    "print(f\"Fine-tuned model path: {GDRIVE_FINETUNED_MODEL_PATH}\")\n",
    "print(f\"FAISS index path (Drive): {GDRIVE_FAISS_INDEX_PATH}\")\n",
    "\n",
    "if not os.path.exists(GDRIVE_FINETUNED_MODEL_PATH):\n",
    "    print(f\"Warning: Fine-tuned model directory not found at {GDRIVE_FINETUNED_MODEL_PATH}.\")\n",
    "    print(\"Please ensure your fine-tuning script has completed successfully and saved the model to this location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f6d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\aksharaplus\\aksharaplus\\Lib\\site-packages\\huggingface_hub\\file_download.py:834: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcb3484b94f40ae92c79502c1313a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbe36a08e9b4b0996226ebbf3b94258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b012e70037f4492b3cf36b94cb3ed90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ab99b031d7468585d810414cbfeab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2836a6c59ff04f1da9959800d1372e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda20ed71b58422da6784f4ad06f8bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7727585601c0453ab9942dbed4dc1dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cb2147090f4ecaa4e9755ff947da2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bac1bdb0fe446b913a29baed22452b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577fa9ce4fe041abb4f5b5e9664ee608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8821d01c3540dd867192c63ac77c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\aksharaplus\\\\qwen2.5-1.5b-instruct'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting RAG & Gradio library installation process...\")\n",
    "!pip install sentence-transformers -q\n",
    "print(\"Installed sentence-transformers.\")\n",
    "import torch # Important for checking GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU detected. Installing faiss-gpu...\")\n",
    "    !pip install faiss-gpu -q\n",
    "else:\n",
    "    print(\"No GPU detected. Installing faiss-cpu...\")\n",
    "    !pip install faiss-cpu -q\n",
    "print(\"Installed FAISS.\")\n",
    "!pip install gradio -q\n",
    "print(\"Installed Gradio.\")\n",
    "!pip install -U bitsandbytes accelerate -q # Crucial for 8-bit quantization\n",
    "print(\"Installed/Upgraded bitsandbytes and accelerate.\")\n",
    "!pip install numpy huggingface_hub -q\n",
    "print(\"Installed numpy and huggingface_hub.\")\n",
    "print(\"\\n--- ALL REQUIRED LIBRARY INSTALLATIONS COMPLETE ---\")\n",
    "print(\"Important: Please RESTART RUNTIME (Runtime -> Restart runtime) now,\")\n",
    "print(\"then re-run cells from the top (starting with Drive Mount)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-gptq\n",
      "  Downloading auto_gptq-0.7.1.tar.gz (126 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/90/e5/b22697903982284fe284568fb2663a2196694a8eee637f5cf4ccfe435a38/auto_gptq-0.7.1.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested auto-gptq from https://files.pythonhosted.org/packages/90/e5/b22697903982284fe284568fb2663a2196694a8eee637f5cf4ccfe435a38/auto_gptq-0.7.1.tar.gz has inconsistent version: expected '0.7.1', but metadata has '0.7.1+cu121'\n",
      "  Downloading auto_gptq-0.7.0.tar.gz (124 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/34/71/c3e73cf17681f6ff4754ef8f4cb8b67af3def230fc8711eac1250bbd78d5/auto_gptq-0.7.0.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested auto-gptq from https://files.pythonhosted.org/packages/34/71/c3e73cf17681f6ff4754ef8f4cb8b67af3def230fc8711eac1250bbd78d5/auto_gptq-0.7.0.tar.gz has inconsistent version: expected '0.7.0', but metadata has '0.7.0+cu121'\n",
      "  Downloading auto_gptq-0.6.0.tar.gz (120 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/49/af/02b66e55dfd9aeb0ece923843043724ed7432ec0c649ea0f3b9fa1dd90c6/auto_gptq-0.6.0.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested auto-gptq from https://files.pythonhosted.org/packages/49/af/02b66e55dfd9aeb0ece923843043724ed7432ec0c649ea0f3b9fa1dd90c6/auto_gptq-0.6.0.tar.gz has inconsistent version: expected '0.6.0', but metadata has '0.6.0+cu121'\n",
      "  Downloading auto_gptq-0.5.1.tar.gz (112 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/db/77/ec5a16c5625b0791dccfe5e42356171332ed3537c1df505d64a162148c8f/auto_gptq-0.5.1.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested auto-gptq from https://files.pythonhosted.org/packages/db/77/ec5a16c5625b0791dccfe5e42356171332ed3537c1df505d64a162148c8f/auto_gptq-0.5.1.tar.gz has inconsistent version: expected '0.5.1', but metadata has '0.5.1+cu121'\n",
      "  Downloading auto_gptq-0.5.0.tar.gz (111 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/3d/fa/c2cd09965b2dbf4e454d9f073376922f7139a574f617f70a22adb203eced/auto_gptq-0.5.0.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested auto-gptq from https://files.pythonhosted.org/packages/3d/fa/c2cd09965b2dbf4e454d9f073376922f7139a574f617f70a22adb203eced/auto_gptq-0.5.0.tar.gz has inconsistent version: expected '0.5.0', but metadata has '0.5.0+cu121'\n",
      "  Downloading auto_gptq-0.3.2.tar.gz (63 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0): Requested auto-gptq from https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz has inconsistent version: expected '0.3.2', but metadata has '0.3.2+cu121'\n",
      "  Downloading auto_gptq-0.3.1.tar.gz (63 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers in .\\aksharaplus\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: accelerate>=0.19.0 in .\\aksharaplus\\lib\\site-packages (from auto-gptq) (1.7.0)\n",
      "Collecting datasets (from auto-gptq)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in .\\aksharaplus\\lib\\site-packages (from auto-gptq) (1.26.4)\n",
      "Collecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in .\\aksharaplus\\lib\\site-packages (from auto-gptq) (2.5.1+cu121)\n",
      "Requirement already satisfied: safetensors in .\\aksharaplus\\lib\\site-packages (from auto-gptq) (0.5.3)\n",
      "Collecting peft (from auto-gptq)\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in .\\aksharaplus\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in .\\aksharaplus\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\aksharaplus\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\aksharaplus\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\aksharaplus\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in .\\aksharaplus\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in .\\aksharaplus\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\aksharaplus\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in .\\aksharaplus\\lib\\site-packages (from accelerate>=0.19.0->auto-gptq) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\aksharaplus\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\aksharaplus\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in .\\aksharaplus\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in .\\aksharaplus\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.1.6)\n",
      "Requirement already satisfied: setuptools in .\\aksharaplus\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in .\\aksharaplus\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\aksharaplus\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Requirement already satisfied: colorama in .\\aksharaplus\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in .\\aksharaplus\\lib\\site-packages (from datasets->auto-gptq) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->auto-gptq)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in .\\aksharaplus\\lib\\site-packages (from datasets->auto-gptq) (2.2.3)\n",
      "Collecting xxhash (from datasets->auto-gptq)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->auto-gptq)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\aksharaplus\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\aksharaplus\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\aksharaplus\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\aksharaplus\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: six in .\\aksharaplus\\lib\\site-packages (from rouge->auto-gptq) (1.17.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq)\n",
      "  Downloading aiohttp-3.12.7-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\aksharaplus\\lib\\site-packages (from jinja2->torch>=1.13.0->auto-gptq) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\aksharaplus\\lib\\site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\aksharaplus\\lib\\site-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\aksharaplus\\lib\\site-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\aksharaplus\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq) (25.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq)\n",
      "  Downloading multidict-6.4.4-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->auto-gptq)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-win_amd64.whl.metadata (74 kB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Downloading aiohttp-3.12.7-cp312-cp312-win_amd64.whl (445 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-win_amd64.whl (120 kB)\n",
      "Downloading multidict-6.4.4-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-win_amd64.whl (92 kB)\n",
      "Building wheels for collected packages: auto-gptq\n",
      "  Building wheel for auto-gptq (setup.py): started\n",
      "  Building wheel for auto-gptq (setup.py): finished with status 'done'\n",
      "  Created wheel for auto-gptq: filename=auto_gptq-0.3.1-py3-none-any.whl size=67604 sha256=e9a417aea4eb0ef09e5045e4c095ba97486bab68a2cacdfd6351366469090f88\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\10\\02\\c8\\42debdb543e006982dc5ca308d6c1dd3668953455c839faaef\n",
      "Successfully built auto-gptq\n",
      "Installing collected packages: xxhash, rouge, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, peft, datasets, auto-gptq\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.7 aiosignal-1.3.2 auto-gptq-0.3.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 multidict-6.4.4 multiprocess-0.70.16 peft-0.15.2 propcache-0.3.1 rouge-1.0.1 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: RAG Pipeline Functions (MODIFIED FOR EXPLICIT PATH PASSING)\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Load the knowledge base with nested \"Introduction\" structure\n",
    "# Now takes 'knowledge_base_path' as an argument\n",
    "def load_knowledge_base(knowledge_base_path):\n",
    "    print(f\"Attempting to load knowledge base from: {knowledge_base_path}\")\n",
    "    try:\n",
    "        with open(knowledge_base_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                f.seek(0)\n",
    "                data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "        chunks = []\n",
    "        if isinstance(data, dict):\n",
    "            data = [data]\n",
    "        for item in data:\n",
    "            for topic, sections in item.items():\n",
    "                for section_name, levels in sections.items():\n",
    "                    for i in range(1, 4):\n",
    "                        level_key = f\"chunks_level{i}\"\n",
    "                        if level_key in levels and isinstance(levels[level_key], list):\n",
    "                            for chunk in levels[level_key]:\n",
    "                                if isinstance(chunk, dict) and \"text\" in chunk:\n",
    "                                    chunks.append({\n",
    "                                        \"text\": chunk[\"text\"],\n",
    "                                        \"title\": topic,\n",
    "                                        \"section\": section_name\n",
    "                                    })\n",
    "                                else:\n",
    "                                    print(f\"Warning: Skipping malformed chunk in {level_key}: {chunk}\")\n",
    "                        elif level_key in levels:\n",
    "                             print(f\"Warning: Expected list for {level_key}, found {type(levels[level_key])}. Skipping.\")\n",
    "        if not chunks:\n",
    "            raise ValueError(\"No valid chunks found in knowledge base after parsing.\")\n",
    "        print(f\"Loaded {len(chunks)} chunks from knowledge base.\")\n",
    "        return chunks\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Knowledge base not found at {knowledge_base_path}. Please check the path and ensure it's in your Google Drive or copied locally.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON/JSONL format in {knowledge_base_path}: {e}. Inspect the file for malformed lines.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading knowledge base: {e}\")\n",
    "\n",
    "# Build retriever with Sentence-BERT and FAISS (and save/load)\n",
    "# Now takes 'faiss_index_path' as an argument for saving/loading the index\n",
    "def build_retriever(chunks, faiss_index_path):\n",
    "    print(\"Building retriever model and FAISS index...\")\n",
    "\n",
    "    # Try loading existing FAISS index first\n",
    "    if os.path.exists(faiss_index_path):\n",
    "        try:\n",
    "            index = faiss.read_index(faiss_index_path)\n",
    "            retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            print(f\"FAISS index loaded from {faiss_index_path}.\")\n",
    "            if index.ntotal != len(chunks):\n",
    "                print(\"Warning: Loaded FAISS index size does not match current chunk count. Rebuilding index.\")\n",
    "                raise ValueError(\"Index mismatch\")\n",
    "            return retriever_model, index, chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading FAISS index: {e}. Rebuilding index.\")\n",
    "            pass\n",
    "\n",
    "    retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    embeddings = retriever_model.encode(chunk_texts, convert_to_tensor=False)\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    try:\n",
    "        faiss.write_index(index, faiss_index_path)\n",
    "        print(f\"FAISS index built and saved to {faiss_index_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save FAISS index to Google Drive: {e}\")\n",
    "        print(\"Continuing without saving FAISS index.\")\n",
    "\n",
    "    return retriever_model, index, chunks\n",
    "\n",
    "# Retrieve top-k relevant chunks (No change)\n",
    "def retrieve_chunks(question, retriever_model, index, chunks, k=3):\n",
    "    print(f\"Retrieving top {k} chunks for the question...\")\n",
    "    question_embedding = retriever_model.encode([question], convert_to_tensor=False)[0].astype('float32')\n",
    "    distances, indices = index.search(np.array([question_embedding]), k)\n",
    "    retrieved_chunks = [chunks[idx] for idx in indices[0]]\n",
    "    print(f\"Retrieved {len(retrieved_chunks)} chunks.\")\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Load fine-tuned Qwen model (max_new_tokens updated)\n",
    "# Now takes 'finetuned_model_path' as an argument\n",
    "def load_qwen_local(finetuned_model_path):\n",
    "    print(f\"Loading fine-tuned Qwen model from: {finetuned_model_path}\")\n",
    "    try:\n",
    "        base_model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path, trust_remote_code=True)\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Base Qwen model loaded.\")\n",
    "\n",
    "        model = PeftModel.from_pretrained(base_model, finetuned_model_path, trust_remote_code=True)\n",
    "        print(\"PEFT adapter loaded and merged.\")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        print(\"Text generation pipeline created.\")\n",
    "        return generator\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load fine-tuned Qwen model: {e}\")\n",
    "\n",
    "# Generate answer with guidance (max_new_tokens for prompt adjusted)\n",
    "def generate_answer(question, retrieved_chunks, generator):\n",
    "    context = \"\\n\".join([f\"From {chunk['title']} - {chunk['section']}:\\n{chunk['text']}\" for chunk in retrieved_chunks])\n",
    "    if any(word in question.lower() for word in [\"explain\", \"how\"]):\n",
    "        guidance = \"Explain the answer for the given question precisely in step-by-step manner and do not hallucinate the answer, give answer in 200 tokens\"\n",
    "    elif any(word in question.lower() for word in [\"why\"]):\n",
    "        guidance = \"Explain the answer for the given question in step-by-step manner, give answer in 200 tokens\"\n",
    "    elif any(word in question.lower() for word in [\"derive\", \"prove\"]):\n",
    "        guidance = \"Provide a mathematical or logical derivation and justify each step clearly, give answer in 200 tokens\"\n",
    "    elif \"difference\" in question.lower() or \"compare\" in question.lower():\n",
    "        guidance = \"Compare the concepts side by side, listing their differences and similarities clearly, give answer in 200 tokens\"\n",
    "    elif any(word in question.lower() for word in [\"what is\", \"define\"]):\n",
    "        guidance = \"Provide a clear and concise definition with relevant examples, give answer in 200 tokens\"\n",
    "    else:\n",
    "        guidance = \"Give a direct, informative, and relevant answer, give answer in 200 tokens\"\n",
    "\n",
    "    prompt = f\"\"\"You are an AI tutor helping a student learn machine learning. Answer concisely and clearly.\n",
    "\n",
    "    Question: {question}\n",
    "    Context from the textbook:\n",
    "    {context}\n",
    "\n",
    "    Instruction: {guidance}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    print(f\"Generating answer with prompt length: {len(prompt)} characters.\")\n",
    "    response = generator(prompt, max_new_tokens=200, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "    answer = response[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "    return answer\n",
    "\n",
    "# Global variables for single loading (optimization)\n",
    "_chunks = None\n",
    "_retriever_model = None\n",
    "_index = None\n",
    "_generator = None\n",
    "\n",
    "# Main RAG pipeline function\n",
    "def rag_pipeline(question):\n",
    "    print(f\"\\n--- Running RAG pipeline for: '{question}' ---\")\n",
    "    try:\n",
    "        global _chunks, _retriever_model, _index, _generator\n",
    "\n",
    "        if _generator is None:\n",
    "            print(\"Initializing RAG components for the first time...\")\n",
    "\n",
    "            # Use the global variables (defined in Cell 2) to pass as arguments\n",
    "            _chunks = load_knowledge_base(KNOWLEDGE_BASE_PATH_TO_USE)\n",
    "            _retriever_model, _index, _chunks = build_retriever(_chunks, GDRIVE_FAISS_INDEX_PATH)\n",
    "            _generator = load_qwen_local(GDRIVE_FINETUNED_MODEL_PATH)\n",
    "            print(\"RAG components initialized.\")\n",
    "\n",
    "        retrieved_chunks = retrieve_chunks(question, _retriever_model, _index, _chunks, k=3)\n",
    "        answer = generate_answer(question, retrieved_chunks, _generator)\n",
    "\n",
    "        print(f\"\\n📌 Question: {question}\")\n",
    "        print(f\"💡 Answer: {answer}\")\n",
    "        print(\"\\n🔍 Retrieved Chunks (showing first 200 chars):\")\n",
    "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "            print(f\"{i}. From {chunk['title']} - {chunk['section']}:\\n{chunk['text'][:200]}...\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG pipeline: {e}\")\n",
    "\n",
    "# Example usage (no change)\n",
    "if __name__ == \"__main__\":\n",
    "    questions = [\n",
    "        \"What is logistic regression? Explain in short\",\n",
    "        # \"How does SVM work? Explain in short\",\n",
    "        # \"What are evaluation metrics in machine learning? Explain in short\",\n",
    "        # \"how logistic regression works? explain in short\",\n",
    "        # \"what does k stands in K-NN, explain in short\",\n",
    "        # \"how accuracy is calculated?, answer in short.\",\n",
    "        # \"what is the difference between ridge and Lasso regularization, explain in short\",\n",
    "        # \"how naive bayes algorithem works, explain in short\",\n",
    "        # \"explain me how gradient descent works, explain in short\",\n",
    "        # \"what is OvO in multiclass classification\",\n",
    "        # \"derive the gradient descent\",\n",
    "        # \"In simple terms, what is the role of the sigmoid function in a logistic regression model?\",\n",
    "        # \"Explain how the decision boundary transforms the logit calculation into a probability.\",\n",
    "        # \"Prove the sigmoid’s optimality for logistic regression's text categorization using model training\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        rag_pipeline(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811a1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyngrok\n",
    "# Cell 5: Gradio Interface with Ngrok (MODIFIED FOR EXPLICIT PATH PASSING)\n",
    "import gradio as gr\n",
    "from pyngrok import ngrok\n",
    "import os\n",
    "# from google.colab import userdata # Uncomment if using Colab Secrets for ngrok\n",
    "\n",
    "# Set your ngrok auth token (replace with your actual token!)\n",
    "# os.environ[\"NGROK_AUTH_TOKEN\"] = userdata.get(\"NGROK_AUTH_TOKEN\") # if using Colab Secrets\n",
    "# ngrok.set_auth_token(os.environ[\"2yMKm5Da2PQFR0VDJopwwhv0F5E_2r6Zy6QRYSkinWggCrFoF\"]) # Or directly set it\n",
    "ngrok.set_auth_token(\"2yMKm5Da2PQFR0VDJopwwhv0F5E_2r6Zy6QRYSkinWggCrFoF\")\n",
    "\n",
    "# Wrapper function for the RAG pipeline to be used by Gradio\n",
    "def rag_inference(question):\n",
    "    global _chunks, _retriever_model, _index, _generator\n",
    "\n",
    "    if _generator is None:\n",
    "        print(\"Initializing RAG components for the first time (Gradio context)...\")\n",
    "        try:\n",
    "            print(f\"DEBUG (Gradio): Attempting to load knowledge base from {KNOWLEDGE_BASE_PATH_TO_USE}\")\n",
    "            _chunks = load_knowledge_base(KNOWLEDGE_BASE_PATH_TO_USE)\n",
    "            print(\"DEBUG (Gradio): Knowledge base loaded.\")\n",
    "\n",
    "            print(f\"DEBUG (Gradio): Attempting to build retriever from {GDRIVE_FAISS_INDEX_PATH}\")\n",
    "            _retriever_model, _index, _chunks = build_retriever(_chunks, GDRIVE_FAISS_INDEX_PATH)\n",
    "            print(\"DEBUG (Gradio): Retriever built.\")\n",
    "\n",
    "            print(f\"DEBUG (Gradio): Attempting to load Qwen model from {GDRIVE_FINETUNED_MODEL_PATH}\")\n",
    "            _generator = load_qwen_local(GDRIVE_FINETUNED_MODEL_PATH)\n",
    "            print(\"DEBUG (Gradio): Qwen model loaded.\")\n",
    "\n",
    "            print(\"RAG components initialized in Gradio context.\")\n",
    "        except Exception as init_e_gradio:\n",
    "            print(f\"CRITICAL ERROR (Gradio) during RAG component initialization: {init_e_gradio}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # This will print the full traceback\n",
    "            return f\"Error during initialization: {init_e_gradio}\", \"Initialization failed.\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        print(\"DEBUG (Gradio): Components are ready. Proceeding with retrieval and generation.\")\n",
    "        retrieved_chunks = retrieve_chunks(question, _retriever_model, _index, _chunks, k=3)\n",
    "        answer = generate_answer(question, retrieved_chunks, _generator)\n",
    "\n",
    "        retrieved_text = \"\\n\\n\".join([f\"**From {chunk['title']} - {chunk['section']}:**\\n{chunk['text']}\" for chunk in retrieved_chunks])\n",
    "        return answer, retrieved_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Gradio RAG inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # This will print the full traceback\n",
    "        return f\"Error: {e}\", \"Could not retrieve chunks.\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=rag_inference,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your machine learning question here...\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Generated Answer\"),\n",
    "        gr.Textbox(label=\"Retrieved Chunks (Context)\"),\n",
    "    ],\n",
    "    title=\"Qwen RAG Machine Learning Tutor\",\n",
    "    description=\"Ask questions about machine learning, and I'll retrieve relevant information and answer using a fine-tuned Qwen model.\",\n",
    "    examples=[\n",
    "        \"What is logistic regression?\",\n",
    "        \"How does SVM work?\",\n",
    "        \"Explain gradient descent.\",\n",
    "        \"What is the difference between ridge and Lasso regularization?\",\n",
    "        \"Derive the formula for accuracy.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Launching Gradio interface...\")\n",
    "iface.launch(share=True, debug=True)\n",
    "\n",
    "print(\"\\n--- Gradio interface launched. Look for the public URL above ---\")\n",
    "print(\"It will be something like 'Running on public URL: https://[random-string].gradio.live'\")\n",
    "print(\"Copy this URL to use it from VS Code or a web browser.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
