{
    "Logistic_regression": {
        "title": "Logistic_regression",
        "chunks": [
            {
                "text": "Example graph of a logistic regression curve fitted to data. The curve shows the estimated probability of passing an exam (binary dependent variable) versus hours studying (scalar independent variable). See for worked details. In statistics, A logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations). In binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name."
            },
            {
                "text": "The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See and for formal mathematics, and for a worked example. Binary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see ), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc. ), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier."
            },
            {
                "text": "Analogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see . The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the \"simplest\" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see . The parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see . Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see for discussion."
            },
            {
                "text": "The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in , where he coined \"logit\"; see . Applications General Logistic regression is used in various fields, including machine learning, most medical fields, and social sciences. For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd using logistic regression. Many other medical scales used to assess severity of a patient have been developed using logistic regression. Logistic regression may be used to predict the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.). Another example might be to predict whether a Nepalese voter will vote Nepali Congress or Communist Party of Nepal or Any Other Party, based on age, income, sex, race, state of residence, votes in previous elections, etc. The technique can also be used in engineering, especially for predicting the probability of failure of a given process, system or product."
            },
            {
                "text": "It is also used in marketing applications such as prediction of a customer's propensity to purchase a product or halt a subscription, etc. In economics, it can be used to predict the likelihood of a person ending up in the labor force, and a business application would be to predict the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in natural language processing. Disaster planners and engineers rely on these models to predict decisions taken by householders or building occupants in small-scale and large-scales evacuations, such as building fires, wildfires, hurricanes among others. These models help in the development of reliable disaster managing plans and safer design for the built environment. Supervised machine learning Logistic regression is a supervised machine learning algorithm widely used for binary classification tasks, such as identifying whether an email is spam or not and diagnosing diseases by assessing the presence or absence of specific conditions based on patient test results. This approach utilizes the logistic (or sigmoid) function to transform a linear combination of input features into a probability value ranging between 0 and 1."
            },
            {
                "text": "This probability indicates the likelihood that a given input corresponds to one of two predefined categories. The essential mechanism of logistic regression is grounded in the logistic function's ability to model the probability of binary outcomes accurately. With its distinctive S-shaped curve, the logistic function effectively maps any real-valued number to a value within the 0 to 1 interval. This feature renders it particularly suitable for binary classification tasks, such as sorting emails into \"spam\" or \"not spam\". By calculating the probability that the dependent variable will be categorized into a specific group, logistic regression provides a probabilistic framework that supports informed decision-making. Example Problem As a simple example, we can use a logistic regression with one explanatory variable and two categories to answer the following question: A group of 20 students spends between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability of the student passing the exam? The reason for using logistic regression for this problem is that the values of the dependent variable, pass and fail, while represented by \"1\" and \"0\", are not cardinal numbers."
            },
            {
                "text": "If the problem was changed so that pass/fail was replaced with the grade 0–100 (cardinal numbers), then simple regression analysis could be used. The table shows the number of hours each student spent studying, and whether they passed (1) or failed (0). Hours (xk) 0.50 0.75 1.00 1.25 1.50 1.75 1.75 2.00 2.25 2.50 2.75 3.00 3.25 3.50 4.00 4.25 4.50 4.75 5.00 5.50 Pass (yk) 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 We wish to fit a logistic function to the data consisting of the hours studied (xk) and the outcome of the test (yk =1 for pass, 0 for fail). The data points are indexed by the subscript k which runs from $k=1$ to $k=K=20$. The x variable is called the \"explanatory variable\", and the y variable is called the \"categorical variable\" consisting of two categories: \"pass\" or \"fail\" corresponding to the categorical values 1 and 0 respectively. Model Graph of a logistic regression curve fitted to the (xm,ym) data."
            },
            {
                "text": "The curve shows the probability of passing an exam versus hours studying. The logistic function is of the form: $p(x)=\\frac{1}{1+e^{-(x-\\mu)/s}}$ where μ is a location parameter (the midpoint of the curve, where $p(\\mu)=1/2$) and s is a scale parameter. This expression may be rewritten as: $p(x)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1 x)}}$ where $\\beta_0 = -\\mu/s$ and is known as the intercept (it is the vertical intercept or y-intercept of the line $y = \\beta_0+\\beta_1 x$), and $\\beta_1= 1/s$ (inverse scale parameter or rate parameter): these are the y-intercept and slope of the log-odds as a function of x. Conversely, $\\mu=-\\beta_0/\\beta_1$ and $s=1/\\beta_1$. Remark: This model is actually an oversimplification, since it assumes everybody will pass if they learn long enough (limit = 1). The limit value should be a variable parameter too, if you want to make it more realistic."
            },
            {
                "text": "Fit The usual measure of goodness of fit for a logistic regression uses logistic loss (or log loss), the negative log-likelihood. For a given xk and yk, write $p_k=p(x_k)$. The are the probabilities that the corresponding will equal one and are the probabilities that they will be zero (see Bernoulli distribution). We wish to find the values of and which give the \"best fit\" to the data. In the case of linear regression, the sum of the squared deviations of the fit from the data points (yk), the squared error loss, is taken as a measure of the goodness of fit, and the best fit is obtained when that function is minimized. The log loss for the k-th point is: $\\ell_k = \\begin{cases} -\\ln p_k & \\text{ if } y_k = 1, \\\\ -\\ln (1 - p_k) & \\text{ if } y_k = 0. \\end{cases}$ The log loss can be interpreted as the \"surprisal\" of the actual outcome relative to the prediction , and is a measure of information content."
            },
            {
                "text": "Log loss is always greater than or equal to 0, equals 0 only in case of a perfect prediction (i.e., when $p_k = 1$ and $y_k = 1$, or $p_k = 0$ and $y_k = 0$), and approaches infinity as the prediction gets worse (i.e., when $y_k = 1$ and $p_k \\to 0$ or $y_k = 0 $ and $p_k \\to 1$), meaning the actual outcome is \"more surprising\". Since the value of the logistic function is always strictly between zero and one, the log loss is always greater than zero and less than infinity. Unlike in a linear regression, where the model can have zero loss at a point by passing through a data point (and zero loss overall if all points are on a line), in a logistic regression it is not possible to have zero loss at any points, since is either 0 or 1, but . These can be combined into a single expression: $\\ell_k = -y_k\\ln p_k - (1 - y_k)\\ln (1 - p_k).$ This expression is more formally known as the cross-entropy of the predicted distribution $\\big(p_k, (1-p_k)\\big)$ from the actual distribution $\\big(y_k, (1-y_k)\\big)$, as probability distributions on the two-element space of (pass, fail)."
            },
            {
                "text": "The sum of these, the total loss, is the overall negative log-likelihood , and the best fit is obtained for those choices of and for which is minimized. Alternatively, instead of minimizing the loss, one can maximize its inverse, the (positive) log-likelihood: $\\ell = \\sum_{k:y_k=1}\\ln(p_k) + \\sum_{k:y_k=0}\\ln(1-p_k) = \\sum_{k=1}^K \\left(\\,y_k \\ln(p_k)+(1-y_k)\\ln(1-p_k)\\right)$ or equivalently maximize the likelihood function itself, which is the probability that the given data set is produced by a particular logistic function: $L = \\prod_{k:y_k=1}p_k\\,\\prod_{k:y_k=0}(1-p_k)$ This method is known as maximum likelihood estimation. Parameter estimation Since ℓ is nonlinear in and , determining their optimum values will require numerical methods. One method of maximizing ℓ is to require the derivatives of ℓ with respect to and to be zero: $0 = \\frac{\\partial \\ell}{\\partial \\beta_0} = \\sum_{k=1}^K(y_k-p_k)$ $0 = \\frac{\\partial \\ell}{\\partial \\beta_1} = \\sum_{k=1}^K(y_k-p_k)x_k$ and the maximization procedure can be accomplished by solving the above two equations for and , which, again, will generally require the use of numerical methods."
            },
            {
                "text": "The values of and which maximize ℓ and L using the above data are found to be: $\\beta_0 \\approx -4.1$ $\\beta_1 \\approx 1.5$ which yields a value for μ and s of: $\\mu = -\\beta_0/\\beta_1 \\approx 2.7$ $s = 1/\\beta_1 \\approx 0.67$ Predictions The and coefficients may be entered into the logistic regression equation to estimate the probability of passing the exam. For example, for a student who studies 2 hours, entering the value $x = 2$ into the equation gives the estimated probability of passing the exam of 0.25: $ t = \\beta_0+2\\beta_1 \\approx - 4.1 + 2 \\cdot 1.5 = -1.1 $ $ p = \\frac{1}{1 + e^{-t} } \\approx 0.25 = \\text{Probability of passing exam} $ Similarly, for a student who studies 4 hours, the estimated probability of passing the exam is 0.87: $t = \\beta_0+4\\beta_1 \\approx - 4.1 + 4 \\cdot 1.5 = 1.9$ $p = \\frac{1}{1 + e^{-t} } \\approx 0.87 = \\text{Probability of passing exam} $ This table shows the estimated probability of passing the exam for several values of hours studying."
            },
            {
                "text": "Hoursof study(x) Passing exam Log-odds (t) Odds (et) Probability (p) 1 −2.57 0.076 ≈ 1:13.1 0.07 2 −1.07 0.34 ≈ 1:2.91 0.26 0 1 = 0.50 3 0.44 1.55 0.61 4 1.94 6.96 0.87 5 3.45 31.4 0.97 Model evaluation The logistic regression analysis gives the following output. Coefficient Std. Error z-value p-value (Wald) Intercept (β0) −4.1 1.8 −2.3 0.021 Hours (β1) 1.5 0.9 2.4 0.017 By the Wald test, the output indicates that hours studying is significantly associated with the probability of passing the exam ($p = 0.017$). Rather than the Wald method, the recommended method to calculate the p-value for logistic regression is the likelihood-ratio test (LRT), which for these data give $p \\approx 0.00064$ (see below). Generalizations This simple model is an example of binary logistic regression, and has one explanatory variable and a binary categorical variable which can assume one of two categorical values. Multinomial logistic regression is the generalization of binary logistic regression to include any number of explanatory variables and any number of categories."
            },
            {
                "text": "Background right|Figure 1. The standard logistic function $\\sigma (t)$; $\\sigma (t) \\in (0,1)$ for all $t$. Definition of the logistic function An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a sigmoid function, which takes any real input $t$, and outputs a value between zero and one. For the logit, this is interpreted as taking input log-odds and having output probability. The standard logistic function $\\sigma:\\mathbb R\\rightarrow (0,1)$ is defined as follows: $\\sigma (t) = \\frac{e^t}{e^t+1} = \\frac{1}{1+e^{-t}}$ A graph of the logistic function on the t-interval (−6,6) is shown in Figure 1. Let us assume that $t$ is a linear function of a single explanatory variable $x$ (the case where $t$ is a linear combination of multiple explanatory variables is treated similarly). We can then express $t$ as follows: $t = \\beta_0 + \\beta_1 x$ And the general logistic function $p:\\mathbb R \\rightarrow (0,1)$ can now be written as: $p(x) = \\sigma(t)= \\frac {1}{1+e^{-(\\beta_0 + \\beta_1 x)}}$ In the logistic model, $p(x)$ is interpreted as the probability of the dependent variable $Y$ equaling a success/case rather than a failure/non-case."
            },
            {
                "text": "It is clear that the response variables $Y_i$ are not identically distributed: $P(Y_i = 1\\mid X)$ differs from one data point $X_i$ to another, though they are independent given design matrix $X$ and shared parameters $\\beta$. Definition of the inverse of the logistic function We can now define the logit (log odds) function as the inverse $g = \\sigma^{-1}$ of the standard logistic function. It is easy to see that it satisfies: $g(p(x)) = \\sigma^{-1} (p(x)) = \\operatorname{logit} p(x) = \\ln \\left( \\frac{p(x)}{1 - p(x)} \\right) = \\beta_0 + \\beta_1 x ,$ and equivalently, after exponentiating both sides we have the odds: $\\frac{p(x)}{1 - p(x)} = e^{\\beta_0 + \\beta_1 x}.$ Interpretation of these terms In the above equations, the terms are as follows: $g$ is the logit function."
            },
            {
                "text": "The equation for $g(p(x))$ illustrates that the logit (i.e., log-odds or natural logarithm of the odds) is equivalent to the linear regression expression. $\\ln$ denotes the natural logarithm. $p(x)$ is the probability that the dependent variable equals a case, given some linear combination of the predictors. The formula for $p(x)$ illustrates that the probability of the dependent variable equaling a case is equal to the value of the logistic function of the linear regression expression. This is important in that it shows that the value of the linear regression expression can vary from negative to positive infinity and yet, after transformation, the resulting expression for the probability $p(x)$ ranges between 0 and 1. $\\beta_0$ is the intercept from the linear regression equation (the value of the criterion when the predictor is equal to zero). $\\beta_1 x$ is the regression coefficient multiplied by some value of the predictor. base $e$ denotes the exponential function."
            },
            {
                "text": "Definition of the odds The odds of the dependent variable equaling a case (given some linear combination $x$ of the predictors) is equivalent to the exponential function of the linear regression expression. This illustrates how the logit serves as a link function between the probability and the linear regression expression. Given that the logit ranges between negative and positive infinity, it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds. So we define odds of the dependent variable equaling a case (given some linear combination $x$ of the predictors) as follows: $\\text{odds} = e^{\\beta_0 + \\beta_1 x}.$ The odds ratio For a continuous independent variable the odds ratio can be defined as: thumb|The image represents an outline of what an odds ratio looks like in writing, through a template in addition to the test score example in the \"Example\" section of the contents."
            },
            {
                "text": "In simple terms, if we hypothetically get an odds ratio of 2 to 1, we can say... \"For every one-unit increase in hours studied, the odds of passing (group 1) or failing (group 0) are (expectedly) 2 to 1 (Denis, 2019).$ \\mathrm{OR} = \\frac{\\operatorname{odds}(x+1)}{\\operatorname{odds}(x)} = \\frac{\\left(\\frac{p(x+1)}{1 - p(x+1)}\\right)}{\\left(\\frac{p(x)}{1 - p(x)}\\right)} = \\frac{e^{\\beta_0 + \\beta_1 (x+1)}}{e^{\\beta_0 + \\beta_1 x}} = e^{\\beta_1}$ This exponential relationship provides an interpretation for $\\beta_1$: The odds multiply by $e^{\\beta_1}$ for every 1-unit increase in x."
            },
            {
                "text": "For a binary independent variable the odds ratio is defined as $\\frac{ad}{bc}$ where a, b, c and d are cells in a 2×2 contingency table. Multiple explanatory variables If there are multiple explanatory variables, the above expression $\\beta_0+\\beta_1x$ can be revised to $\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_mx_m = \\beta_0+ \\sum_{i=1}^m \\beta_ix_i$. Then when this is used in the equation relating the log odds of a success to the values of the predictors, the linear regression will be a multiple regression with m explanators; the parameters $\\beta_i$ for all $i = 0, 1, 2, \\dots, m$ are all estimated. Again, the more traditional equations are: $\\log \\frac{p}{1-p} = \\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_mx_m$ and $p = \\frac{1}{1+b^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_mx_m )}}$ where usually $b=e$. Definition A dataset contains N points. Each point i consists of a set of m input variables x1,i ... xm,i (also called independent variables, explanatory variables, predictor variables, features, or attributes), and a binary outcome variable Yi (also known as a dependent variable, response variable, output variable, or class), i.e."
            },
            {
                "text": "it can assume only the two possible values 0 (often meaning \"no\" or \"failure\") or 1 (often meaning \"yes\" or \"success\"). The goal of logistic regression is to use the dataset to create a predictive model of the outcome variable. As in linear regression, the outcome variables Yi are assumed to depend on the explanatory variables x1,i ... xm,i. Explanatory variables The explanatory variables may be of any type: real-valued, binary, categorical, etc. The main distinction is between continuous variables and discrete variables. (Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables), that is, separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning \"variable does have the given value\" and a 0 meaning \"variable does not have that value\".) Outcome variables Formally, the outcomes Yi are described as being Bernoulli-distributed data, where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand, but related to the explanatory variables."
            },
            {
                "text": "This can be expressed in any of the following equivalent forms: $ \\begin{align} Y_i\\mid x_{1,i},\\ldots,x_{m,i} \\ & \\sim \\operatorname{Bernoulli}(p_i) \\\\[5pt] \\operatorname{\\mathbb E}[Y_i\\mid x_{1,i},\\ldots,x_{m,i}] &= p_i \\\\[5pt] \\Pr(Y_i=y\\mid x_{1,i},\\ldots,x_{m,i}) &= \\begin{cases} p_i & \\text{if }y=1 \\\\ 1-p_i & \\text{if }y=0 \\end{cases} \\\\[5pt] \\Pr(Y_i=y\\mid x_{1,i},\\ldots,x_{m,i}) &= p_i^y (1-p_i)^{(1-y)} \\end{align} $ The meanings of these four lines are: The first line expresses the probability distribution of each Yi : conditioned on the explanatory variables, it follows a Bernoulli distribution with parameters pi, the probability of the outcome of 1 for trial i."
            },
            {
                "text": "As noted above, each separate trial has its own probability of success, just as each trial has its own explanatory variables. The probability of success pi is not observed, only the outcome of an individual Bernoulli trial using that probability. The second line expresses the fact that the expected value of each Yi is equal to the probability of success pi, which is a general property of the Bernoulli distribution. In other words, if we run a large number of Bernoulli trials using the same probability of success pi, then take the average of all the 1 and 0 outcomes, then the result would be close to pi. This is because doing an average this way simply computes the proportion of successes seen, which we expect to converge to the underlying probability of success. The third line writes out the probability mass function of the Bernoulli distribution, specifying the probability of seeing each of the two possible outcomes. The fourth line is another way of writing the probability mass function, which avoids having to write separate cases and is more convenient for certain types of calculations."
            },
            {
                "text": "This relies on the fact that Yi can take only the value 0 or 1. In each case, one of the exponents will be 1, \"choosing\" the value under it, while the other is 0, \"canceling out\" the value under it. Hence, the outcome is either pi or 1 − pi, as in the previous line. Linear predictor function The basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using a linear predictor function, i.e. a linear combination of the explanatory variables and a set of regression coefficients that are specific to the model at hand but the same for all trials. The linear predictor function $f(i)$ for a particular data point i is written as: $f(i) = \\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_m x_{m,i},$ where $\\beta_0, \\ldots, \\beta_m$ are regression coefficients indicating the relative effect of a particular explanatory variable on the outcome."
            },
            {
                "text": "The model is usually put into a more compact form as follows: The regression coefficients β0, β1, ..., βm are grouped into a single vector β of size m + 1. For each data point i, an additional explanatory pseudo-variable x0,i is added, with a fixed value of 1, corresponding to the intercept coefficient β0. The resulting explanatory variables x0,i, x1,i, ..., xm,i are then grouped into a single vector Xi of size m + 1. This makes it possible to write the linear predictor function as follows: $f(i)= \\boldsymbol\\beta \\cdot \\mathbf{X}_i,$ using the notation for a dot product between two vectors. This is an example of an SPSS output for a logistic regression model using three explanatory variables (coffee use per week, energy drink use per week, and soda use per week) and two categories (male and female). Many explanatory variables, two categories The above example of binary logistic regression on one explanatory variable can be generalized to binary logistic regression on any number of explanatory variables x1, x2,... and any number of categorical values $y=0,1,2,\\dots$."
            },
            {
                "text": "To begin with, we may consider a logistic model with M explanatory variables, x1, x2 ... xM and, as in the example above, two categorical values (y = 0 and 1). For the simple binary logistic regression model, we assumed a linear relationship between the predictor variable and the log-odds (also called logit) of the event that $y=1$. This linear relationship may be extended to the case of M explanatory variables: $t = \\log_b \\frac{p}{1-p} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2+ \\cdots +\\beta_M x_M $ where t is the log-odds and $\\beta_i$ are parameters of the model. An additional generalization has been introduced in which the base of the model (b) is not restricted to Euler's number e. In most applications, the base $b$ of the logarithm is usually taken to be e. However, in some cases it can be easier to communicate results by working in base 2 or base 10. For a more compact notation, we will specify the explanatory variables and the β coefficients as -dimensional vectors: $\\boldsymbol{x}=\\{x_0,x_1,x_2,\\dots,x_M\\}$ $\\boldsymbol{\\beta}=\\{\\beta_0,\\beta_1,\\beta_2,\\dots,\\beta_M\\}$ with an added explanatory variable x0 =1."
            },
            {
                "text": "The logit may now be written as: $t =\\sum_{m=0}^{M} \\beta_m x_m = \\boldsymbol{\\beta} \\cdot x$ Solving for the probability p that $y=1$ yields: $p(\\boldsymbol{x}) = \\frac{b^{\\boldsymbol{\\beta} \\cdot \\boldsymbol{x}}}{1+b^{\\boldsymbol{\\beta} \\cdot \\boldsymbol{x}}}= \\frac{1}{1+b^{-\\boldsymbol{\\beta} \\cdot \\boldsymbol{x}}}=S_b(t)$, where $S_b$ is the sigmoid function with base $b$. The above formula shows that once the $\\beta_m$ are fixed, we can easily compute either the log-odds that $y=1$ for a given observation, or the probability that $y=1$ for a given observation. The main use-case of a logistic model is to be given an observation $\\boldsymbol{x}$, and estimate the probability $p(\\boldsymbol{x})$ that $y=1$. The optimum beta coefficients may again be found by maximizing the log-likelihood."
            },
            {
                "text": "For K measurements, defining $\\boldsymbol{x}_k$ as the explanatory vector of the k-th measurement, and $y_k$ as the categorical outcome of that measurement, the log likelihood may be written in a form very similar to the simple $M=1$ case above: $\\ell = \\sum_{k=1}^K y_k \\log_b(p(\\boldsymbol{x_k}))+\\sum_{k=1}^K (1-y_k) \\log_b(1-p(\\boldsymbol{x_k}))$ As in the simple example above, finding the optimum β parameters will require numerical methods. One useful technique is to equate the derivatives of the log likelihood with respect to each of the β parameters to zero yielding a set of equations which will hold at the maximum of the log likelihood: $\\frac{\\partial \\ell}{\\partial \\beta_m} = 0 = \\sum_{k=1}^K y_k x_{mk} - \\sum_{k=1}^K p(\\boldsymbol{x}_k)x_{mk}$ where xmk is the value of the xm explanatory variable from the k-th measurement."
            },
            {
                "text": "Consider an example with $M=2$ explanatory variables, $b=10$, and coefficients $\\beta_0=-3$, $\\beta_1=1$, and $\\beta_2=2$ which have been determined by the above method. To be concrete, the model is: $t=\\log_{10}\\frac{p}{1 - p} = -3 + x_1 + 2 x_2$ $p = \\frac{b^{\\boldsymbol{\\beta} \\cdot \\boldsymbol{x}}}{1+b^{\\boldsymbol{\\beta} \\cdot x}} = \\frac{b^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2}}{1+b^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2} } = \\frac{1}{1 + b^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)}}$, where p is the probability of the event that $y=1$. This can be interpreted as follows: $\\beta_0 = -3$ is the y-intercept. It is the log-odds of the event that $y=1$, when the predictors $x_1=x_2=0$. By exponentiating, we can see that when $x_1=x_2=0$ the odds of the event that $y=1$ are 1-to-1000, or $10^{-3}$."
            },
            {
                "text": "Similarly, the probability of the event that $y=1$ when $x_1=x_2=0$ can be computed as $ 1/(1000 + 1) = 1/1001.$ $\\beta_1 = 1$ means that increasing $x_1$ by 1 increases the log-odds by $1$. So if $x_1$ increases by 1, the odds that $y=1$ increase by a factor of $10^1$. The probability of $y=1$ has also increased, but it has not increased by as much as the odds have increased. $\\beta_2 = 2$ means that increasing $x_2$ by 1 increases the log-odds by $2$. So if $x_2$ increases by 1, the odds that $y=1$ increase by a factor of $10^2.$ Note how the effect of $x_2$ on the log-odds is twice as great as the effect of $x_1$, but the effect on the odds is 10 times greater. But the effect on the probability of $y=1$ is not as much as 10 times greater, it's only the effect on the odds that is 10 times greater."
            },
            {
                "text": "Multinomial logistic regression: Many explanatory variables and many categories In the above cases of two categories (binomial logistic regression), the categories were indexed by \"0\" and \"1\", and we had two probabilities: The probability that the outcome was in category 1 was given by $p(\\boldsymbol{x})$and the probability that the outcome was in category 0 was given by $1-p(\\boldsymbol{x})$. The sum of these probabilities equals 1, which must be true, since \"0\" and \"1\" are the only possible categories in this setup. In general, if we have explanatory variables (including x0) and categories, we will need separate probabilities, one for each category, indexed by n, which describe the probability that the categorical outcome y will be in category y=n, conditional on the vector of covariates x. The sum of these probabilities over all categories must equal 1."
            },
            {
                "text": "Using the mathematically convenient base e, these probabilities are: $p_n(\\boldsymbol{x}) = \\frac{e^{\\boldsymbol{\\beta}_n\\cdot \\boldsymbol{x}}}{1+\\sum_{u=1}^N e^{\\boldsymbol{\\beta}_u\\cdot \\boldsymbol{x}}}$ for $n=1,2,\\dots,N$ $p_0(\\boldsymbol{x}) = 1-\\sum_{n=1}^N p_n(\\boldsymbol{x})=\\frac{1}{1+\\sum_{u=1}^N e^{\\boldsymbol{\\beta}_u\\cdot \\boldsymbol{x}}}$ Each of the probabilities except $p_0(\\boldsymbol{x})$ will have their own set of regression coefficients $\\boldsymbol{\\beta}_n$."
            },
            {
                "text": "It can be seen that, as required, the sum of the $p_n(\\boldsymbol{x})$ over all categories n is 1. The selection of $p_0(\\boldsymbol{x})$ to be defined in terms of the other probabilities is artificial. Any of the probabilities could have been selected to be so defined. This special value of n is termed the \"pivot index\", and the log-odds (tn) are expressed in terms of the pivot probability and are again expressed as a linear combination of the explanatory variables: $t_n = \\ln\\left(\\frac{p_n(\\boldsymbol{x})}{p_0(\\boldsymbol{x})}\\right) = \\boldsymbol{\\beta}_n \\cdot \\boldsymbol{x}$ Note also that for the simple case of $N=1$, the two-category case is recovered, with $p(\\boldsymbol{x})=p_1(\\boldsymbol{x})$ and $p_0(\\boldsymbol{x})=1-p_1(\\boldsymbol{x})$. The log-likelihood that a particular set of K measurements or data points will be generated by the above probabilities can now be calculated."
            },
            {
                "text": "Indexing each measurement by k, let the k-th set of measured explanatory variables be denoted by $\\boldsymbol{x}_k$ and their categorical outcomes be denoted by $y_k$ which can be equal to any integer in [0,N]. The log-likelihood is then: $\\ell = \\sum_{k=1}^K \\sum_{n=0}^N \\Delta(n,y_k)\\,\\ln(p_n(\\boldsymbol{x}_k))$ where $\\Delta(n,y_k)$ is an indicator function which equals 1 if yk = n and zero otherwise. In the case of two explanatory variables, this indicator function was defined as yk when n = 1 and 1-yk when n = 0. This was convenient, but not necessary.For example, the indicator function in this case could be defined as $\\Delta(n,y)=1-(y-n)^2$ Again, the optimum beta coefficients may be found by maximizing the log-likelihood function generally using numerical methods."
            },
            {
                "text": "A possible method of solution is to set the derivatives of the log-likelihood with respect to each beta coefficient equal to zero and solve for the beta coefficients: $\\frac{\\partial \\ell}{\\partial \\beta_{nm}} = 0 = \\sum_{k=1}^K \\Delta(n,y_k)x_{mk} - \\sum_{k=1}^K p_n(\\boldsymbol{x}_k)x_{mk}$ where $\\beta_{nm}$ is the m-th coefficient of the $\\boldsymbol{\\beta}_n$ vector and $x_{mk}$ is the m-th explanatory variable of the k-th measurement."
            },
            {
                "text": "Once the beta coefficients have been estimated from the data, we will be able to estimate the probability that any subsequent set of explanatory variables will result in any of the possible outcome categories. Interpretations There are various equivalent specifications and interpretations of logistic regression, which fit into different types of more general models, and allow different generalizations. As a generalized linear model The particular model used by logistic regression, which distinguishes it from standard linear regression and from other types of regression analysis used for binary-valued outcomes, is the way the probability of a particular outcome is linked to the linear predictor function: $\\operatorname{logit}(\\operatorname{\\mathbb E}[Y_i\\mid x_{1,i},\\ldots,x_{m,i}]) = \\operatorname{logit}(p_i) = \\ln \\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_m x_{m,i}$ Written using the more compact notation described above, this is: $\\operatorname{logit}(\\operatorname{\\mathbb E}[Y_i\\mid \\mathbf{X}_i]) = \\operatorname{logit}(p_i)=\\ln\\left(\\frac{p_i}{1-p_i}\\right) = \\boldsymbol\\beta \\cdot \\mathbf{X}_i$ This formulation expresses logistic regression as a type of generalized linear model, which predicts variables with various types of probability distributions by fitting a linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable."
            },
            {
                "text": "The intuition for transforming using the logit function (the natural log of the odds) was explained above. It also has the practical effect of converting the probability (which is bounded to be between 0 and 1) to a variable that ranges over $(-\\infty,+\\infty)$ — thereby matching the potential range of the linear prediction function on the right side of the equation. Both the probabilities pi and the regression coefficients are unobserved, and the means of determining them is not part of the model itself. They are typically determined by some sort of optimization procedure, e.g. maximum likelihood estimation, that finds values that best fit the observed data (i.e. that give the most accurate predictions for the data already observed), usually subject to regularization conditions that seek to exclude unlikely values, e.g. extremely large values for any of the regression coefficients. The use of a regularization condition is equivalent to doing maximum a posteriori (MAP) estimation, an extension of maximum likelihood. (Regularization is most commonly done using a squared regularizing function, which is equivalent to placing a zero-mean Gaussian prior distribution on the coefficients, but other regularizers are also possible.)"
            },
            {
                "text": "Whether or not regularization is used, it is usually not possible to find a closed-form solution; instead, an iterative numerical method must be used, such as iteratively reweighted least squares (IRLS) or, more commonly these days, a quasi-Newton method such as the L-BFGS method. The interpretation of the βj parameter estimates is as the additive effect on the log of the odds for a unit change in the j the explanatory variable. In the case of a dichotomous explanatory variable, for instance, gender $e^\\beta$ is the estimate of the odds of having the outcome for, say, males compared with females. An equivalent formula uses the inverse of the logit function, which is the logistic function, i.e."
            },
            {
                "text": ": $\\operatorname{\\mathbb E}[Y_i\\mid \\mathbf{X}_i] = p_i = \\operatorname{logit}^{-1}(\\boldsymbol\\beta \\cdot \\mathbf{X}_i) = \\frac{1}{1+e^{-\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}$ The formula can also be written as a probability distribution (specifically, using a probability mass function): $\\Pr(Y_i=y\\mid \\mathbf{X}_i) = {p_i}^y(1-p_i)^{1-y} =\\left(\\frac{e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}{1+e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}\\right)^{y} \\left(1-\\frac{e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}{1+e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}\\right)^{1-y} = \\frac{e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i \\cdot y} }{1+e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}$ As a latent-variable model The logistic model has an equivalent formulation as a latent-variable model."
            },
            {
                "text": "This formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple, correlated choices, as well as to compare logistic regression to the closely related probit model. Imagine that, for each trial i, there is a continuous latent variable Yi* (i.e. an unobserved random variable) that is distributed as follows: $ Y_i^\\ast = \\boldsymbol\\beta \\cdot \\mathbf{X}_i + \\varepsilon_i \\, $ where $\\varepsilon_i \\sim \\operatorname{Logistic}(0,1) \\, $ i.e. the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to a standard logistic distribution. Then Yi can be viewed as an indicator for whether this latent variable is positive: $ Y_i = \\begin{cases} 1 & \\text{if }Y_i^\\ast > 0 \\ \\text{ i.e. } {- \\varepsilon_i} < \\boldsymbol\\beta \\cdot \\mathbf{X}_i, \\\\ 0 &\\text{otherwise.} \\end{cases} $ The choice of modeling the error variable specifically with a standard logistic distribution, rather than a general logistic distribution with the location and scale set to arbitrary values, seems restrictive, but in fact, it is not."
            },
            {
                "text": "It must be kept in mind that we can choose the regression coefficients ourselves, and very often can use them to offset changes in the parameters of the error variable's distribution. For example, a logistic error-variable distribution with a non-zero location parameter μ (which sets the mean) is equivalent to a distribution with a zero location parameter, where μ has been added to the intercept coefficient. Both situations produce the same value for Yi* regardless of settings of explanatory variables. Similarly, an arbitrary scale parameter s is equivalent to setting the scale parameter to 1 and then dividing all regression coefficients by s. In the latter case, the resulting value of Yi* will be smaller by a factor of s than in the former case, for all sets of explanatory variables — but critically, it will always remain on the same side of 0, and hence lead to the same Yi choice. (This predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available.) It turns out that this formulation is exactly equivalent to the preceding one, phrased in terms of the generalized linear model and without any latent variables."
            },
            {
                "text": "This can be shown as follows, using the fact that the cumulative distribution function (CDF) of the standard logistic distribution is the logistic function, which is the inverse of the logit function, i.e. $\\Pr(\\varepsilon_i < x) = \\operatorname{logit}^{-1}(x)$ Then: $ \\begin{align} \\Pr(Y_i=1\\mid\\mathbf{X}_i) &= \\Pr(Y_i^\\ast > 0\\mid\\mathbf{X}_i) \\\\[5pt] &= \\Pr(\\boldsymbol\\beta \\cdot \\mathbf{X}_i + \\varepsilon_i > 0) \\\\[5pt] &= \\Pr(\\varepsilon_i > -\\boldsymbol\\beta \\cdot \\mathbf{X}_i) \\\\[5pt] &= \\Pr(\\varepsilon_i < \\boldsymbol\\beta \\cdot \\mathbf{X}_i) & & \\text{(because the logistic distribution is symmetric)} \\\\[5pt] &= \\operatorname{logit}^{-1}(\\boldsymbol\\beta \\cdot \\mathbf{X}_i) & \\\\[5pt] &= p_i & & \\text{(see above)} \\end{align} $ This formulation—which is standard in discrete choice models—makes clear the relationship between logistic regression (the \"logit model\") and the probit model, which uses an error variable distributed according to a standard normal distribution instead of a standard logistic distribution."
            },
            {
                "text": "Both the logistic and normal distributions are symmetric with a basic unimodal, \"bell curve\" shape. The only difference is that the logistic distribution has somewhat heavier tails, which means that it is less sensitive to outlying data (and hence somewhat more robust to model mis-specifications or erroneous data). Two-way latent-variable model Yet another formulation uses two separate latent variables: $ \\begin{align} Y_i^{0\\ast} &= \\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i + \\varepsilon_0 \\, \\\\ Y_i^{1\\ast} &= \\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i + \\varepsilon_1 \\, \\end{align} $ where $ \\begin{align} \\varepsilon_0 & \\sim \\operatorname{EV}_1(0,1) \\\\ \\varepsilon_1 & \\sim \\operatorname{EV}_1(0,1) \\end{align} $ where EV1(0,1) is a standard type-1 extreme value distribution: i.e. $\\Pr(\\varepsilon_0=x) = \\Pr(\\varepsilon_1=x) = e^{-x} e^{-e^{-x}}$ Then $ Y_i = \\begin{cases} 1 & \\text{if }Y_i^{1\\ast} > Y_i^{0\\ast}, \\\\ 0 &\\text{otherwise.}"
            },
            {
                "text": "\\end{cases} $ This model has a separate latent variable and a separate set of regression coefficients for each possible outcome of the dependent variable. The reason for this separation is that it makes it easy to extend logistic regression to multi-outcome categorical variables, as in the multinomial logit model. In such a model, it is natural to model each possible outcome using a different set of regression coefficients. It is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice, and thus motivate logistic regression in terms of utility theory. (In terms of utility theory, a rational actor always chooses the choice with the greatest associated utility.) This is the approach taken by economists when formulating discrete choice models, because it both provides a theoretically strong foundation and facilitates intuitions about the model, which in turn makes it easy to consider various sorts of extensions. (See the example below.) The choice of the type-1 extreme value distribution seems fairly arbitrary, but it makes the mathematics work out, and it may be possible to justify its use through rational choice theory."
            },
            {
                "text": "It turns out that this model is equivalent to the previous model, although this seems non-obvious, since there are now two sets of regression coefficients and error variables, and the error variables have a different distribution. In fact, this model reduces directly to the previous one with the following substitutions: $\\boldsymbol\\beta = \\boldsymbol\\beta_1 - \\boldsymbol\\beta_0$ $\\varepsilon = \\varepsilon_1 - \\varepsilon_0$ An intuition for this comes from the fact that, since we choose based on the maximum of two values, only their difference matters, not the exact values — and this effectively removes one degree of freedom. Another critical fact is that the difference of two type-1 extreme-value-distributed variables is a logistic distribution, i.e."
            },
            {
                "text": "$\\varepsilon = \\varepsilon_1 - \\varepsilon_0 \\sim \\operatorname{Logistic}(0,1) .$ We can demonstrate the equivalent as follows: $\\begin{align} \\Pr(Y_i=1\\mid\\mathbf{X}_i) = {} & \\Pr \\left (Y_i^{1\\ast} > Y_i^{0\\ast}\\mid\\mathbf{X}_i \\right ) & \\\\[5pt] = {} & \\Pr \\left (Y_i^{1\\ast} - Y_i^{0\\ast} > 0\\mid\\mathbf{X}_i \\right ) & \\\\[5pt] = {} & \\Pr \\left (\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i + \\varepsilon_1 - \\left (\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i + \\varepsilon_0 \\right ) > 0 \\right ) & \\\\[5pt] = {} & \\Pr \\left ((\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i - \\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i) + (\\varepsilon_1 - \\varepsilon_0) > 0 \\right ) & \\\\[5pt] = {} & \\Pr((\\boldsymbol\\beta_1 - \\boldsymbol\\beta_0) \\cdot \\mathbf{X}_i + (\\varepsilon_1 - \\varepsilon_0) > 0) & \\\\[5pt] = {} & \\Pr((\\boldsymbol\\beta_1 - \\boldsymbol\\beta_0) \\cdot \\mathbf{X}_i + \\varepsilon > 0) & & \\text{(substitute } \\varepsilon\\text{ as above)} \\\\[5pt] = {} & \\Pr(\\boldsymbol\\beta \\cdot \\mathbf{X}_i + \\varepsilon > 0) & & \\text{(substitute }\\boldsymbol\\beta\\text{ as above)} \\\\[5pt] = {} & \\Pr(\\varepsilon > -\\boldsymbol\\beta \\cdot \\mathbf{X}_i) & & \\text{(now, same as above model)}\\\\[5pt] = {} & \\Pr(\\varepsilon < \\boldsymbol\\beta \\cdot \\mathbf{X}_i) & \\\\[5pt] = {} & \\operatorname{logit}^{-1}(\\boldsymbol\\beta \\cdot \\mathbf{X}_i) \\\\[5pt] = {} & p_i \\end{align}$ Example As an example, consider a province-level election where the choice is between a right-of-center party, a left-of-center party, and a secessionist party (e.g."
            },
            {
                "text": "the Parti Québécois, which wants Quebec to secede from Canada). We would then use three latent variables, one for each choice. Then, in accordance with utility theory, we can then interpret the latent variables as expressing the utility that results from making each of the choices. We can also interpret the regression coefficients as indicating the strength that the associated factor (i.e. explanatory variable) has in contributing to the utility — or more correctly, the amount by which a unit change in an explanatory variable changes the utility of a given choice. A voter might expect that the right-of-center party would lower taxes, especially on rich people. This would give low-income people no benefit, i.e. no change in utility (since they usually don't pay taxes); would cause moderate benefit (i.e. somewhat more money, or moderate utility increase) for middle-incoming people; would cause significant benefits for high-income people. On the other hand, the left-of-center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes."
            },
            {
                "text": "This would cause significant positive benefit to low-income people, perhaps a weak benefit to middle-income people, and significant negative benefit to high-income people. Finally, the secessionist party would take no direct actions on the economy, but simply secede. A low-income or middle-income voter might expect basically no clear utility gain or loss from this, but a high-income voter might expect negative utility since he/she is likely to own companies, which will have a harder time doing business in such an environment and probably lose money. These intuitions can be expressed as follows: +Estimated strength of regression coefficient for different outcomes (party choices) and different values of explanatory variables Center-right Center-left Secessionist High-income strong + strong − strong − Middle-income moderate + weak + Low-income strong + This clearly shows that Separate sets of regression coefficients need to exist for each choice. When phrased in terms of utility, this can be seen very easily. Different choices have different effects on net utility; furthermore, the effects vary in complex ways that depend on the characteristics of each individual, so there need to be separate sets of coefficients for each characteristic, not simply a single extra per-choice characteristic."
            },
            {
                "text": "Even though income is a continuous variable, its effect on utility is too complex for it to be treated as a single variable. Either it needs to be directly split up into ranges, or higher powers of income need to be added so that polynomial regression on income is effectively done. As a \"log-linear\" model Yet another formulation combines the two-way latent variable formulation above with the original formulation higher up without latent variables, and in the process provides a link to one of the standard formulations of the multinomial logit. Here, instead of writing the logit of the probabilities pi as a linear predictor, we separate the linear predictor into two, one for each of the two outcomes: $ \\begin{align} \\ln \\Pr(Y_i=0) &= \\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i - \\ln Z \\\\ \\ln \\Pr(Y_i=1) &= \\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i - \\ln Z \\end{align} $ Two separate sets of regression coefficients have been introduced, just as in the two-way latent variable model, and the two equations appear a form that writes the logarithm of the associated probability as a linear predictor, with an extra term $- \\ln Z$ at the end."
            },
            {
                "text": "This term, as it turns out, serves as the normalizing factor ensuring that the result is a distribution. This can be seen by exponentiating both sides: $ \\begin{align} \\Pr(Y_i=0) &= \\frac{1}{Z} e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} \\\\[5pt] \\Pr(Y_i=1) &= \\frac{1}{Z} e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i} \\end{align} $ In this form it is clear that the purpose of Z is to ensure that the resulting distribution over Yi is in fact a probability distribution, i.e. it sums to 1. This means that Z is simply the sum of all un-normalized probabilities, and by dividing each probability by Z, the probabilities become \"normalized\". That is: $ Z = e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}$ and the resulting equations are $ \\begin{align} \\Pr(Y_i=0) &= \\frac{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i}}{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}} \\\\[5pt] \\Pr(Y_i=1) &= \\frac{e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}."
            },
            {
                "text": "\\end{align} $ Or generally: $\\Pr(Y_i=c) = \\frac{e^{\\boldsymbol\\beta_c \\cdot \\mathbf{X}_i}}{\\sum_h e^{\\boldsymbol\\beta_h \\cdot \\mathbf{X}_i}}$ This shows clearly how to generalize this formulation to more than two outcomes, as in multinomial logit. This general formulation is exactly the softmax function as in $\\Pr(Y_i=c) = \\operatorname{softmax}(c, \\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i, \\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i, \\dots) .$ In order to prove that this is equivalent to the previous model, the above model is overspecified, in that $\\Pr(Y_i=0)$ and $\\Pr(Y_i=1)$ cannot be independently specified: rather $\\Pr(Y_i=0) + \\Pr(Y_i=1) = 1$ so knowing one automatically determines the other. As a result, the model is nonidentifiable, in that multiple combinations of β0 and β1 will produce the same probabilities for all possible explanatory variables."
            },
            {
                "text": "In fact, it can be seen that adding any constant vector to both of them will produce the same probabilities: $ \\begin{align} \\Pr(Y_i=1) &= \\frac{e^{(\\boldsymbol\\beta_1 +\\mathbf{C}) \\cdot \\mathbf{X}_i}}{e^{(\\boldsymbol\\beta_0 +\\mathbf{C})\\cdot \\mathbf{X}_i} + e^{(\\boldsymbol\\beta_1 +\\mathbf{C}) \\cdot \\mathbf{X}_i}} \\\\[5pt] &= \\frac{e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i} e^{\\mathbf{C} \\cdot \\mathbf{X}_i}}{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} e^{\\mathbf{C} \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i} e^{\\mathbf{C} \\cdot \\mathbf{X}_i}} \\\\[5pt] &= \\frac{e^{\\mathbf{C} \\cdot \\mathbf{X}_i}e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}{e^{\\mathbf{C} \\cdot \\mathbf{X}_i}(e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i})} \\\\[5pt] &= \\frac{e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}."
            },
            {
                "text": "\\end{align} $ As a result, we can simplify matters, and restore identifiability, by picking an arbitrary value for one of the two vectors. We choose to set $\\boldsymbol\\beta_0 = \\mathbf{0} .$ Then, $e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} = e^{\\mathbf{0} \\cdot \\mathbf{X}_i} = 1$ and so $ \\Pr(Y_i=1) = \\frac{e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}{1 + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}} = \\frac{1}{1+e^{-\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}} = p_i$ which shows that this formulation is indeed equivalent to the previous formulation. (As in the two-way latent variable formulation, any settings where $\\boldsymbol\\beta = \\boldsymbol\\beta_1 - \\boldsymbol\\beta_0$ will produce equivalent results.) Most treatments of the multinomial logit model start out either by extending the \"log-linear\" formulation presented here or the two-way latent variable formulation presented above, since both clearly show the way that the model could be extended to multi-way outcomes."
            },
            {
                "text": "In general, the presentation with latent variables is more common in econometrics and political science, where discrete choice models and utility theory reign, while the \"log-linear\" formulation here is more common in computer science, e.g. machine learning and natural language processing. As a single-layer perceptron The model has an equivalent formulation $p_i = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}}. \\, $ This functional form is commonly called a single-layer perceptron or single-layer artificial neural network. A single-layer neural network computes a continuous output instead of a step function. The derivative of pi with respect to X = (x1, ..., xk) is computed from the general form: $y = \\frac{1}{1+e^{-f(X)}}$ where f(X) is an analytic function in X. With this choice, the single-layer neural network is identical to the logistic regression model. This function has a continuous derivative, which allows it to be used in backpropagation."
            },
            {
                "text": "This function is also preferred because its derivative is easily calculated: $\\frac{\\mathrm{d}y}{\\mathrm{d}X} = y(1-y)\\frac{\\mathrm{d}f}{\\mathrm{d}X}. \\, $ In terms of binomial data A closely related model assumes that each i is associated not with a single Bernoulli trial but with ni independent identically distributed trials, where the observation Yi is the number of successes observed (the sum of the individual Bernoulli-distributed random variables), and hence follows a binomial distribution: $Y_i \\,\\sim \\operatorname{Bin}(n_i,p_i),\\text{ for }i = 1, \\dots , n$ An example of this distribution is the fraction of seeds (pi) that germinate after ni are planted."
            },
            {
                "text": "In terms of expected values, this model is expressed as follows: $p_i = \\operatorname{\\mathbb E}\\left[\\left.\\frac{Y_i}{n_{i}}\\,\\right|\\,\\mathbf{X}_i \\right]\\,, $ so that $\\operatorname{logit}\\left(\\operatorname{\\mathbb E}\\left[\\left.\\frac{Y_i}{n_i}\\,\\right|\\,\\mathbf{X}_i \\right]\\right) = \\operatorname{logit}(p_i) = \\ln \\left(\\frac{p_i}{1-p_i}\\right) = \\boldsymbol\\beta \\cdot \\mathbf{X}_i\\,,$ Or equivalently: $\\Pr(Y_i=y\\mid \\mathbf{X}_i) = {n_i \\choose y} p_i^y(1-p_i)^{n_i-y} ={n_i \\choose y} \\left(\\frac{1}{1+e^{-\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}\\right)^y \\left(1-\\frac{1}{1+e^{-\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}\\right)^{n_i-y}\\,.$ This model can be fit using the same sorts of methods as the above more basic model."
            },
            {
                "text": "Model fitting Maximum likelihood estimation (MLE) The regression coefficients are usually estimated using maximum likelihood estimation. Unlike linear regression with normally distributed residuals, it is not possible to find a closed-form expression for the coefficient values that maximize the likelihood function so an iterative process must be used instead; for example Newton's method. This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until no more improvement is made, at which point the process is said to have converged. In some instances, the model may not reach convergence. Non-convergence of a model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions. A failure to converge may occur for a number of reasons: having a large ratio of predictors to cases, multicollinearity, sparseness, or complete separation. Having a large ratio of variables to cases results in an overly conservative Wald statistic (discussed below) and can lead to non-convergence. Regularized logistic regression is specifically intended to be used in this situation."
            },
            {
                "text": "Multicollinearity refers to unacceptably high correlations between predictors. As multicollinearity increases, coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases. To detect multicollinearity amongst the predictors, one can conduct a linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic used to assess whether multicollinearity is unacceptably high. Sparseness in the data refers to having a large proportion of empty cells (cells with zero counts). Zero cell counts are particularly problematic with categorical predictors. With continuous predictors, the model can infer values for the zero cell counts, but this is not the case with categorical predictors. The model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value so that the final solution to the model cannot be reached. To remedy this problem, researchers may collapse categories in a theoretically meaningful way or add a constant to all cells. Another numerical problem that may lead to a lack of convergence is complete separation, which refers to the instance in which the predictors perfectly predict the criterion – all cases are accurately classified and the likelihood maximized with infinite coefficients."
            },
            {
                "text": "In such instances, one should re-examine the data, as there may be some kind of error. One can also take semi-parametric or non-parametric approaches, e.g., via local-likelihood or nonparametric quasi-likelihood methods, which avoid assumptions of a parametric form for the index function and is robust to the choice of the link function (e.g., probit or logit). Iteratively reweighted least squares (IRLS) Binary logistic regression ($y=0$ or $ y=1$) can, for example, be calculated using iteratively reweighted least squares (IRLS), which is equivalent to maximizing the log-likelihood of a Bernoulli distributed process using Newton's method."
            },
            {
                "text": "If the problem is written in vector matrix form, with parameters $\\mathbf{w}^T=[\\beta_0,\\beta_1,\\beta_2, \\ldots]$, explanatory variables $\\mathbf{x}(i)=[1, x_1(i), x_2(i), \\ldots]^T$ and expected value of the Bernoulli distribution $\\mu(i)=\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}(i)}}$, the parameters $\\mathbf{w}$ can be found using the following iterative algorithm: $\\mathbf{w}_{k+1} = \\left(\\mathbf{X}^T\\mathbf{S}_k\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\left(\\mathbf{S}_k \\mathbf{X} \\mathbf{w}_k + \\mathbf{y} - \\mathbf{\\boldsymbol\\mu}_k\\right) $ where $\\mathbf{S}=\\operatorname{diag}(\\mu(i)(1-\\mu(i)))$ is a diagonal weighting matrix, $\\boldsymbol\\mu=[\\mu(1), \\mu(2),\\ldots]$ the vector of expected values, $\\mathbf{X}=\\begin{bmatrix} 1 & x_1(1) & x_2(1) & \\ldots\\\\ 1 & x_1(2) & x_2(2) & \\ldots\\\\ \\vdots & \\vdots & \\vdots \\end{bmatrix}$ The regressor matrix and $\\mathbf{y}(i)=[y(1),y(2),\\ldots]^T$ the vector of response variables."
            },
            {
                "text": "More details can be found in the literature. Bayesian right|300px|thumb|Comparison of logistic function with a scaled inverse probit function (i.e. the CDF of the normal distribution), comparing $\\sigma(x)$ vs. , which makes the slopes the same at the origin. This shows the heavier tails of the logistic distribution. In a Bayesian statistics context, prior distributions are normally placed on the regression coefficients, for example in the form of Gaussian distributions. There is no conjugate prior of the likelihood function in logistic regression. When Bayesian inference was performed analytically, this made the posterior distribution difficult to calculate except in very low dimensions. Now, though, automatic software such as OpenBUGS, JAGS, PyMC, Stan or Turing.jl allows these posteriors to be computed using simulation, so lack of conjugacy is not a concern. However, when the sample size or the number of parameters is large, full Bayesian simulation can be slow, and people often use approximate methods such as variational Bayesian methods and expectation propagation. \"Rule of ten\" Widely used, the \"one in ten rule\", states that logistic regression models give stable values for the explanatory variables if based on a minimum of about 10 events per explanatory variable (EPV); where event denotes the cases belonging to the less frequent category in the dependent variable."
            },
            {
                "text": "Thus a study designed to use $k$ explanatory variables for an event (e.g. myocardial infarction) expected to occur in a proportion $p$ of participants in the study will require a total of $10k/p$ participants. However, there is considerable debate about the reliability of this rule, which is based on simulation studies and lacks a secure theoretical underpinning. According to some authors the rule is overly conservative in some circumstances, with the authors stating, \"If we (somewhat subjectively) regard confidence interval coverage less than 93 percent, type I error greater than 7 percent, or relative bias greater than 15 percent as problematic, our results indicate that problems are fairly frequent with 2–4 EPV, uncommon with 5–9 EPV, and still observed with 10–16 EPV. The worst instances of each problem were not severe with 5–9 EPV and usually comparable to those with 10–16 EPV\". Others have found results that are not consistent with the above, using different criteria. A useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in a new sample as it appeared to achieve in the model development sample."
            },
            {
                "text": "For that criterion, 20 events per candidate variable may be required. Also, one can argue that 96 observations are needed only to estimate the model's intercept precisely enough that the margin of error in predicted probabilities is ±0.1 with a 0.95 confidence level. Error and significance of fit Deviance and likelihood ratio test ─ a simple case In any fitting procedure, the addition of another fitting parameter to a model (e.g. the beta parameters in a logistic regression model) will almost always improve the ability of the model to predict the measured outcomes. This will be true even if the additional term has no predictive value, since the model will simply be \"overfitting\" to the noise in the data. The question arises as to whether the improvement gained by the addition of another fitting parameter is significant enough to recommend the inclusion of the additional term, or whether the improvement is simply that which may be expected from overfitting. In short, for logistic regression, a statistic known as the deviance is defined which is a measure of the error between the logistic model fit and the outcome data."
            },
            {
                "text": "In the limit of a large number of data points, the deviance is chi-squared distributed, which allows a chi-squared test to be implemented in order to determine the significance of the explanatory variables. Linear regression and logistic regression have many similarities. For example, in simple linear regression, a set of K data points (xk, yk) are fitted to a proposed model function of the form $y=b_0+b_1 x$. The fit is obtained by choosing the b parameters which minimize the sum of the squares of the residuals (the squared error term) for each data point: $\\varepsilon^2=\\sum_{k=1}^K (b_0+b_1 x_k-y_k)^2.$ The minimum value which constitutes the fit will be denoted by $\\hat{\\varepsilon}^2$ The idea of a null model may be introduced, in which it is assumed that the x variable is of no use in predicting the yk outcomes: The data points are fitted to a null model function of the form y = b0 with a squared error term: $\\varepsilon^2=\\sum_{k=1}^K (b_0-y_k)^2.$ The fitting process consists of choosing a value of b0 which minimizes $\\varepsilon^2$ of the fit to the null model, denoted by $\\varepsilon_\\varphi^2$ where the $\\varphi$ subscript denotes the null model."
            },
            {
                "text": "It is seen that the null model is optimized by $b_0=\\overline{y}$ where $\\overline{y}$ is the mean of the yk values, and the optimized $\\varepsilon_\\varphi^2$ is: $\\hat{\\varepsilon}_\\varphi^2=\\sum_{k=1}^K (\\overline{y}-y_k)^2$ which is proportional to the square of the (uncorrected) sample standard deviation of the yk data points. We can imagine a case where the yk data points are randomly assigned to the various xk, and then fitted using the proposed model. Specifically, we can consider the fits of the proposed model to every permutation of the yk outcomes. It can be shown that the optimized error of any of these fits will never be less than the optimum error of the null model, and that the difference between these minimum error will follow a chi-squared distribution, with degrees of freedom equal those of the proposed model minus those of the null model which, in this case, will be $2-1=1$. Using the chi-squared test, we may then estimate how many of these permuted sets of yk will yield a minimum error less than or equal to the minimum error using the original yk, and so we can estimate how significant an improvement is given by the inclusion of the x variable in the proposed model."
            },
            {
                "text": "For logistic regression, the measure of goodness-of-fit is the likelihood function L, or its logarithm, the log-likelihood ℓ. The likelihood function L is analogous to the $\\varepsilon^2$ in the linear regression case, except that the likelihood is maximized rather than minimized. Denote the maximized log-likelihood of the proposed model by $\\hat{\\ell}$. In the case of simple binary logistic regression, the set of K data points are fitted in a probabilistic sense to a function of the form: $p(x)=\\frac{1}{1+e^{-t}}$ where is the probability that $y=1$."
            },
            {
                "text": "The log-odds are given by: $t=\\beta_0+\\beta_1 x$ and the log-likelihood is: $\\ell=\\sum_{k=1}^K \\left( y_k \\ln(p(x_k))+(1-y_k) \\ln(1-p(x_k))\\right)$ For the null model, the probability that $y=1$ is given by: $p_\\varphi(x)=\\frac{1}{1+e^{-t_\\varphi}}$ The log-odds for the null model are given by: $t_\\varphi=\\beta_0$ and the log-likelihood is: $\\ell_\\varphi=\\sum_{k=1}^K \\left( y_k \\ln(p_\\varphi)+(1-y_k) \\ln(1-p_\\varphi)\\right)$ Since we have $p_\\varphi=\\overline{y}$ at the maximum of L, the maximum log-likelihood for the null model is $\\hat{\\ell}_\\varphi=K(\\,\\overline{y} \\ln(\\overline{y}) + (1-\\overline{y})\\ln(1-\\overline{y}))$ The optimum $\\beta_0$ is: $\\beta_0=\\ln\\left(\\frac{\\overline{y}}{1-\\overline{y}}\\right)$ where $\\overline{y}$ is again the mean of the yk values."
            },
            {
                "text": "Again, we can conceptually consider the fit of the proposed model to every permutation of the yk and it can be shown that the maximum log-likelihood of these permutation fits will never be smaller than that of the null model: $ \\hat{\\ell} \\ge \\hat{\\ell}_\\varphi$ Also, as an analog to the error of the linear regression case, we may define the deviance of a logistic regression fit as: $D=\\ln\\left(\\frac{\\hat{L}^2}{\\hat{L}_\\varphi^2}\\right) = 2(\\hat{\\ell}-\\hat{\\ell}_\\varphi)$ which will always be positive or zero. The reason for this choice is that not only is the deviance a good measure of the goodness of fit, it is also approximately chi-squared distributed, with the approximation improving as the number of data points (K) increases, becoming exactly chi-square distributed in the limit of an infinite number of data points. As in the case of linear regression, we may use this fact to estimate the probability that a random set of data points will give a better fit than the fit obtained by the proposed model, and so have an estimate how significantly the model is improved by including the xk data points in the proposed model."
            },
            {
                "text": "For the simple model of student test scores described above, the maximum value of the log-likelihood of the null model is $\\hat{\\ell}_\\varphi= -13.8629\\ldots$ The maximum value of the log-likelihood for the simple model is $\\hat{\\ell}=-8.02988\\ldots$ so that the deviance is $D = 2(\\hat{\\ell}-\\hat{\\ell}_\\varphi)=11.6661\\ldots$ Using the chi-squared test of significance, the integral of the chi-squared distribution with one degree of freedom from 11.6661... to infinity is equal to 0.00063649... This effectively means that about 6 out of a 10,000 fits to random yk can be expected to have a better fit (smaller deviance) than the given yk and so we can conclude that the inclusion of the x variable and data in the proposed model is a very significant improvement over the null model. In other words, we reject the null hypothesis with $1-D\\approx 99.94 \\%$ confidence. Goodness of fit summary Goodness of fit in linear regression models is generally measured using R2. Since this has no direct analog in logistic regression, various methods including the following can be used instead."
            },
            {
                "text": "Deviance and likelihood ratio tests In linear regression analysis, one is concerned with partitioning variance via the sum of squares calculations – variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance. In logistic regression analysis, deviance is used in lieu of a sum of squares calculations. Deviance is analogous to the sum of squares calculations in linear regression and is a measure of the lack of fit to the data in a logistic regression model. When a \"saturated\" model is available (a model with a theoretically perfect fit), deviance is calculated by comparing a given model with the saturated model. This computation gives the likelihood-ratio test: $ D = -2\\ln \\frac{\\text{likelihood of the fitted model}} {\\text{likelihood of the saturated model}}.$ In the above equation, represents the deviance and ln represents the natural logarithm. The log of this likelihood ratio (the ratio of the fitted model to the saturated model) will produce a negative value, hence the need for a negative sign."
            },
            {
                "text": "can be shown to follow an approximate chi-squared distribution. Smaller values indicate better fit as the fitted model deviates less from the saturated model. When assessed upon a chi-square distribution, nonsignificant chi-square values indicate very little unexplained variance and thus, good model fit. Conversely, a significant chi-square value indicates that a significant amount of the variance is unexplained. When the saturated model is not available (a common case), deviance is calculated simply as −2·(log likelihood of the fitted model), and the reference to the saturated model's log likelihood can be removed from all that follows without harm. Two measures of deviance are particularly important in logistic regression: null deviance and model deviance. The null deviance represents the difference between a model with only the intercept (which means \"no predictors\") and the saturated model. The model deviance represents the difference between a model with at least one predictor and the saturated model. In this respect, the null model provides a baseline upon which to compare predictor models. Given that deviance is a measure of the difference between a given model and the saturated model, smaller values indicate better fit."
            },
            {
                "text": "Thus, to assess the contribution of a predictor or set of predictors, one can subtract the model deviance from the null deviance and assess the difference on a $\\chi^2_{s-p},$ chi-square distribution with degrees of freedom equal to the difference in the number of parameters estimated. Let $\\begin{align} D_{\\text{null}} &=-2\\ln \\frac{\\text{likelihood of null model}} {\\text{likelihood of the saturated model}}\\\\[6pt] D_{\\text{fitted}} &=-2\\ln \\frac{\\text{likelihood of fitted model}} {\\text{likelihood of the saturated model}}. \\end{align} $ Then the difference of both is: $\\begin{align} D_\\text{null}- D_\\text{fitted} &= -2 \\left(\\ln \\frac{\\text{likelihood of null model}} {\\text{likelihood of the saturated model}}-\\ln \\frac{\\text{likelihood of fitted model}} {\\text{likelihood of the saturated model}}\\right)\\\\[6pt] &= -2 \\ln \\frac{ \\left( \\dfrac{\\text{likelihood of null model}}{\\text{likelihood of the saturated model}}\\right)}{\\left(\\dfrac{\\text{likelihood of fitted model}}{\\text{likelihood of the saturated model}}\\right)}\\\\[6pt] &= -2 \\ln \\frac{\\text{likelihood of the null model}}{\\text{likelihood of fitted model}}."
            },
            {
                "text": "\\end{align}$ If the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improve the model's fit. This is analogous to the -test used in linear regression analysis to assess the significance of prediction. Pseudo-R-squared In linear regression the squared multiple correlation, 2 is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors. In logistic regression analysis, there is no agreed upon analogous measure, but there are several competing measures each with limitations. Four of the most commonly used indices and one less commonly used one are examined on this page: Likelihood ratio 2 Cox and Snell 2 Nagelkerke 2 McFadden 2 Tjur 2 Hosmer–Lemeshow test The Hosmer–Lemeshow test uses a test statistic that asymptotically follows a $\\chi^2$ distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population. This test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power."
            },
            {
                "text": "Coefficient significance After fitting the model, it is likely that researchers will want to examine the contribution of individual predictors. To do so, they will want to examine the regression coefficients. In linear regression, the regression coefficients represent the change in the criterion for each unit change in the predictor. In logistic regression, however, the regression coefficients represent the change in the logit for each unit change in the predictor. Given that the logit is not intuitive, researchers are likely to focus on a predictor's effect on the exponential function of the regression coefficient – the odds ratio (see definition). In linear regression, the significance of a regression coefficient is assessed by computing a t test. In logistic regression, there are several different tests designed to assess the significance of an individual predictor, most notably the likelihood ratio test and the Wald statistic. Likelihood ratio test The likelihood-ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual \"predictors\" to a given model. In the case of a single predictor model, one simply compares the deviance of the predictor model with that of the null model on a chi-square distribution with a single degree of freedom."
            },
            {
                "text": "If the predictor model has significantly smaller deviance (c.f. chi-square using the difference in degrees of freedom of the two models), then one can conclude that there is a significant association between the \"predictor\" and the outcome. Although some common statistical packages (e.g. SPSS) do provide likelihood ratio test statistics, without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case. To assess the contribution of individual predictors one can enter the predictors hierarchically, comparing each new model with the previous to determine the contribution of each predictor. There is some debate among statisticians about the appropriateness of so-called \"stepwise\" procedures. The fear is that they may not preserve nominal statistical properties and may become misleading. Wald statistic Alternatively, when assessing the contribution of individual predictors in a given model, one may examine the significance of the Wald statistic. The Wald statistic, analogous to the t-test in linear regression, is used to assess the significance of coefficients. The Wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as a chi-square distribution."
            },
            {
                "text": "$W_j = \\frac{\\beta^2_j} {SE^2_{\\beta_j}}$ Although several statistical packages (e.g., SPSS, SAS) report the Wald statistic to assess the contribution of individual predictors, the Wald statistic has limitations. When the regression coefficient is large, the standard error of the regression coefficient also tends to be larger increasing the probability of Type-II error. The Wald statistic also tends to be biased when data are sparse. Case-control sampling Suppose cases are rare. Then we might wish to sample them more frequently than their prevalence in the population. For example, suppose there is a disease that affects 1 person in 10,000 and to collect our data we need to do a complete physical. It may be too expensive to do thousands of physicals of healthy people in order to obtain data for only a few diseased individuals. Thus, we may evaluate more diseased individuals, perhaps all of the rare outcomes. This is also retrospective sampling, or equivalently it is called unbalanced data. As a rule of thumb, sampling controls at a rate of five times the number of cases will produce sufficient control data.https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/classification.pdf slide 16 Logistic regression is unique in that it may be estimated on unbalanced data, rather than randomly sampled data, and still yield correct coefficient estimates of the effects of each independent variable on the outcome."
            },
            {
                "text": "That is to say, if we form a logistic model from such data, if the model is correct in the general population, the $\\beta_j$ parameters are all correct except for $\\beta_0$. We can correct $\\beta_0$ if we know the true prevalence as follows: $\\widehat{\\beta}_0^* = \\widehat{\\beta}_0+\\log \\frac \\pi {1 - \\pi} - \\log{ \\tilde{\\pi} \\over {1 - \\tilde{\\pi}} } $ where $\\pi$ is the true prevalence and $\\tilde{\\pi}$ is the prevalence in the sample. Discussion Like other forms of regression analysis, logistic regression makes use of one or more predictor variables that may be either continuous or categorical. Unlike ordinary linear regression, however, logistic regression is used for predicting dependent variables that take membership in one of a limited number of categories (treating the dependent variable in the binomial case as the outcome of a Bernoulli trial) rather than a continuous outcome. Given this difference, the assumptions of linear regression are violated."
            },
            {
                "text": "In particular, the residuals cannot be normally distributed. In addition, linear regression may make nonsensical predictions for a binary dependent variable. What is needed is a way to convert a binary variable into a continuous one that can take on any real value (negative or positive). To do that, binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable, and then takes its logarithm to create a continuous criterion as a transformed version of the dependent variable. The logarithm of the odds is the $logit$ of the probability, the $logit$ is defined as follows: Although the dependent variable in logistic regression is Bernoulli, the logit is on an unrestricted scale. The logit function is the link function in this kind of generalized linear model, i.e. is the Bernoulli-distributed response variable and is the predictor variable; the values are the linear parameters. The $logit$ of the probability of success is then fitted to the predictors. The predicted value of the $logit$ is converted back into predicted odds, via the inverse of the natural logarithm – the exponential function."
            },
            {
                "text": "Thus, although the observed dependent variable in binary logistic regression is a 0-or-1 variable, the logistic regression estimates the odds, as a continuous variable, that the dependent variable is a 'success'. In some applications, the odds are all that is needed. In others, a specific yes-or-no prediction is needed for whether the dependent variable is or is not a 'success'; this categorical prediction can be based on the computed odds of success, with predicted odds above some chosen cutoff value being translated into a prediction of success. Maximum entropy Of all the functional forms used for estimating the probabilities of a particular categorical outcome which optimize the fit by maximizing the likelihood function (e.g. probit regression, Poisson regression, etc. ), the logistic regression solution is unique in that it is a maximum entropy solution. This is a case of a general property: an exponential family of distributions maximizes entropy, given an expected value. In the case of the logistic model, the logistic function is the natural parameter of the Bernoulli distribution (it is in \"canonical form\", and the logistic function is the canonical link function), while other sigmoid functions are non-canonical link functions; this underlies its mathematical elegance and ease of optimization."
            },
            {
                "text": "See for details. Proof In order to show this, we use the method of Lagrange multipliers. The Lagrangian is equal to the entropy plus the sum of the products of Lagrange multipliers times various constraint expressions. The general multinomial case will be considered, since the proof is not made that much simpler by considering simpler cases. Equating the derivative of the Lagrangian with respect to the various probabilities to zero yields a functional form for those probabilities which corresponds to those used in logistic regression. As in the above section on multinomial logistic regression, we will consider explanatory variables denoted and which include $x_0=1$. There will be a total of K data points, indexed by $k=\\{1,2,\\dots,K\\}$, and the data points are given by $x_{mk}$ and . The xmk will also be represented as an -dimensional vector $\\boldsymbol{x}_k = \\{x_{0k},x_{1k},\\dots,x_{Mk}\\}$. There will be possible values of the categorical variable y ranging from 0 to N. Let pn(x) be the probability, given explanatory variable vector x, that the outcome will be $y=n$."
            },
            {
                "text": "Define $p_{nk}=p_n(\\boldsymbol{x}_k)$ which is the probability that for the k-th measurement, the categorical outcome is n. The Lagrangian will be expressed as a function of the probabilities pnk and will minimized by equating the derivatives of the Lagrangian with respect to these probabilities to zero. An important point is that the probabilities are treated equally and the fact that they sum to 1 is part of the Lagrangian formulation, rather than being assumed from the beginning. The first contribution to the Lagrangian is the entropy: $\\mathcal{L}_{ent}=-\\sum_{k=1}^K\\sum_{n=0}^N p_{nk}\\ln(p_{nk})$ The log-likelihood is: $\\ell=\\sum_{k=1}^K\\sum_{n=0}^N \\Delta(n,y_k)\\ln(p_{nk})$ Assuming the multinomial logistic function, the derivative of the log-likelihood with respect the beta coefficients was found to be: $\\frac{\\partial \\ell}{\\partial \\beta_{nm}}=\\sum_{k=1}^K ( p_{nk}x_{mk}-\\Delta(n,y_k)x_{mk})$ A very important point here is that this expression is (remarkably) not an explicit function of the beta coefficients."
            },
            {
                "text": "It is only a function of the probabilities pnk and the data. Rather than being specific to the assumed multinomial logistic case, it is taken to be a general statement of the condition at which the log-likelihood is maximized and makes no reference to the functional form of pnk. There are then (M+1)(N+1) fitting constraints and the fitting constraint term in the Lagrangian is then: $\\mathcal{L}_{fit}=\\sum_{n=0}^N\\sum_{m=0}^M \\lambda_{nm}\\sum_{k=1}^K (p_{nk}x_{mk}-\\Delta(n,y_k)x_{mk})$ where the λnm are the appropriate Lagrange multipliers. There are K normalization constraints which may be written: $\\sum_{n=0}^N p_{nk}=1$ so that the normalization term in the Lagrangian is: $\\mathcal{L}_{norm}=\\sum_{k=1}^K \\alpha_k \\left(1-\\sum_{n=1}^N p_{nk}\\right) $ where the αk are the appropriate Lagrange multipliers."
            },
            {
                "text": "The Lagrangian is then the sum of the above three terms: $\\mathcal{L}=\\mathcal{L}_{ent} + \\mathcal{L}_{fit} + \\mathcal{L}_{norm}$ Setting the derivative of the Lagrangian with respect to one of the probabilities to zero yields: $\\frac{\\partial \\mathcal{L}}{\\partial p_{n'k'}}=0=-\\ln(p_{n'k'})-1+\\sum_{m=0}^M (\\lambda_{n'm}x_{mk'})-\\alpha_{k'}$ Using the more condensed vector notation: $\\sum_{m=0}^M \\lambda_{nm}x_{mk} = \\boldsymbol{\\lambda}_n\\cdot\\boldsymbol{x}_k$ and dropping the primes on the n and k indices, and then solving for $p_{nk}$ yields: $p_{nk}=e^{\\boldsymbol{\\lambda}_n\\cdot\\boldsymbol{x}_k}/Z_k$ where: $Z_k=e^{1+\\alpha_k}$ Imposing the normalization constraint, we can solve for the Zk and write the probabilities as: $p_{nk}=\\frac{e^{\\boldsymbol{\\lambda}_n\\cdot\\boldsymbol{x}_k}}{\\sum_{u=0}^N e^{\\boldsymbol{\\lambda}_u\\cdot\\boldsymbol{x}_k}}$ The $\\boldsymbol{\\lambda}_n$ are not all independent."
            },
            {
                "text": "We can add any constant -dimensional vector to each of the $\\boldsymbol{\\lambda}_n$ without changing the value of the $p_{nk}$ probabilities so that there are only N rather than independent $\\boldsymbol{\\lambda}_n$. In the multinomial logistic regression section above, the $\\boldsymbol{\\lambda}_0$ was subtracted from each $\\boldsymbol{\\lambda}_n$ which set the exponential term involving $\\boldsymbol{\\lambda}_0$ to 1, and the beta coefficients were given by $\\boldsymbol{\\beta}_n=\\boldsymbol{\\lambda}_n-\\boldsymbol{\\lambda}_0$. Other approaches In machine learning applications where logistic regression is used for binary classification, the MLE minimises the cross-entropy loss function. Logistic regression is an important machine learning algorithm. The goal is to model the probability of a random variable $Y$ being 0 or 1 given experimental data. Consider a generalized linear model function parameterized by $\\theta$, $ h_\\theta(X) = \\frac{1}{1 + e^{-\\theta^TX}} = \\Pr(Y=1 \\mid X; \\theta) $ Therefore, $ \\Pr(Y=0 \\mid X; \\theta) = 1 - h_\\theta(X) $ and since $ Y \\in \\{0,1\\}$, we see that $ \\Pr(y\\mid X;\\theta) $ is given by $ \\Pr(y \\mid X; \\theta) = h_\\theta(X)^y(1 - h_\\theta(X))^{(1-y)}."
            },
            {
                "text": "$ We now calculate the likelihood function assuming that all the observations in the sample are independently Bernoulli distributed, $\\begin{align} L(\\theta \\mid y; x) &= \\Pr(Y \\mid X; \\theta) \\\\ &= \\prod_i \\Pr(y_i \\mid x_i; \\theta) \\\\ &= \\prod_i h_\\theta(x_i)^{y_i}(1 - h_\\theta(x_i))^{(1-y_i)} \\end{align}$ Typically, the log likelihood is maximized, $ N^{-1} \\log L(\\theta \\mid y; x) = N^{-1} \\sum_{i=1}^N \\log \\Pr(y_i \\mid x_i; \\theta) $ which is maximized using optimization techniques such as gradient descent."
            },
            {
                "text": "Assuming the $(x, y)$ pairs are drawn uniformly from the underlying distribution, then in the limit of large N, $\\begin{align} & \\lim \\limits_{N \\rightarrow +\\infty} N^{-1} \\sum_{i=1}^N \\log \\Pr(y_i \\mid x_i; \\theta) \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} \\Pr(X=x, Y=y) \\log \\Pr(Y=y \\mid Xx; \\theta) \\\\[6pt] {} & \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} \\Pr(X=x, Y=y) \\left( - \\log\\frac{\\Pr(Y=y \\mid X=x)}{\\Pr(Y=y \\mid X=x; \\theta)} + \\log \\Pr(Y=y \\mid Xx) \\right) \\\\[6pt] = {} & - D_\\text{KL}( Y \\parallel Y_\\theta ) - H(Y \\mid X) \\end{align}$ where $H(Y\\mid X)$ is the conditional entropy and $D_\\text{KL}$ is the Kullback–Leibler divergence."
            },
            {
                "text": "This leads to the intuition that by maximizing the log-likelihood of a model, you are minimizing the KL divergence of your model from the maximal entropy distribution. Intuitively searching for the model that makes the fewest assumptions in its parameters. Comparison with linear regression Logistic regression can be seen as a special case of the generalized linear model and thus analogous to linear regression. The model of logistic regression, however, is based on quite different assumptions (about the relationship between the dependent and independent variables) from those of linear regression. In particular, the key differences between these two models can be seen in the following two features of logistic regression. First, the conditional distribution $y \\mid x$ is a Bernoulli distribution rather than a Gaussian distribution, because the dependent variable is binary. Second, the predicted values are probabilities and are therefore restricted to (0,1) through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves. Alternatives A common alternative to the logistic model (logit model) is the probit model, as the related names suggest."
            },
            {
                "text": "From the perspective of generalized linear models, these differ in the choice of link function: the logistic model uses the logit function (inverse logistic function), while the probit model uses the probit function (inverse error function). Equivalently, in the latent variable interpretations of these two methods, the first assumes a standard logistic distribution of errors and the second a standard normal distribution of errors. Other sigmoid functions or error distributions can be used instead. Logistic regression is an alternative to Fisher's 1936 method, linear discriminant analysis. If the assumptions of linear discriminant analysis hold, the conditioning can be reversed to produce logistic regression. The converse is not true, however, because logistic regression does not require the multivariate normal assumption of discriminant analysis. The assumption of linear predictor effects can easily be relaxed using techniques such as spline functions. History A detailed history of the logistic regression is given in . The logistic function was developed as a model of population growth and named \"logistic\" by Pierre François Verhulst in the 1830s and 1840s, under the guidance of Adolphe Quetelet; see for details."
            },
            {
                "text": "In his earliest paper (1838), Verhulst did not specify how he fit the curves to the data. In his more detailed paper (1845), Verhulst determined the three parameters of the model by making the curve pass through three observed points, which yielded poor predictions. The logistic function was independently developed in chemistry as a model of autocatalysis (Wilhelm Ostwald, 1883). An autocatalytic reaction is one in which one of the products is itself a catalyst for the same reaction, while the supply of one of the reactants is fixed. This naturally gives rise to the logistic equation for the same reason as population growth: the reaction is self-reinforcing but constrained. The logistic function was independently rediscovered as a model of population growth in 1920 by Raymond Pearl and Lowell Reed, published as , which led to its use in modern statistics. They were initially unaware of Verhulst's work and presumably learned about it from L. Gustave du Pasquier, but they gave him little credit and did not adopt his terminology. Verhulst's priority was acknowledged and the term \"logistic\" revived by Udny Yule in 1925 and has been followed since."
            },
            {
                "text": "Pearl and Reed first applied the model to the population of the United States, and also initially fitted the curve by making it pass through three points; as with Verhulst, this again yielded poor results. In the 1930s, the probit model was developed and systematized by Chester Ittner Bliss, who coined the term \"probit\" in , and by John Gaddum in , and the model fit by maximum likelihood estimation by Ronald A. Fisher in , as an addendum to Bliss's work. The probit model was principally used in bioassay, and had been preceded by earlier work dating to 1860; see . The probit model influenced the subsequent development of the logit model and these models competed with each other. The logistic model was likely first used as an alternative to the probit model in bioassay by Edwin Bidwell Wilson and his student Jane Worcester in . However, the development of the logistic model as a general alternative to the probit model was principally due to the work of Joseph Berkson over many decades, beginning in , where he coined \"logit\", by analogy with \"probit\", and continuing through and following years."
            },
            {
                "text": "The logit model was initially dismissed as inferior to the probit model, but \"gradually achieved an equal footing with the probit\", particularly between 1960 and 1970. By 1970, the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it. This relative popularity was due to the adoption of the logit outside of bioassay, rather than displacing the probit within bioassay, and its informal use in practice; the logit's popularity is credited to the logit model's computational simplicity, mathematical properties, and generality, allowing its use in varied fields. Various refinements occurred during that time, notably by David Cox, as in . The multinomial logit model was introduced independently in and , which greatly increased the scope of application and the popularity of the logit model. In 1973 Daniel McFadden linked the multinomial logit to the theory of discrete choice, specifically Luce's choice axiom, showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences; this gave a theoretical foundation for the logistic regression. Extensions There are large numbers of extensions: Multinomial logistic regression (or multinomial logit) handles the case of a multi-way categorical dependent variable (with unordered values, also called \"classification\"). The general case of having dependent variables with more than two values is termed polytomous regression. Ordered logistic regression (or ordered logit) handles ordinal dependent variables (ordered values). Mixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable."
            },
            {
                "text": "An extension of the logistic model to sets of interdependent variables is the conditional random field. Conditional logistic regression handles matched or stratified data when the strata are small. It is mostly used in the analysis of observational studies. See also Logistic function Discrete choice Jarrow–Turnbull model Limited dependent variable Multinomial logit model Ordered logit Hosmer–Lemeshow test Brier score mlpack - contains a C++ implementation of logistic regression Local case-control sampling Logistic model tree References Sources Published in: External links by Mark Thoma Logistic Regression tutorial mlelr: software in C for teaching purposes Category:Predictive analytics Category:Regression models"
            }
        ],
        "latex_formulas": [
            "[[logit]]",
            "logit",
            "logit",
            "logit",
            "k=1",
            "k=K=20",
            "p(x)=\\frac{1}{1+e^{-(x-\\mu)/s}}",
            "p(\\mu)=1/2",
            "p(x)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1 x)}}",
            "\\beta_0 = -\\mu/s",
            "y = \\beta_0+\\beta_1 x",
            "\\beta_1= 1/s",
            "\\mu=-\\beta_0/\\beta_1",
            "s=1/\\beta_1",
            "p_k=p(x_k)",
            "\\ell_k = \\begin{cases}\n-\\ln p_k & \\text{ if } y_k = 1, \\\\\n-\\ln (1 - p_k) & \\text{ if } y_k = 0.\n\\end{cases}",
            "p_k = 1",
            "y_k = 1",
            "p_k = 0",
            "y_k = 0",
            "y_k = 1",
            "p_k \\to 0",
            "y_k = 0",
            "p_k \\to 1",
            "\\ell_k = -y_k\\ln p_k - (1 - y_k)\\ln (1 - p_k).",
            "\\big(p_k, (1-p_k)\\big)",
            "\\big(y_k, (1-y_k)\\big)",
            "\\ell = \\sum_{k:y_k=1}\\ln(p_k) + \\sum_{k:y_k=0}\\ln(1-p_k) = \\sum_{k=1}^K \\left(\\,y_k \\ln(p_k)+(1-y_k)\\ln(1-p_k)\\right)",
            "L = \\prod_{k:y_k=1}p_k\\,\\prod_{k:y_k=0}(1-p_k)",
            "0 = \\frac{\\partial \\ell}{\\partial \\beta_0} = \\sum_{k=1}^K(y_k-p_k)",
            "0 = \\frac{\\partial \\ell}{\\partial \\beta_1} = \\sum_{k=1}^K(y_k-p_k)x_k",
            "\\beta_0 \\approx -4.1",
            "\\beta_1 \\approx 1.5",
            "\\mu = -\\beta_0/\\beta_1 \\approx 2.7",
            "s = 1/\\beta_1 \\approx 0.67",
            "x = 2",
            "t = \\beta_0+2\\beta_1 \\approx - 4.1 + 2 \\cdot 1.5  = -1.1",
            "p = \\frac{1}{1 + e^{-t} } \\approx 0.25 = \\text{Probability of passing exam}",
            "t = \\beta_0+4\\beta_1 \\approx - 4.1 + 4 \\cdot 1.5  = 1.9",
            "p = \\frac{1}{1 + e^{-t} } \\approx 0.87 = \\text{Probability of passing exam}",
            "p = 0.017",
            "p \\approx 0.00064",
            "\\sigma (t)",
            "\\sigma (t) \\in (0,1)",
            "t",
            "t",
            "\\sigma:\\mathbb R\\rightarrow (0,1)",
            "\\sigma (t) = \\frac{e^t}{e^t+1} = \\frac{1}{1+e^{-t}}",
            "t",
            "x",
            "t",
            "t",
            "t = \\beta_0 + \\beta_1 x",
            "p:\\mathbb R \\rightarrow (0,1)",
            "p(x) = \\sigma(t)= \\frac {1}{1+e^{-(\\beta_0 + \\beta_1 x)}}",
            "p(x)",
            "Y",
            "Y_i",
            "P(Y_i = 1\\mid X)",
            "X_i",
            "X",
            "\\beta",
            "g = \\sigma^{-1}",
            "g(p(x)) = \\sigma^{-1} (p(x)) = \\operatorname{logit} p(x) = \\ln \\left( \\frac{p(x)}{1 - p(x)} \\right) = \\beta_0 + \\beta_1 x ,",
            "\\frac{p(x)}{1 - p(x)} = e^{\\beta_0 + \\beta_1 x}.",
            "g",
            "g(p(x))",
            "\\ln",
            "p(x)",
            "p(x)",
            "p(x)",
            "\\beta_0",
            "\\beta_1 x",
            "e",
            "x",
            "x",
            "\\text{odds} = e^{\\beta_0 + \\beta_1 x}.",
            "\\mathrm{OR} = \\frac{\\operatorname{odds}(x+1)}{\\operatorname{odds}(x)} = \\frac{\\left(\\frac{p(x+1)}{1 - p(x+1)}\\right)}{\\left(\\frac{p(x)}{1 - p(x)}\\right)}\n                                        = \\frac{e^{\\beta_0 + \\beta_1 (x+1)}}{e^{\\beta_0 + \\beta_1 x}} = e^{\\beta_1}",
            "\\beta_1",
            "e^{\\beta_1}",
            "\\frac{ad}{bc}",
            "\\beta_0+\\beta_1x",
            "\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_mx_m = \\beta_0+ \\sum_{i=1}^m \\beta_ix_i",
            "\\beta_i",
            "i = 0, 1, 2, \\dots, m",
            "\\log \\frac{p}{1-p} = \\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_mx_m",
            "p = \\frac{1}{1+b^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_mx_m )}}",
            "b=e",
            "\\begin{align}\nY_i\\mid x_{1,i},\\ldots,x_{m,i} \\ & \\sim  \\operatorname{Bernoulli}(p_i) \\\\[5pt]\n\\operatorname{\\mathbb E}[Y_i\\mid x_{1,i},\\ldots,x_{m,i}] &= p_i  \\\\[5pt]\n\\Pr(Y_i=y\\mid x_{1,i},\\ldots,x_{m,i}) &=\n\\begin{cases}\np_i & \\text{if }y=1 \\\\\n1-p_i & \\text{if }y=0\n\\end{cases}\n\\\\[5pt]\n\\Pr(Y_i=y\\mid x_{1,i},\\ldots,x_{m,i}) &= p_i^y (1-p_i)^{(1-y)}\n\\end{align}",
            "f(i)",
            "f(i) = \\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_m x_{m,i},",
            "\\beta_0, \\ldots, \\beta_m",
            "f(i)= \\boldsymbol\\beta \\cdot \\mathbf{X}_i,",
            "y=0,1,2,\\dots",
            "y=1",
            "t = \\log_b \\frac{p}{1-p} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2+ \\cdots +\\beta_M x_M",
            "\\beta_i",
            "b",
            "\\boldsymbol{x}=\\{x_0,x_1,x_2,\\dots,x_M\\}",
            "\\boldsymbol{\\beta}=\\{\\beta_0,\\beta_1,\\beta_2,\\dots,\\beta_M\\}",
            "t =\\sum_{m=0}^{M} \\beta_m x_m = \\boldsymbol{\\beta} \\cdot x",
            "y=1",
            "p(\\boldsymbol{x}) = \\frac{b^{\\boldsymbol{\\beta} \\cdot \\boldsymbol{x}}}{1+b^{\\boldsymbol{\\beta} \\cdot \\boldsymbol{x}}}= \\frac{1}{1+b^{-\\boldsymbol{\\beta} \\cdot \\boldsymbol{x}}}=S_b(t)",
            "S_b",
            "b",
            "\\beta_m",
            "y=1",
            "y=1",
            "\\boldsymbol{x}",
            "p(\\boldsymbol{x})",
            "y=1",
            "\\boldsymbol{x}_k",
            "y_k",
            "M=1",
            "\\ell = \\sum_{k=1}^K y_k \\log_b(p(\\boldsymbol{x_k}))+\\sum_{k=1}^K (1-y_k) \\log_b(1-p(\\boldsymbol{x_k}))",
            "\\frac{\\partial  \\ell}{\\partial  \\beta_m} = 0 = \\sum_{k=1}^K y_k x_{mk} - \\sum_{k=1}^K p(\\boldsymbol{x}_k)x_{mk}",
            "M=2",
            "b=10",
            "\\beta_0=-3",
            "\\beta_1=1",
            "\\beta_2=2",
            "t=\\log_{10}\\frac{p}{1 - p} = -3 + x_1 + 2 x_2",
            "p = \\frac{b^{\\boldsymbol{\\beta} \\cdot \\boldsymbol{x}}}{1+b^{\\boldsymbol{\\beta} \\cdot x}} = \\frac{b^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2}}{1+b^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2} } = \\frac{1}{1 + b^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)}}",
            "y=1",
            "\\beta_0 = -3",
            "y=1",
            "x_1=x_2=0",
            "x_1=x_2=0",
            "y=1",
            "10^{-3}",
            "y=1",
            "x_1=x_2=0",
            "1/(1000 + 1) = 1/1001.",
            "\\beta_1 = 1",
            "x_1",
            "1",
            "x_1",
            "y=1",
            "10^1",
            "y=1",
            "\\beta_2 = 2",
            "x_2",
            "2",
            "x_2",
            "y=1",
            "10^2.",
            "x_2",
            "x_1",
            "y=1",
            "p(\\boldsymbol{x})",
            "1-p(\\boldsymbol{x})",
            "p_n(\\boldsymbol{x}) = \\frac{e^{\\boldsymbol{\\beta}_n\\cdot \\boldsymbol{x}}}{1+\\sum_{u=1}^N e^{\\boldsymbol{\\beta}_u\\cdot \\boldsymbol{x}}}",
            "n=1,2,\\dots,N",
            "p_0(\\boldsymbol{x}) = 1-\\sum_{n=1}^N p_n(\\boldsymbol{x})=\\frac{1}{1+\\sum_{u=1}^N e^{\\boldsymbol{\\beta}_u\\cdot \\boldsymbol{x}}}",
            "p_0(\\boldsymbol{x})",
            "\\boldsymbol{\\beta}_n",
            "p_n(\\boldsymbol{x})",
            "p_0(\\boldsymbol{x})",
            "t_n = \\ln\\left(\\frac{p_n(\\boldsymbol{x})}{p_0(\\boldsymbol{x})}\\right) = \\boldsymbol{\\beta}_n \\cdot \\boldsymbol{x}",
            "N=1",
            "p(\\boldsymbol{x})=p_1(\\boldsymbol{x})",
            "p_0(\\boldsymbol{x})=1-p_1(\\boldsymbol{x})",
            "\\boldsymbol{x}_k",
            "y_k",
            "\\ell = \\sum_{k=1}^K \\sum_{n=0}^N \\Delta(n,y_k)\\,\\ln(p_n(\\boldsymbol{x}_k))",
            "\\Delta(n,y_k)",
            "\\Delta(n,y)=1-(y-n)^2",
            "\\frac{\\partial \\ell}{\\partial  \\beta_{nm}} = 0 = \\sum_{k=1}^K \\Delta(n,y_k)x_{mk} - \\sum_{k=1}^K p_n(\\boldsymbol{x}_k)x_{mk}",
            "\\beta_{nm}",
            "\\boldsymbol{\\beta}_n",
            "x_{mk}",
            "\\operatorname{logit}(\\operatorname{\\mathbb E}[Y_i\\mid x_{1,i},\\ldots,x_{m,i}]) = \\operatorname{logit}(p_i) = \\ln \\left(\\frac{p_i}{1-p_i}\\right) = \\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_m x_{m,i}",
            "\\operatorname{logit}(\\operatorname{\\mathbb E}[Y_i\\mid \\mathbf{X}_i]) = \\operatorname{logit}(p_i)=\\ln\\left(\\frac{p_i}{1-p_i}\\right) = \\boldsymbol\\beta \\cdot \\mathbf{X}_i",
            "(-\\infty,+\\infty)",
            "e^\\beta",
            "\\operatorname{\\mathbb E}[Y_i\\mid \\mathbf{X}_i] = p_i = \\operatorname{logit}^{-1}(\\boldsymbol\\beta \\cdot \\mathbf{X}_i) = \\frac{1}{1+e^{-\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}",
            "\\Pr(Y_i=y\\mid \\mathbf{X}_i) = {p_i}^y(1-p_i)^{1-y} =\\left(\\frac{e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}{1+e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}\\right)^{y} \\left(1-\\frac{e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}{1+e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}\\right)^{1-y} = \\frac{e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i \\cdot y}  }{1+e^{\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}",
            "Y_i^\\ast = \\boldsymbol\\beta \\cdot \\mathbf{X}_i + \\varepsilon_i \\,",
            "\\varepsilon_i \\sim \\operatorname{Logistic}(0,1) \\,",
            "Y_i = \\begin{cases} 1 & \\text{if }Y_i^\\ast > 0 \\ \\text{ i.e. } {- \\varepsilon_i} < \\boldsymbol\\beta \\cdot \\mathbf{X}_i, \\\\\n0 &\\text{otherwise.} \\end{cases}",
            "\\Pr(\\varepsilon_i < x) = \\operatorname{logit}^{-1}(x)",
            "\\begin{align}\n\\Pr(Y_i=1\\mid\\mathbf{X}_i) &= \\Pr(Y_i^\\ast > 0\\mid\\mathbf{X}_i) \\\\[5pt]\n&= \\Pr(\\boldsymbol\\beta \\cdot \\mathbf{X}_i + \\varepsilon_i > 0) \\\\[5pt]\n&= \\Pr(\\varepsilon_i > -\\boldsymbol\\beta \\cdot \\mathbf{X}_i) \\\\[5pt]\n&= \\Pr(\\varepsilon_i < \\boldsymbol\\beta \\cdot \\mathbf{X}_i) & & \\text{(because the logistic distribution is symmetric)} \\\\[5pt]\n&= \\operatorname{logit}^{-1}(\\boldsymbol\\beta \\cdot \\mathbf{X}_i) & \\\\[5pt]\n&= p_i & & \\text{(see above)}\n\\end{align}",
            "\\begin{align}\nY_i^{0\\ast} &= \\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i + \\varepsilon_0 \\, \\\\\nY_i^{1\\ast} &= \\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i + \\varepsilon_1 \\,\n\\end{align}",
            "\\begin{align}\n\\varepsilon_0 & \\sim \\operatorname{EV}_1(0,1) \\\\\n\\varepsilon_1 & \\sim \\operatorname{EV}_1(0,1)\n\\end{align}",
            "\\Pr(\\varepsilon_0=x) = \\Pr(\\varepsilon_1=x) = e^{-x} e^{-e^{-x}}",
            "Y_i = \\begin{cases} 1 & \\text{if }Y_i^{1\\ast} > Y_i^{0\\ast}, \\\\\n0 &\\text{otherwise.} \\end{cases}",
            "\\boldsymbol\\beta = \\boldsymbol\\beta_1 - \\boldsymbol\\beta_0",
            "\\varepsilon = \\varepsilon_1 - \\varepsilon_0",
            "\\varepsilon = \\varepsilon_1 - \\varepsilon_0 \\sim \\operatorname{Logistic}(0,1) .",
            "\\begin{align}\n\\Pr(Y_i=1\\mid\\mathbf{X}_i) = {} & \\Pr \\left (Y_i^{1\\ast} > Y_i^{0\\ast}\\mid\\mathbf{X}_i \\right ) & \\\\[5pt]\n= {} & \\Pr \\left (Y_i^{1\\ast} - Y_i^{0\\ast} > 0\\mid\\mathbf{X}_i \\right ) & \\\\[5pt]\n= {} & \\Pr \\left (\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i + \\varepsilon_1 - \\left (\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i + \\varepsilon_0 \\right ) > 0 \\right ) & \\\\[5pt]\n= {} & \\Pr \\left ((\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i - \\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i) + (\\varepsilon_1 - \\varepsilon_0) > 0 \\right ) & \\\\[5pt]\n= {} & \\Pr((\\boldsymbol\\beta_1 - \\boldsymbol\\beta_0) \\cdot \\mathbf{X}_i + (\\varepsilon_1 - \\varepsilon_0) > 0) & \\\\[5pt]\n= {} & \\Pr((\\boldsymbol\\beta_1 - \\boldsymbol\\beta_0) \\cdot \\mathbf{X}_i + \\varepsilon > 0) & & \\text{(substitute } \\varepsilon\\text{ as above)} \\\\[5pt]\n= {} & \\Pr(\\boldsymbol\\beta \\cdot \\mathbf{X}_i + \\varepsilon > 0) & & \\text{(substitute }\\boldsymbol\\beta\\text{ as above)} \\\\[5pt]\n= {} & \\Pr(\\varepsilon > -\\boldsymbol\\beta \\cdot \\mathbf{X}_i) & & \\text{(now, same as above model)}\\\\[5pt]\n= {} & \\Pr(\\varepsilon < \\boldsymbol\\beta \\cdot \\mathbf{X}_i) & \\\\[5pt]\n= {} & \\operatorname{logit}^{-1}(\\boldsymbol\\beta \\cdot \\mathbf{X}_i) \\\\[5pt]\n= {} & p_i\n\\end{align}",
            "\\begin{align}\n\\ln \\Pr(Y_i=0) &= \\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i - \\ln Z \\\\\n\\ln \\Pr(Y_i=1) &= \\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i - \\ln Z\n\\end{align}",
            "- \\ln Z",
            "\\begin{align}\n\\Pr(Y_i=0) &= \\frac{1}{Z} e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} \\\\[5pt]\n\\Pr(Y_i=1) &= \\frac{1}{Z} e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}\n\\end{align}",
            "Z = e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}",
            "\\begin{align}\n\\Pr(Y_i=0) &= \\frac{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i}}{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}} \\\\[5pt]\n\\Pr(Y_i=1) &= \\frac{e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}.\n\\end{align}",
            "\\Pr(Y_i=c) = \\frac{e^{\\boldsymbol\\beta_c \\cdot \\mathbf{X}_i}}{\\sum_h e^{\\boldsymbol\\beta_h \\cdot \\mathbf{X}_i}}",
            "\\Pr(Y_i=c) = \\operatorname{softmax}(c, \\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i, \\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i, \\dots) .",
            "\\Pr(Y_i=0)",
            "\\Pr(Y_i=1)",
            "\\Pr(Y_i=0) + \\Pr(Y_i=1) = 1",
            "\\begin{align}\n\\Pr(Y_i=1) &= \\frac{e^{(\\boldsymbol\\beta_1 +\\mathbf{C}) \\cdot \\mathbf{X}_i}}{e^{(\\boldsymbol\\beta_0  +\\mathbf{C})\\cdot \\mathbf{X}_i} + e^{(\\boldsymbol\\beta_1 +\\mathbf{C}) \\cdot \\mathbf{X}_i}} \\\\[5pt]\n&= \\frac{e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i} e^{\\mathbf{C} \\cdot \\mathbf{X}_i}}{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} e^{\\mathbf{C} \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i} e^{\\mathbf{C} \\cdot \\mathbf{X}_i}} \\\\[5pt]\n&= \\frac{e^{\\mathbf{C} \\cdot \\mathbf{X}_i}e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}{e^{\\mathbf{C} \\cdot \\mathbf{X}_i}(e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i})} \\\\[5pt]\n&= \\frac{e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}{e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}.\n\\end{align}",
            "\\boldsymbol\\beta_0 = \\mathbf{0} .",
            "e^{\\boldsymbol\\beta_0 \\cdot \\mathbf{X}_i} = e^{\\mathbf{0} \\cdot \\mathbf{X}_i} = 1",
            "\\Pr(Y_i=1) = \\frac{e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}}{1 + e^{\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}} = \\frac{1}{1+e^{-\\boldsymbol\\beta_1 \\cdot \\mathbf{X}_i}} = p_i",
            "\\boldsymbol\\beta = \\boldsymbol\\beta_1 - \\boldsymbol\\beta_0",
            "p_i = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}}. \\,",
            "y = \\frac{1}{1+e^{-f(X)}}",
            "\\frac{\\mathrm{d}y}{\\mathrm{d}X} = y(1-y)\\frac{\\mathrm{d}f}{\\mathrm{d}X}. \\,",
            "Y_i \\,\\sim  \\operatorname{Bin}(n_i,p_i),\\text{ for }i = 1, \\dots , n",
            "p_i = \\operatorname{\\mathbb E}\\left[\\left.\\frac{Y_i}{n_{i}}\\,\\right|\\,\\mathbf{X}_i \\right]\\,,",
            "\\operatorname{logit}\\left(\\operatorname{\\mathbb E}\\left[\\left.\\frac{Y_i}{n_i}\\,\\right|\\,\\mathbf{X}_i \\right]\\right) = \\operatorname{logit}(p_i) = \\ln \\left(\\frac{p_i}{1-p_i}\\right) = \\boldsymbol\\beta \\cdot \\mathbf{X}_i\\,,",
            "\\Pr(Y_i=y\\mid \\mathbf{X}_i) = {n_i \\choose y} p_i^y(1-p_i)^{n_i-y} ={n_i \\choose y} \\left(\\frac{1}{1+e^{-\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}\\right)^y \\left(1-\\frac{1}{1+e^{-\\boldsymbol\\beta \\cdot \\mathbf{X}_i}}\\right)^{n_i-y}\\,.",
            "y=0",
            "y=1",
            "\\mathbf{w}^T=[\\beta_0,\\beta_1,\\beta_2, \\ldots]",
            "\\mathbf{x}(i)=[1, x_1(i), x_2(i), \\ldots]^T",
            "\\mu(i)=\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}(i)}}",
            "\\mathbf{w}",
            "\\mathbf{w}_{k+1} = \\left(\\mathbf{X}^T\\mathbf{S}_k\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\left(\\mathbf{S}_k \\mathbf{X} \\mathbf{w}_k + \\mathbf{y} - \\mathbf{\\boldsymbol\\mu}_k\\right)",
            "\\mathbf{S}=\\operatorname{diag}(\\mu(i)(1-\\mu(i)))",
            "\\boldsymbol\\mu=[\\mu(1), \\mu(2),\\ldots]",
            "\\mathbf{X}=\\begin{bmatrix}\n1 & x_1(1) & x_2(1) & \\ldots\\\\\n1 & x_1(2) & x_2(2) & \\ldots\\\\\n\\vdots & \\vdots & \\vdots   \n\\end{bmatrix}",
            "\\mathbf{y}(i)=[y(1),y(2),\\ldots]^T",
            "\\sigma(x)",
            "k",
            "p",
            "10k/p",
            "y=b_0+b_1 x",
            "\\varepsilon^2=\\sum_{k=1}^K (b_0+b_1 x_k-y_k)^2.",
            "\\hat{\\varepsilon}^2",
            "\\varepsilon^2=\\sum_{k=1}^K (b_0-y_k)^2.",
            "\\varepsilon^2",
            "\\varepsilon_\\varphi^2",
            "\\varphi",
            "b_0=\\overline{y}",
            "\\overline{y}",
            "\\varepsilon_\\varphi^2",
            "\\hat{\\varepsilon}_\\varphi^2=\\sum_{k=1}^K (\\overline{y}-y_k)^2",
            "2-1=1",
            "\\varepsilon^2",
            "\\hat{\\ell}",
            "p(x)=\\frac{1}{1+e^{-t}}",
            "y=1",
            "t=\\beta_0+\\beta_1 x",
            "\\ell=\\sum_{k=1}^K \\left( y_k \\ln(p(x_k))+(1-y_k) \\ln(1-p(x_k))\\right)",
            "y=1",
            "p_\\varphi(x)=\\frac{1}{1+e^{-t_\\varphi}}",
            "t_\\varphi=\\beta_0",
            "\\ell_\\varphi=\\sum_{k=1}^K \\left( y_k \\ln(p_\\varphi)+(1-y_k) \\ln(1-p_\\varphi)\\right)",
            "p_\\varphi=\\overline{y}",
            "\\hat{\\ell}_\\varphi=K(\\,\\overline{y} \\ln(\\overline{y}) + (1-\\overline{y})\\ln(1-\\overline{y}))",
            "\\beta_0",
            "\\beta_0=\\ln\\left(\\frac{\\overline{y}}{1-\\overline{y}}\\right)",
            "\\overline{y}",
            "\\hat{\\ell} \\ge \\hat{\\ell}_\\varphi",
            "D=\\ln\\left(\\frac{\\hat{L}^2}{\\hat{L}_\\varphi^2}\\right) = 2(\\hat{\\ell}-\\hat{\\ell}_\\varphi)",
            "\\hat{\\ell}_\\varphi= -13.8629\\ldots",
            "\\hat{\\ell}=-8.02988\\ldots",
            "D = 2(\\hat{\\ell}-\\hat{\\ell}_\\varphi)=11.6661\\ldots",
            "1-D\\approx 99.94 \\%",
            "D = -2\\ln \\frac{\\text{likelihood of the fitted model}} {\\text{likelihood of the saturated model}}.",
            "\\chi^2_{s-p},",
            "\\begin{align}\n    D_{\\text{null}} &=-2\\ln \\frac{\\text{likelihood of null model}} {\\text{likelihood of the saturated model}}\\\\[6pt]\n   D_{\\text{fitted}} &=-2\\ln \\frac{\\text{likelihood of fitted model}} {\\text{likelihood of the saturated model}}.\n\\end{align}",
            "\\begin{align} \nD_\\text{null}- D_\\text{fitted} &= -2 \\left(\\ln \\frac{\\text{likelihood of null model}} {\\text{likelihood of the saturated model}}-\\ln \\frac{\\text{likelihood of fitted model}} {\\text{likelihood of the saturated model}}\\right)\\\\[6pt]\n&= -2 \\ln \\frac{ \\left( \\dfrac{\\text{likelihood of null model}}{\\text{likelihood of the saturated model}}\\right)}{\\left(\\dfrac{\\text{likelihood of fitted model}}{\\text{likelihood of the saturated model}}\\right)}\\\\[6pt]\n&= -2 \\ln \\frac{\\text{likelihood of the null model}}{\\text{likelihood of fitted model}}.\n\\end{align}",
            "\\chi^2",
            "W_j = \\frac{\\beta^2_j} {SE^2_{\\beta_j}}",
            "\\beta_j",
            "\\beta_0",
            "\\beta_0",
            "\\widehat{\\beta}_0^* = \\widehat{\\beta}_0+\\log \\frac \\pi {1 - \\pi} - \\log{ \\tilde{\\pi} \\over {1 - \\tilde{\\pi}} }",
            "\\pi",
            "\\tilde{\\pi}",
            "x_0=1",
            "k=\\{1,2,\\dots,K\\}",
            "x_{mk}",
            "\\boldsymbol{x}_k = \\{x_{0k},x_{1k},\\dots,x_{Mk}\\}",
            "y=n",
            "p_{nk}=p_n(\\boldsymbol{x}_k)",
            "\\mathcal{L}_{ent}=-\\sum_{k=1}^K\\sum_{n=0}^N p_{nk}\\ln(p_{nk})",
            "\\ell=\\sum_{k=1}^K\\sum_{n=0}^N \\Delta(n,y_k)\\ln(p_{nk})",
            "\\frac{\\partial  \\ell}{\\partial  \\beta_{nm}}=\\sum_{k=1}^K ( p_{nk}x_{mk}-\\Delta(n,y_k)x_{mk})",
            "\\mathcal{L}_{fit}=\\sum_{n=0}^N\\sum_{m=0}^M \\lambda_{nm}\\sum_{k=1}^K (p_{nk}x_{mk}-\\Delta(n,y_k)x_{mk})",
            "\\sum_{n=0}^N p_{nk}=1",
            "\\mathcal{L}_{norm}=\\sum_{k=1}^K \\alpha_k \\left(1-\\sum_{n=1}^N p_{nk}\\right)",
            "\\mathcal{L}=\\mathcal{L}_{ent} + \\mathcal{L}_{fit} + \\mathcal{L}_{norm}",
            "\\frac{\\partial \\mathcal{L}}{\\partial  p_{n'k'}}=0=-\\ln(p_{n'k'})-1+\\sum_{m=0}^M (\\lambda_{n'm}x_{mk'})-\\alpha_{k'}",
            "\\sum_{m=0}^M \\lambda_{nm}x_{mk} = \\boldsymbol{\\lambda}_n\\cdot\\boldsymbol{x}_k",
            "p_{nk}",
            "p_{nk}=e^{\\boldsymbol{\\lambda}_n\\cdot\\boldsymbol{x}_k}/Z_k",
            "Z_k=e^{1+\\alpha_k}",
            "p_{nk}=\\frac{e^{\\boldsymbol{\\lambda}_n\\cdot\\boldsymbol{x}_k}}{\\sum_{u=0}^N e^{\\boldsymbol{\\lambda}_u\\cdot\\boldsymbol{x}_k}}",
            "\\boldsymbol{\\lambda}_n",
            "\\boldsymbol{\\lambda}_n",
            "p_{nk}",
            "\\boldsymbol{\\lambda}_n",
            "\\boldsymbol{\\lambda}_0",
            "\\boldsymbol{\\lambda}_n",
            "\\boldsymbol{\\lambda}_0",
            "\\boldsymbol{\\beta}_n=\\boldsymbol{\\lambda}_n-\\boldsymbol{\\lambda}_0",
            "Y",
            "\\theta",
            "h_\\theta(X) = \\frac{1}{1 + e^{-\\theta^TX}} = \\Pr(Y=1 \\mid X; \\theta)",
            "\\Pr(Y=0 \\mid X; \\theta) = 1 - h_\\theta(X)",
            "Y \\in \\{0,1\\}",
            "\\Pr(y\\mid X;\\theta)",
            "\\Pr(y \\mid X; \\theta) = h_\\theta(X)^y(1 - h_\\theta(X))^{(1-y)}.",
            "\\begin{align}\nL(\\theta \\mid y; x) &= \\Pr(Y \\mid X; \\theta) \\\\\n  &= \\prod_i \\Pr(y_i \\mid x_i; \\theta) \\\\\n  &= \\prod_i h_\\theta(x_i)^{y_i}(1 - h_\\theta(x_i))^{(1-y_i)}\n\\end{align}",
            "N^{-1} \\log L(\\theta \\mid y; x) = N^{-1} \\sum_{i=1}^N \\log \\Pr(y_i \\mid x_i; \\theta)",
            "(x, y)",
            "\\begin{align}\n& \\lim \\limits_{N \\rightarrow +\\infty} N^{-1} \\sum_{i=1}^N \\log \\Pr(y_i \\mid x_i; \\theta) \n= \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} \\Pr(X=x, Y=y) \\log \\Pr(Y=y \\mid X=x; \\theta) \\\\[6pt]\n= {} & \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} \\Pr(X=x, Y=y) \\left( - \\log\\frac{\\Pr(Y=y \\mid X=x)}{\\Pr(Y=y \\mid X=x; \\theta)} + \\log \\Pr(Y=y \\mid X=x) \\right) \\\\[6pt]\n= {} &  - D_\\text{KL}( Y \\parallel Y_\\theta ) - H(Y \\mid X) \n\\end{align}",
            "H(Y\\mid X)",
            "D_\\text{KL}",
            "y \\mid x"
        ]
    },
    "Support_vector_machine": {
        "title": "Support_vector_machine",
        "chunks": [
            {
                "text": "In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories, SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). In addition to performing linear classification, SVMs can efficiently perform non-linear classification using the kernel trick, representing the data only through a set of pairwise similarity comparisons between the original data points using a kernel function, which transforms them into coordinates in a higher-dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces, where linear classification can be performed. Being max-margin models, SVMs are resilient to noisy data (e.g., misclassified examples). SVMs can also be used for regression tasks, where the objective becomes $\\epsilon$-sensitive. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
            },
            {
                "text": "These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data into groups, and then to map new data according to these clusters. The popularity of SVMs is likely due to their amenability to theoretical analysis, and their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression. Motivation H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximal margin. Classifying data is a common task in machine learning. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a $p$-dimensional vector (a list of $p$ numbers), and we want to know whether we can separate such points with a $(p-1)$-dimensional hyperplane."
            },
            {
                "text": "This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability. More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier. A lower generalization error means that the implementer is less likely to experience overfitting. thumb|Kernel machine Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space."
            },
            {
                "text": "For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function $k(x, y)$ selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters $\\alpha_i$ of images of feature vectors $x_i$ that occur in the data base. With this choice of a hyperplane, the points $x$ in the feature space that are mapped into the hyperplane are defined by the relation $\\textstyle\\sum_i \\alpha_i k(x_i, x) = \\text{constant}.$ Note that if $k(x, y)$ becomes small as $y$ grows further away from $x$, each term in the sum measures the degree of closeness of the test point $x$ to the corresponding data base point $x_i$."
            },
            {
                "text": "In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points $x$ mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space. Applications SVMs can be used to solve various real-world problems: SVMs are helpful in text and hypertext categorization, as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings. Some methods for shallow semantic parsing are based on support vector machines. Classification of images can also be performed using SVMs. Experimental results show that SVMs achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback. This is also true for image segmentation systems, including those using a modified version SVM that uses the privileged approach as suggested by Vapnik.Vapnik, Vladimir N.: Invited Speaker."
            },
            {
                "text": "IPMU Information Processing and Management 2014). Classification of satellite data like SAR data using supervised SVM. Hand-written characters can be recognized using SVM. The SVM algorithm has been widely applied in the biological and other sciences. They have been used to classify proteins with up to 90% of the compounds classified correctly. Permutation tests based on SVM weights have been suggested as a mechanism for interpretation of SVM models. Support vector machine weights have also been used to interpret SVM models in the past.Statnikov, Alexander; Hardin, Douglas; & Aliferis, Constantin; (2006); \"Using SVM weight-based methods to identify causally relevant and non-causally relevant variables\", Sign, 1, 4. Posthoc interpretation of support vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences. History The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1964. In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes."
            },
            {
                "text": "The \"soft margin\" incarnation, as is commonly used in software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995. Linear SVM alt=|300x300px We are given a training dataset of $n$ points of the form where the $y_i$ are either 1 or −1, each indicating the class to which the point $\\mathbf{x}_i $ belongs. Each $\\mathbf{x}_i $ is a $p$-dimensional real vector. We want to find the \"maximum-margin hyperplane\" that divides the group of points $\\mathbf{x}_i$ for which $y_i = 1$ from the group of points for which $y_i = -1$, which is defined so that the distance between the hyperplane and the nearest point $\\mathbf{x}_i $ from either group is maximized. Any hyperplane can be written as the set of points $\\mathbf{x}$ satisfying where $\\mathbf{w}$ is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that $\\mathbf{w}$ is not necessarily a unit vector."
            },
            {
                "text": "The parameter $\\tfrac{b}{\\|\\mathbf{w}\\|}$ determines the offset of the hyperplane from the origin along the normal vector $\\mathbf{w}$. Warning: most of the literature on the subject defines the bias so that Hard-margin If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the \"margin\", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations $\\mathbf{w}^\\mathsf{T} \\mathbf{x} - b = 1$ (anything on or above this boundary is of one class, with label 1) and $\\mathbf{w}^\\mathsf{T} \\mathbf{x} - b = -1$ (anything on or below this boundary is of the other class, with label −1). Geometrically, the distance between these two hyperplanes is $\\tfrac{2}{\\|\\mathbf{w}\\|}$, so to maximize the distance between the planes we want to minimize $\\|\\mathbf{w}\\|$."
            },
            {
                "text": "The distance is computed using the distance from a point to a plane equation. We also have to prevent data points from falling into the margin, we add the following constraint: for each $i$ either or These constraints state that each data point must lie on the correct side of the margin. This can be rewritten as We can put this together to get the optimization problem: $\\begin{align} &\\underset{\\mathbf{w},\\;b}{\\operatorname{minimize}} && \\frac{1}{2}\\|\\mathbf{w}\\|^2\\\\ &\\text{subject to} && y_i(\\mathbf{w}^\\top \\mathbf{x}_i - b) \\geq 1 \\quad \\forall i \\in \\{1,\\dots,n\\} \\end{align}$ The $\\mathbf{w}$ and $b$ that solve this problem determine the final classifier, $\\mathbf{x} \\mapsto \\sgn(\\mathbf{w}^\\mathsf{T} \\mathbf{x} - b)$, where $\\sgn(\\cdot)$ is the sign function."
            },
            {
                "text": "An important consequence of this geometric description is that the max-margin hyperplane is completely determined by those $\\mathbf{x}_i$ that lie nearest to it (explained below). These $\\mathbf{x}_i$ are called support vectors. Soft-margin To extend SVM to cases in which the data are not linearly separable, the hinge loss function is helpful Note that $y_i$ is the i-th target (i.e., in this case, 1 or −1), and $\\mathbf{w}^\\mathsf{T} \\mathbf{x}_i - b$ is the i-th output. This function is zero if the constraint in is satisfied, in other words, if $\\mathbf{x}_i$ lies on the correct side of the margin. For data on the wrong side of the margin, the function's value is proportional to the distance from the margin. The goal of the optimization then is to minimize: where the parameter $C > 0$ determines the trade-off between increasing the margin size and ensuring that the $\\mathbf{x}_i$ lie on the correct side of the margin (Note we can add a weight to either term in the equation above)."
            },
            {
                "text": "By deconstructing the hinge loss, this optimization problem can be formulated into the following: Thus, for large values of $C$, it will behave similar to the hard-margin SVM, if the input data are linearly classifiable, but will still learn if a classification rule is viable or not. Nonlinear kernels thumb|Kernel machine The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.) to maximum-margin hyperplanes. The kernel trick, where dot products are replaced by kernels, is easily derived in the dual representation of the SVM problem. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high-dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space. It is noteworthy that working in a higher-dimensional feature space increases the generalization error of support vector machines, although given enough samples the algorithm still performs well."
            },
            {
                "text": "Some common kernels include: Polynomial (homogeneous): $k(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j)^d$. Particularly, when $d = 1$, this becomes the linear kernel. Polynomial (inhomogeneous): $k(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d$. Gaussian radial basis function: $k(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma \\left\\|\\mathbf{x}_i - \\mathbf{x}_j\\right\\|^2\\right)$ for $\\gamma > 0$. Sometimes parametrized using $\\gamma = 1/(2\\sigma^2)$. Sigmoid function (Hyperbolic tangent): $k(\\mathbf{x_i}, \\mathbf{x_j}) = \\tanh(\\kappa \\mathbf{x}_i \\cdot \\mathbf{x}_j + c)$ for some (not every) $\\kappa > 0 $ and $c < 0$."
            },
            {
                "text": "The kernel is related to the transform $\\varphi(\\mathbf{x}_i)$ by the equation $k(\\mathbf{x}_i, \\mathbf{x}_j) = \\varphi(\\mathbf{x}_i) \\cdot \\varphi(\\mathbf{x}_j)$. The value $w$ is also in the transformed space, with . Dot products with $w$ for classification can again be computed by the kernel trick, i.e. . Computing the SVM classifier Computing the (soft-margin) SVM classifier amounts to minimizing an expression of the form We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for $\\lambda$ yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing to a quadratic programming problem, is detailed below. Then, more recent approaches such as sub-gradient descent and coordinate descent will be discussed. Primal Minimizing can be rewritten as a constrained optimization problem with a differentiable objective function in the following way. For each $i \\in \\{1,\\,\\ldots,\\,n\\}$ we introduce a variable $ \\zeta_i = \\max\\left(0, 1 - y_i(\\mathbf{w}^\\mathsf{T} \\mathbf{x}_i - b)\\right)$."
            },
            {
                "text": "Note that $ \\zeta_i$ is the smallest nonnegative number satisfying $ y_i(\\mathbf{w}^\\mathsf{T} \\mathbf{x}_i - b) \\geq 1 - \\zeta_i.$ Thus we can rewrite the optimization problem as follows This is called the primal problem. Dual By solving for the Lagrangian dual of the above problem, one obtains the simplified problem This is called the dual problem. Since the dual maximization problem is a quadratic function of the $ c_i$ subject to linear constraints, it is efficiently solvable by quadratic programming algorithms. Here, the variables $ c_i$ are defined such that Moreover, $ c_i = 0$ exactly when $ \\mathbf{x}_i$ lies on the correct side of the margin, and $ 0 < c_i <(2n\\lambda)^{-1}$ when $ \\mathbf{x}_i$ lies on the margin's boundary. It follows that $\\mathbf{w}$ can be written as a linear combination of the support vectors. The offset, $ b$, can be recovered by finding an $ \\mathbf{x}_i$ on the margin's boundary and solving (Note that $y_i^{-1}=y_i$ since $y_i=\\pm 1$.)"
            },
            {
                "text": "Kernel trick thumbnail|right|A training example of SVM with kernel given by φ((a, b)) = (a, b, a2 + b2) Suppose now that we would like to learn a nonlinear classification rule which corresponds to a linear classification rule for the transformed data points $ \\varphi(\\mathbf{x}_i).$ Moreover, we are given a kernel function $ k$ which satisfies $ k(\\mathbf{x}_i, \\mathbf{x}_j) = \\varphi(\\mathbf{x}_i) \\cdot \\varphi(\\mathbf{x}_j)$. We know the classification vector $\\mathbf{w}$ in the transformed space satisfies where, the $c_i$ are obtained by solving the optimization problem The coefficients $ c_i$ can be solved for using quadratic programming, as before. Again, we can find some index $ i$ such that $ 0 < c_i <(2n\\lambda)^{-1}$, so that $ \\varphi(\\mathbf{x}_i)$ lies on the boundary of the margin in the transformed space, and then solve Finally, Modern methods Recent algorithms for finding the SVM classifier include sub-gradient descent and coordinate descent."
            },
            {
                "text": "Both techniques have proven to offer significant advantages over the traditional approach when dealing with large, sparse datasets—sub-gradient methods are especially efficient when there are many training examples, and coordinate descent when the dimension of the feature space is high. Sub-gradient descent Sub-gradient descent algorithms for the SVM work directly with the expression Note that $f$ is a convex function of $\\mathbf{w}$ and $b$. As such, traditional gradient descent (or SGD) methods can be adapted, where instead of taking a step in the direction of the function's gradient, a step is taken in the direction of a vector selected from the function's sub-gradient. This approach has the advantage that, for certain implementations, the number of iterations does not scale with $n$, the number of data points. Coordinate descent Coordinate descent algorithms for the SVM work from the dual problem For each $ i \\in \\{1,\\, \\ldots,\\, n\\}$, iteratively, the coefficient $ c_i$ is adjusted in the direction of $ \\partial f/ \\partial c_i$."
            },
            {
                "text": "Then, the resulting vector of coefficients $ (c_1',\\,\\ldots,\\,c_n')$ is projected onto the nearest vector of coefficients that satisfies the given constraints. (Typically Euclidean distances are used.) The process is then repeated until a near-optimal vector of coefficients is obtained. The resulting algorithm is extremely fast in practice, although few performance guarantees have been proven. Empirical risk minimization The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties. Risk minimization In supervised learning, one is given a set of training examples $X_1 \\ldots X_n$ with labels $y_1 \\ldots y_n$, and wishes to predict $y_{n+1}$ given $X_{n+1}$."
            },
            {
                "text": "To do so one forms a hypothesis, $f$, such that $f(X_{n+1})$ is a \"good\" approximation of $y_{n+1}$. A \"good\" approximation is usually defined with the help of a loss function, $\\ell(y,z)$, which characterizes how bad $z$ is as a prediction of $y$. We would then like to choose a hypothesis that minimizes the expected risk: In most cases, we don't know the joint distribution of $X_{n+1},\\,y_{n+1}$ outright. In these cases, a common strategy is to choose the hypothesis that minimizes the empirical risk: Under certain assumptions about the sequence of random variables $X_k,\\, y_k$ (for example, that they are generated by a finite Markov process), if the set of hypotheses being considered is small enough, the minimizer of the empirical risk will closely approximate the minimizer of the expected risk as $n$ grows large."
            },
            {
                "text": "This approach is called empirical risk minimization, or ERM. Regularization and stability In order for the minimization problem to have a well-defined solution, we have to place constraints on the set $\\mathcal{H}$ of hypotheses being considered. If $\\mathcal{H}$ is a normed space (as is the case for SVM), a particularly effective technique is to consider only those hypotheses $ f$ for which $\\lVert f \\rVert_{\\mathcal H} < k$ . This is equivalent to imposing a regularization penalty $\\mathcal R(f) = \\lambda_k\\lVert f \\rVert_{\\mathcal H}$, and solving the new optimization problem This approach is called Tikhonov regularization. More generally, $\\mathcal{R}(f)$ can be some measure of the complexity of the hypothesis $f$, so that simpler hypotheses are preferred. SVM and the hinge loss Recall that the (soft-margin) SVM classifier $\\hat\\mathbf{w}, b: \\mathbf{x} \\mapsto \\sgn(\\hat\\mathbf{w}^\\mathsf{T} \\mathbf{x} - b)$ is chosen to minimize the following expression: In light of the above discussion, we see that the SVM technique is equivalent to empirical risk minimization with Tikhonov regularization, where in this case the loss function is the hinge loss From this perspective, SVM is closely related to other fundamental classification algorithms such as regularized least-squares and logistic regression."
            },
            {
                "text": "The difference between the three lies in the choice of loss function: regularized least-squares amounts to empirical risk minimization with the square-loss, $\\ell_{sq}(y,z) = (y-z)^2$; logistic regression employs the log-loss, Target functions The difference between the hinge loss and these other loss functions is best stated in terms of target functions - the function that minimizes expected risk for a given pair of random variables $X,\\,y$. In particular, let $y_x$ denote $y$ conditional on the event that $X = x$. In the classification setting, we have: The optimal classifier is therefore: For the square-loss, the target function is the conditional expectation function, $f_{sq}(x) = \\mathbb{E}\\left[y_x\\right]$; For the logistic loss, it's the logit function, $f_{\\log}(x) = \\ln\\left(p_x / ({1-p_x})\\right)$. While both of these target functions yield the correct classifier, as $\\sgn(f_{sq}) = \\sgn(f_\\log) = f^*$, they give us more information than we need."
            },
            {
                "text": "In fact, they give us enough information to completely describe the distribution of $ y_x$. On the other hand, one can check that the target function for the hinge loss is exactly $f^*$. Thus, in a sufficiently rich hypothesis space—or equivalently, for an appropriately chosen kernel—the SVM classifier will converge to the simplest function (in terms of $\\mathcal{R}$) that correctly classifies the data. This extends the geometric interpretation of SVM—for linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier. Properties SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron.R. Collobert and S. Bengio (2004). Links between Perceptrons, MLPs and SVMs. Proc. Int'l Conf. on Machine Learning (ICML). They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers."
            },
            {
                "text": "A comparison of the SVM to other classifiers has been made by Meyer, Leisch and Hornik. Parameter selection The effectiveness of SVM depends on the selection of kernel, the kernel's parameters, and soft margin parameter $\\lambda$. A common choice is a Gaussian kernel, which has a single parameter $\\gamma$. The best combination of $\\lambda$ and $\\gamma$ is often selected by a grid search with exponentially growing sequences of $\\lambda$ and $\\gamma$, for example, $\\lambda \\in \\{ 2^{-5}, 2^{-3}, \\dots, 2^{13},2^{15} \\}$; $\\gamma \\in \\{ 2^{-15},2^{-13}, \\dots, 2^{1},2^{3} \\}$. Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in Bayesian optimization can be used to select $\\lambda$ and $\\gamma$ , often requiring the evaluation of far fewer parameter combinations than grid search."
            },
            {
                "text": "The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters. Issues Potential drawbacks of the SVM include the following aspects: Requires full labeling of input data Uncalibrated class membership probabilities—SVM stems from Vapnik's theory which avoids estimating probabilities on finite data The SVM is only directly applicable for two-class tasks. Therefore, algorithms that reduce the multi-class task to several binary problems have to be applied; see the multi-class SVM section. Parameters of a solved model are difficult to interpret. Extensions Multiclass SVM Multiclass SVM aims to assign labels to instances by using support vector machines, where the labels are drawn from a finite set of several elements. The dominant approach for doing so is to reduce the single multiclass problem into multiple binary classification problems. Common methods for such reduction include: Building binary classifiers that distinguish between one of the labels and the rest (one-versus-all) or between every pair of classes (one-versus-one). Classification of new instances for the one-versus-all case is done by a winner-takes-all strategy, in which the classifier with the highest-output function assigns the class (it is important that the output functions be calibrated to produce comparable scores)."
            },
            {
                "text": "For the one-versus-one approach, classification is done by a max-wins voting strategy, in which every classifier assigns the instance to one of the two classes, then the vote for the assigned class is increased by one vote, and finally the class with the most votes determines the instance classification. Directed acyclic graph SVM (DAGSVM) Error-correcting output codes Crammer and Singer proposed a multiclass SVM method which casts the multiclass classification problem into a single optimization problem, rather than decomposing it into multiple binary classification problems. See also Lee, Lin and Wahba and Van den Burg and Groenen. Transductive support vector machines Transductive support vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. Here, in addition to the training set $\\mathcal{D}$, the learner is also given a set of test examples to be classified. Formally, a transductive support vector machine is defined by the following primal optimization problem: Minimize (in $\\mathbf{w}, b, \\mathbf{y}^\\star$) subject to (for any $i = 1, \\dots, n$ and any $j = 1, \\dots, k$) and Transductive support vector machines were introduced by Vladimir N. Vapnik in 1998."
            },
            {
                "text": "Structured SVM Structured support-vector machine is an extension of the traditional SVM model. While the SVM model is primarily designed for binary classification, multiclass classification, and regression tasks, structured SVM broadens its application to handle general structured output labels, for example parse trees, classification with taxonomies, sequence alignment and many more. Regression thumbnail|right|Support vector regression (prediction) with different thresholds ε. As ε increases, the prediction becomes less sensitive to errors. A version of SVM for regression was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola.Drucker, Harris; Burges, Christ. C.; Kaufman, Linda; Smola, Alexander J.; and Vapnik, Vladimir N. (1997); \"Support Vector Regression Machines\", in Advances in Neural Information Processing Systems 9, NIPS 1996, 155–161, MIT Press. This method is called support vector regression (SVR). The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin."
            },
            {
                "text": "Analogously, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction. Another SVM version known as least-squares support vector machine (LS-SVM) has been proposed by Suykens and Vandewalle.Suykens, Johan A. K.; Vandewalle, Joos P. L.; \"Least squares support vector machine classifiers\", Neural Processing Letters, vol. 9, no. 3, Jun. 1999, pp. 293–300. Training the original SVR means solving minimize $\\tfrac{1}{2} \\|w\\|^2 $ subject to $ | y_i - \\langle w, x_i \\rangle - b | \\le \\varepsilon $ where $x_i$ is a training sample with target value $y_i$. The inner product plus intercept $\\langle w, x_i \\rangle + b$ is the prediction for that sample, and $\\varepsilon$ is a free parameter that serves as a threshold: all predictions have to be within an $\\varepsilon$ range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible."
            },
            {
                "text": "Bayesian SVM In 2011 it was shown by Polson and Scott that the SVM admits a Bayesian interpretation through the technique of data augmentation. In this approach the SVM is viewed as a graphical model (where the parameters are connected via probability distributions). This extended view allows the application of Bayesian techniques to SVMs, such as flexible feature modeling, automatic hyperparameter tuning, and predictive uncertainty quantification. Recently, a scalable version of the Bayesian SVM was developed by Florian Wenzel, enabling the application of Bayesian SVMs to big data. Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM.Florian Wenzel; Matthäus Deutsch; Théo Galy-Fajou; Marius Kloft; ”Scalable Approximate Inference for the Bayesian Nonlinear Support Vector Machine” Implementation The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks."
            },
            {
                "text": "Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems. Instead of solving a sequence of broken-down problems, this approach directly solves the problem altogether. To avoid solving a linear system involving the large kernel matrix, a low-rank approximation to the matrix is often used in the kernel trick. Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems. The special case of linear support vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS) and coordinate descent (e.g., LIBLINEAR). LIBLINEAR has some attractive training-time properties. Each convergence iteration takes time linear in the time taken to read the train data, and the iterations also have a Q-linear convergence property, making the algorithm extremely fast."
            },
            {
                "text": "The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM), especially when parallelization is allowed. Kernel SVMs are available in many machine-learning toolkits, including LIBSVM, MATLAB, SAS, SVMlight, kernlab, scikit-learn, Shogun, Weka, Shark, JKernelMachines, OpenCV and others. Preprocessing of data (standardization) is highly recommended to enhance accuracy of classification. There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score. Subtraction of mean and division by variance of each feature is usually used for SVM. See also In situ adaptive tabulation Kernel machines Fisher kernel Platt scaling Polynomial kernel Predictive analytics Regularization perspectives on support vector machines Relevance vector machine, a probabilistic sparse-kernel model identical in functional form to SVM Sequential minimal optimization Space mapping Winnow (algorithm) References Further reading External links libsvm, LIBSVM is a popular library of SVM learners liblinear is a library for large linear classification including some SVMs SVM light is a collection of software tools for learning and classification using SVM SVMJS live demo is a GUI demo for JavaScript implementation of SVMs Category:Classification algorithms Category:Statistical classification"
            }
        ],
        "latex_formulas": [
            "'''w'''",
            "'''w'''",
            "\\epsilon",
            "p",
            "p",
            "(p-1)",
            "k(x, y)",
            "\\alpha_i",
            "x_i",
            "x",
            "\\textstyle\\sum_i \\alpha_i k(x_i, x) = \\text{constant}.",
            "k(x, y)",
            "y",
            "x",
            "x",
            "x_i",
            "x",
            "n",
            "y_i",
            "\\mathbf{x}_i",
            "\\mathbf{x}_i",
            "p",
            "\\mathbf{x}_i",
            "y_i = 1",
            "y_i = -1",
            "\\mathbf{x}_i",
            "\\mathbf{x}",
            "\\mathbf{w}",
            "\\mathbf{w}",
            "\\tfrac{b}{\\|\\mathbf{w}\\|}",
            "\\mathbf{w}",
            "\\mathbf{w}^\\mathsf{T} \\mathbf{x} - b = 1",
            "\\mathbf{w}^\\mathsf{T} \\mathbf{x} - b = -1",
            "\\tfrac{2}{\\|\\mathbf{w}\\|}",
            "\\frac{2}{\\|\\mathbf{w}\\|}",
            "\\|\\mathbf{w}\\|",
            "i",
            "\\begin{align}\n&\\underset{\\mathbf{w},\\;b}{\\operatorname{minimize}} && \\frac{1}{2}\\|\\mathbf{w}\\|^2\\\\\n&\\text{subject to} && y_i(\\mathbf{w}^\\top \\mathbf{x}_i - b) \\geq 1 \\quad \\forall i \\in \\{1,\\dots,n\\}\n\\end{align}",
            "\\mathbf{w}",
            "b",
            "\\mathbf{x} \\mapsto \\sgn(\\mathbf{w}^\\mathsf{T} \\mathbf{x} - b)",
            "\\sgn(\\cdot)",
            "\\mathbf{x}_i",
            "\\mathbf{x}_i",
            "y_i",
            "\\mathbf{w}^\\mathsf{T} \\mathbf{x}_i - b",
            "\\mathbf{x}_i",
            "C > 0",
            "\\mathbf{x}_i",
            "C",
            "k(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j)^d",
            "d = 1",
            "k(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d",
            "k(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\gamma \\left\\|\\mathbf{x}_i - \\mathbf{x}_j\\right\\|^2\\right)",
            "\\gamma > 0",
            "\\gamma = 1/(2\\sigma^2)",
            "k(\\mathbf{x_i}, \\mathbf{x_j}) = \\tanh(\\kappa \\mathbf{x}_i \\cdot \\mathbf{x}_j + c)",
            "\\kappa > 0",
            "c < 0",
            "\\varphi(\\mathbf{x}_i)",
            "k(\\mathbf{x}_i, \\mathbf{x}_j) = \\varphi(\\mathbf{x}_i) \\cdot \\varphi(\\mathbf{x}_j)",
            "\\lambda",
            "i \\in \\{1,\\,\\ldots,\\,n\\}",
            "\\zeta_i = \\max\\left(0, 1 - y_i(\\mathbf{w}^\\mathsf{T} \\mathbf{x}_i - b)\\right)",
            "\\zeta_i",
            "y_i(\\mathbf{w}^\\mathsf{T} \\mathbf{x}_i - b) \\geq 1 - \\zeta_i.",
            "c_i",
            "c_i",
            "c_i = 0",
            "\\mathbf{x}_i",
            "0 < c_i <(2n\\lambda)^{-1}",
            "\\mathbf{x}_i",
            "\\mathbf{w}",
            "b",
            "\\mathbf{x}_i",
            "y_i^{-1}=y_i",
            "y_i=\\pm 1",
            "\\varphi(\\mathbf{x}_i).",
            "k",
            "k(\\mathbf{x}_i, \\mathbf{x}_j) = \\varphi(\\mathbf{x}_i) \\cdot \\varphi(\\mathbf{x}_j)",
            "\\mathbf{w}",
            "c_i",
            "c_i",
            "i",
            "0 < c_i <(2n\\lambda)^{-1}",
            "\\varphi(\\mathbf{x}_i)",
            "f",
            "\\mathbf{w}",
            "b",
            "n",
            "i \\in \\{1,\\, \\ldots,\\, n\\}",
            "c_i",
            "\\partial f/ \\partial c_i",
            "(c_1',\\,\\ldots,\\,c_n')",
            "X_1 \\ldots X_n",
            "y_1 \\ldots y_n",
            "y_{n+1}",
            "X_{n+1}",
            "f",
            "f(X_{n+1})",
            "y_{n+1}",
            "\\ell(y,z)",
            "z",
            "y",
            "X_{n+1},\\,y_{n+1}",
            "X_k,\\, y_k",
            "n",
            "\\mathcal{H}",
            "\\mathcal{H}",
            "f",
            "\\lVert f \\rVert_{\\mathcal H} < k",
            "\\mathcal R(f) = \\lambda_k\\lVert f \\rVert_{\\mathcal H}",
            "\\mathcal{R}(f)",
            "f",
            "\\hat\\mathbf{w}, b: \\mathbf{x} \\mapsto \\sgn(\\hat\\mathbf{w}^\\mathsf{T} \\mathbf{x} - b)",
            "\\ell_{sq}(y,z) = (y-z)^2",
            "X,\\,y",
            "y_x",
            "y",
            "X = x",
            "f_{sq}(x) = \\mathbb{E}\\left[y_x\\right]",
            "f_{\\log}(x) = \\ln\\left(p_x / ({1-p_x})\\right)",
            "\\sgn(f_{sq}) = \\sgn(f_\\log) = f^*",
            "y_x",
            "f^*",
            "\\mathcal{R}",
            "\\lambda",
            "\\gamma",
            "\\lambda",
            "\\gamma",
            "\\lambda",
            "\\gamma",
            "\\lambda \\in \\{ 2^{-5}, 2^{-3}, \\dots, 2^{13},2^{15} \\}",
            "\\gamma \\in \\{ 2^{-15},2^{-13}, \\dots, 2^{1},2^{3} \\}",
            "\\lambda",
            "\\gamma",
            "\\mathcal{D}",
            "\\mathbf{w}, b, \\mathbf{y}^\\star",
            "i = 1, \\dots, n",
            "j = 1, \\dots, k",
            "\\tfrac{1}{2} \\|w\\|^2",
            "| y_i - \\langle w, x_i \\rangle  - b | \\le \\varepsilon",
            "x_i",
            "y_i",
            "\\langle w, x_i \\rangle + b",
            "\\varepsilon",
            "\\varepsilon"
        ]
    },
    "K-nearest_neighbors_algorithm": {
        "title": "K-nearest_neighbors_algorithm",
        "chunks": [
            {
                "text": "In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method. It was first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. Most often, it is used for classification, as a k-NN classifier, the output of which is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. The k-NN algorithm can also be generalized for regression. In -NN regression, also known as nearest neighbor smoothing, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k = 1, then the output is simply assigned to the value of that single nearest neighbor, also known as nearest neighbor interpolation. For both classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that nearer neighbors contribute more to the average than distant ones."
            },
            {
                "text": "For example, a common weighting scheme consists of giving each neighbor a weight of 1/d, where d is the distance to the neighbor.This scheme is a generalization of linear interpolation. The input consists of the k closest training examples in a data set. The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required. A peculiarity (sometimes even a disadvantage) of the k-NN algorithm is its sensitivity to the local structure of the data. In k-NN classification the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance, if the features represent different physical units or come in vastly different scales, then feature-wise normalizing of the training data can greatly improve its accuracy. Statistical setting Suppose we have pairs $(X_1,Y_1), (X_2,Y_2), \\dots, (X_n, Y_n) $ taking values in $\\mathbb{R}^d \\times \\{1,2\\}$, where is the class label of , so that $X|Y=r \\sim P_r$ for $r = 1,2$ (and probability distributions $P_r$)."
            },
            {
                "text": "Given some norm $\\|\\cdot\\|$ on $\\mathbb{R}^d$ and a point $x \\in \\mathbb{R}^d$, let $(X_{(1)},Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)}) $ be a reordering of the training data such that $ \\|X_{(1)}-x\\| \\leq \\dots \\leq \\|X_{(n)}-x\\| $. Algorithm Example of k-NN classification. The test sample (green dot) should be classified either to blue squares or to red triangles. If k = 3 (solid line circle) it is assigned to the red triangles because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the blue squares (3 squares vs. 2 triangles inside the outer circle). The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples."
            },
            {
                "text": "In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point. Application of a k-NN classifier considering k = 3 neighbors. Left - Given the test point \"? \", the algorithm seeks the 3 closest points in the training set, and adopts the majority vote to classify it as \"class red\". Right - By iteratively repeating the prediction over the whole feature space (X1, X2), one can depict the \"decision surface\". A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance). In the context of gene expression microarray data, for example, k-NN has been employed with correlation coefficients, such as Pearson and Spearman, as a metric. Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis."
            },
            {
                "text": "thumb|An animated visualization of K-means clustering with k = 3, grouping countries based on life expectancy, GDP, and happiness—demonstrating how KNN operates in higher dimensions. Click to view the animation.Helliwell, J. F., Layard, R., Sachs, J. D., Aknin, L. B., De Neve, J.-E., & Wang, S. (Eds.). (2023). World Happiness Report 2023 (11th ed.). Sustainable Development Solutions Network. A drawback of the basic \"majority voting\" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number. One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point."
            },
            {
                "text": "Another way to overcome skew is by abstraction in data representation. For example, in a self-organizing map (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. K-NN can then be applied to the SOM. Parameter selection The best choice of k depends upon the data; generally, larger values of k reduces effect of the noise on the classification,Everitt, Brian S.; Landau, Sabine; Leese, Morven; and Stahl, Daniel (2011) \"Miscellaneous Clustering Methods\", in Cluster Analysis, 5th Edition, John Wiley & Sons, Ltd., Chichester, UK but make boundaries between classes less distinct. A good k can be selected by various heuristic techniques (see hyperparameter optimization). The special case where the class is predicted to be the class of the closest training sample (i.e. when k = 1) is called the nearest neighbor algorithm. The accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance."
            },
            {
                "text": "Much research effort has been put into selecting or scaling features to improve classification. A particularly popular approach is the use of evolutionary algorithms to optimize feature scaling. Another popular approach is to scale features by the mutual information of the training data with the training classes. In binary (two class) classification problems, it is helpful to choose k to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal k in this setting is via bootstrap method. The 1-nearest neighbor classifier The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point to the class of its closest neighbour in the feature space, that is $C_n^{1nn}(x) = Y_{(1)}$. As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data). The weighted nearest neighbour classifier The -nearest neighbour classifier can be viewed as assigning the nearest neighbours a weight $1/k$ and all others weight."
            },
            {
                "text": "This can be generalised to weighted nearest neighbour classifiers. That is, where the th nearest neighbour is assigned a weight $w_{ni}$, with . An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds. Let $C^{wnn}_n$ denote the weighted nearest classifier with weights $\\{w_{ni}\\}_{i=1}^n$. Subject to regularity conditions, which in asymptotic theory are conditional variables which require assumptions to differentiate among parameters with some criteria. On the class distributions the excess risk has the following asymptotic expansion for constants $B_1$ and $B_2$ where $s_n^2 = \\sum_{i=1}^n w_{ni}^2$ and $t_n = n^{-2/d}\\sum_{i=1}^n w_{ni} \\left\\{i^{1+2/d} - (i-1)^{1+2/d}\\right\\}$. The optimal weighting scheme $\\{w_{ni}^*\\}_{i=1}^n$, that balances the two terms in the display above, is given as follows: set $k^* = \\lfloor B n^{\\frac 4 {d+4}} \\rfloor$, for $i=1,2,\\dots,k^*$ and for $ i = k^*+1,\\dots,n$."
            },
            {
                "text": "With optimal weights the dominant term in the asymptotic expansion of the excess risk is $\\mathcal{O}(n^{-\\frac 4 {d+4}})$. Similar results are true when using a bagged nearest neighbour classifier. Properties k-NN is a special case of a variable-bandwidth, kernel density \"balloon\" estimator with a uniform kernel. The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate nearest neighbor search algorithm makes k-NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed. k-NN has some strong consistency results. As the amount of data approaches infinity, the two-class k-NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data). Various improvements to the k-NN speed are possible by using proximity graphs."
            },
            {
                "text": "For multi-class k-NN classification, Cover and Hart (1967) prove an upper bound error rate of where $R^*$ is the Bayes error rate (which is the minimal error rate possible), $R_{kNN}$ is the asymptotic k-NN error rate, and is the number of classes in the problem. This bound is tight in the sense that both the lower and upper bounds are achievable by some distribution.Devroye, L., Gyorfi, L. & Lugosi, G. A Probabilistic Theory of Pattern Recognition. Discrete Appl Math 73, 192–194 (1997). For $M=2$ and as the Bayesian error rate $R^*$ approaches zero, this limit reduces to \"not more than twice the Bayesian error rate\". Error rates There are many results on the error rate of the nearest neighbour classifiers. The -nearest neighbour classifier is strongly (that is for any joint distribution on $ (X, Y)$) consistent provided $k:=k_n$ diverges and $k_n/n$ converges to zero as $n \\to \\infty$. Let $ C_n^{knn} $ denote the nearest neighbour classifier based on a training set of size ."
            },
            {
                "text": "Under certain regularity conditions, the excess risk yields the following asymptotic expansion for some constants $ B_1 $ and $ B_2 $. The choice $k^* = \\left\\lfloor B n^{\\frac 4 {d+4}} \\right\\rfloor$ offers a trade off between the two terms in the above display, for which the $k^*$-nearest neighbour error converges to the Bayes error at the optimal (minimax) rate $\\mathcal{O}\\left(n^{-\\frac 4 {d+4}}\\right)$. Metric learning The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are neighbourhood components analysis and large margin nearest neighbor. Supervised metric learning algorithms use the label information to learn a new metric or pseudo-metric. Feature extraction When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called feature extraction."
            },
            {
                "text": "If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying k-NN algorithm on the transformed data in feature space. An example of a typical computer vision computation pipeline for face recognition using k-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with OpenCV): Haar face detection Mean-shift tracking analysis PCA or Fisher LDA projection into feature space, followed by k-NN classification Dimension reduction For high-dimensional data (e.g., with number of dimensions more than 10) dimension reduction is usually performed prior to applying the k-NN algorithm in order to avoid the effects of the curse of dimensionality. The curse of dimensionality in the k-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same)."
            },
            {
                "text": "Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k-NN on feature vectors in reduced-dimension space. This process is also called low-dimensional embedding. For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate k-NN search using locality sensitive hashing, \"random projections\", \"sketches\"Ryan, Donna (editor); High Performance Discovery in Time Series, Berlin: Springer, 2004, or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option. Decision boundary Nearest neighbor rules in effect implicitly compute the decision boundary. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity. Data reduction Data reduction is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification."
            },
            {
                "text": "Those data are called the prototypes and can be found as follows: Select the class-outliers, that is, training data that are classified incorrectly by k-NN (for a given k) Separate the rest of the data into two sets: (i) the prototypes that are used for the classification decisions and (ii) the absorbed points that can be correctly classified by k-NN using prototypes. The absorbed points can then be removed from the training set. Selection of class-outliers A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include: random error insufficient training examples of this class (an isolated example appears instead of a cluster) missing important features (the classes are separated in other dimensions which we don't know) too many training examples of other classes (unbalanced classes) that create a \"hostile\" background for the given small class Class outliers with k-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, k>r>0, a training example is called a (k,r)NN class-outlier if its k nearest neighbors include more than r examples of other classes."
            },
            {
                "text": "Condensed Nearest Neighbor for data reduction Condensed nearest neighbor (CNN, the Hart algorithm) is an algorithm designed to reduce the data set for k-NN classification. It selects the set of prototypes U from the training data, such that 1NN with U can classify the examples almost as accurately as 1NN does with the whole data set. right|Calculation of the border ratio right|Three types of points: prototypes, class-outliers, and absorbed points. Given a training set X, CNN works iteratively: Scan all elements of X, looking for an element x whose nearest prototype from U has a different label than x. Remove x from X and add it to U Repeat the scan until no more prototypes are added to U. Use U instead of X for classification. The examples that are not prototypes are called \"absorbed\" points. It is efficient to scan the training examples in order of decreasing border ratio.Mirkes, Evgeny M.; KNN and Potential Energy: applet , University of Leicester, 2011 The border ratio of a training example x is defined as where $ is the distance to the closest example y having a different color than x, and $ is the distance from y to its closest example x' with the same label as x."
            },
            {
                "text": "The border ratio is in the interval [0,1] because $ never exceeds $. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes U. A point of a different label than x is called external to x. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is x and its label is red. External points are blue and green. The closest to x external point is y. The closest to y red point is x' . The border ratio $1= a(x) = is the attribute of the initial point x. Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors)."
            },
            {
                "text": "Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet. k-NN regression In k-NN regression, also known as k-NN smoothing, the k-NN algorithm is used for estimating continuous variables. One such algorithm uses a weighted average of the k nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows: Compute the Euclidean or Mahalanobis distance from the query example to the labeled examples. Order the labeled examples by increasing distance. Find a heuristically optimal number k of nearest neighbors, based on RMSE. This is done using cross validation. Calculate an inverse distance weighted average with the k-nearest multivariate neighbors. k-NN outlier The distance to the kth nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in anomaly detection. The larger the distance to the k-NN, the lower the local density, the more likely the query point is an outlier."
            },
            {
                "text": "Although quite simple, this outlier model, along with another classic data mining method, local outlier factor, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis. Validation of results A confusion matrix or \"matching matrix\" is often used as a tool to validate the accuracy of k-NN classification. More robust statistical methods such as likelihood-ratio test can also be applied. See also Nearest centroid classifier Closest pair of points problem Nearest neighbor graph Segmentation-based object categorization References Further reading Category:Classification algorithms Category:Search algorithms Category:Machine learning algorithms Category:Statistical classification Category:Nonparametric statistics"
            }
        ],
        "latex_formulas": [
            "''a''(''x'') = {{sfrac|{{norm|''x'-y''}}|{{norm|''x-y''}}}}",
            "{{norm|''x-y''}}",
            "{{norm|''x'-y''}}",
            "{{norm|''x'-y''}}",
            "{{norm|''x-y''}}",
            "''a''(''x'') = {{norm|''x'-y''}} / {{norm|''x-y''}}",
            "(X_1,Y_1), (X_2,Y_2), \\dots, (X_n, Y_n)",
            "\\mathbb{R}^d \\times \\{1,2\\}",
            "X|Y=r \\sim P_r",
            "r = 1,2",
            "P_r",
            "\\|\\cdot\\|",
            "\\mathbb{R}^d",
            "x \\in \\mathbb{R}^d",
            "(X_{(1)},Y_{(1)}), \\dots, (X_{(n)}, Y_{(n)})",
            "\\|X_{(1)}-x\\| \\leq \\dots \\leq \\|X_{(n)}-x\\|",
            "C_n^{1nn}(x) =  Y_{(1)}",
            "1/k",
            "w_{ni}",
            "C^{wnn}_n",
            "\\{w_{ni}\\}_{i=1}^n",
            "B_1",
            "B_2",
            "s_n^2 = \\sum_{i=1}^n w_{ni}^2",
            "t_n = n^{-2/d}\\sum_{i=1}^n w_{ni} \\left\\{i^{1+2/d} - (i-1)^{1+2/d}\\right\\}",
            "\\{w_{ni}^*\\}_{i=1}^n",
            "k^* = \\lfloor B n^{\\frac 4 {d+4}} \\rfloor",
            "i=1,2,\\dots,k^*",
            "i = k^*+1,\\dots,n",
            "\\mathcal{O}(n^{-\\frac 4 {d+4}})",
            "R^*",
            "R_{kNN}",
            "M=2",
            "R^*",
            "(X, Y)",
            "k:=k_n",
            "k_n/n",
            "n \\to \\infty",
            "C_n^{knn}",
            "B_1",
            "B_2",
            "k^* = \\left\\lfloor B n^{\\frac 4 {d+4}} \\right\\rfloor",
            "k^*",
            "\\mathcal{O}\\left(n^{-\\frac 4 {d+4}}\\right)"
        ]
    },
    "Naive_Bayes_classifier": {
        "title": "Naive_Bayes_classifier",
        "chunks": [
            {
                "text": "thumb|Example of a naive Bayes classifier depicted as a Bayesian Network In statistics, naive (sometimes simple or idiot's) Bayes classifiers are a family of \"probabilistic classifiers\" which assumes that the features are conditionally independent, given the target class. In other words, a naive Bayes model assumes the information about the class provided by each variable is unrelated to the information from the others, with no information shared between the predictors. The highly unrealistic nature of this assumption, called the naive independence assumption, is what gives the classifier its name. These classifiers are some of the simplest Bayesian network models. Naive Bayes classifiers generally perform worse than more advanced models like logistic regressions, especially at quantifying uncertainty (with naive Bayes models often producing wildly overconfident probabilities). However, they are highly scalable, requiring only one parameter for each feature or predictor in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression (simply by counting observations in each group), rather than the expensive iterative approximation algorithms required by most other models. Despite the use of Bayes' theorem in the classifier's decision rule, naive Bayes is not (necessarily) a Bayesian method, and naive Bayes models can be fit to data using either Bayesian or frequentist methods."
            },
            {
                "text": "Introduction Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features. In many practical applications, parameter estimation for naive Bayes models uses the method of maximum likelihood; in other words, one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods. Despite their naive design and apparently oversimplified assumptions, naive Bayes classifiers have worked quite well in many complex real-world situations."
            },
            {
                "text": "In 2004, an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers. Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests. An advantage of naive Bayes is that it only requires a small amount of training data to estimate the parameters necessary for classification. Probabilistic model Abstractly, naive Bayes is a conditional probability model: it assigns probabilities $p(C_k \\mid x_1, \\ldots, x_n)$ for each of the possible outcomes or classes $C_k$ given a problem instance to be classified, represented by a vector $\\mathbf{x} = (x_1, \\ldots, x_n)$ encoding some features (independent variables). The problem with the above formulation is that if the number of features is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible. The model must therefore be reformulated to make it more tractable."
            },
            {
                "text": "Using Bayes' theorem, the conditional probability can be decomposed as: In plain English, using Bayesian probability terminology, the above equation can be written as In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on $C$ and the values of the features $x_i$ are given, so that the denominator is effectively constant. The numerator is equivalent to the joint probability model which can be rewritten as follows, using the chain rule for repeated applications of the definition of conditional probability: Now the \"naive\" conditional independence assumptions come into play: assume that all features in $\\mathbf{x}$ are mutually independent, conditional on the category $C_k$. Under this assumption, Thus, the joint model can be expressed as where $\\varpropto$ denotes proportionality since the denominator $p(\\mathbf{x})$ is omitted. This means that under the above independence assumptions, the conditional distribution over the class variable $C$ is: where the evidence $Z = p(\\mathbf{x}) = \\sum_k p(C_k) \\ p(\\mathbf{x} \\mid C_k)$ is a scaling factor dependent only on $x_1, \\ldots, x_n$, that is, a constant if the values of the feature variables are known."
            },
            {
                "text": "Constructing a classifier from the probability model The discussion so far has derived the independent feature model, that is, the naive Bayes probability model. The naive Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable so as to minimize the probability of misclassification; this is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label $\\hat{y} = C_k$ for some as follows: thumb|Likelihood functions $p(\\mathbf{x} \\mid Y)$, Confusion matrix and ROC curve. For the naive Bayes classifier and given that the a priori probabilities $p(Y)$ are the same for all classes, then the decision boundary (green line) would be placed on the point where the two probability densities intersect, due to Parameter estimation and event models A class's prior may be calculated by assuming equiprobable classes, i.e., $p(C_k) = \\frac{1}{K}$, or by calculating an estimate for the class probability from the training set: To estimate the parameters for a feature's distribution, one must assume a distribution or generate nonparametric models for the features from the training set."
            },
            {
                "text": "The assumptions on distributions of features are called the \"event model\" of the naive Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), multinomial and Bernoulli distributions are popular. These assumptions lead to two distinct models, which are often confused. Gaussian naive Bayes When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution. For example, suppose the training data contains a continuous attribute, $x$. The data is first segmented by the class, and then the mean and variance of $x$ is computed in each class. Let $\\mu_k$ be the mean of the values in $x$ associated with class $C_k$, and let $\\sigma^2_k$ be the Bessel corrected variance of the values in $x$ associated with class $C_k$. Suppose one has collected some observation value $v$. Then, the probability density of $v$ given a class $C_k$, i.e., $p(x=v \\mid C_k)$, can be computed by plugging $v$ into the equation for a normal distribution parameterized by $\\mu_k$ and $\\sigma^2_k$."
            },
            {
                "text": "Formally, Another common technique for handling continuous values is to use binning to discretize the feature values and obtain a new set of Bernoulli-distributed features. Some literature suggests that this is required in order to use naive Bayes, but it is not true, as the discretization may throw away discriminative information. Sometimes the distribution of class-conditional marginal densities is far from normal. In these cases, kernel density estimation can be used for a more realistic estimate of the marginal densities of each class. This method, which was introduced by John and Langley, can boost the accuracy of the classifier considerably. Multinomial naive Bayes With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial $(p_1, \\dots, p_n)$ where $p_i$ is the probability that event occurs (or such multinomials in the multiclass case). A feature vector $\\mathbf{x} = (x_1, \\dots, x_n)$ is then a histogram, with $x_i$ counting the number of times event was observed in a particular instance."
            },
            {
                "text": "This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption). The likelihood of observing a histogram $x$ is given by: where $p_{ki} := p(i \\mid C_k)$. The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space: where $b = \\log p(C_k)$ and $w_{ki} = \\log p_{ki}$. Estimating the parameters in log space is advantageous since multiplying a large number of small values can lead to significant rounding error. Applying a log transform reduces the effect of this rounding error. If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, because the probability estimate is directly proportional to the number of occurrences of a feature's value. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero."
            },
            {
                "text": "This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case. Rennie et al. discuss problems with the multinomial assumption in the context of document classification and possible ways to alleviate those problems, including the use of tf–idf weights instead of raw term frequencies and document length normalization, to produce a naive Bayes classifier that is competitive with support vector machines. Bernoulli naive Bayes In the multivariate Bernoulli event model, features are independent Boolean variables (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence features are used rather than term frequencies. If $x_i$ is a Boolean expressing the occurrence or absence of the 'th term from the vocabulary, then the likelihood of a document given a class $C_k$ is given by: where $p_{ki}$ is the probability of class $C_k$ generating the term $x_i$. This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms."
            },
            {
                "text": "Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one. Semi-supervised parameter estimation Given a way to train a naive Bayes classifier from labeled data, it's possible to construct a semi-supervised training algorithm that can learn from a combination of labeled and unlabeled data by running the supervised learning algorithm in a loop: Given a collection $D = L \\uplus U$ of labeled samples and unlabeled samples , start by training a naive Bayes classifier on . Until convergence, do: Predict class probabilities $P(C \\mid x)$ for all examples in $D$. Re-train the model based on the probabilities (not the labels) predicted in the previous step. Convergence is determined based on improvement to the model likelihood $P(D \\mid \\theta)$, where $\\theta$ denotes the parameters of the naive Bayes model. This training algorithm is an instance of the more general expectation–maximization algorithm (EM): the prediction step inside the loop is the E-step of EM, while the re-training of naive Bayes is the M-step."
            },
            {
                "text": "The algorithm is formally justified by the assumption that the data are generated by a mixture model, and the components of this mixture model are exactly the classes of the classification problem. Discussion Despite the fact that the far-reaching independence assumptions are often inaccurate, the naive Bayes classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While naive Bayes often fails to produce a good estimate for the correct class probabilities, this may not be a requirement for many applications. For example, the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is predicted as more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model."
            },
            {
                "text": "Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below. Relation to logistic regression In the case of discrete inputs (indicator or frequency features for discrete events), naive Bayes classifiers form a generative-discriminative pair with multinomial logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood $p(C, \\mathbf{x})$, while logistic regression fits the same probability model to optimize the conditional $p(C \\mid \\mathbf{x})$. More formally, we have the following: The link between the two can be seen by observing that the decision function for naive Bayes (in the binary case) can be rewritten as \"predict class $C_1$ if the odds of $p(C_1 \\mid \\mathbf{x})$ exceed those of $p(C_2 \\mid \\mathbf{x})$\". Expressing this in log-space gives: The left-hand side of this equation is the log-odds, or logit, the quantity predicted by the linear model that underlies logistic regression."
            },
            {
                "text": "Since naive Bayes is also a linear model for the two \"discrete\" event models, it can be reparametrised as a linear function $b + \\mathbf{w}^\\top x > 0$. Obtaining the probabilities is then a matter of applying the logistic function to $b + \\mathbf{w}^\\top x$, or in the multiclass case, the softmax function. Discriminative classifiers have lower asymptotic error than generative ones; however, research by Ng and Jordan has shown that in some practical cases naive Bayes can outperform logistic regression because it reaches its asymptotic error faster. Examples Person classification Problem: classify whether a given person is a male or a female based on the measured features. The features include height, weight, and foot size. Although with NB classifier we treat them as independent, they are not in reality. Training Example training set below."
            },
            {
                "text": "Person height (feet) weight (lbs) foot size (inches) male 6 180 12 male 5.92 (5'11\") 190 11 male 5.58 (5'7\") 170 12 male 5.92 (5'11\") 165 10 female 5 100 6 female 5.5 (5'6\") 150 8 female 5.42 (5'5\") 130 7 female 5.75 (5'9\") 150 9 The classifier created from the training set using a Gaussian distribution assumption would be (given variances are unbiased sample variances): Person mean (height) variance (height) mean (weight) variance (weight) mean (foot size) variance (foot size) male 5.855 3.5033 × 10−2 176.25 1.2292 × 102 11.25 9.1667 × 10−1 female 5.4175 9.7225 × 10−2 132.5 5.5833 × 102 7.5 1.6667 The following example assumes equiprobable classes so that P(male)= P(female) = 0.5."
            },
            {
                "text": "This prior probability distribution might be based on prior knowledge of frequencies in the larger population or in the training set. Testing Below is a sample to be classified as male or female. Person height (feet) weight (lbs) foot size (inches) sample 6 130 8 In order to classify the sample, one has to determine which posterior is greater, male or female. For the classification as male the posterior is given by For the classification as female the posterior is given by The evidence (also termed normalizing constant) may be calculated: However, given the sample, the evidence is a constant and thus scales both posteriors equally. It therefore does not affect classification and can be ignored. The probability distribution for the sex of the sample can now be determined: where $\\mu = 5.855$ and $\\sigma^2 = 3.5033 \\cdot 10^{-2}$ are the parameters of normal distribution which have been previously determined from the training set. Note that a value greater than 1 is OK here – it is a probability density rather than a probability, because height is a continuous variable."
            },
            {
                "text": "Since posterior numerator is greater in the female case, the prediction is that the sample is female. Document classification Here is a worked example of naive Bayesian classification to the document classification problem. Consider the problem of classifying documents by their content, for example into spam and non-spam e-mails. Imagine that documents are drawn from a number of classes of documents which can be modeled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class C can be written as (For this treatment, things are further simplified by assuming that words are randomly distributed in the document - that is, words are not dependent on the length of the document, position within the document with relation to other words, or other document-context.) Then the probability that a given document D contains all of the words $w_i$, given a class C, is The question that has to be answered is: \"what is the probability that a given document D belongs to a given class C?\" In other words, what is $p(C \\mid D)\\,$?"
            },
            {
                "text": "Now by definition and Bayes' theorem manipulates these into a statement of probability in terms of likelihood. Assume for the moment that there are only two mutually exclusive classes, S and ¬S (e.g. spam and not spam), such that every element (email) is in either one or the other; and Using the Bayesian result above, one can write: Dividing one by the other gives: Which can be re-factored as: Thus, the probability ratio p(S | D) / p(¬S | D) can be expressed in terms of a series of likelihood ratios. The actual probability p(S | D) can be easily computed from log (p(S | D) / p(¬S | D)) based on the observation that p(S | D) + p(¬S | D) = 1. Taking the logarithm of all these ratios, one obtains: (This technique of \"log-likelihood ratios\" is a common technique in statistics. In the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a sigmoid curve: see logit for details.)"
            },
            {
                "text": "Finally, the document can be classified as follows. It is spam if $p(S\\mid D) > p(\\neg S\\mid D)$ (i. e., $\\ln{p(S\\mid D) \\over p(\\neg S\\mid D)} > 0$), otherwise it is not spam. Spam filtering Naive Bayes classifiers are a popular statistical technique of e-mail filtering. They typically use bag-of-words features to identify email spam, an approach commonly used in text classification. Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam. Naive Bayes spam filtering is a baseline technique for dealing with spam that can tailor itself to the email needs of individual users and give low false positive spam detection rates that are generally acceptable to users. Bayesian algorithms were used for email filtering as early as 1996. Although naive Bayesian filters did not become popular until later, multiple programs were released in 1998 to address the growing problem of unwanted email."
            },
            {
                "text": "The first scholarly publication on Bayesian spam filtering was by Sahami et al. in 1998. Variants of the basic technique have been implemented in a number of research works and commercial software products. Many modern mail clients implement Bayesian spam filtering. Users can also install separate email filtering programs. Server-side email filters, such as DSPAM, SpamAssassin, SpamBayes, Bogofilter, and ASSP, make use of Bayesian spam filtering techniques, and the functionality is sometimes embedded within mail server software itself. CRM114, oft cited as a Bayesian filter, is not intended to use a Bayes filter in production, but includes the ″unigram″ feature for reference. Dealing with rare words In the case a word has never been met during the learning phase, both the numerator and the denominator are equal to zero, both in the general formula and in the spamicity formula. The software can decide to discard such words for which there is no information available. More generally, the words that were encountered only a few times during the learning phase cause a problem, because it would be an error to trust blindly the information they provide."
            },
            {
                "text": "A simple solution is to simply avoid taking such unreliable words into account as well. Applying again Bayes' theorem, and assuming the classification between spam and ham of the emails containing a given word (\"replica\") is a random variable with beta distribution, some programs decide to use a corrected probability: $\\Pr'(S|W) = \\frac{s \\cdot \\Pr(S) + n \\cdot \\Pr(S|W)}{s + n }$ where: $\\Pr'(S|W)$ is the corrected probability for the message to be spam, knowing that it contains a given word ; $s$ is the strength we give to background information about incoming spam ; $\\Pr(S)$ is the probability of any incoming message to be spam ; $n$ is the number of occurrences of this word during the learning phase ; $\\Pr(S|W)$ is the spamicity of this word. (Demonstration:) This corrected probability is used instead of the spamicity in the combining formula. This formula can be extended to the case where n is equal to zero (and where the spamicity is not defined), and evaluates in this case to $Pr(S)$."
            },
            {
                "text": "Other heuristics \"Neutral\" words like \"the\", \"a\", \"some\", or \"is\" (in English), or their equivalents in other languages, can be ignored. These are also known as Stop words. More generally, some bayesian filtering filters simply ignore all the words which have a spamicity next to 0.5, as they contribute little to a good decision. The words taken into consideration are those whose spamicity is next to 0.0 (distinctive signs of legitimate messages), or next to 1.0 (distinctive signs of spam). A method can be for example to keep only those ten words, in the examined message, which have the greatest absolute value |0.5 − pI|. Some software products take into account the fact that a given word appears several times in the examined message, others don't. Some software products use patterns (sequences of words) instead of isolated natural languages words. For example, with a \"context window\" of four words, they compute the spamicity of \"Viagra is good for\", instead of computing the spamicities of \"Viagra\", \"is\", \"good\", and \"for\"."
            },
            {
                "text": "This method gives more sensitivity to context and eliminates the Bayesian noise better, at the expense of a bigger database. Disadvantages Depending on the implementation, Bayesian spam filtering may be susceptible to Bayesian poisoning, a technique used by spammers in an attempt to degrade the effectiveness of spam filters that rely on Bayesian filtering. A spammer practicing Bayesian poisoning will send out emails with large amounts of legitimate text (gathered from legitimate news or literary sources). Spammer tactics include insertion of random innocuous words that are not normally associated with spam, thereby decreasing the email's spam score, making it more likely to slip past a Bayesian spam filter. However, with (for example) Paul Graham's scheme only the most significant probabilities are used, so that padding the text out with non-spam-related words does not affect the detection probability significantly. Words that normally appear in large quantities in spam may also be transformed by spammers. For example, «Viagra» would be replaced with «Viaagra» or «V!agra» in the spam message. The recipient of the message can still read the changed words, but each of these words is met more rarely by the Bayesian filter, which hinders its learning process."
            },
            {
                "text": "As a general rule, this spamming technique does not work very well, because the derived words end up recognized by the filter just like the normal ones.Paul Graham (2002), A Plan for Spam Another technique used to try to defeat Bayesian spam filters is to replace text with pictures, either directly included or linked. The whole text of the message, or some part of it, is replaced with a picture where the same text is \"drawn\". The spam filter is usually unable to analyze this picture, which would contain the sensitive words like «Viagra». However, since many mail clients disable the display of linked pictures for security reasons, the spammer sending links to distant pictures might reach fewer targets. Also, a picture's size in bytes is bigger than the equivalent text's size, so the spammer needs more bandwidth to send messages directly including pictures. Some filters are more inclined to decide that a message is spam if it has mostly graphical contents. A solution used by Google in its Gmail email system is to perform an OCR (Optical Character Recognition) on every mid to large size image, analyzing the text inside. See also AODE Anti-spam techniques Bayes classifier Bayesian network Bayesian poisoning Email filtering Linear classifier Logistic regression Markovian discrimination Mozilla Thunderbird mail client with native implementation of Bayes filters Perceptron Random naive Bayes Take-the-best heuristic References Further reading External links Book Chapter: Naive Bayes text classification, Introduction to Information Retrieval Naive Bayes for Text Classification with Unbalanced Classes Category:Classification algorithms Category:Statistical classification Category:Bayesian statistics"
            }
        ],
        "latex_formulas": [
            "'''x'''",
            "p(C_k \\mid x_1, \\ldots, x_n)",
            "C_k",
            "\\mathbf{x} = (x_1, \\ldots, x_n)",
            "C",
            "x_i",
            "\\mathbf{x}",
            "C_k",
            "\\varpropto",
            "p(\\mathbf{x})",
            "C",
            "Z = p(\\mathbf{x}) = \\sum_k p(C_k) \\ p(\\mathbf{x} \\mid C_k)",
            "x_1, \\ldots, x_n",
            "\\hat{y} = C_k",
            "p(\\mathbf{x} \\mid Y)",
            "p(Y)",
            "p(Y \\mid \\mathbf{x}) = \\frac{p(Y) \\ p(\\mathbf{x} \\mid Y)}{p(\\mathbf{x})} \\propto p(\\mathbf{x} \\mid Y)",
            "p(C_k) = \\frac{1}{K}",
            "x",
            "x",
            "\\mu_k",
            "x",
            "C_k",
            "\\sigma^2_k",
            "x",
            "C_k",
            "v",
            "v",
            "C_k",
            "p(x=v \\mid C_k)",
            "v",
            "\\mu_k",
            "\\sigma^2_k",
            "(p_1, \\dots, p_n)",
            "p_i",
            "\\mathbf{x} = (x_1, \\dots, x_n)",
            "x_i",
            "p_{ki} := p(i \\mid C_k)",
            "b = \\log p(C_k)",
            "w_{ki} = \\log p_{ki}",
            "x_i",
            "C_k",
            "p_{ki}",
            "C_k",
            "x_i",
            "D = L \\uplus U",
            "P(C \\mid x)",
            "D",
            "P(D \\mid \\theta)",
            "\\theta",
            "p(C, \\mathbf{x})",
            "p(C \\mid \\mathbf{x})",
            "Y\\in \\{1, ..., n\\}",
            "C_1",
            "p(C_1 \\mid \\mathbf{x})",
            "p(C_2 \\mid \\mathbf{x})",
            "b + \\mathbf{w}^\\top x > 0",
            "b + \\mathbf{w}^\\top x",
            "\\mu = 5.855",
            "\\sigma^2 = 3.5033 \\cdot 10^{-2}",
            "w_i",
            "p(C \\mid D)\\,",
            "p(S\\mid D) > p(\\neg S\\mid D)",
            "\\ln{p(S\\mid D) \\over p(\\neg S\\mid D)} > 0",
            "\\Pr'(S|W) = \\frac{s \\cdot \\Pr(S) + n \\cdot \\Pr(S|W)}{s + n }",
            "\\Pr'(S|W)",
            "s",
            "\\Pr(S)",
            "n",
            "\\Pr(S|W)",
            "Pr(S)"
        ]
    },
    "Decision_tree": {
        "title": "Decision_tree",
        "chunks": [
            {
                "text": "right|thumb|Traditionally, decision trees have been created manually. A decision tree is a decision support recursive partitioning structure that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning. Overview A decision tree is a flowchart-like structure in which each internal node represents a test on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules. In decision analysis, a decision tree and the closely related influence diagram are used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated."
            },
            {
                "text": "A decision tree consists of three types of nodes: Decision nodes – typically represented by squares Chance nodes – typically represented by circles End nodes – typically represented by triangles Decision trees are commonly used in operations research and operations management. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating conditional probabilities. Decision trees, influence diagrams, utility functions, and other decision analysis tools and methods are taught to undergraduate students in schools of business, health economics, and public health, and are examples of operations research or management science methods. These tools are also used to predict decisions of householders in normal and emergency scenarios. Decision-tree building blocks Decision-tree elements center| Drawn from left to right, a decision tree has only burst nodes (splitting paths) but no sink nodes (converging paths). So used manually they can grow very big and are then often hard to draw fully by hand."
            },
            {
                "text": "Traditionally, decision trees have been created manually – as the aside example shows – although increasingly, specialized software is employed. Decision rules The decision tree can be linearized into decision rules, where the outcome is the contents of the leaf node, and the conditions along the path form a conjunction in the if clause. In general, the rules have the form: if condition1 and condition2 and condition3 then outcome. Decision rules can be generated by constructing association rules with the target variable on the right. They can also denote temporal or causal relations.K. Karimi and H.J. Hamilton (2011), \"Generation and Interpretation of Temporal Decision Rules\", International Journal of Computer Information Systems and Industrial Management Applications, Volume 3 Decision tree using flowchart symbols Commonly a decision tree is drawn using flowchart symbols as it is easier for many to read and understand. Note there is a conceptual error in the \"Proceed\" calculation of the tree shown below; the error relates to the calculation of \"costs\" awarded in a legal action. center Analysis example Analysis can take into account the decision maker's (e.g., the company's) preference or utility function, for example: center The basic interpretation in this situation is that the company prefers B's risk and payoffs under realistic risk preference coefficients (greater than $400K—in that range of risk aversion, the company would need to model a third strategy, \"Neither A nor B\")."
            },
            {
                "text": "Another example, commonly used in operations research courses, is the distribution of lifeguards on beaches (a.k.a. the \"Life's a Beach\" example). The example describes two beaches with lifeguards to be distributed on each beach. There is maximum budget B that can be distributed among the two beaches (in total), and using a marginal returns table, analysts can decide how many lifeguards to allocate to each beach. +Lifeguards on each beachDrownings prevented in total, beach #1Drownings prevented in total, beach #2131204 In this example, a decision tree can be drawn to illustrate the principles of diminishing returns on beach #1. alt=|none|thumb|Beach decision tree The decision tree illustrates that when sequentially distributing lifeguards, placing a first lifeguard on beach #1 would be optimal if there is only the budget for 1 lifeguard. But if there is a budget for two guards, then placing both on beach #2 would prevent more overall drownings. alt=|center Influence diagram Much of the information in a decision tree can be represented more compactly as an influence diagram, focusing attention on the issues and relationships between events."
            },
            {
                "text": "center|frame|The rectangle on the left represents a decision, the ovals represent actions, and the diamond represents results. Association rule induction Decision trees can also be seen as generative models of induction rules from empirical data. An optimal decision tree is then defined as a tree that accounts for most of the data, while minimizing the number of levels (or \"questions\").R. Quinlan, \"Learning efficient classification procedures\", Machine Learning: an artificial intelligence approach, Michalski, Carbonell & Mitchell (eds. ), Morgan Kaufmann, 1983, p. 463–482. Several algorithms to generate such optimal trees have been devised, such as ID3/4/5,Utgoff, P. E. (1989). Incremental induction of decision trees. Machine learning, 4(2), 161–186. CLS, ASSISTANT, and CART. Advantages and disadvantages Among decision support tools, decision trees (and influence diagrams) have several advantages. Decision trees: Are simple to understand and interpret. People are able to understand decision tree models after a brief explanation. Have value even with little hard data."
            },
            {
                "text": "Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes. Help determine worst, best, and expected values for different scenarios. Use a white box model. If a given result is provided by a model. Can be combined with other decision techniques. The action of more than one decision-maker can be considered. Disadvantages of decision trees: They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree. They are often relatively inaccurate. Many other predictors perform better with similar data. This can be remedied by replacing a single decision tree with a random forest of decision trees, but a random forest is not as easy to interpret as a single decision tree. For data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of those attributes with more levels. Calculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked."
            },
            {
                "text": "Optimizing a decision tree A few things should be considered when improving the accuracy of the decision tree classifier. The following are some possible optimizations to consider when looking to make sure the decision tree model produced makes the correct decision or classification. Note that these things are not the only things to consider but only some. Increasing the number of levels of the tree The accuracy of the decision tree can change based on the depth of the decision tree. In many cases, the tree’s leaves are pure nodes. When a node is pure, it means that all the data in that node belongs to a single class. For example, if the classes in the data set are Cancer and Non-Cancer a leaf node would be considered pure when all the sample data in a leaf node is part of only one class, either cancer or non-cancer. It is important to note that a deeper tree is not always better when optimizing the decision tree. A deeper tree can influence the runtime in a negative way. If a certain classification algorithm is being used, then a deeper tree could mean the runtime of this classification algorithm is significantly slower."
            },
            {
                "text": "There is also the possibility that the actual algorithm building the decision tree will get significantly slower as the tree gets deeper. If the tree-building algorithm being used splits pure nodes, then a decrease in the overall accuracy of the tree classifier could be experienced. Occasionally, going deeper in the tree can cause an accuracy decrease in general, so it is very important to test modifying the depth of the decision tree and selecting the depth that produces the best results. To summarize, observe the points below, we will define the number D as the depth of the tree. Possible advantages of increasing the number D: Accuracy of the decision-tree classification model increases. Possible disadvantages of increasing D Runtime issues Decrease in accuracy in general Pure node splits while going deeper can cause issues. The ability to test the differences in classification results when changing D is imperative. We must be able to easily change and test the variables that could affect the accuracy and reliability of the decision tree-model. The choice of node-splitting functions The node splitting function used can have an impact on improving the accuracy of the decision tree."
            },
            {
                "text": "For example, using the information-gain function may yield better results than using the phi function. The phi function is known as a measure of “goodness” of a candidate split at a node in the decision tree. The information gain function is known as a measure of the “reduction in entropy”. In the following, we will build two decision trees. One decision tree will be built using the phi function to split the nodes and one decision tree will be built using the information gain function to split the nodes. The main advantages and disadvantages of information gain and phi function One major drawback of information gain is that the feature that is chosen as the next node in the tree tends to have more unique values. An advantage of information gain is that it tends to choose the most impactful features that are close to the root of the tree. It is a very good measure for deciding the relevance of some features. The phi function is also a good measure for deciding the relevance of some features based on \"goodness\". This is the information gain function formula."
            },
            {
                "text": "The formula states the information gain is a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree. $I_{\\textrm{gain}}(s) = H(t) - H(s,t)$ This is the phi function formula. The phi function is maximized when the chosen feature splits the samples in a way that produces homogenous splits and have around the same number of samples in each split. $\\Phi(s,t) = (2*P_L*P_R ) * Q(s|t)$ We will set D, which is the depth of the decision tree we are building, to three (D = 3). We also have the following data set of cancer and non-cancer samples and the mutation features that the samples either have or do not have. If a sample has a feature mutation then the sample is positive for that mutation, and it will be represented by one. If a sample does not have a feature mutation then the sample is negative for that mutation, and it will be represented by zero."
            },
            {
                "text": "To summarize, C stands for cancer and NC stands for non-cancer. The letter M stands for mutation, and if a sample has a particular mutation it will show up in the table as a one and otherwise zero. +The sample dataM1M2M3M4M5C101011NC100000NC200110NC300000C211111NC400010 Now, we can use the formulas to calculate the phi function values and information gain values for each M in the dataset. Once all the values are calculated the tree can be produced. The first thing to be done is to select the root node. In information gain and the phi function we consider the optimal split to be the mutation that produces the highest value for information gain or the phi function. Now assume that M1 has the highest phi function value and M4 has the highest information gain value. The M1 mutation will be the root of our phi function tree and M4 will be the root of our information gain tree. You can observe the root nodes below center|frameless|220x220px|Figure 1: The left node is the root node of the tree we are building using the phi function to split the nodes. The right node is the root node of the tree we are building using information gain to split the nodes."
            },
            {
                "text": "Now, once we have chosen the root node we can split the samples into two groups based on whether a sample is positive or negative for the root node mutation. The groups will be called group A and group B. For example, if we use M1 to split the samples in the root node we get NC2 and C2 samples in group A and the rest of the samples NC4, NC3, NC1, C1 in group B. Disregarding the mutation chosen for the root node, proceed to place the next best features that have the highest values for information gain or the phi function in the left or right child nodes of the decision tree. Once we choose the root node and the two child nodes for the tree of depth = 3 we can just add the leaves. The leaves will represent the final classification decision the model has produced based on the mutations a sample either has or does not have. The left tree is the decision tree we obtain from using information gain to split the nodes and the right tree is what we obtain from using the phi function to split the nodes."
            },
            {
                "text": "left|frameless|500x500px|The resulting tree from using information gain to split the nodes frameless|500x500px|center Now assume the classification results from both trees are given using a confusion matrix. Information gain confusion matrix: + C NC C11 NC04 Phi function confusion matrix: + C NC C20 NC13 The tree using information gain has the same results when using the phi function when calculating the accuracy. When we classify the samples based on the model using information gain we get one true positive, one false positive, zero false negatives, and four true negatives. For the model using the phi function we get two true positives, zero false positives, one false negative, and three true negatives. The next step is to evaluate the effectiveness of the decision tree using some key metrics that will be discussed in the evaluating a decision tree section below. The metrics that will be discussed below can help determine the next steps to be taken when optimizing the decision tree. Other techniques The above information is not where it ends for building and optimizing a decision tree. There are many techniques for improving the decision tree classification models we build."
            },
            {
                "text": "One of the techniques is making our decision tree model from a bootstrapped dataset. The bootstrapped dataset helps remove the bias that occurs when building a decision tree model with the same data the model is tested with. The ability to leverage the power of random forests can also help significantly improve the overall accuracy of the model being built. This method generates many decisions from many decision trees and tallies up the votes from each decision tree to make the final classification. There are many techniques, but the main objective is to test building your decision tree model in different ways to make sure it reaches the highest performance level possible. Evaluating a decision tree It is important to know the measurements used to evaluate decision trees. The main metrics used are accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate. All these measurements are derived from the number of true positives, false positives, True negatives, and false negatives obtained when running a set of samples through the decision tree classification model. Also, a confusion matrix can be made to display these results."
            },
            {
                "text": "All these main metrics tell something different about the strengths and weaknesses of the classification model built based on your decision tree. For example, a low sensitivity with high specificity could indicate the classification model built from the decision tree does not do well identifying cancer samples over non-cancer samples. Let us take the confusion matrix below. + C NC C11(true positives)45(false negatives) NC1(false positive)105(true negatives) We will now calculate the values accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate. Accuracy: Sensitivity (TPR – true positive rate): Specificity (TNR – true negative rate): Precision (PPV – positive predictive value): Miss Rate (FNR – false negative rate): False discovery rate (FDR): False omission rate (FOR): Once we have calculated the key metrics we can make some initial conclusions on the performance of the decision tree model built. The accuracy that we calculated was 71.60%. The accuracy value is good to start but we would like to get our models as accurate as possible while maintaining the overall performance. The sensitivity value of 19.64% means that out of everyone who was actually positive for cancer tested positive. If we look at the specificity value of 99.06% we know that out of all the samples that were negative for cancer actually tested negative."
            },
            {
                "text": "When it comes to sensitivity and specificity it is important to have a balance between the two values, so if we can decrease our specificity to increase the sensitivity that would prove to be beneficial. These are just a few examples on how to use these values and the meanings behind them to evaluate the decision tree model and improve upon the next iteration. See also - Application of the technique in valuations of computation References External links Extensive Decision Tree tutorials and examples Gallery of example decision trees Gradient Boosted Decision Trees Category:Decision analysis"
            }
        ],
        "latex_formulas": [
            "I_{\\textrm{gain}}(s) = H(t) - H(s,t)",
            "\\Phi(s,t) = (2*P_L*P_R ) * Q(s|t)"
        ]
    },
    "Statistical_classification": {
        "title": "Statistical_classification",
        "chunks": [
            {
                "text": "When classification is performed by a computer, statistical methods are normally used to develop the algorithm. Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type), ordinal (e.g. \"large\", \"medium\" or \"small\"), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function. An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term \"classifier\" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category. Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc."
            },
            {
                "text": "), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. Other fields may use different terminology: e.g. in community ecology, the term \"classification\" normally refers to cluster analysis. Relation to other problems Classification and clustering are examples of the more general problem of pattern recognition, which is the assignment of some sort of output value to a given input value. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc. A common subclass of classification is probabilistic classification."
            },
            {
                "text": "Algorithms of this nature use statistical inference to find the best class for a given instance. Unlike other algorithms, which simply output a \"best\" class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes. The best class is normally then selected as the one with the highest probability. However, such an algorithm has numerous advantages over non-probabilistic classifiers: It can output a confidence value associated with its choice (in general, a classifier that can do this is known as a confidence-weighted classifier). Correspondingly, it can abstain when its confidence of choosing any particular output is too low. Because of the probabilities which are generated, probabilistic classifiers can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation. Frequentist procedures Early work on statistical classification was undertaken by Fisher, in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation.Gnanadesikan, R. (1977) Methods for Statistical Data Analysis of Multivariate Observations, Wiley."
            },
            {
                "text": "(p. 83–86) This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two groups has also been considered with a restriction imposed that the classification rule should be linear.Rao, C.R. (1952) Advanced Statistical Methods in Multivariate Analysis, Wiley. (Section 9c) Later work for the multivariate normal distribution allowed the classifier to be nonlinear:Anderson, T.W. (1958) An Introduction to Multivariate Statistical Analysis, Wiley. several classification rules can be derived based on different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation. Bayesian procedures Unlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the different groups within the overall population. Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised. Some Bayesian procedures involve the calculation of group-membership probabilities: these provide a more informative outcome than a simple attribution of a single group-label to each new observation."
            },
            {
                "text": "Binary and multiclass classification Classification can be thought of as two separate problems – binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes.Har-Peled, S., Roth, D., Zimak, D. (2003) \"Constraint Classification for Multiclass Classification and Ranking.\" In: Becker, B., Thrun, S., Obermayer, K. (Eds) Advances in Neural Information Processing Systems 15: Proceedings of the 2002 Conference, MIT Press. Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers. Feature vectors Most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance. Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or may not be statistically independent). Features may variously be binary (e.g. \"on\" or \"off\"); categorical (e.g."
            },
            {
                "text": "\"A\", \"B\", \"AB\" or \"O\", for blood type); ordinal (e.g. \"large\", \"medium\" or \"small\"); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure). If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words. Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be discretized into groups (e.g. less than 5, between 5 and 10, or greater than 10). Linear classifiers A large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights, using a dot product. The predicted category is the one with the highest score."
            },
            {
                "text": "This type of score function is known as a linear predictor function and has the following general form: where Xi is the feature vector for instance i, βk is the vector of weights corresponding to category k, and score(Xi, k) is the score associated with assigning instance i to category k. In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person i choosing category k. Algorithms with this basic setup are known as linear classifiers. What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted. Examples of such algorithms include The perceptron algorithm Algorithms Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms has been developed. The most commonly used include: Choices between different possible algorithms are frequently made on the basis of quantitative evaluation of accuracy. Application domains Classification has many applications. In some of these, it is employed as a data mining procedure, while in others more detailed statistical modeling is undertaken. identification Medical image analysis and Drug discovery and Internet Micro-array classification See also References *"
            }
        ],
        "latex_formulas": []
    },
    "Predictive_modelling": {
        "title": "Predictive_modelling",
        "chunks": [
            {
                "text": "Predictive modelling uses statistics to predict outcomes. Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place. In many cases, the model is chosen on the basis of detection theory to try to guess the probability of an outcome given a set amount of input data, for example given an email determining how likely that it is spam. Models can use one or more classifiers in trying to determine the probability of a set of data belonging to another set. For example, a model might be used to determine whether an email is spam or \"ham\" (non-spam). Depending on definitional boundaries, predictive modelling is synonymous with, or largely overlapping with, the field of machine learning, as it is more commonly referred to in academic or research and development contexts. When deployed commercially, predictive modelling is often referred to as predictive analytics."
            },
            {
                "text": "Predictive modelling is often contrasted with causal modelling/analysis. In the former, one may be entirely satisfied to make use of indicators of, or proxies for, the outcome of interest. In the latter, one seeks to determine true cause-and-effect relationships. This distinction has given rise to a burgeoning literature in the fields of research methods and statistics and to the common statement that \"correlation does not imply causation\". Models Nearly any statistical model can be used for prediction purposes. Broadly speaking, there are two classes of predictive models: parametric and non-parametric. A third class, semi-parametric models, includes features of both. Parametric models make \"specific assumptions with regard to one or more of the population parameters that characterize the underlying distribution(s)\". Non-parametric models \"typically involve fewer assumptions of structure and distributional form [than parametric models] but usually contain strong assumptions about independencies\". Applications Uplift modelling Uplift modelling is a technique for modelling the change in probability caused by an action. Typically this is a marketing action such as an offer to buy a product, to use a product more or to re-sign a contract."
            },
            {
                "text": "For example, in a retention campaign you wish to predict the change in probability that a customer will remain a customer if they are contacted. A model of the change in probability allows the retention campaign to be targeted at those customers on whom the change in probability will be beneficial. This allows the retention programme to avoid triggering unnecessary churn or customer attrition without wasting money contacting people who would act anyway. Archaeology Predictive modelling in archaeology gets its foundations from Gordon Willey's mid-fifties work in the Virú Valley of Peru.Willey, Gordon R. (1953), \"Prehistoric Settlement Patterns in the Virú Valley, Peru\", Bulletin 155. Bureau of American Ethnology Complete, intensive surveys were performed then covariability between cultural remains and natural features such as slope and vegetation were determined. Development of quantitative methods and a greater availability of applicable data led to growth of the discipline in the 1960s and by the late 1980s, substantial progress had been made by major land managers worldwide. Generally, predictive modelling in archaeology is establishing statistically valid causal or covariable relationships between natural proxies such as soil types, elevation, slope, vegetation, proximity to water, geology, geomorphology, etc., and the presence of archaeological features."
            },
            {
                "text": "Through analysis of these quantifiable attributes from land that has undergone archaeological survey, sometimes the \"archaeological sensitivity\" of unsurveyed areas can be anticipated based on the natural proxies in those areas. Large land managers in the United States, such as the Bureau of Land Management (BLM), the Department of Defense (DOD),Heidelberg, Kurt, et al. \"An Evaluation of the Archaeological Sample Survey Program at the Nevada Test and Training Range\", SRI Technical Report 02-16, 2002Jeffrey H. Altschul, Lynne Sebastian, and Kurt Heidelberg, \"Predictive Modeling in the Military: Similar Goals, Divergent Paths\", Preservation Research Series 1, SRI Foundation, 2004 and numerous highway and parks agencies, have successfully employed this strategy. By using predictive modelling in their cultural resource management plans, they are capable of making more informed decisions when planning for activities that have the potential to require ground disturbance and subsequently affect archaeological sites. Customer relationship management Predictive modelling is used extensively in analytical customer relationship management and data mining to produce customer-level models that describe the likelihood that a customer will take a particular action."
            },
            {
                "text": "The actions are usually sales, marketing and customer retention related. For example, a large consumer organization such as a mobile telecommunications operator will have a set of predictive models for product cross-sell, product deep-sell (or upselling) and churn. It is also now more common for such an organization to have a model of savability using an uplift model. This predicts the likelihood that a customer can be saved at the end of a contract period (the change in churn probability) as opposed to the standard churn prediction model. Auto insurance Predictive modelling is utilised in vehicle insurance to assign risk of incidents to policy holders from information obtained from policy holders. This is extensively employed in usage-based insurance solutions where predictive models utilise telemetry-based data to build a model of predictive risk for claim likelihood. Black-box auto insurance predictive models utilise GPS or accelerometer sensor input only. Some models include a wide range of predictive input beyond basic telemetry including advanced driving behaviour, independent crash records, road history, and user profiles to provide improved risk models. Health care In 2009 Parkland Health & Hospital System began analyzing electronic medical records in order to use predictive modeling to help identify patients at high risk of readmission."
            },
            {
                "text": "Initially, the hospital focused on patients with congestive heart failure, but the program has expanded to include patients with diabetes, acute myocardial infarction, and pneumonia. In 2018, Banerjee et al. proposed a deep learning model for estimating short-term life expectancy (>3 months) of the patients by analyzing free-text clinical notes in the electronic medical record, while maintaining the temporal visit sequence. The model was trained on a large dataset (10,293 patients) and validated on a separated dataset (1818 patients). It achieved an area under the ROC (Receiver Operating Characteristic) curve of 0.89. To provide explain-ability, they developed an interactive graphical tool that may improve physician understanding of the basis for the model's predictions. The high accuracy and explain-ability of the PPES-Met model may enable the model to be used as a decision support tool to personalize metastatic cancer treatment and provide valuable assistance to physicians. The first clinical prediction model reporting guidelines were published in 2015 (Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD)), and have since been updated."
            },
            {
                "text": "Predictive modelling has been used to estimate surgery duration. Algorithmic trading Predictive modeling in trading is a modeling process wherein the probability of an outcome is predicted using a set of predictor variables. Predictive models can be built for different assets like stocks, futures, currencies, commodities etc. Predictive modeling is still extensively used by trading firms to devise strategies and trade. It utilizes mathematically advanced software to evaluate indicators on price, volume, open interest and other historical data, to discover repeatable patterns. Lead tracking systems Predictive modelling gives lead generators a head start by forecasting data-driven outcomes for each potential campaign. This method saves time and exposes potential blind spots to help client make smarter decisions. Notable failures of predictive modeling Although not widely discussed by the mainstream predictive modeling community, predictive modeling is a methodology that has been widely used in the financial industry in the past and some of the major failures contributed to the financial crisis of 2007–2008. These failures exemplify the danger of relying exclusively on models that are essentially backward looking in nature. The following examples are by no mean a complete list: Bond rating."
            },
            {
                "text": "S&P, Moody's and Fitch quantify the probability of default of bonds with discrete variables called rating. The rating can take on discrete values from AAA down to D. The rating is a predictor of the risk of default based on a variety of variables associated with the borrower and historical macroeconomic data. The rating agencies failed with their ratings on the US$600 billion mortgage backed Collateralized Debt Obligation (CDO) market. Almost the entire AAA sector (and the super-AAA sector, a new rating the rating agencies provided to represent super safe investment) of the CDO market defaulted or severely downgraded during 2008, many of which obtained their ratings less than just a year previously. So far, no statistical models that attempt to predict equity market prices based on historical data are considered to consistently make correct predictions over the long term. One particularly memorable failure is that of Long Term Capital Management, a fund that hired highly qualified analysts, including a Nobel Memorial Prize in Economic Sciences winner, to develop a sophisticated statistical model that predicted the price spreads between different securities."
            },
            {
                "text": "The models produced impressive profits until a major debacle that caused the then Federal Reserve chairman Alan Greenspan to step in to broker a rescue plan by the Wall Street broker dealers in order to prevent a meltdown of the bond market. Possible fundamental limitations of predictive models based on data fitting History cannot always accurately predict the future. Using relations derived from historical data to predict the future implicitly assumes there are certain lasting conditions or constants in a complex system. This almost always leads to some imprecision when the system involves people. Unknown unknowns are an issue. In all data collection, the collector first defines the set of variables for which data is collected. However, no matter how extensive the collector considers his/her selection of the variables, there is always the possibility of new variables that have not been considered or even defined, yet are critical to the outcome. Algorithms can be defeated adversarially. After an algorithm becomes an accepted standard of measurement, it can be taken advantage of by people who understand the algorithm and have the incentive to fool or manipulate the outcome. This is what happened to the CDO rating described above. The CDO dealers actively fulfilled the rating agencies' input to reach an AAA or super-AAA on the CDO they were issuing, by cleverly manipulating variables that were \"unknown\" to the rating agencies' \"sophisticated\" models. See also Calibration (statistics) Prediction interval Predictive analytics Predictive inference Statistical learning theory Statistical model References Further reading Category:Statistical classification Category:Statistical models Category:Predictive analytics Category:Business intelligence"
            }
        ],
        "latex_formulas": []
    },
    "Machine_learning": {
        "title": "Machine_learning",
        "chunks": [
            {
                "text": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning. History The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.R. Kohavi and F. Provost, \"Glossary of terms\", Machine Learning, vol."
            },
            {
                "text": "30, no. 2–3, pp. 271–274, 1998. The synonym self-teaching computers was also used in this time period. Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning."
            },
            {
                "text": "It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. \"Science: The Goof Button\", Time (magazine), 18 August 1961. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.Nilsson N. Learning Machines, McGraw Hill, 1965. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.Duda, R., Hart P. Pattern Recognition and Scene Analysis, Wiley Interscience, 1973 In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.S. Bozinovski \"Teaching space: A representation concept for adaptive pattern classification\" COINS Technical Report No. 81-28, Computer and Information Science Department, University of Massachusetts at Amherst, MA, 1981. https://web.cs.umass.edu/publication/docs/1981/UM-CS-1981-028.pdf Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms."
            },
            {
                "text": "This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\". Modern-day machine learning has two objectives. One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions. Relationships to other fields Artificial intelligence thumb|Machine learning as subfield of AI As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics."
            },
            {
                "text": "Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature."
            },
            {
                "text": "It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. Data compression Data mining Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge."
            },
            {
                "text": "Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimization: Many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples). Generalization Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms. Statistics Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics."
            },
            {
                "text": "He also suggested the term data science as a placeholder to call the overall field. Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.Hung et al. Algorithms to Measure Surgeon Performance and Anticipate Clinical Outcomes in Robotic Surgery. JAMA Surg. 2018 Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning. Statistical physics Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks."
            },
            {
                "text": "Statistical physics is thus finding applications in the area of medical diagnostics. Theory A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error. For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data."
            },
            {
                "text": "If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer. In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. Approaches In supervised learning, the training data is labeled with the expected answers, while in unsupervised learning, the model identifies patterns or structures in unlabeled data. Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system: Supervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs."
            },
            {
                "text": "Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize. Although each algorithm has advantages and limitations, no single algorithm works for all problems. Supervised learning thumb|A support-vector machine is a supervised learning model that divides the data into regions separated by a linear boundary. Here, the linear boundary divides the black circles from the white. Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal."
            },
            {
                "text": "In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data."
            },
            {
                "text": "Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters."
            },
            {
                "text": "Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself. Semi-supervised learning Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning right|frameless Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms."
            },
            {
                "text": "In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Dimensionality reduction Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization."
            },
            {
                "text": "Other types Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning. Self-learning Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).Bozinovski, S. (1982). \"A self-learning system using secondary reinforcement\". In Trappl, Robert (ed.). Cybernetics and Systems Research: Proceedings of the Sixth European Meeting on Cybernetics and Systems Research. North-Holland. pp. 397–402. .Bozinovski, S. (1999) \"Crossbar Adaptive Array: The first connectionist network that solved the delayed reinforcement learning problem\" In A. Dobnikar, N. Steele, D. Pearson, R. Albert (eds.) Artificial Neural Networks and Genetic Algorithms, Springer Verlag, p. 320-325, ISBN 3-211-83364-1 It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent."
            },
            {
                "text": "The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.Bozinovski, Stevo (2014) \"Modeling mechanisms of cognition-emotion interaction in artificial neural networks, since 1981.\" Procedia Computer Science p. 255-263 The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: in situation s perform action a receive a consequence situation s compute emotion of being in the consequence situation v(s') update crossbar memory w'(a,s) = w(a,s) + v(s') It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment."
            },
            {
                "text": "After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.Bozinovski, S. (2001) \"Self-learning agents: A connectionist theory of emotion based on crossbar value judgment.\" Cybernetics and Systems 32(6) 637–667. Feature learning Several learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data."
            },
            {
                "text": "Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features."
            },
            {
                "text": "An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.Aharon, M, M Elad, and A Bruckstein. 2006. \"K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation .\" Signal Processing, IEEE Transactions on 54 (11): 4311–4322 Anomaly detection In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data."
            },
            {
                "text": "Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions. In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns. Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection)."
            },
            {
                "text": "Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model. Robot learning Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML). Association rules Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".Piatetsky-Shapiro, Gregory (1991), Discovery, analysis, and presentation of strong rules, in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., Knowledge Discovery in Databases, AAAI/MIT Press, Cambridge, MA. Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system."
            },
            {
                "text": "This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule $\\{\\mathrm{onions, potatoes}\\} \\Rightarrow \\{\\mathrm{burger}\\}$ found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions."
            },
            {
                "text": "Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions. Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs. Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.Plotkin G.D. Automatic Methods of Inductive Inference , PhD thesis, University of Edinburgh, 1970.Shapiro, Ehud Y. Inductive inference of theories from facts , Research Report 192, Yale University, Department of Computer Science, 1981."
            },
            {
                "text": "Reprinted in J.-L. Lassez, G. Plotkin (Eds. ), Computational Logic, The MIT Press, Cambridge, MA, 1991, pp. 199–254.Shapiro, Ehud Y. (1983). Algorithmic program debugging. Cambridge, Mass: MIT Press. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.Shapiro, Ehud Y. \"The model inference system .\" Proceedings of the 7th international joint conference on Artificial intelligence-Volume 2. Morgan Kaufmann Publishers Inc., 1981. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. Models A ''' is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned."
            },
            {
                "text": "Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection. Artificial neural networks An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another. Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules. An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it."
            },
            {
                "text": "In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis."
            },
            {
                "text": "Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.Honglak Lee, Roger Grosse, Rajesh Ranganath, Andrew Y. Ng. \"Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations \" Proceedings of the 26th Annual International Conference on Machine Learning, 2009. Decision trees thumb|A decision tree showing survival probability of passengers on the Titanic Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees."
            },
            {
                "text": "In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making. Random forest regression Random forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data of from training set. This random selection of RFR for training enables model to reduce bias predictions and achieve accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor task. This makes RFR compatible to be used in various application. Support-vector machines Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category."
            },
            {
                "text": "An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Regression analysis Illustration of linear regression on a data set Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space. Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously."
            },
            {
                "text": "This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional. Bayesian networks A simple Bayesian network. Rain influences whether the sprinkler is activated, and both rain and the sprinkler influence whether the grass is wet. A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams."
            },
            {
                "text": "Gaussian processes thumbnail|right|An example of Gaussian Process Regression (prediction) compared with other regression modelsThe documentation for scikit-learn also has similar examples . A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations. Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point. Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization. Genetic algorithms A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s."
            },
            {
                "text": "Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms. Belief functions The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g., Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches."
            },
            {
                "text": "Rule-based models Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems, association rule learning,Zhang, C. and Zhang, S., 2002. Association rule mining: models and algorithms. Springer-Verlag. artificial immune systems,De Castro, Leandro Nunes, and Jonathan Timmis. Artificial immune systems: a new computational intelligence approach. Springer Science & Business Media, 2002. and other similar models. These methods extract patterns from data and evolve rules over time. Training models Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service."
            },
            {
                "text": "Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams. Federated learning Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google. Applications There are many applications for machine learning, including: Agriculture Anatomy Adaptive website Affective computing Astronomy Automated decision-making Banking Behaviorism Bioinformatics Brain–machine interfaces Cheminformatics Citizen Science Climate Science Computer networks Computer vision Credit-card fraud detection Data quality DNA sequence classification Economics Financial market analysisMachine learning is included in the CFA Curriculum (discussion is top-down); see: Kathleen DeRose and Christophe Le Lanno (2020)."
            },
            {
                "text": "\"Machine Learning\" . General game playing Handwriting recognition Healthcare Information retrieval Insurance Internet fraud detection Knowledge graph embedding Linguistics Machine learning control Machine perception Machine translation Material Engineering Marketing Medical diagnosis Natural language processing Natural language understanding Online advertising Optimization Recommender systems Robot locomotion Search engines Sentiment analysis Sequence mining Software engineering Speech recognition Structural health monitoring Syntactic pattern recognition Telecommunications Theorem proving Time-series forecasting Tomographic reconstruction User behavior analytics In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. \"BelKor Home Page\" research.att.com Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis."
            },
            {
                "text": "In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists.When A Machine Learning Algorithm Studied Fine Art Paintings, It Saw Things Art Historians Had Never Noticed , The Physics at ArXiv blog In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behavior of travelers. Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS."
            },
            {
                "text": "Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes. Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires. Limitations Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems. The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data."
            },
            {
                "text": "The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes. In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users. Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves. Explainability Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI."
            },
            {
                "text": "It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. Overfitting thumb|The blue line could be an example of overfitting a linear function due to random noise. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is. Other limitations and vulnerabilities Learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects."
            },
            {
                "text": "Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies. Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning. Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access. Model assessments Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model."
            },
            {
                "text": "In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy. In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model. Ethics Bias Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices."
            },
            {
                "text": "For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data. While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world."
            },
            {
                "text": "Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI. Language models learned from data have been shown to contain human-like biases. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language. In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\". In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognize gorillas. Similar issues with recognizing non-white people have been found in many other systems. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains."
            },
            {
                "text": "Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financial incentives There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated."
            },
            {
                "text": "Hardware Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months. Tensor Processing Units (TPUs) Tensor Processing Units (TPUs) are specialized hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimized for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency."
            },
            {
                "text": "Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments. Neuromorphic computing Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialized hardware architectures. physical neural networks A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses. Embedded machine learning Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets."
            },
            {
                "text": "Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimization. Common optimization techniques include pruning, quantization, knowledge distillation, low-rank factorization, network architecture search, and parameter sharing. Software Software suites containing a variety of machine learning algorithms include the following: Free and open-source software Caffe Deeplearning4j DeepSpeed ELKI Google JAX Infer.NET Keras Kubeflow LightGBM Mahout Mallet Microsoft Cognitive Toolkit ML.NET mlpack MXNet OpenNN Orange pandas (software) ROOT (TMVA with ROOT) scikit-learn Shogun Spark MLlib SystemML TensorFlow Torch / PyTorch Weka / MOA XGBoost Yooreeka Proprietary software with free and open-source editions KNIME RapidMiner Proprietary software Amazon Machine Learning Angoss KnowledgeSTUDIO Azure Machine Learning IBM Watson Studio Google Cloud Vertex AI Google Prediction API IBM SPSS Modeler KXEN Modeler LIONsolver Mathematica MATLAB Neural Designer NeuroSolutions Oracle Data Mining Oracle AI Platform Cloud Service PolyAnalyst RCASE SAS Enterprise Miner SequenceL Splunk STATISTICA Data Miner Journals Journal of Machine Learning Research Machine Learning Nature Machine Intelligence Neural Computation IEEE Transactions on Pattern Analysis and Machine Intelligence Conferences AAAI Conference on Artificial Intelligence Association for Computational Linguistics (ACL) European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB) International Conference on Machine Learning (ICML) International Conference on Learning Representations (ICLR) International Conference on Intelligent Robots and Systems (IROS) Conference on Knowledge Discovery and Data Mining (KDD) Conference on Neural Information Processing Systems (NeurIPS) See also Deep learning — branch of ML concerned with artificial neural networks M-theory (learning framework) Machine unlearning References Sources ."
            },
            {
                "text": "Further reading Alpaydin, Ethem (2020). Introduction to Machine Learning, (4th edition) MIT Press, . Bishop, Christopher (1995). Neural Networks for Pattern Recognition, Oxford University Press. . Bishop, Christopher (2006) Pattern Recognition and Machine Learning, Springer. Domingos, Pedro (September 2015), The Master Algorithm, Basic Books, Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001) Pattern classification (2nd edition), Wiley, New York, . Hastie, Trevor; Tibshirani, Robert & Friedman, Jerome H. (2009) The Elements of Statistical Learning, Springer. . MacKay, David J. C. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. Murphy, Kevin P. (2021). Probabilistic Machine Learning: An Introduction , MIT Press. Nilsson, Nils J. (2015) Introduction to Machine Learning . Russell, Stuart & Norvig, Peter (2020). Artificial Intelligence – A Modern Approach. (4th edition) Pearson, . Solomonoff, Ray, (1956) An Inductive Inference Machine A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI. Witten, Ian H. & Frank, Eibe (2011). Data Mining: Practical machine learning tools and techniques'' Morgan Kaufmann, 664pp., . External links International Machine Learning Society mloss is an academic database of open-source machine learning software. Category:Cybernetics Category:Learning Category:Definition"
            }
        ],
        "latex_formulas": [
            "\\{\\mathrm{onions, potatoes}\\} \\Rightarrow \\{\\mathrm{burger}\\}"
        ]
    },
    "Text_classification": {
        "title": "Text_classification",
        "chunks": [
            {
                "text": "REDIRECT Document classification"
            }
        ],
        "latex_formulas": []
    },
    "Natural_language_processing": {
        "title": "Natural_language_processing",
        "chunks": [
            {
                "text": "Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation. History Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. Symbolic NLP (1950s – early 1990s) The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts."
            },
            {
                "text": "1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe\"ALPAC: the (in)famous report\", John Hutchins, MT News International, no. 14, June 1996, pp. 9–12.) until the late 1980s when the first statistical machine translation systems were developed. 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction."
            },
            {
                "text": "When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time., see also : \"Early programs were necessarily limited in scope by the size and speed of memory\" 1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY). 1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP."
            },
            {
                "text": "Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering TheoryJoshi, A. K., & Weinstein, S. (1981, August). Control of Inference: Role of Some Aspects of Discourse Structure-Centering. In IJCAI (pp. 385–387).) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period. Statistical NLP (1990s–present) Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g."
            },
            {
                "text": "transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.Chomskyan linguistics encourages the investigation of \"corner cases\" that stress the limits of its theoretical models (comparable to pathological phenomena in mathematics), typically created using thought experiments, rather than the systematic investigation of typical phenomena that occur in real-world data, as is the case in corpus linguistics. The creation and use of such corpora of real-world data is a fundamental part of machine-learning algorithms for natural language processing. In addition, theoretical underpinnings of Chomskyan linguistics such as the so-called \"poverty of the stimulus\" argument entail that general learning algorithms, as are typically used in machine learning, cannot be successful in language processing. As a result, the Chomskyan paradigm discouraged the application of such models to language processing. 1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government."
            },
            {
                "text": "However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data. 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical. 2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)"
            },
            {
                "text": "2010: Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling, and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy. Approaches: Symbolic, statistical, neural networks Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: such as by writing grammars or devising heuristic rules for stemming. Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally."
            },
            {
                "text": "language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce. the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems. Rule-based systems are commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization, or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses. Statistical approach In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.Mark Johnson."
            },
            {
                "text": "How the statistical revolution changes (computational) linguistics. Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics.Philip Resnik. Four revolutions. Language Log, February 5, 2011. The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach. Neural networks A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015, This was an early Deep Learning tutorial at the ACL 2012 and met with both interest and (at the time) skepticism by most participants. Until then, neural learning was basically rejected because of its lack of statistical interpretability. Until 2015, deep learning had evolved into the major framework of NLP. [Link is broken, try http://web.stanford.edu/class/cs224n/] the statistical approach has been replaced by the neural networks approach, using semantic networks and word embeddings to capture semantic properties of words. Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore."
            },
            {
                "text": "Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation. Common NLP tasks The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. Text and speech processing Optical character recognition (OCR) Given an image representing printed text, determine the corresponding text. Speech recognition Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process."
            },
            {
                "text": "Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. Speech segmentation Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it. Text-to-speech Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired. Word segmentation (Tokenization) Tokenization is a process used in text analysis that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language."
            },
            {
                "text": "Sometimes this process is also used in cases like bag of words (BOW) creation in data mining. Morphological analysis Lemmatization The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form. Morphological segmentation Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms."
            },
            {
                "text": "Part-of-speech tagging Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech. Stemming The process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary. Syntactic analysis Grammar induction Generate a formal grammar that describes a language's syntax. Sentence breaking (also known as \"sentence boundary disambiguation\") Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations)."
            },
            {
                "text": "Parsing Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar). Lexical semantics (of individual words in context) Lexical semantics What is the computational meaning of individual words in context? Distributional semantics How can we learn semantic representations from data? Named entity recognition (NER) Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient."
            },
            {
                "text": "For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. Another name for this task is token classification. Sentiment analysis (see also Multimodal sentiment analysis) Sentiment analysis is a computational method used to identify and classify the emotional intent behind text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms."
            },
            {
                "text": "Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet. Entity linking Many words—typically proper names—refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context. Relational semantics (semantics of individual sentences) Relationship extraction Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom). Semantic parsing Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below)."
            },
            {
                "text": "Semantic role labelling (see also implicit semantic role labelling below) Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles). Discourse (semantics beyond individual sentences) Coreference resolution Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to). Discourse analysis This rubric includes several related tasks."
            },
            {
                "text": "One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no question, content question, statement, assertion, etc.). Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages. Recognizing textual entailment Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.PASCAL Recognizing Textual Entailment Challenge (RTE-7) https://tac.nist.gov//2011/RTE/ Topic segmentation and recognition Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment."
            },
            {
                "text": "Argument mining The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse. Higher-level NLP applications Automatic summarization (text summarization) Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper. Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications."
            },
            {
                "text": "Logic translation Translate a text from a natural language into formal logic. Machine translation (MT) Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly. Natural-language understanding (NLU) Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization."
            },
            {
                "text": "Natural-language generation (NLG): Convert information from computer databases or semantic intents into readable human language. Book generation Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. Document AI A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants."
            },
            {
                "text": "Dialogue management Computer systems intended to converse with a human. Question answering Given a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada? \"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Text-to-image generation Given a description of an image, generate an image that matches the description. Text-to-scene generation Given a description of a scene, generate a 3D model of the scene. Text-to-video Given a description of a video, generate a video that matches the description. General tendencies and (possible) future directions Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: Interest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing)."
            },
            {
                "text": "Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages) Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems) Cognition Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics."
            },
            {
                "text": "Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects: Apply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience. When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance. The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information. Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG)."
            },
            {
                "text": "The mathematical equation for such algorithms is presented in US Patent 9269353: $ {RMM(token_N)} = {PMM(token_N)} \\times \\frac{1}{2d} \\left (\\sum_{i=-d}^d {((PMM(token_{N})} \\times {PF(token_{N-i},token_N,token_{N+i}))_i}\\right ) $ Where RMM is the relative measure of meaning token is any block of text, sentence, phrase or word N is the number of tokens being analyzed PMM is the probable measure of meaning based on a corpora d is the non zero location of the token along the sequence of N tokens PF is the probability function specific to a language Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar,Rodríguez, F. C., & Mairal-Usón, R. (2016)."
            },
            {
                "text": "Building an RRG computational grammar. Onomazein, (34), 86–117. construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) and developments in artificial intelligence, specifically tools and technologies using large language model approaches and new directions in artificial general intelligence based on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston. See also 1 the Road Artificial intelligence detection software Automated essay scoring Biomedical text mining Compound term processing Computational linguistics Computer-assisted reviewing Controlled natural language Deep learning Deep linguistic processing Distributional semantics Foreign language reading aid Foreign language writing aid Information extraction Information retrieval Language and Communication Technologies Language model Language technology Latent semantic indexing Multi-agent system Native-language identification Natural-language programming Natural-language understanding Natural-language search Outline of natural language processing Query expansion Query understanding Reification (linguistics) Speech processing Spoken dialogue systems Text-proofing Text simplification Transformer (machine learning model) Truecasing Question answering Word2vec References Further reading Steven Bird, Ewan Klein, and Edward Loper (2009)."
            },
            {
                "text": "Natural Language Processing with Python. O'Reilly Media. . Kenna Hughes-Castleberry, \"A Murder Mystery Puzzle: The literary puzzle Cain's Jawbone, which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\", Scientific American, vol. 329, no. 4 (November 2023), pp. 81–82. \"This murder mystery competition has revealed that although NLP (natural-language processing) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages. In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose.\" (p. 82.) Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing, 2nd edition. Pearson Prentice Hall. . Mohamed Zakaria Kurdi (2016). Natural Language Processing and Computational Linguistics: speech, morphology, and syntax, Volume 1. ISTE-Wiley. . Mohamed Zakaria Kurdi (2017). Natural Language Processing and Computational Linguistics: semantics, discourse, and applications, Volume 2. ISTE-Wiley. . Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (2008). Introduction to Information Retrieval. Cambridge University Press. . Official html and pdf versions available without charge. Christopher D. Manning and Hinrich Schütze (1999). Foundations of Statistical Natural Language Processing. The MIT Press. . David M. W. Powers and Christopher C. R. Turk (1989). Machine Learning of Natural Language."
            },
            {
                "text": "Springer-Verlag. . External links Category:Computational fields of study Category:Computational linguistics Category:Speech recognition"
            }
        ],
        "latex_formulas": [
            "{RMM(token_N)}\n=\n{PMM(token_N)} \n\\times  \n\\frac{1}{2d}\n\\left (\\sum_{i=-d}^d {((PMM(token_{N})} \\times {PF(token_{N-i},token_N,token_{N+i}))_i}\\right )"
        ]
    },
    "Supervised_learning": {
        "title": "Supervised_learning",
        "chunks": [
            {
                "text": "In supervised learning, the training data is labeled with the expected answers, while in unsupervised learning, the model identifies patterns or structures in unlabeled data. In machine learning, supervised learning (SL) is a paradigm where a model is trained using input objects (e.g. a vector of predictor variables) and desired output values (also known as a supervisory signal), which are often human-made labels. The training process builds a function that maps new data to expected output values.Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar (2012) Foundations of Machine Learning, The MIT Press . An optimal scenario will allow for the algorithm to accurately determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a reasonable way (see inductive bias). This statistical quality of an algorithm is measured via a generalization error. Steps to follow To solve a given problem of supervised learning, the following steps must be performed: Determine the type of training samples. Before doing anything else, the user should decide what kind of data is to be used as a training set."
            },
            {
                "text": "In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting, or a full paragraph of handwriting. Gather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered together with corresponding outputs, either from human experts or from measurements. Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output. Determine the structure of the learned function and corresponding learning algorithm. For example, one may choose to use support-vector machines or decision trees. Complete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters."
            },
            {
                "text": "These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation. Evaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set. Algorithm choice A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem). There are four major issues to consider in supervised learning: Bias–variance tradeoff A first issue is the tradeoff between bias and variance.S. Geman, E. Bienenstock, and R. Doursat (1992). Neural networks and the bias/variance dilemma. Neural Computation 4, 1–58. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input $x$ if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for $x$."
            },
            {
                "text": "A learning algorithm has high variance for a particular input $x$ if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.G. James (2003) Variance and Bias for General Loss Functions, Machine Learning 51, 115-135. (http://www-bcf.usc.edu/~gareth/research/bv.pdf) Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust). Function complexity and amount of training data The second issue is of the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function). If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data."
            },
            {
                "text": "But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a \"flexible\" learning algorithm with low bias and high variance. Dimensionality of the input space A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones."
            },
            {
                "text": "This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm. Noise in the output values A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \"corrupts\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator. In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm."
            },
            {
                "text": "There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.C.E. Brodely and M.A. Friedl (1999). Identifying and Eliminating Mislabeled Training Instances, Journal of Artificial Intelligence Research 11, 131-167. (http://jair.org/media/606/live-606-1803-jair.pdf) Other factors to consider Other factors to consider when choosing and applying a learning algorithm include the following: Heterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including support-vector machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support-vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data. Redundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance-based methods) will perform poorly because of numerical instabilities."
            },
            {
                "text": "These problems can often be solved by imposing some form of regularization. Presence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, support-vector machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support-vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them. When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross-validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms."
            },
            {
                "text": "Algorithms The most widely used learning algorithms are: Support-vector machines Linear regression Logistic regression Naive Bayes Linear discriminant analysis Decision trees k-nearest neighbors algorithm Neural networks (e.g., Multilayer perceptron) Similarity learning How supervised learning algorithms work Given a set of $N$ training examples of the form $\\{(x_1, y_1), ..., (x_N,\\; y_N)\\}$ such that $x_i$ is the feature vector of the $i$-th example and $y_i$ is its label (i.e., class), a learning algorithm seeks a function $g: X \\to Y$, where $X$ is the input space and $Y$ is the output space."
            },
            {
                "text": "The function $g$ is an element of some space of possible functions $G$, usually called the hypothesis space. It is sometimes convenient to represent $g$ using a scoring function $f: X \\times Y \\to \\mathbb{R}$ such that $g$ is defined as returning the $y$ value that gives the highest score: $g(x) = \\underset{y}{\\arg\\max} \\; f(x,y)$. Let $F$ denote the space of scoring functions. Although $G$ and $F$ can be any space of functions, many learning algorithms are probabilistic models where $g$ takes the form of a conditional probability model $g(x) = \\underset{y}{\\arg\\max} \\; P(y|x)$, or $f$ takes the form of a joint probability model $f(x,y) = P(x,y)$. For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model."
            },
            {
                "text": "There are two basic approaches to choosing $f$ or $g$: empirical risk minimization and structural risk minimization.Vapnik, V. N. The Nature of Statistical Learning Theory (2nd Ed. ), Springer Verlag, 2000. Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff. In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, $(x_i, \\;y_i)$. In order to measure how well a function fits the training data, a loss function $L: Y \\times Y \\to \\mathbb{R}^{\\ge 0}$ is defined. For training example $(x_i,\\;y_i)$, the loss of predicting the value $\\hat{y}$ is $L(y_i,\\hat{y})$. The risk $R(g)$ of function $g$ is defined as the expected loss of $g$. This can be estimated from the training data as $R_{emp}(g) = \\frac{1}{N} \\sum_i L(y_i, g(x_i))$."
            },
            {
                "text": "Empirical risk minimization In empirical risk minimization, the supervised learning algorithm seeks the function $g$ that minimizes $R(g)$. Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find $g$. When $g$ is a conditional probability distribution $P(y|x)$ and the loss function is the negative log likelihood: $L(y, \\hat{y}) = -\\log P(y | x)$, then empirical risk minimization is equivalent to maximum likelihood estimation. When $G$ contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well (overfitting). Structural risk minimization Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones. A wide variety of penalties have been employed that correspond to different definitions of complexity."
            },
            {
                "text": "For example, consider the case where the function $g$ is a linear function of the form $ g(x) = \\sum_{j=1}^d \\beta_j x_j$. A popular regularization penalty is $\\sum_j \\beta_j^2$, which is the squared Euclidean norm of the weights, also known as the $L_2$ norm. Other norms include the $L_1$ norm, $\\sum_j |\\beta_j|$, and the $L_0$ \"norm\", which is the number of non-zero $\\beta_j$s. The penalty will be denoted by $C(g)$. The supervised learning optimization problem is to find the function $g$ that minimizes $ J(g) = R_{emp}(g) + \\lambda C(g).$ The parameter $\\lambda$ controls the bias-variance tradeoff. When $\\lambda = 0$, this gives empirical risk minimization with low bias and high variance. When $\\lambda$ is large, the learning algorithm will have high bias and low variance. The value of $\\lambda$ can be chosen empirically via cross-validation."
            },
            {
                "text": "The complexity penalty has a Bayesian interpretation as the negative log prior probability of $g$, $-\\log P(g)$, in which case $J(g)$ is the posterior probability of $g$. Generative training The training methods described above are discriminative training methods, because they seek to find a function $g$ that discriminates well between the different output values (see discriminative model). For the special case where $f(x,y) = P(x,y)$ is a joint probability distribution and the loss function is the negative log likelihood $- \\sum_i \\log P(x_i, y_i),$ a risk minimization algorithm is said to perform generative training, because $f$ can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis. Generalizations Tendency for a task to employ supervised vs. unsupervised methods."
            },
            {
                "text": "Task names straddling circle boundaries is intentional. It shows that the classical division of imaginative tasks (left) employing unsupervised methods is blurred in today's learning schemes.There are several ways in which the standard supervised learning problem can be generalized: Semi-supervised learning or weak supervision: the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Active learning: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user. Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning. Structured prediction: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended. Learning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended. Approaches and algorithms Analytical learning Artificial neural network Backpropagation Boosting (meta-algorithm) Bayesian statistics Case-based reasoning Decision tree learning Inductive logic programming Gaussian process regression Genetic programming Group method of data handling Kernel estimators Learning automata Learning classifier systems Learning vector quantization Minimum message length (decision trees, decision graphs, etc.)"
            },
            {
                "text": "Multilinear subspace learning Naive Bayes classifier Maximum entropy classifier Conditional random field Nearest neighbor algorithm Probably approximately correct learning (PAC) learning Ripple down rules, a knowledge acquisition methodology Symbolic machine learning algorithms Subsymbolic machine learning algorithms Support vector machines Minimum complexity machines (MCM) Random forests Ensembles of classifiers Ordinal classification Data pre-processing Handling imbalanced datasets Statistical relational learning Proaftn, a multicriteria classification algorithm Applications Bioinformatics Cheminformatics Quantitative structure–activity relationship Database marketing Handwriting recognition Information retrieval Learning to rank Information extraction Object recognition in computer vision Optical character recognition Spam detection Pattern recognition Speech recognition Supervised learning is a special case of downward causation in biological systems Landform classification using satellite imagery Spend classification in procurement processes General issues Computational learning theory Inductive bias Overfitting (Uncalibrated) class membership probabilities Version spaces See also List of datasets for machine-learning research Unsupervised learning References External links Machine Learning Open Source Software (MLOSS)"
            }
        ],
        "latex_formulas": [
            "x",
            "x",
            "x",
            "N",
            "\\{(x_1, y_1), ..., (x_N,\\; y_N)\\}",
            "x_i",
            "i",
            "y_i",
            "g: X \\to Y",
            "X",
            "Y",
            "g",
            "G",
            "g",
            "f: X \\times Y \\to \\mathbb{R}",
            "g",
            "y",
            "g(x) = \\underset{y}{\\arg\\max} \\; f(x,y)",
            "F",
            "G",
            "F",
            "g",
            "g(x) = \\underset{y}{\\arg\\max} \\; P(y|x)",
            "f",
            "f(x,y) = P(x,y)",
            "f",
            "g",
            "(x_i, \\;y_i)",
            "L: Y \\times Y \\to\n\\mathbb{R}^{\\ge 0}",
            "(x_i,\\;y_i)",
            "\\hat{y}",
            "L(y_i,\\hat{y})",
            "R(g)",
            "g",
            "g",
            "R_{emp}(g) = \\frac{1}{N} \\sum_i L(y_i, g(x_i))",
            "g",
            "R(g)",
            "g",
            "g",
            "P(y|x)",
            "L(y, \\hat{y}) = -\\log P(y | x)",
            "G",
            "g",
            "g(x) = \\sum_{j=1}^d \\beta_j x_j",
            "\\sum_j \\beta_j^2",
            "L_2",
            "L_1",
            "\\sum_j |\\beta_j|",
            "L_0",
            "\\beta_j",
            "C(g)",
            "g",
            "J(g) = R_{emp}(g) + \\lambda C(g).",
            "\\lambda",
            "\\lambda = 0",
            "\\lambda",
            "\\lambda",
            "g",
            "-\\log P(g)",
            "J(g)",
            "g",
            "g",
            "f(x,y) = P(x,y)",
            "- \\sum_i \\log P(x_i, y_i),",
            "f"
        ]
    },
    "Feature_(machine_learning)": {
        "title": "Feature_(machine_learning)",
        "chunks": [
            {
                "text": "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a data set. Choosing informative, discriminating, and independent features is crucial to produce effective algorithms for pattern recognition, classification, and regression tasks. Features are usually numeric, but other types such as strings and graphs are used in syntactic pattern recognition, after some pre-processing step such as one-hot encoding. The concept of \"features\" is related to that of explanatory variables used in statistical techniques such as linear regression. Feature types In feature engineering, two types of features are commonly used: numerical and categorical. Numerical features are continuous values that can be measured on a scale. Examples of numerical features include age, height, weight, and income. Numerical features can be used in machine learning algorithms directly. Categorical features are discrete values that can be grouped into categories. Examples of categorical features include gender, color, and zip code. Categorical features typically need to be converted to numerical features before they can be used in machine learning algorithms. This can be done using a variety of techniques, such as one-hot encoding, label encoding, and ordinal encoding."
            },
            {
                "text": "The type of feature that is used in feature engineering depends on the specific machine learning algorithm that is being used. Some machine learning algorithms, such as decision trees, can handle both numerical and categorical features. Other machine learning algorithms, such as linear regression, can only handle numerical features. Classification A numeric feature can be conveniently described by a feature vector. One way to achieve binary classification is using a linear predictor function (related to the perceptron) with a feature vector as input. The method consists of calculating the scalar product between the feature vector and a vector of weights, qualifying those observations whose result exceeds a threshold. Algorithms for classification from a feature vector include nearest neighbor classification, neural networks, and statistical techniques such as Bayesian approaches. Examples In character recognition, features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others. In speech recognition, features for recognizing phonemes can include noise ratios, length of sounds, relative power, filter matches and many others."
            },
            {
                "text": "In spam detection algorithms, features may include the presence or absence of certain email headers, the email structure, the language, the frequency of specific terms, the grammatical correctness of the text. In computer vision, there are a large number of possible features, such as edges and objects. Feature vectors In pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis. When representing images, the feature values might correspond to the pixels of an image, while when representing texts the features might be the frequencies of occurrence of textual terms. Feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression. Feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction. The vector space associated with these vectors is often called the feature space. In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed."
            },
            {
                "text": "Higher-level features can be obtained from already available features and added to the feature vector; for example, for the study of diseases the feature 'Age' is useful and is defined as Age = 'Year of death' minus 'Year of birth' . This process is referred to as feature construction.Liu, H., Motoda H. (1998) Feature Selection for Knowledge Discovery and Data Mining., Kluwer Academic Publishers. Norwell, MA, USA. 1998.Piramuthu, S., Sikora R. T. Iterative feature construction for improving inductive learning algorithms. In Journal of Expert Systems with Applications. Vol. 36 , Iss. 2 (March 2009), pp. 3401-3406, 2009 Feature construction is the application of a set of constructive operators to a set of existing features resulting in construction of new features. Examples of such constructive operators include checking for the equality conditions {=, ≠}, the arithmetic operators {+,−,×, /}, the array operators {max(S), min(S), average(S)} as well as other more sophisticated operators, for example count(S,C)Bloedorn, E., Michalski, R. Data-driven constructive induction: a methodology and its applications."
            },
            {
                "text": "IEEE Intelligent Systems, Special issue on Feature Transformation and Subset Selection, pp. 30-37, March/April, 1998 that counts the number of features in the feature vector S satisfying some condition C or, for example, distances to other recognition classes generalized by some accepting device. Feature construction has long been considered a powerful tool for increasing both accuracy and understanding of structure, particularly in high-dimensional problems.Breiman, L. Friedman, T., Olshen, R., Stone, C. (1984) Classification and regression trees, Wadsworth Applications include studies of disease and emotion recognition from speech.Sidorova, J., Badia T. Syntactic learning for ESEDA.1, tool for enhanced speech emotion detection and analysis. Internet Technology and Secured Transactions Conference 2009 (ICITST-2009), London, November 9–12. IEEE Selection and extraction The initial set of raw features can be redundant and large enough that estimation and optimization is made difficult or ineffective. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and interpretability. Extracting or selecting features is a combination of art and science; developing systems to do so is known as feature engineering. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert. Automating this process is feature learning, where a machine not only uses features for learning, but learns the features itself."
            },
            {
                "text": "See also Covariate Dimensionality reduction Feature engineering Hashing trick Statistical classification Explainable artificial intelligence References Category:Data mining Category:Machine learning Category:Pattern recognition"
            }
        ],
        "latex_formulas": []
    },
    "Binary_classification": {
        "title": "Binary_classification",
        "chunks": [
            {
                "text": "Binary classification is the task of classifying the elements of a set into one of two groups (each called class). Typical binary classification problems include: Medical testing to determine if a patient has a certain disease or not; Quality control in industry, deciding whether a specification has been met; In information retrieval, deciding whether a page should be in the result set of a search or not In administration, deciding whether someone should be issued with a driving licence or not In cognition, deciding whether an object is food or not food. When measuring the accuracy of a binary classifier, the simplest way is to count the errors. But in the real world often one of the two classes is more important, so that the number of both of the different types of errors is of interest. For example, in medical testing, detecting a disease when it is not present (a false positive) is considered differently from not detecting a disease when it is present (a false negative). right|In this set of tested instances, the instances left of the divider have the condition being tested; the right half do not."
            },
            {
                "text": "The oval bounds those instances that a test algorithm classifies as having the condition. The green areas highlight the instances that the test algorithm correctly classified. Labels refer to: TP=true positive; TN=true negative; FP=false positive (type I error); FN=false negative (type II error); TPR=set of instances to determine true positive rate; FPR=set of instances to determine false positive rate; PPV=positive predictive value; NPV=negative predictive value. Four outcomes Given a classification of a specific data set, there are four basic combinations of actual data category and assigned category: true positives TP (correct positive assignments), true negatives TN (correct negative assignments), false positives FP (incorrect positive assignments), and false negatives FN (incorrect negative assignments). Test outcome positive Test outcome negative Condition positive True positive False negative Condition negative False positive True negative These can be arranged into a 2×2 contingency table, with rows corresponding to actual value – condition positive or condition negative – and columns corresponding to classification value – test outcome positive or test outcome negative. Evaluation From tallies of the four basic outcomes, there are many approaches that can be used to measure the accuracy of a classifier or predictor."
            },
            {
                "text": "Different fields have different preferences. The eight basic ratios A common approach to evaluation is to begin by computing two ratios of a standard pattern. There are eight basic ratios of this form that one can compute from the contingency table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form \"true positive row ratio\" or \"false negative column ratio\". There are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair – the other four numbers are the complements. The row ratios are: true positive rate (TPR) = (TP/(TP+FN)), aka sensitivity or recall. These are the proportion of the population with the condition for which the test is correct. with complement the false negative rate (FNR) = (FN/(TP+FN)) true negative rate (TNR) = (TN/(TN+FP), aka specificity (SPC), with complement false positive rate (FPR) = (FP/(TN+FP)), also called independent of prevalence The column ratios are: positive predictive value (PPV, aka precision) (TP/(TP+FP))."
            },
            {
                "text": "These are the proportion of the population with a given test result for which the test is correct. with complement the false discovery rate (FDR) (FP/(TP+FP)) negative predictive value (NPV) (TN/(TN+FN)) with complement the false omission rate (FOR) (FN/(TN+FN)), also called dependence on prevalence. In diagnostic testing, the main ratios used are the true column ratios – true positive rate and true negative rate – where they are known as sensitivity and specificity. In informational retrieval, the main ratios are the true positive ratios (row and column) – positive predictive value and true positive rate – where they are known as precision and recall. Cullerne Bown has suggested a flow chart for determining which pair of indicators should be used when. Otherwise, there is no general rule for deciding. There is also no general agreement on how the pair of indicators should be used to decide on concrete questions, such as when to prefer one classifier over another. One can take ratios of a complementary pair of ratios, yielding four likelihood ratios (two column ratio of ratios, two row ratio of ratios)."
            },
            {
                "text": "This is primarily done for the column (condition) ratios, yielding likelihood ratios in diagnostic testing. Taking the ratio of one of these groups of ratios yields a final ratio, the diagnostic odds ratio (DOR). This can also be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN); this has a useful interpretation – as an odds ratio – and is prevalence-independent. Other metrics There are a number of other metrics, most simply the accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; the complement is the Fraction Incorrect (FiC). The F-score combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score (F1 score). Some metrics come from regression coefficients: the markedness and the informedness, and their geometric mean, the Matthews correlation coefficient. Other metrics include Youden's J statistic, the uncertainty coefficient, the phi coefficient, and Cohen's kappa. Statistical binary classification Statistical classification is a problem studied in machine learning in which the classification is performed on the basis of a classification rule."
            },
            {
                "text": "It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. When there are only two categories the problem is known as statistical binary classification. Some of the methods commonly used for binary classification are: Decision trees Random forests Bayesian networks Support vector machines Neural networks Logistic regression Probit model Genetic Programming Multi expression programming Linear genetic programming Each classifier is best in only a select domain based upon the number of observations, the dimensionality of the feature vector, the noise in the data and many other factors. For example, random forests perform better than SVM classifiers for 3D point clouds. Converting continuous values to binary Binary classification may be a form of dichotomization in which a continuous function is transformed into a binary variable. Tests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff."
            },
            {
                "text": "However, such conversion causes a loss of information, as the resultant binary classification does not tell how much above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as \"positive\" with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as \"positive\" as the one of 52 mIU/ml. See also Approximate membership query filter Examples of Bayesian inference Classification rule Confusion matrix Detection theory Kernel methods Multiclass classification Multi-label classification One-class classification Prosecutor's fallacy Receiver operating characteristic Thresholding (image processing) Uncertainty coefficient, aka proficiency Qualitative property Precision and recall (equivalent classification schema) References Bibliography Nello Cristianini and John Shawe-Taylor."
            },
            {
                "text": "An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000. ( SVM Book) John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. (Website for the book) Bernhard Schölkopf and A. J. Smola: Learning with Kernels. MIT Press, Cambridge, Massachusetts, 2002. Category:Statistical classification Category:Machine learning"
            }
        ],
        "latex_formulas": []
    },
    "Sigmoid_function": {
        "title": "Sigmoid_function",
        "chunks": [
            {
                "text": "thumb|The logistic curve thumb|Plot of the error function A sigmoid function is any mathematical function whose graph has a characteristic S-shaped or sigmoid curve. A common example of a sigmoid function is the logistic function, which is defined by the formula $\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1 + e^x} = 1 - \\sigma(-x).$ Other sigmoid functions are given in the Examples section. In some fields, most notably in the context of artificial neural networks, the term \"sigmoid function\" is used as a synonym for \"logistic function\". Special cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams). Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing. Sigmoid functions most often show a return value (y axis) in the range 0 to 1."
            },
            {
                "text": "Another commonly used range is from −1 to 1. A wide variety of sigmoid functions including the logistic and hyperbolic tangent functions have been used as the activation function of artificial neurons. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from 0 to 1), such as the integrals of the logistic density, the normal density, and Student's t probability density functions. The logistic sigmoid function is invertible, and its inverse is the logit function. Definition A sigmoid function is a bounded, differentiable, real function that is defined for all real input values and has a positive derivative at each point and exactly 1/4 at the inflection point. Properties In general, a sigmoid function is monotonic, and has a first derivative which is bell shaped. Conversely, the integral of any continuous, non-negative, bell-shaped function (with one local maximum and no local minimum, unless degenerate) will be sigmoidal. Thus the cumulative distribution functions for many common probability distributions are sigmoidal. One such example is the error function, which is related to the cumulative distribution function of a normal distribution; another is the arctan function, which is related to the cumulative distribution function of a Cauchy distribution."
            },
            {
                "text": "A sigmoid function is constrained by a pair of horizontal asymptotes as $x \\rightarrow \\pm \\infty$. A sigmoid function is convex for values less than a particular point, and it is concave for values greater than that point: in many of the examples here, that point is 0. Examples right|Some sigmoid functions compared. In the drawing all functions are normalized in such a way that their slope at the origin is 1. Logistic function Hyperbolic tangent (shifted and scaled version of the logistic function, above) Arctangent function Gudermannian function Error function Generalised logistic function Smoothstep function Some algebraic functions, for example and in a more general form Up to shifts and scaling, many sigmoids are special cases of where is the inverse of the negative Box–Cox transformation, and $\\alpha < 1$ and $\\beta < 1$ are shape parameters. Smooth transition function normalized to (−1,1): using the hyperbolic tangent mentioned above. Here, $m$ is a free parameter encoding the slope at $x=0$, which must be greater than or equal to $\\sqrt{3}$ because any smaller value will result in a function with multiple inflection points, which is therefore not a true sigmoid."
            },
            {
                "text": "This function is unusual because it actually attains the limiting values of −1 and 1 within a finite range, meaning that its value is constant at −1 for all $x \\leq -1$ and at 1 for all $x \\geq 1$. Nonetheless, it is smooth (infinitely differentiable, $C^\\infty$) everywhere, including at $x = \\pm 1$. Applications 320px|Inverted logistic S-curve to model the relation between wheat yield and soil salinity Many natural processes, such as those of complex system learning curves, exhibit a progression from small beginnings that accelerates and approaches a climax over time. When a specific mathematical model is lacking, a sigmoid function is often used. The van Genuchten–Gupta model is based on an inverted S-curve and applied to the response of crop yield to soil salinity. Examples of the application of the logistic S-curve to the response of crop yield (wheat) to both the soil salinity and depth to water table in the soil are shown in modeling crop response in agriculture. In artificial neural networks, sometimes non-smooth functions are used instead for efficiency; these are known as hard sigmoids."
            },
            {
                "text": "In audio signal processing, sigmoid functions are used as waveshaper transfer functions to emulate the sound of analog circuitry clipping. In biochemistry and pharmacology, the Hill and Hill–Langmuir equations are sigmoid functions. In computer graphics and real-time rendering, some of the sigmoid functions are used to blend colors or geometry between two values, smoothly and without visible seams or discontinuities. Titration curves between strong acids and strong bases have a sigmoid shape due to the logarithmic nature of the pH scale. The logistic function can be calculated efficiently by utilizing type III Unums. An hierarchy of sigmoid growth models with increasing complexity (number of parameters) was built with the primary goal to re-analyze kinetic data, the so called N-t curves, from heterogeneous nucleation experiments, in electrochemistry. The hierarchy includes at present three models, with 1, 2 and 3 parameters, if not counting the maximal number of nuclei Nmax, respectively—a tanh2 based model called α21 originally devised to describe diffusion-limited crystal growth (not aggregation!) in 2D, the Johnson-Mehl-Avrami-Kolmogorov (JMAKn) model, and the Richards model. It was shown that for the concrete purpose even the simplest model works and thus it was implied that the experiments revisited are an example of two-step nucleation with the first step being the growth of the metastable phase in which the nuclei of the stable phase form. See also References Further reading . (NB. In particular see \"Chapter 4: Artificial Neural Networks\" (in particular pp."
            },
            {
                "text": "96–97) where Mitchell uses the word \"logistic function\" and the \"sigmoid function\" synonymously – this function he also calls the \"squashing function\" – and the sigmoid (aka logistic) function is used to compress the outputs of the \"neurons\" in multi-layer neural nets.) (NB. Properties of the sigmoid, including how it can shift along axes and how its domain may be transformed.) External links Category:Elementary special functions Category:Artificial neural networks"
            }
        ],
        "latex_formulas": [
            "\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1 + e^x} = 1 - \\sigma(-x).",
            "x \\rightarrow \\pm \\infty",
            "\\alpha < 1",
            "\\beta < 1",
            "m",
            "x=0",
            "\\sqrt{3}",
            "x \\leq -1",
            "x \\geq 1",
            "C^\\infty",
            "x = \\pm 1"
        ]
    },
    "Gradient_descent": {
        "title": "Gradient_descent",
        "chunks": [
            {
                "text": "Gradient Descent in 2D Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a trajectory that maximizes that function; the procedure is then known as gradient ascent. It is particularly useful in machine learning for minimizing the cost or loss function. Gradient descent should not be confused with local search algorithms, although both are iterative methods for optimization. Gradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades. A simple extension of gradient descent, stochastic gradient descent, serves as the most basic algorithm used for training most deep networks today."
            },
            {
                "text": "Description Illustration of gradient descent on a series of level sets Gradient descent is based on the observation that if the multi-variable function $F(\\mathbf{x})$ is defined and differentiable in a neighborhood of a point $\\mathbf{a}$, then $F(\\mathbf{x})$ decreases fastest if one goes from $\\mathbf{a}$ in the direction of the negative gradient of $F$ at $\\mathbf{a}, -\\nabla F(\\mathbf{a})$. It follows that, if $ \\mathbf{a}_{n+1} = \\mathbf{a}_n-\\gamma\\nabla F(\\mathbf{a}_n)$ for a small enough step size or learning rate $\\gamma \\in \\R_{+}$, then $F(\\mathbf{a_n})\\geq F(\\mathbf{a_{n+1}})$. In other words, the term $\\gamma\\nabla F(\\mathbf{a})$ is subtracted from $\\mathbf{a}$ because we want to move against the gradient, toward the local minimum."
            },
            {
                "text": "With this observation in mind, one starts with a guess $\\mathbf{x}_0$ for a local minimum of $F$, and considers the sequence $\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_2, \\ldots$ such that $\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n \\nabla F(\\mathbf{x}_n),\\ n \\ge 0.$ We have a monotonic sequence $F(\\mathbf{x}_0)\\ge F(\\mathbf{x}_1)\\ge F(\\mathbf{x}_2)\\ge \\cdots,$ so the sequence $(\\mathbf{x}_n)$ converges to the desired local minimum. Note that the value of the step size $\\gamma$ is allowed to change at every iteration. It is possible to guarantee the convergence to a local minimum under certain assumptions on the function $F$ (for example, $F$ convex and $\\nabla F$ Lipschitz) and particular choices of $\\gamma$."
            },
            {
                "text": "Those include the sequence $\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$ as in the Barzilai-Borwein method, or a sequence $ \\gamma_n$ satisfying the Wolfe conditions (which can be found by using line search). When the function $F$ is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution. This process is illustrated in the adjacent picture. Here, $F$ is assumed to be defined on the plane, and that its graph has a bowl shape. The blue curves are the contour lines, that is, the regions on which the value of $F$ is constant."
            },
            {
                "text": "A red arrow originating at a point shows the direction of the negative gradient at that point. Note that the (negative) gradient at a point is orthogonal to the contour line going through that point. We see that gradient descent leads us to the bottom of the bowl, that is, to the point where the value of the function $F$ is minimal. An analogy for understanding gradient descent thumb|Fog in the mountains The basic intuition behind gradient descent can be illustrated by a hypothetical scenario. People are stuck in the mountains and are trying to get down (i.e., trying to find the global minimum). There is heavy fog such that visibility is extremely low. Therefore, the path down the mountain is not visible, so they must use local information to find the minimum. They can use the method of gradient descent, which involves looking at the steepness of the hill at their current position, then proceeding in the direction with the steepest descent (i.e., downhill). If they were trying to find the top of the mountain (i.e., the maximum), then they would proceed in the direction of steepest ascent (i.e., uphill)."
            },
            {
                "text": "Using this method, they would eventually find their way down the mountain or possibly get stuck in some hole (i.e., local minimum or saddle point), like a mountain lake. However, assume also that the steepness of the hill is not immediately obvious with simple observation, but rather it requires a sophisticated instrument to measure, which the persons happen to have at the moment. It takes quite some time to measure the steepness of the hill with the instrument, thus they should minimize their use of the instrument if they wanted to get down the mountain before sunset. The difficulty then is choosing the frequency at which they should measure the steepness of the hill so not to go off track. In this analogy, the persons represent the algorithm, and the path taken down the mountain represents the sequence of parameter settings that the algorithm will explore. The steepness of the hill represents the slope of the function at that point. The instrument used to measure steepness is differentiation. The direction they choose to travel in aligns with the gradient of the function at that point."
            },
            {
                "text": "The amount of time they travel before taking another measurement is the step size. If they step off a cliff in the fog they will have optimised their descent path. Choosing the step size and descent direction Since using a step size $\\gamma$ that is too small would slow convergence, and a $\\gamma$ too large would lead to overshoot and divergence, finding a good setting of $\\gamma$ is an important practical problem. Philip Wolfe also advocated using \"clever choices of the [descent] direction\" in practice. While using a direction that deviates from the steepest descent direction may seem counter-intuitive, the idea is that the smaller slope may be compensated for by being sustained over a much longer distance. To reason about this mathematically, consider a direction $ \\mathbf{p}_n$ and step size $ \\gamma_n$ and consider the more general update: $ \\mathbf{a}_{n+1} = \\mathbf{a}_n-\\gamma_n\\,\\mathbf{p}_n$. Finding good settings of $ \\mathbf{p}_n$ and $ \\gamma_n$ requires some thought."
            },
            {
                "text": "First of all, we would like the update direction to point downhill. Mathematically, letting $ \\theta_n$ denote the angle between $-\\nabla F(\\mathbf{a_n})$ and $ \\mathbf{p}_n$, this requires that $ \\cos \\theta_n > 0.$ To say more, we need more information about the objective function that we are optimising. Under the fairly weak assumption that $F$ is continuously differentiable, we may prove that: This inequality implies that the amount by which we can be sure the function $F$ is decreased depends on a trade off between the two terms in square brackets. The first term in square brackets measures the angle between the descent direction and the negative gradient. The second term measures how quickly the gradient changes along the descent direction. In principle inequality () could be optimized over $ \\mathbf{p}_n$ and $ \\gamma_n$ to choose an optimal step size and direction. The problem is that evaluating the second term in square brackets requires evaluating $ \\nabla F(\\mathbf{a}_n - t \\gamma_n \\mathbf{p}_n)$, and extra gradient evaluations are generally expensive and undesirable."
            },
            {
                "text": "Some ways around this problem are: Forgo the benefits of a clever descent direction by setting $\\mathbf{p}_n = \\nabla F(\\mathbf{a_n})$, and use line search to find a suitable step-size $ \\gamma_n$, such as one that satisfies the Wolfe conditions. A more economic way of choosing learning rates is backtracking line search, a method that has both good theoretical guarantees and experimental results. Note that one does not need to choose $\\mathbf{p}_n $ to be the gradient; any direction that has positive inner product with the gradient will result in a reduction of the function value (for a sufficiently small value of $ \\gamma_n$). Assuming that $F$ is twice-differentiable, use its Hessian $\\nabla^2 F$ to estimate $ \\|\\nabla F(\\mathbf{a}_n - t \\gamma_n \\mathbf{p}_n) - \\nabla F(\\mathbf{a}_n)\\|_2 \\approx \\| t \\gamma_n \\nabla^2 F(\\mathbf{a}_n) \\mathbf{p}_n\\|.$Then choose $ \\mathbf{p}_n$ and $ \\gamma_n$ by optimising inequality ()."
            },
            {
                "text": "Assuming that $\\nabla F$ is Lipschitz, use its Lipschitz constant $ L$ to bound $ \\|\\nabla F(\\mathbf{a}_n - t \\gamma_n \\mathbf{p}_n) - \\nabla F(\\mathbf{a}_n)\\|_2 \\leq L t \\gamma_n \\|\\mathbf{p}_n\\|.$ Then choose $ \\mathbf{p}_n$ and $ \\gamma_n$ by optimising inequality (). Build a custom model of $ \\max_{t\\in[0,1]} \\frac{\\|\\nabla F(\\mathbf{a}_n - t \\gamma_n \\mathbf{p}_n) - \\nabla F(\\mathbf{a}_n)\\|_2}{\\| \\nabla F(\\mathbf{a}_n) \\|_2}$ for $F$. Then choose $ \\mathbf{p}_n$ and $ \\gamma_n$ by optimising inequality (). Under stronger assumptions on the function $F$ such as convexity, more advanced techniques may be possible. Usually by following one of the recipes above, convergence to a local minimum can be guaranteed. When the function $F$ is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution."
            },
            {
                "text": "Solution of a linear system The steepest descent algorithm applied to the Wiener filterHaykin, Simon S. Adaptive filter theory. Pearson Education India, 2008. - p. 108-142, 217-242 Gradient descent can be used to solve a system of linear equations $A\\mathbf{x}-\\mathbf{b}=0$ reformulated as a quadratic minimization problem. If the system matrix $A$ is real symmetric and positive-definite, an objective function is defined as the quadratic function, with minimization of $F(\\mathbf{x})=\\mathbf{x}^T A\\mathbf{x}-2\\mathbf{x}^T\\mathbf{b},$ so that $\\nabla F(\\mathbf{x})=2(A\\mathbf{x}-\\mathbf{b}).$ For a general real matrix $A$, linear least squares define $F(\\mathbf{x})=\\left\\|A\\mathbf{x}-\\mathbf{b}\\right\\|^2.$ In traditional linear least squares for real $A$ and $\\mathbf{b}$ the Euclidean norm is used, in which case $\\nabla F(\\mathbf{x})=2A^T(A\\mathbf{x}-\\mathbf{b}).$ The line search minimization, finding the locally optimal step size $\\gamma$ on every iteration, can be performed analytically for quadratic functions, and explicit formulas for the locally optimal $\\gamma$ are known."
            },
            {
                "text": "For example, for real symmetric and positive-definite matrix $A$, a simple algorithm can be as follows, $\\begin{align} & \\text{repeat in the loop:} \\\\ & \\qquad \\mathbf{r} := \\mathbf{b} - \\mathbf{A x} \\\\ & \\qquad \\gamma := {\\mathbf{r}^\\mathsf{T} \\mathbf{r}}/{\\mathbf{r}^\\mathsf{T} \\mathbf{A r}} \\\\ & \\qquad \\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r} \\\\ & \\qquad \\hbox{if } \\mathbf{r}^\\mathsf{T} \\mathbf{r} \\text{ is sufficiently small, then exit loop} \\\\ & \\text{end repeat loop} \\\\ & \\text{return } \\mathbf{x} \\text{ as the result} \\end{align}$ To avoid multiplying by $A$ twice per iteration, we note that $\\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r}$ implies $\\mathbf{r} := \\mathbf{r} - \\gamma \\mathbf{A r}$, which gives the traditional algorithm, $\\begin{align} & \\mathbf{r} := \\mathbf{b} - \\mathbf{A x} \\\\ & \\text{repeat in the loop:} \\\\ & \\qquad \\gamma := {\\mathbf{r}^\\mathsf{T} \\mathbf{r}}/{\\mathbf{r}^\\mathsf{T} \\mathbf{A r}} \\\\ & \\qquad \\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r} \\\\ & \\qquad \\hbox{if } \\mathbf{r}^\\mathsf{T} \\mathbf{r} \\text{ is sufficiently small, then exit loop} \\\\ & \\qquad \\mathbf{r} := \\mathbf{r} - \\gamma \\mathbf{A r} \\\\ & \\text{end repeat loop} \\\\ & \\text{return } \\mathbf{x} \\text{ as the result} \\end{align}$ The method is rarely used for solving linear equations, with the conjugate gradient method being one of the most popular alternatives."
            },
            {
                "text": "The number of gradient descent iterations is commonly proportional to the spectral condition number $\\kappa(A)$ of the system matrix $A$ (the ratio of the maximum to minimum eigenvalues of , while the convergence of conjugate gradient method is typically determined by a square root of the condition number, i.e., is much faster. Both methods can benefit from preconditioning, where gradient descent may require less assumptions on the preconditioner. Solution of a non-linear system Gradient descent can also be used to solve a system of nonlinear equations. Below is an example that shows how to use the gradient descent to solve for three unknown variables, x1, x2, and x3. This example shows one iteration of the gradient descent."
            },
            {
                "text": "Consider the nonlinear system of equations $ \\begin{cases} 3x_1-\\cos(x_2x_3)-\\tfrac{3}{2} =0 \\\\ 4x_1^2-625x_2^2+2x_2-1 = 0 \\\\ \\exp(-x_1x_2)+20x_3+\\tfrac{10\\pi-3}{3} =0 \\end{cases}$ Let us introduce the associated function $G(\\mathbf{x}) = \\begin{bmatrix} 3x_1-\\cos(x_2x_3)-\\tfrac{3}{2} \\\\ 4x_1^2-625x_2^2+2x_2-1 \\\\ \\exp(-x_1x_2)+20x_3+\\tfrac{10\\pi-3}{3} \\\\ \\end{bmatrix}, $ where $ \\mathbf{x} =\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\end{bmatrix}.$ One might now define the objective function $\\begin{align}F(\\mathbf{x}) &= \\frac{1}{2} G^\\mathrm{T}(\\mathbf{x}) G(\\mathbf{x}) \\\\&=\\frac{1}{2} \\left[ \\left (3x_1-\\cos(x_2x_3)-\\frac{3}{2} \\right)^2 + \\left(4x_1^2-625x_2^2+2x_2-1 \\right)^2 +\\right.\\\\ &{}\\qquad\\left."
            },
            {
                "text": "\\left(\\exp(-x_1x_2) + 20x_3 + \\frac{10\\pi-3}{3} \\right)^2 \\right],\\end{align}$ which we will attempt to minimize."
            },
            {
                "text": "As an initial guess, let us use $ \\mathbf{x}^{(0)}= \\mathbf{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix}.$ We know that $\\mathbf{x}^{(1)}=\\mathbf{0}-\\gamma_0 \\nabla F(\\mathbf{0}) = \\mathbf{0}-\\gamma_0 J_G(\\mathbf{0})^\\mathrm{T} G(\\mathbf{0}),$ where the Jacobian matrix $J_G$ is given by $J_G(\\mathbf{x}) = \\begin{bmatrix} 3 & \\sin(x_2x_3)x_3 & \\sin(x_2x_3)x_2 \\\\ 8x_1 & -1250x_2+2 & 0 \\\\ -x_2\\exp{(-x_1x_2)} & -x_1\\exp(-x_1x_2) & 20\\\\ \\end{bmatrix}.$ We calculate: $J_G(\\mathbf{0}) = \\begin{bmatrix} 3 & 0 & 0\\\\ 0 & 2 & 0\\\\ 0 & 0 & 20 \\end{bmatrix}, \\qquad G(\\mathbf{0}) = \\begin{bmatrix} -2.5\\\\ -1\\\\ 10.472 \\end{bmatrix}.$ Thus $\\mathbf{x}^{(1)}= \\mathbf{0}-\\gamma_0 \\begin{bmatrix} -7.5\\\\ -2\\\\ 209.44 \\end{bmatrix},$ and $F(\\mathbf{0}) = 0.5 \\left( (-2.5)^2 + (-1)^2 + (10.472)^2 \\right) = 58.456.$ 350px|An animation showing the first 83 iterations of gradient descent applied to this example."
            },
            {
                "text": "Surfaces are isosurfaces of $F(\\mathbf{x}^{(n)})$ at current guess $\\mathbf{x}^{(n)}$, and arrows show the direction of descent. Due to a small and constant step size, the convergence is slow. Now, a suitable $\\gamma_0$ must be found such that $F\\left (\\mathbf{x}^{(1)}\\right ) \\le F\\left (\\mathbf{x}^{(0)}\\right ) = F(\\mathbf{0}).$ This can be done with any of a variety of line search algorithms. One might also simply guess $\\gamma_0=0.001,$ which gives $ \\mathbf{x}^{(1)}=\\begin{bmatrix} 0.0075 \\\\ 0.002 \\\\ -0.20944 \\\\ \\end{bmatrix}.$ Evaluating the objective function at this value, yields $F \\left (\\mathbf{x}^{(1)}\\right ) = 0.5 \\left ((-2.48)^2 + (-1.00)^2 + (6.28)^2 \\right ) = 23.306.$ The decrease from $F(\\mathbf{0})=58.456$ to the next step's value of $ F\\left (\\mathbf{x}^{(1)}\\right ) =23.306 $ is a sizable decrease in the objective function."
            },
            {
                "text": "Further steps would reduce its value further until an approximate solution to the system was found. Comments Gradient descent works in spaces of any number of dimensions, even in infinite-dimensional ones. In the latter case, the search space is typically a function space, and one calculates the Fréchet derivative of the functional to be minimized to determine the descent direction. That gradient descent works in any number of dimensions (finite number at least) can be seen as a consequence of the Cauchy-Schwarz inequality, i.e. the magnitude of the inner (dot) product of two vectors of any dimension is maximized when they are colinear. In the case of gradient descent, that would be when the vector of independent variable adjustments is proportional to the gradient vector of partial derivatives. The gradient descent can take many iterations to compute a local minimum with a required accuracy, if the curvature in different directions is very different for the given function. For such functions, preconditioning, which changes the geometry of the space to shape the function level sets like concentric circles, cures the slow convergence."
            },
            {
                "text": "Constructing and applying preconditioning can be computationally expensive, however. The gradient descent can be modified via momentums (Nesterov, Polyak, and Frank-Wolfe) and heavy-ball parameters (exponential moving averages and positive-negative momentum). The main examples of such optimizers are Adam, DiffGrad, Yogi, AdaBelief, etc. Methods based on Newton's method and inversion of the Hessian using conjugate gradient techniques can be better alternatives. Generally, such methods converge in fewer iterations, but the cost of each iteration is higher. An example is the BFGS method which consists in calculating on every step a matrix by which the gradient vector is multiplied to go into a \"better\" direction, combined with a more sophisticated line search algorithm, to find the \"best\" value of $\\gamma.$ For extremely large problems, where the computer-memory issues dominate, a limited-memory method such as L-BFGS should be used instead of BFGS or the steepest descent. While it is sometimes possible to substitute gradient descent for a local search algorithm, gradient descent is not in the same family: although it is an iterative method for local optimization, it relies on an objective function’s gradient rather than an explicit exploration of a solution space."
            },
            {
                "text": "Gradient descent can be viewed as applying Euler's method for solving ordinary differential equations $x'(t)=-\\nabla f(x(t))$ to a gradient flow. In turn, this equation may be derived as an optimal controller for the control system $x'(t) = u(t)$ with $u(t)$ given in feedback form $u(t) = -\\nabla f(x(t))$. Modifications Gradient descent can converge to a local minimum and slow down in a neighborhood of a saddle point. Even for unconstrained quadratic minimization, gradient descent develops a zig-zag pattern of subsequent iterates as iterations progress, resulting in slow convergence. Multiple modifications of gradient descent have been proposed to address these deficiencies. Fast gradient methods Yurii Nesterov has proposed a simple modification that enables faster convergence for convex problems and has been since further generalized. For unconstrained smooth problems, the method is called the fast gradient method (FGM) or the accelerated gradient method (AGM). Specifically, if the differentiable function $F$ is convex and $\\nabla F$ is Lipschitz, and it is not assumed that $F$ is strongly convex, then the error in the objective value generated at each step $k$ by the gradient descent method will be bounded by ."
            },
            {
                "text": "Using the Nesterov acceleration technique, the error decreases at . It is known that the rate $\\mathcal{O}\\left({k^{-2}}\\right)$ for the decrease of the cost function is optimal for first-order optimization methods. Nevertheless, there is the opportunity to improve the algorithm by reducing the constant factor. The optimized gradient method (OGM) reduces that constant by a factor of two and is an optimal first-order method for large-scale problems. For constrained or non-smooth problems, Nesterov's FGM is called the fast proximal gradient method (FPGM), an acceleration of the proximal gradient method. Momentum or heavy ball method Trying to break the zig-zag pattern of gradient descent, the momentum or heavy ball method uses a momentum term in analogy to a heavy ball sliding on the surface of values of the function being minimized, or to mass movement in Newtonian dynamics through a viscous medium in a conservative force field. Gradient descent with momentum remembers the solution update at each iteration, and determines the next update as a linear combination of the gradient and the previous update."
            },
            {
                "text": "For unconstrained quadratic minimization, a theoretical convergence rate bound of the heavy ball method is asymptotically the same as that for the optimal conjugate gradient method. This technique is used in stochastic gradient descent and as an extension to the backpropagation algorithms used to train artificial neural networks. Part of a lecture series for the Coursera online course Neural Networks for Machine Learning . In the direction of updating, stochastic gradient descent adds a stochastic property. The weights can be used to calculate the derivatives. Extensions Gradient descent can be extended to handle constraints by including a projection onto the set of constraints. This method is only feasible when the projection is efficiently computable on a computer. Under suitable assumptions, this method converges. This method is a specific case of the forward-backward algorithm for monotone inclusions (which includes convex programming and variational inequalities). Gradient descent is a special case of mirror descent using the squared Euclidean distance as the given Bregman divergence. Theoretical properties The properties of gradient descent depend on the properties of the objective function and the variant of gradient descent used (for example, if a line search step is used). The assumptions made affect the convergence rate, and other properties, that can be proven for gradient descent. For example, if the objective is assumed to be strongly convex and lipschitz smooth, then gradient descent converges linearly with a fixed step size. Looser assumptions lead to either weaker convergence guarantees or require a more sophisticated step size selection."
            },
            {
                "text": "See also Backtracking line search Conjugate gradient method Stochastic gradient descent Rprop Delta rule Wolfe conditions Preconditioning Broyden–Fletcher–Goldfarb–Shanno algorithm Davidon–Fletcher–Powell formula Nelder–Mead method Gauss–Newton algorithm Hill climbing Quantum annealing CLS (continuous local search) Neuroevolution References Further reading External links Using gradient descent in C++, Boost, Ublas for linear regression Series of Khan Academy videos discusses gradient ascent Online book teaching gradient descent in deep neural network context Archived at Ghostarchive and the Wayback Machine: Handbook of Convergence Theorems for (Stochastic) Gradient Methods Category:Mathematical optimization Category:First order methods Category:Optimization algorithms and methods Category:Gradient methods"
            }
        ],
        "latex_formulas": [
            "F(\\mathbf{x})",
            "\\mathbf{a}",
            "F(\\mathbf{x})",
            "\\mathbf{a}",
            "F",
            "\\mathbf{a}, -\\nabla F(\\mathbf{a})",
            "\\mathbf{a}_{n+1} = \\mathbf{a}_n-\\gamma\\nabla F(\\mathbf{a}_n)",
            "\\gamma \\in \\R_{+}",
            "F(\\mathbf{a_n})\\geq F(\\mathbf{a_{n+1}})",
            "\\gamma\\nabla F(\\mathbf{a})",
            "\\mathbf{a}",
            "\\mathbf{x}_0",
            "F",
            "\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_2, \\ldots",
            "\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n \\nabla F(\\mathbf{x}_n),\\ n \\ge 0.",
            "F(\\mathbf{x}_0)\\ge F(\\mathbf{x}_1)\\ge F(\\mathbf{x}_2)\\ge \\cdots,",
            "(\\mathbf{x}_n)",
            "\\gamma",
            "F",
            "F",
            "\\nabla F",
            "\\gamma",
            "\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}",
            "\\gamma_n",
            "F",
            "F",
            "F",
            "F",
            "\\gamma",
            "\\gamma",
            "\\gamma",
            "\\mathbf{p}_n",
            "\\gamma_n",
            "\\mathbf{a}_{n+1} = \\mathbf{a}_n-\\gamma_n\\,\\mathbf{p}_n",
            "\\mathbf{p}_n",
            "\\gamma_n",
            "\\theta_n",
            "-\\nabla F(\\mathbf{a_n})",
            "\\mathbf{p}_n",
            "\\cos \\theta_n > 0.",
            "F",
            "F(\\mathbf{a}_{n+1}) \\leq F(\\mathbf{a}_n) - \\gamma_n \\|\\nabla F(\\mathbf{a}_n)\\|_2 \\|\\mathbf{p}_n\\|_2 \\left[\\cos \\theta_n - \\max_{t\\in[0,1]} \\frac{\\|\\nabla F(\\mathbf{a}_n - t \\gamma_n \\mathbf{p}_n) - \\nabla F(\\mathbf{a}_n)\\|_2}{\\| \\nabla F(\\mathbf{a}_n) \\|_2}\\right]",
            "F",
            "\\mathbf{p}_n",
            "\\gamma_n",
            "\\nabla F(\\mathbf{a}_n - t \\gamma_n \\mathbf{p}_n)",
            "\\mathbf{p}_n = \\nabla F(\\mathbf{a_n})",
            "\\gamma_n",
            "\\mathbf{p}_n",
            "\\gamma_n",
            "F",
            "\\nabla^2 F",
            "\\|\\nabla F(\\mathbf{a}_n - t \\gamma_n \\mathbf{p}_n) - \\nabla F(\\mathbf{a}_n)\\|_2 \\approx \\| t \\gamma_n \\nabla^2 F(\\mathbf{a}_n) \\mathbf{p}_n\\|.",
            "\\mathbf{p}_n",
            "\\gamma_n",
            "\\nabla F",
            "L",
            "\\|\\nabla F(\\mathbf{a}_n - t \\gamma_n \\mathbf{p}_n) - \\nabla F(\\mathbf{a}_n)\\|_2 \\leq L t \\gamma_n \\|\\mathbf{p}_n\\|.",
            "\\mathbf{p}_n",
            "\\gamma_n",
            "\\max_{t\\in[0,1]} \\frac{\\|\\nabla F(\\mathbf{a}_n - t \\gamma_n \\mathbf{p}_n) - \\nabla F(\\mathbf{a}_n)\\|_2}{\\| \\nabla F(\\mathbf{a}_n) \\|_2}",
            "F",
            "\\mathbf{p}_n",
            "\\gamma_n",
            "F",
            "F",
            "A\\mathbf{x}-\\mathbf{b}=0",
            "A",
            "F(\\mathbf{x})=\\mathbf{x}^T A\\mathbf{x}-2\\mathbf{x}^T\\mathbf{b},",
            "\\nabla F(\\mathbf{x})=2(A\\mathbf{x}-\\mathbf{b}).",
            "A",
            "F(\\mathbf{x})=\\left\\|A\\mathbf{x}-\\mathbf{b}\\right\\|^2.",
            "A",
            "\\mathbf{b}",
            "\\nabla F(\\mathbf{x})=2A^T(A\\mathbf{x}-\\mathbf{b}).",
            "\\gamma",
            "\\gamma",
            "A",
            "\\begin{align}\n& \\text{repeat in the loop:} \\\\\n& \\qquad \\mathbf{r} := \\mathbf{b} - \\mathbf{A x} \\\\\n& \\qquad \\gamma := {\\mathbf{r}^\\mathsf{T} \\mathbf{r}}/{\\mathbf{r}^\\mathsf{T} \\mathbf{A r}}  \\\\\n& \\qquad \\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r} \\\\\n& \\qquad \\hbox{if } \\mathbf{r}^\\mathsf{T} \\mathbf{r} \\text{ is sufficiently small, then exit loop} \\\\\n& \\text{end repeat loop} \\\\\n& \\text{return } \\mathbf{x} \\text{ as the result}\n\\end{align}",
            "A",
            "\\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r}",
            "\\mathbf{r} := \\mathbf{r} - \\gamma \\mathbf{A r}",
            "\\begin{align}\n& \\mathbf{r} := \\mathbf{b} - \\mathbf{A x} \\\\\n& \\text{repeat in the loop:} \\\\\n& \\qquad \\gamma := {\\mathbf{r}^\\mathsf{T} \\mathbf{r}}/{\\mathbf{r}^\\mathsf{T} \\mathbf{A r}}  \\\\\n& \\qquad \\mathbf{x} := \\mathbf{x} + \\gamma \\mathbf{r} \\\\\n& \\qquad \\hbox{if } \\mathbf{r}^\\mathsf{T} \\mathbf{r} \\text{ is sufficiently small, then exit loop} \\\\\n& \\qquad \\mathbf{r} := \\mathbf{r} - \\gamma \\mathbf{A r} \\\\\n& \\text{end repeat loop} \\\\\n& \\text{return } \\mathbf{x} \\text{ as the result}\n\\end{align}",
            "\\kappa(A)",
            "A",
            "A^TA",
            "\\begin{cases}\n3x_1-\\cos(x_2x_3)-\\tfrac{3}{2} =0 \\\\\n4x_1^2-625x_2^2+2x_2-1 = 0  \\\\\n\\exp(-x_1x_2)+20x_3+\\tfrac{10\\pi-3}{3} =0\n\\end{cases}",
            "G(\\mathbf{x}) = \\begin{bmatrix}\n3x_1-\\cos(x_2x_3)-\\tfrac{3}{2} \\\\\n4x_1^2-625x_2^2+2x_2-1 \\\\\n\\exp(-x_1x_2)+20x_3+\\tfrac{10\\pi-3}{3} \\\\\n\\end{bmatrix},",
            "\\mathbf{x} =\\begin{bmatrix}\n  x_1 \\\\\n  x_2 \\\\\n  x_3 \\\\\n\\end{bmatrix}.",
            "\\begin{align}F(\\mathbf{x}) &= \\frac{1}{2} G^\\mathrm{T}(\\mathbf{x}) G(\\mathbf{x}) \\\\&=\\frac{1}{2} \\left[ \\left (3x_1-\\cos(x_2x_3)-\\frac{3}{2} \\right)^2  + \\left(4x_1^2-625x_2^2+2x_2-1 \\right)^2 +\\right.\\\\\n&{}\\qquad\\left. \\left(\\exp(-x_1x_2) + 20x_3 + \\frac{10\\pi-3}{3} \\right)^2 \\right],\\end{align}",
            "\\mathbf{x}^{(0)}= \\mathbf{0} = \\begin{bmatrix}\n  0 \\\\\n  0 \\\\\n  0 \\\\ \\end{bmatrix}.",
            "\\mathbf{x}^{(1)}=\\mathbf{0}-\\gamma_0 \\nabla F(\\mathbf{0}) = \\mathbf{0}-\\gamma_0 J_G(\\mathbf{0})^\\mathrm{T} G(\\mathbf{0}),",
            "J_G",
            "J_G(\\mathbf{x}) = \\begin{bmatrix}\n  3 & \\sin(x_2x_3)x_3 & \\sin(x_2x_3)x_2   \\\\\n  8x_1 & -1250x_2+2 & 0 \\\\\n  -x_2\\exp{(-x_1x_2)} & -x_1\\exp(-x_1x_2) & 20\\\\\n\\end{bmatrix}.",
            "J_G(\\mathbf{0}) = \\begin{bmatrix}\n  3 & 0 & 0\\\\\n  0 & 2 & 0\\\\\n  0 & 0 & 20\n\\end{bmatrix}, \\qquad G(\\mathbf{0}) = \\begin{bmatrix}\n  -2.5\\\\\n  -1\\\\\n  10.472\n\\end{bmatrix}.",
            "\\mathbf{x}^{(1)}= \\mathbf{0}-\\gamma_0 \\begin{bmatrix}\n  -7.5\\\\\n  -2\\\\\n  209.44\n\\end{bmatrix},",
            "F(\\mathbf{0}) = 0.5 \\left( (-2.5)^2 + (-1)^2 + (10.472)^2 \\right) = 58.456.",
            "F(\\mathbf{x}^{(n)})",
            "\\mathbf{x}^{(n)}",
            "\\gamma_0",
            "F\\left (\\mathbf{x}^{(1)}\\right ) \\le F\\left (\\mathbf{x}^{(0)}\\right ) = F(\\mathbf{0}).",
            "\\gamma_0=0.001,",
            "\\mathbf{x}^{(1)}=\\begin{bmatrix}\n   0.0075  \\\\\n   0.002   \\\\\n  -0.20944 \\\\\n\\end{bmatrix}.",
            "F \\left (\\mathbf{x}^{(1)}\\right ) = 0.5 \\left ((-2.48)^2 + (-1.00)^2 + (6.28)^2 \\right ) = 23.306.",
            "F(\\mathbf{0})=58.456",
            "F\\left (\\mathbf{x}^{(1)}\\right ) =23.306",
            "\\gamma.",
            "x'(t)=-\\nabla f(x(t))",
            "x'(t) = u(t)",
            "u(t)",
            "u(t) = -\\nabla f(x(t))",
            "F",
            "\\nabla F",
            "F",
            "k",
            "\\mathcal{O}\\left({k^{-2}}\\right)"
        ]
    },
    "Regularization_(mathematics)": {
        "title": "Regularization_(mathematics)",
        "chunks": [
            {
                "text": "The green and blue functions both incur zero loss on the given data points. A learned model can be induced to prefer the green function, which may generalize better to more points drawn from the underlying unknown distribution, by adjusting $\\lambda$, the weight of the regularization term. In mathematics, statistics, finance, and computer science, particularly in machine learning and inverse problems, regularization is a process that converts the answer of a problem to a simpler one. It is often used in solving ill-posed problems or to prevent overfitting. Although regularization procedures can be divided in many ways, the following delineation is particularly helpful: Explicit regularization is regularization whenever one explicitly adds a term to the optimization problem. These terms could be priors, penalties, or constraints. Explicit regularization is commonly employed with ill-posed optimization problems. The regularization term, or penalty, imposes a cost on the optimization function to make the optimal solution unique. Implicit regularization is all other forms of regularization. This includes, for example, early stopping, using a robust loss function, and discarding outliers."
            },
            {
                "text": "Implicit regularization is essentially ubiquitous in modern machine learning approaches, including stochastic gradient descent for training deep neural networks, and ensemble methods (such as random forests and gradient boosted trees). In explicit regularization, independent of the problem or model, there is always a data term, that corresponds to a likelihood of the measurement, and a regularization term that corresponds to a prior. By combining both using Bayesian statistics, one can compute a posterior, that includes both information sources and therefore stabilizes the estimation process. By trading off both objectives, one chooses to be more aligned to the data or to enforce regularization (to prevent overfitting). There is a whole research branch dealing with all possible regularizations. In practice, one usually tries a specific regularization and then figures out the probability density that corresponds to that regularization to justify the choice. It can also be physically motivated by common sense or intuition. In machine learning, the data term corresponds to the training data and the regularization is either the choice of the model or modifications to the algorithm."
            },
            {
                "text": "It is always intended to reduce the generalization error, i.e. the error score with the trained model on the evaluation set (testing data) and not the training data. One of the earliest uses of regularization is Tikhonov regularization (ridge regression), related to the method of least squares. Regularization in machine learning In machine learning, a key challenge is enabling models to accurately predict outcomes on unseen data, not just on familiar training data. Regularization is crucial for addressing overfitting—where a model memorizes training data details but cannot generalize to new data. The goal of regularization is to encourage models to learn the broader patterns within the data rather than memorizing it. Techniques like early stopping, L1 and L2 regularization, and dropout are designed to prevent overfitting and underfitting, thereby enhancing the model's ability to adapt to and perform well with new data, thus improving model generalization. Early Stopping Stops training when validation performance deteriorates, preventing overfitting by halting before the model memorizes training data. L1 and L2 Regularization Adds penalty terms to the cost function to discourage complex models: L1 regularization (also called LASSO) leads to sparse models by adding a penalty based on the absolute value of coefficients."
            },
            {
                "text": "L2 regularization (also called ridge regression) encourages smaller, more evenly distributed weights by adding a penalty based on the square of the coefficients. Dropout In the context of neural networks, the Dropout technique repeatedly ignores random subsets of neurons during training, which simulates the training of multiple neural network architectures at once to improve generalization. Classification Empirical learning of classifiers (from a finite data set) is always an underdetermined problem, because it attempts to infer a function of any $x$ given only examples $x_1, x_2, \\dots, x_n$. A regularization term (or regularizer) $R(f)$ is added to a loss function: where $V$ is an underlying loss function that describes the cost of predicting $f(x)$ when the label is $y$, such as the square loss or hinge loss; and $\\lambda$ is a parameter which controls the importance of the regularization term. $R(f)$ is typically chosen to impose a penalty on the complexity of $f$."
            },
            {
                "text": "Concrete notions of complexity used include restrictions for smoothness and bounds on the vector space norm. A theoretical justification for regularization is that it attempts to impose Occam's razor on the solution (as depicted in the figure above, where the green function, the simpler one, may be preferred). From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters.For the connection between maximum a posteriori estimation and ridge regression, see Regularization can serve multiple purposes, including learning simpler models, inducing models to be sparse and introducing group structure into the learning problem. The same idea arose in many fields of science. A simple form of regularization applied to integral equations (Tikhonov regularization) is essentially a trade-off between fitting the data and reducing a norm of the solution. More recently, non-linear regularization methods, including total variation regularization, have become popular. Generalization Regularization can be motivated as a technique to improve the generalizability of a learned model. The goal of this learning problem is to find a function that fits or predicts the outcome (label) that minimizes the expected error over all possible inputs and labels."
            },
            {
                "text": "The expected error of a function $f_n$ is: where $X$ and $Y$ are the domains of input data $x$ and their labels $y$ respectively. Typically in learning problems, only a subset of input data and labels are available, measured with some noise. Therefore, the expected error is unmeasurable, and the best surrogate available is the empirical error over the $ N $ available samples: Without bounds on the complexity of the function space (formally, the reproducing kernel Hilbert space) available, a model will be learned that incurs zero loss on the surrogate empirical error. If measurements (e.g. of $x_i$) were made with noise, this model may suffer from overfitting and display poor expected error. Regularization introduces a penalty for exploring certain regions of the function space used to build the model, which can improve generalization. Tikhonov regularization (ridge regression) These techniques are named for Andrey Nikolayevich Tikhonov, who applied regularization to integral equations and made important contributions in many other areas. When learning a linear function $f$, characterized by an unknown vector $w$ such that $f(x) = w \\cdot x$, one can add the $L_2$-norm of the vector $w$ to the loss expression in order to prefer solutions with smaller norms."
            },
            {
                "text": "Tikhonov regularization is one of the most common forms. It is also known as ridge regression. It is expressed as: where $(\\hat x_i, \\hat y_i), \\, 1 \\leq i \\leq n,$ would represent samples used for training. In the case of a general function, the norm of the function in its reproducing kernel Hilbert space is: As the $L_2$ norm is differentiable, learning can be advanced by gradient descent. Tikhonov-regularized least squares The learning problem with the least squares loss function and Tikhonov regularization can be solved analytically. Written in matrix form, the optimal $w$ is the one for which the gradient of the loss function with respect to $w$ is 0. where the third statement is a first-order condition. By construction of the optimization problem, other values of $w$ give larger values for the loss function. This can be verified by examining the second derivative $\\nabla_{ww}$. During training, this algorithm takes $O(d^3 + n d^2)$ time. The terms correspond to the matrix inversion and calculating $X^\\mathsf{T} X$, respectively."
            },
            {
                "text": "Testing takes $O(nd)$ time. Early stopping Early stopping can be viewed as regularization in time. Intuitively, a training procedure such as gradient descent tends to learn more and more complex functions with increasing iterations. By regularizing for time, model complexity can be controlled, improving generalization. Early stopping is implemented using one data set for training, one statistically independent data set for validation and another for testing. The model is trained until performance on the validation set no longer improves and then applied to the test set. Theoretical motivation in least squares Consider the finite approximation of Neumann series for an invertible matrix where $\\left\\| I - A \\right\\| < 1$: This can be used to approximate the analytical solution of unregularized least squares, if is introduced to ensure the norm is less than one. The exact solution to the unregularized least squares learning problem minimizes the empirical error, but may fail. By limiting , the only free parameter in the algorithm above, the problem is regularized for time, which may improve its generalization. The algorithm above is equivalent to restricting the number of gradient descent iterations for the empirical risk with the gradient descent update: The base case is trivial."
            },
            {
                "text": "The inductive case is proved as follows: Regularizers for sparsity Assume that a dictionary $\\phi_j$ with dimension $p$ is given such that a function in the function space can be expressed as: thumb|A comparison between the L1 ball and the L2 ball in two dimensions gives an intuition on how L1 regularization achieves sparsity. Enforcing a sparsity constraint on $w$ can lead to simpler and more interpretable models. This is useful in many real-life applications such as computational biology. An example is developing a simple predictive test for a disease in order to minimize the cost of performing medical tests while maximizing predictive power. A sensible sparsity constraint is the $L_0$ norm $\\|w\\|_0$, defined as the number of non-zero elements in $w$. Solving a $L_0$ regularized learning problem, however, has been demonstrated to be NP-hard. The $L_1$ norm (see also Norms) can be used to approximate the optimal $L_0$ norm via convex relaxation. It can be shown that the $L_1$ norm induces sparsity. In the case of least squares, this problem is known as LASSO in statistics and basis pursuit in signal processing."
            },
            {
                "text": "thumb|Elastic net regularization $L_1$ regularization can occasionally produce non-unique solutions. A simple example is provided in the figure when the space of possible solutions lies on a 45 degree line. This can be problematic for certain applications, and is overcome by combining $L_1$ with $L_2$ regularization in elastic net regularization, which takes the following form: Elastic net regularization tends to have a grouping effect, where correlated input features are assigned equal weights. Elastic net regularization is commonly used in practice and is implemented in many machine learning libraries. Proximal methods While the $L_1$ norm does not result in an NP-hard problem, the $L_1$ norm is convex but is not strictly differentiable due to the kink at x = 0. Subgradient methods which rely on the subderivative can be used to solve $L_1$ regularized learning problems. However, faster convergence can be achieved through proximal methods. For a problem $\\min_{w \\in H} F(w) + R(w)$ such that $F$ is convex, continuous, differentiable, with Lipschitz continuous gradient (such as the least squares loss function), and $R$ is convex, continuous, and proper, then the proximal method to solve the problem is as follows."
            },
            {
                "text": "First define the proximal operator and then iterate The proximal method iteratively performs gradient descent and then projects the result back into the space permitted by $R$. When $R$ is the $L1$ regularizer, the proximal operator is equivalent to the soft-thresholding operator, This allows for efficient computation. Group sparsity without overlaps Groups of features can be regularized by a sparsity constraint, which can be useful for expressing certain prior knowledge into an optimization problem. In the case of a linear model with non-overlapping known groups, a regularizer can be defined: where This can be viewed as inducing a regularizer over the $L_2$ norm over members of each group followed by an $L_1$ norm over groups. This can be solved by the proximal method, where the proximal operator is a block-wise soft-thresholding function: Group sparsity with overlaps The algorithm described for group sparsity without overlaps can be applied to the case where groups do overlap, in certain situations. This will likely result in some groups with all zero elements, and other groups with some non-zero and some zero elements."
            },
            {
                "text": "If it is desired to preserve the group structure, a new regularizer can be defined: For each $w_g$, $\\bar w_g$ is defined as the vector such that the restriction of $\\bar w_g$ to the group $g$ equals $w_g$ and all other entries of $\\bar w_g$ are zero. The regularizer finds the optimal disintegration of $w$ into parts. It can be viewed as duplicating all elements that exist in multiple groups. Learning problems with this regularizer can also be solved with the proximal method with a complication. The proximal operator cannot be computed in closed form, but can be effectively solved iteratively, inducing an inner iteration within the proximal method iteration. Regularizers for semi-supervised learning When labels are more expensive to gather than input examples, semi-supervised learning can be useful. Regularizers have been designed to guide learning algorithms to learn models that respect the structure of unsupervised training samples. If a symmetric weight matrix $W$ is given, a regularizer can be defined: If $W_{ij}$ encodes the result of some distance metric for points $x_i$ and $x_j$, it is desirable that $f(x_i) \\approx f(x_j)$."
            },
            {
                "text": "This regularizer captures this intuition, and is equivalent to: where $L = D- W$ is the Laplacian matrix of the graph induced by $W$. The optimization problem $\\min_{f \\in \\mathbb{R}^m} R(f), m = u + l$ can be solved analytically if the constraint $f(x_i) = y_i$ is applied for all supervised samples. The labeled part of the vector $f$ is therefore obvious. The unlabeled part of $f$ is solved for by: The pseudo-inverse can be taken because $L_{ul}$ has the same range as $L_{uu}$. Regularizers for multitask learning In the case of multitask learning, $T$ problems are considered simultaneously, each related in some way. The goal is to learn $T$ functions, ideally borrowing strength from the relatedness of tasks, that have predictive power. This is equivalent to learning the matrix $W: T \\times D$ . Sparse regularizer on columns This regularizer defines an L2 norm on each column and an L1 norm over all columns."
            },
            {
                "text": "It can be solved by proximal methods. Nuclear norm regularization where $\\sigma(W)$ is the eigenvalues in the singular value decomposition of $W$. Mean-constrained regularization This regularizer constrains the functions learned for each task to be similar to the overall average of the functions across all tasks. This is useful for expressing prior information that each task is expected to share with each other task. An example is predicting blood iron levels measured at different times of the day, where each task represents an individual. Clustered mean-constrained regularization where $I(r)$ is a cluster of tasks. This regularizer is similar to the mean-constrained regularizer, but instead enforces similarity between tasks within the same cluster. This can capture more complex prior information. This technique has been used to predict Netflix recommendations. A cluster would correspond to a group of people who share similar preferences. Graph-based similarity More generally than above, similarity between tasks can be defined by a function. The regularizer encourages the model to learn similar functions for similar tasks. for a given symmetric similarity matrix $M$. Other uses of regularization in statistics and machine learning Bayesian learning methods make use of a prior probability that (usually) gives lower probability to more complex models. Well-known model selection techniques include the Akaike information criterion (AIC), minimum description length (MDL), and the Bayesian information criterion (BIC). Alternative methods of controlling overfitting not involving regularization include cross-validation."
            },
            {
                "text": "Examples of applications of different methods of regularization to the linear model are: ModelFit measureEntropy measureAIC/BICY - X\\beta\\right\\|_2$\\beta\\right\\|_0$LassoY - X\\beta\\right\\|_2$\\beta\\right\\|_1$Ridge regression Y-X\\beta\\right\\|_2$ \\beta\\right\\|_2$Basis pursuit denoising Y - X\\beta\\right\\|_2$ \\beta\\right\\|_1$Rudin–Osher–Fatemi model (TV) Y - X\\beta\\right\\|_2$ \\nabla\\beta\\right\\|_1$ Potts model Y - X\\beta\\right\\|_2$ \\nabla\\beta\\right\\|_0$RLADY - X\\beta\\right\\|_1$ \\beta\\right\\|_1$Dantzig SelectorX^\\mathsf{T} (Y-X\\beta)\\right\\|_\\infty$\\beta\\right\\|_1$SLOPEY - X\\beta\\right\\|_2$ \\beta\\right|_{(i)}$ See also Bayesian interpretation of regularization Bias–variance tradeoff Matrix regularization Regularization by spectral filtering Regularized least squares Lagrange multiplier Variance reduction Notes References Category:Mathematical analysis Category:Inverse problems"
            }
        ],
        "latex_formulas": [
            "''L''<sub>1</sub>",
            "\\lambda",
            "x",
            "x_1, x_2, \\dots, x_n",
            "R(f)",
            "V",
            "f(x)",
            "y",
            "\\lambda",
            "R(f)",
            "f",
            "f_n",
            "X",
            "Y",
            "x",
            "y",
            "N",
            "x_i",
            "f",
            "w",
            "f(x) = w \\cdot x",
            "L_2",
            "w",
            "(\\hat x_i, \\hat y_i), \\, 1 \\leq i \\leq n,",
            "L_2",
            "w",
            "w",
            "w",
            "\\nabla_{ww}",
            "O(d^3 + n d^2)",
            "X^\\mathsf{T} X",
            "O(nd)",
            "\\left\\| I - A \\right\\| < 1",
            "\\phi_j",
            "p",
            "w",
            "L_0",
            "\\|w\\|_0",
            "w",
            "L_0",
            "L_1",
            "L_0",
            "L_1",
            "L_1",
            "L_1",
            "L_2",
            "L_1",
            "L_1",
            "L_1",
            "\\min_{w \\in H} F(w) + R(w)",
            "F",
            "R",
            "R",
            "R",
            "L_2",
            "L_1",
            "w_g",
            "\\bar w_g",
            "\\bar w_g",
            "g",
            "w_g",
            "\\bar w_g",
            "w",
            "W",
            "W_{ij}",
            "x_i",
            "x_j",
            "f(x_i) \\approx f(x_j)",
            "L = D- W",
            "W",
            "\\min_{f \\in \\mathbb{R}^m} R(f), m = u + l",
            "f(x_i) = y_i",
            "f",
            "f",
            "L_{ul}",
            "L_{uu}",
            "T",
            "T",
            "W: T \\times D",
            "\\sigma(W)",
            "W",
            "I(r)",
            "M",
            "\\left\\|Y - X\\beta\\right\\|_2",
            "\\left\\|\\beta\\right\\|_0",
            "\\left\\|Y - X\\beta\\right\\|_2",
            "\\left\\|\\beta\\right\\|_1",
            "\\left\\|Y-X\\beta\\right\\|_2",
            "\\left\\|\\beta\\right\\|_2",
            "\\left\\|Y - X\\beta\\right\\|_2",
            "\\lambda\\left\\|\\beta\\right\\|_1",
            "\\left\\|Y - X\\beta\\right\\|_2",
            "\\lambda\\left\\|\\nabla\\beta\\right\\|_1",
            "\\left\\|Y - X\\beta\\right\\|_2",
            "\\lambda \\left\\|\\nabla\\beta\\right\\|_0",
            "\\left\\|Y - X\\beta\\right\\|_1",
            "\\left\\|\\beta\\right\\|_1",
            "\\left\\|X^\\mathsf{T} (Y-X\\beta)\\right\\|_\\infty",
            "\\left\\|\\beta\\right\\|_1",
            "\\left\\|Y - X\\beta\\right\\|_2",
            "\\sum_{i=1}^p \\lambda_i \\left|\\beta\\right|_{(i)}"
        ]
    },
    "Bayes'_theorem": {
        "title": "Bayes'_theorem",
        "chunks": [
            {
                "text": "Bayes' theorem (alternatively Bayes' law or Bayes' rule, after Thomas Bayes) gives a mathematical rule for inverting conditional probabilities, allowing one to find the probability of a cause given its effect. For example, if the risk of developing health problems is known to increase with age, Bayes' theorem allows the risk to someone of a known age to be assessed more accurately by conditioning it relative to their age, rather than assuming that the person is typical of the population as a whole. Based on Bayes' law, both the prevalence of a disease in a given population and the error rate of an infectious disease test must be taken into account to evaluate the meaning of a positive test result and avoid the base-rate fallacy. One of Bayes' theorem's many applications is Bayesian inference, an approach to statistical inference, where it is used to invert the probability of observations given a model configuration (i.e., the likelihood function) to obtain the probability of the model configuration given the observations (i.e., the posterior probability). History Bayes' theorem is named after Thomas Bayes (), a minister, statistician, and philosopher."
            },
            {
                "text": "Bayes used conditional probability to provide an algorithm (his Proposition 9) that uses evidence to calculate limits on an unknown parameter. His work was published in 1763 as An Essay Towards Solving a Problem in the Doctrine of Chances. Bayes studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology). After Bayes's death, his family gave his papers to a friend, the minister, philosopher, and mathematician Richard Price. Price significantly edited the unpublished manuscript for two years before sending it to a friend who read it aloud at the Royal Society on 23 December 1763. Price edited Bayes's major work \"An Essay Towards Solving a Problem in the Doctrine of Chances\" (1763), which appeared in Philosophical Transactions, and contains Bayes' theorem. Price wrote an introduction to the paper that provides some of the philosophical basis of Bayesian statistics and chose one of the two solutions Bayes offered. In 1765, Price was elected a Fellow of the Royal Society in recognition of his work on Bayes's legacy.Holland, pp."
            },
            {
                "text": "46–7. On 27 April, a letter sent to his friend Benjamin Franklin was read out at the Royal Society, and later published, in which Price applies this work to population and computing 'life-annuities'.. Independently of Bayes, Pierre-Simon Laplace used conditional probability to formulate the relation of an updated posterior probability from a prior probability, given evidence. He reproduced and extended Bayes's results in 1774, apparently unaware of Bayes's work, and summarized his results in Théorie analytique des probabilités (1812). The Bayesian interpretation of probability was developed mainly by Laplace. About 200 years later, Sir Harold Jeffreys put Bayes's algorithm and Laplace's formulation on an axiomatic basis, writing in a 1973 book that Bayes' theorem \"is to the theory of probability what the Pythagorean theorem is to geometry\". Stephen Stigler used a Bayesian argument to conclude that Bayes' theorem was discovered by Nicholas Saunderson, a blind English mathematician, some time before Bayes, but that is disputed. Martyn Hooper and Sharon McGrayne have argued that Richard Price's contribution was substantial: F. Thomas Bruss reviewed Bayes' work \"An essay towards solving a problem in the doctrine of chances\" as communicated by Price."
            },
            {
                "text": "He agrees with Stigler's fine analysis in many points, but not as far as the question of priority is concerned. Bruss underlines the intuitive part of Bayes' formula and adds independent arguments of Bayes' probable motivation for his work. He concludes that, unless the contrary is really proven, we are entitled to be faithful to the name \"Bayes' Theorem\" or \"Bayes' formula\". Statement of theorem Bayes' theorem is stated mathematically as the following equation: where $A$ and $B$ are events and $P(B) \\neq 0$. $P(A\\vert B)$ is a conditional probability: the probability of event $A$ occurring given that $B$ is true. It is also called the posterior probability of $A$ given $B$. $P(B\\vert A)$ is also a conditional probability: the probability of event $B$ occurring given that $A$ is true. It can also be interpreted as the likelihood of $A$ given a fixed $B$ because $P(B\\vert A)=L(A\\vert B)$."
            },
            {
                "text": "$P(A)$ and $P(B)$ are the probabilities of observing $A$ and $B$ respectively without any given conditions; they are known as the prior probability and marginal probability. Proof Visual proof of For events Bayes' theorem may be derived from the definition of conditional probability: $P(A\\vert B)=\\frac{P(A \\cap B)}{P(B)}, \\text{ if } P(B) \\neq 0, $ where $P(A \\cap B)$ is the probability of both A and B being true. Similarly, $P(B\\vert A)=\\frac{P(A \\cap B)}{P(A)}, \\text{ if } P(A) \\neq 0."
            },
            {
                "text": "$ Solving for $P(A \\cap B)$ and substituting into the above expression for $P(A\\vert B)$ yields Bayes' theorem: $P(A\\vert B) = \\frac{P(B\\vert A) P(A)}{P(B)}, \\text{ if } P(B) \\neq 0.$ For continuous random variables For two continuous random variables X and Y, Bayes' theorem may be analogously derived from the definition of conditional density: $f_{X \\vert Y=y} (x) = \\frac{f_{X,Y}(x,y)}{f_Y(y)} $ $f_{Y \\vert X=x}(y) = \\frac{f_{X,Y}(x,y)}{f_X(x)} $ Therefore, $f_{X \\vert Y=y}(x) = \\frac{f_{Y \\vert X=x}(y) f_X(x)}{f_Y(y)}.$ This holds for values $x$ and $y$ within the support of X and Y, ensuring $f_X(x) > 0$ and $f_Y(y)>0$."
            },
            {
                "text": "General case Let $P_Y^x $ be the conditional distribution of $Y$ given $X = x$ and let $P_X$ be the distribution of $X$. The joint distribution is then $P_{X,Y} (dx,dy) = P_Y^x (dy) P_X (dx)$. The conditional distribution $P_X^y $ of $X$ given $Y=y$ is then determined by Existence and uniqueness of the needed conditional expectation is a consequence of the Radon–Nikodym theorem. This was formulated by Kolmogorov in 1933. Kolmogorov underlines the importance of conditional probability, writing, \"I wish to call attention to ... the theory of conditional probabilities and conditional expectations\". Bayes' theorem determines the posterior distribution from the prior distribution. Uniqueness requires continuity assumptions. Bayes' theorem can be generalized to include improper prior distributions such as the uniform distribution on the real line. Modern Markov chain Monte Carlo methods have boosted the importance of Bayes' theorem, including in cases with improper priors. Examples Recreational mathematics Bayes' rule and computing conditional probabilities provide a method to solve a number of popular puzzles, such as the Three Prisoners problem, the Monty Hall problem, the Two Child problem, and the Two Envelopes problem."
            },
            {
                "text": "Drug testing Suppose, a particular test for whether someone has been using cannabis is 90% sensitive, meaning the true positive rate (TPR) = 0.90. Therefore, it leads to 90% true positive results (correct identification of drug use) for cannabis users. The test is also 80% specific, meaning true negative rate (TNR) = 0.80. Therefore, the test correctly identifies 80% of non-use for non-users, but also generates 20% false positives, or false positive rate (FPR) = 0.20, for non-users. Assuming 0.05 prevalence, meaning 5% of people use cannabis, what is the probability that a random person who tests positive is really a cannabis user? The Positive predictive value (PPV) of a test is the proportion of persons who are actually positive out of all those testing positive, and can be calculated from a sample as: PPV = True positive / Tested positive If sensitivity, specificity, and prevalence are known, PPV can be calculated using Bayes' theorem. Let $P(\\text{User}\\vert \\text{Positive}) $ mean \"the probability that someone is a cannabis user given that they test positive\", which is what PPV means."
            },
            {
                "text": "We can write: $ \\begin{align} P(\\text{User}\\vert \\text{Positive}) &= \\frac{P(\\text{Positive}\\vert \\text{User}) P(\\text{User})}{P(\\text{Positive})} \\\\ &= \\frac{P(\\text{Positive}\\vert\\text{User}) P(\\text{User})}{P(\\text{Positive}\\vert\\text{User}) P(\\text{User}) + P(\\text{Positive}\\vert\\text{Non-user}) P(\\text{Non-user})} \\\\[8pt] &= \\frac{0.90 \\times 0.05}{0.90 \\times 0.05 + 0.20 \\times 0.95} = \\frac{0.045}{0.045 + 0.19} \\approx 19\\% \\end{align}$ The denominator $ P(\\text{Positive}) = P(\\text{Positive}\\vert\\text{User}) P(\\text{User}) + P(\\text{Positive}\\vert\\text{Non-user}) P(\\text{Non-user}) $ is a direct application of the Law of Total Probability."
            },
            {
                "text": "In this case, it says that the probability that someone tests positive is the probability that a user tests positive times the probability of being a user, plus the probability that a non-user tests positive, times the probability of being a non-user. This is true because the classifications user and non-user form a partition of a set, namely the set of people who take the drug test. This combined with the definition of conditional probability results in the above statement. In other words, if someone tests positive, the probability that they are a cannabis user is only 19%—because in this group, only 5% of people are users, and most positives are false positives coming from the remaining 95%. Using a frequency box to show $P(\\text{User}\\vert \\text{Positive}) $ visually by comparison of shaded areas. Note how small the pink area of true positives is compared to the blue area of false positives. If 1,000 people were tested: 950 are non-users and 190 of them give false positive (0.20 × 950) 50 of them are users and 45 of them give true positive (0.90 × 50) The 1,000 people thus have 235 positive tests, of which only 45 are genuine, about 19%."
            },
            {
                "text": "Sensitivity or specificity The importance of specificity can be seen by showing that even if sensitivity is raised to 100% and specificity remains at 80%, the probability that someone who tests positive is a cannabis user rises only from 19% to 21%, but if the sensitivity is held at 90% and the specificity is increased to 95%, the probability rises to 49%. PositiveNegative Total User 45 5 50 Non-user 190 760 950 Total 235 765 1000 90% sensitive, 80% specific, PPV=45/235 ≈ 19% PositiveNegative Total User 50 0 50 Non-user 190 760 950 Total 240 760 1000 100% sensitive, 80% specific, PPV=50/240 ≈ 21% PositiveNegative Total User 45 5 50 Non-user 47 903 950 Total 92 908 1000 90% sensitive, 95% specific, PPV=45/92 ≈ 49% Cancer rate If all patients with pancreatic cancer have a certain symptom, it does not follow that anyone who has that symptom has a 100% chance of getting pancreatic cancer. Assuming the incidence rate of pancreatic cancer is 1/100000, while 10/99999 healthy individuals have the same symptoms worldwide, the probability of having pancreatic cancer given the symptoms is 9.1%, and the other 90.9% could be \"false positives\" (that is, falsely said to have cancer; \"positive\" is a confusing term when, as here, the test gives bad news)."
            },
            {
                "text": "Based on incidence rate, the following table presents the corresponding numbers per 100,000 people. YesNo Total Yes 1 0 1 No 10 99989 99999Total 11 99989 100000 Which can then be used to calculate the probability of having cancer when you have the symptoms: $ \\begin{align} P(\\text{Cancer}|\\text{Symptoms}) &= \\frac{P(\\text{Symptoms}|\\text{Cancer}) P(\\text{Cancer})}{P(\\text{Symptoms})} \\\\ &= \\frac{P(\\text{Symptoms}|\\text{Cancer}) P(\\text{Cancer})}{P(\\text{Symptoms}|\\text{Cancer}) P(\\text{Cancer}) + P(\\text{Symptoms}|\\text{Non-Cancer}) P(\\text{Non-Cancer})} \\\\[8pt] &= \\frac{1 \\times 0.00001}{1 \\times 0.00001 + (10/99999) \\times 0.99999} = \\frac1{11} \\approx 9.1\\% \\end{align}$ Defective item rate DefectiveFlawless Total A 10 190 200 B 9 291 300 C 5 495 500 Total 24 976 1000 A factory produces items using three machines—A, B, and C—which account for 20%, 30%, and 50% of its output, respectively."
            },
            {
                "text": "Of the items produced by machine A, 5% are defective, while 3% of B's items and 1% of C's are defective. If a randomly selected item is defective, what is the probability it was produced by machine C? Once again, the answer can be reached without using the formula by applying the conditions to a hypothetical number of cases. For example, if the factory produces 1,000 items, 200 will be produced by A, 300 by B, and 500 by C. Machine A will produce 5% × 200 = 10 defective items, B 3% × 300 = 9, and C 1% × 500 = 5, for a total of 24. Thus 24/1000 (2.4%) of the total output will be defective and the likelihood that a randomly selected defective item was produced by machine C is 5/24 (~20.83%). This problem can also be solved using Bayes' theorem: Let Xi denote the event that a randomly chosen item was made by the i th machine (for i = A,B,C)."
            },
            {
                "text": "Let Y denote the event that a randomly chosen item is defective. Then, we are given the following information: $P(X_A) = 0.2, \\quad P(X_B) = 0.3, \\quad P(X_C) = 0.5.$ If the item was made by the first machine, then the probability that it is defective is 0.05; that is, P(Y | XA) = 0.05. Overall, we have $P(Y| X_A) = 0.05, \\quad P(Y |X_B) = 0.03, \\quad P(Y| X_C) = 0.01.$ To answer the original question, we first find P(Y). That can be done in the following way: $P(Y) = \\sum_i P(Y| X_i) P(X_i) = (0.05)(0.2) + (0.03)(0.3) + (0.01)(0.5) = 0.024.$ Hence, 2.4% of the total output is defective. We are given that Y has occurred and we want to calculate the conditional probability of XC."
            },
            {
                "text": "By Bayes' theorem, $P(X_C|Y) = \\frac{P(Y | X_C) P(X_C)}{P(Y)} = \\frac{0.01 \\cdot 0.50}{0.024} = \\frac{5}{24}$ Given that the item is defective, the probability that it was made by machine C is 5/24. C produces half of the total output but a much smaller fraction of the defective items. Hence the knowledge that the item selected was defective enables us to replace the prior probability P(XC) = 1/2 by the smaller posterior probability P(XC | Y) = 5/24. Interpretations thumb|A geometric visualization of Bayes' theorem using astronauts, from the online game Among Us, who may be suspicious (with eyebrows) and may be assassins (with daggers) The interpretation of Bayes' rule depends on the interpretation of probability ascribed to the terms. The two predominant interpretations are described below. Bayesian interpretation In the Bayesian (or epistemological) interpretation, probability measures a \"degree of belief\"."
            },
            {
                "text": "Bayes' theorem links the degree of belief in a proposition before and after accounting for evidence. For example, suppose it is believed with 50% certainty that a coin is twice as likely to land heads than tails. If the coin is flipped a number of times and the outcomes observed, that degree of belief will probably rise or fall, but might remain the same, depending on the results. For proposition A and evidence B, P (A), the prior, is the initial degree of belief in A. P (A | B), the posterior, is the degree of belief after incorporating news that B is true. the quotient represents the support B provides for A. For more on the application of Bayes' theorem under the Bayesian interpretation of probability, see Bayesian inference. Frequentist interpretation thumb|Illustration of frequentist interpretation with tree diagrams In the frequentist interpretation, probability measures a \"proportion of outcomes\". For example, suppose an experiment is performed many times. P(A) is the proportion of outcomes with property A (the prior) and P(B) is the proportion with property B. P(B | A) is the proportion of outcomes with property B out of outcomes with property A, and P(A | B) is the proportion of those with A out of those with B (the posterior)."
            },
            {
                "text": "The role of Bayes' theorem can be shown with tree diagrams. The two diagrams partition the same outcomes by A and B in opposite orders, to obtain the inverse probabilities. Bayes' theorem links the different partitionings. Example thumb|Tree diagram illustrating the beetle example. R, C, P and $ \\overline{P} $ are the events rare, common, pattern and no pattern. Percentages in parentheses are calculated. Three independent values are given, so it is possible to calculate the inverse tree. An entomologist spots what might, due to the pattern on its back, be a rare subspecies of beetle. A full 98% of the members of the rare subspecies have the pattern, so P(Pattern | Rare) = 98%. Only 5% of members of the common subspecies have the pattern. The rare subspecies is 0.1% of the total population. How likely is the beetle having the pattern to be rare: what is P(Rare | Pattern)? From the extended form of Bayes' theorem (since any beetle is either rare or common), Forms Events Simple form For events A and B, provided that P(B) ≠ 0, $P(A| B) = \\frac{P(B | A) P(A)}{P(B)} ."
            },
            {
                "text": "$ In many applications, for instance in Bayesian inference, the event B is fixed in the discussion and we wish to consider the effect of its having been observed on our belief in various possible events A. In such situations the denominator of the last expression, the probability of the given evidence B, is fixed; what we want to vary is A. Bayes' theorem shows that the posterior probabilities are proportional to the numerator, so the last equation becomes: $P(A| B) \\propto P(A) \\cdot P(B| A) .$ In words, the posterior is proportional to the prior times the likelihood. If events A1, A2, ..., are mutually exclusive and exhaustive, i.e., one of them is certain to occur but no two can occur together, we can determine the proportionality constant by using the fact that their probabilities must add up to one. For instance, for a given event A, the event A itself and its complement ¬A are exclusive and exhaustive. Denoting the constant of proportionality by c, we have: $P(A| B) = c \\cdot P(A) \\cdot P(B| A) \\text{ and } P(\\neg A| B) = c \\cdot P(\\neg A) \\cdot P(B| \\neg A)."
            },
            {
                "text": "$ Adding these two formulas we deduce that: $ 1 = c \\cdot (P(B| A)\\cdot P(A) + P(B| \\neg A) \\cdot P(\\neg A)),$ or $ c = \\frac{1}{P(B| A)\\cdot P(A) + P(B| \\neg A) \\cdot P(\\neg A)} = \\frac 1 {P(B)}."
            },
            {
                "text": "$ Alternative form + Contingency table B (not ) Total $P(B|A)\\cdot P(A)$$= P(A|B)\\cdot P(B)$ $P(\\neg B|A)\\cdot P(A)$$= P(A|\\neg B)\\cdot P(\\neg B)$ (not )$P(B|\\neg A)\\cdot P(\\neg A)$$= P(\\neg A|B)\\cdot P(B)$ $P(\\neg B|\\neg A)\\cdot P(\\neg A)$$= P(\\neg A|\\neg B)\\cdot P(\\neg B)$ $P(\\neg A)$=$1-P(A)$ Total $P(\\neg B) = 1-P(B)$ 1 Another form of Bayes' theorem for two competing statements or hypotheses is: $P(A| B) = \\frac{P(B| A) P(A)}{ P(B| A) P(A) + P(B| \\neg A) P(\\neg A)}.$ For an epistemological interpretation: For proposition A and evidence or background B, $P(A)$ is the prior probability, the initial degree of belief in A."
            },
            {
                "text": "$P(\\neg A)$ is the corresponding initial degree of belief in not-A, that A is false, where $ P(\\neg A) =1-P(A) $ $P(B| A)$ is the conditional probability or likelihood, the degree of belief in B given that A is true. $P(B|\\neg A)$ is the conditional probability or likelihood, the degree of belief in B given that A is false. $P(A| B)$ is the posterior probability, the probability of A after taking into account B. Extended form Often, for some partition {Aj} of the sample space, the event space is given in terms of P(Aj) and P(B | Aj)."
            },
            {
                "text": "It is then useful to compute P(B) using the law of total probability: $P(B)=\\sum_{j}P(B \\cap A_j),$ Or (using the multiplication rule for conditional probability), $P(B) = {\\sum_j P(B| A_j) P(A_j)},$ $\\Rightarrow P(A_i| B) = \\frac{P(B| A_i) P(A_i)}{\\sum\\limits_j P(B| A_j) P(A_j)}\\cdot$ In the special case where A is a binary variable: $P(A| B) = \\frac{P(B| A) P(A)}{ P(B| A) P(A) + P(B| \\neg A) P(\\neg A)}\\cdot$ Random variables thumb|Bayes' theorem applied to an event space generated by continuous random variables X and Y with known probability distributions."
            },
            {
                "text": "There exists an instance of Bayes' theorem for each point in the domain. In practice, these instances might be parametrized by writing the specified probability densities as a function of x and y. Consider a sample space Ω generated by two random variables X and Y with known probability distributions. In principle, Bayes' theorem applies to the events A = {X = x} and B = {Y = y}. $P( X{=}x | Y {=} y) = \\frac{P(Y{=}y | X{=}x) P(X{=}x)}{P(Y{=}y)}$ Terms become 0 at points where either variable has finite probability density. To remain useful, Bayes' theorem can be formulated in terms of the relevant densities (see Derivation). Simple form If X is continuous and Y is discrete, $f_{X | Y{=}y}(x) = \\frac{P(Y{=}y| X{=}x) f_X(x)}{P(Y{=}y)}$ where each $f$ is a density function."
            },
            {
                "text": "If X is discrete and Y is continuous, $ P(X{=}x| Y{=}y) = \\frac{f_{Y | X{=}x}(y) P(X{=}x)}{f_Y(y)}.$ If both X and Y are continuous, $ f_{X| Y{=}y}(x) = \\frac{f_{Y | X{=}x}(y) f_X(x)}{f_Y(y)}.$ Extended form thumb|A way to conceptualize event spaces generated by continuous random variables X and Y A continuous event space is often conceptualized in terms of the numerator terms. It is then useful to eliminate the denominator using the law of total probability. For fY(y), this becomes an integral: $ f_Y(y) = \\int_{-\\infty}^\\infty f_{Y| X = \\xi}(y) f_X(\\xi)\\,d\\xi .$ Bayes' rule in odds form Bayes' theorem in odds form is: $O(A_1:A_2\\vert B) = O(A_1:A_2) \\cdot \\Lambda(A_1:A_2\\vert B) $ where $\\Lambda(A_1:A_2\\vert B) = \\frac{P(B\\vert A_1)}{P(B\\vert A_2)}$ is called the Bayes factor or likelihood ratio."
            },
            {
                "text": "The odds between two events is simply the ratio of the probabilities of the two events. Thus: $O(A_1:A_2) = \\frac{P(A_1)}{P(A_2)},$ $O(A_1:A_2\\vert B) = \\frac{P(A_1\\vert B)}{P(A_2\\vert B)},$ Thus the rule says that the posterior odds are the prior odds times the Bayes factor; in other words, the posterior is proportional to the prior times the likelihood. In the special case that $A_1 = A$ and $A_2 = \\neg A$, one writes $O(A)=O(A:\\neg A) =P(A)/(1-P(A))$, and uses a similar abbreviation for the Bayes factor and for the conditional odds. The odds on $A$ is by definition the odds for and against $A$. Bayes' rule can then be written in the abbreviated form $O(A\\vert B) = O(A) \\cdot \\Lambda(A\\vert B) ,$ or, in words, the posterior odds on $A$ equals the prior odds on $A$ times the likelihood ratio for $A$ given information $B$."
            },
            {
                "text": "In short, posterior odds equals prior odds times likelihood ratio. For example, if a medical test has a sensitivity of 90% and a specificity of 91%, then the positive Bayes factor is $\\Lambda_+ = P(\\text{True Positive})/P(\\text{False Positive}) = 90\\%/(100\\%-91\\%)=10$. Now, if the prevalence of this disease is 9.09%, and if we take that as the prior probability, then the prior odds is about 1:10. So after receiving a positive test result, the posterior odds of having the disease becomes 1:1, which means that the posterior probability of having the disease is 50%. If a second test is performed in serial testing, and that also turns out to be positive, then the posterior odds of having the disease becomes 10:1, which means a posterior probability of about 90.91%. The negative Bayes factor can be calculated to be 91%/(100%-90%)=9.1, so if the second test turns out to be negative, then the posterior odds of having the disease is 1:9.1, which means a posterior probability of about 9.9%."
            },
            {
                "text": "The example above can also be understood with more solid numbers: assume the patient taking the test is from a group of 1,000 people, 91 of whom have the disease (prevalence of 9.1%). If all 1,000 take the test, 82 of those with the disease will get a true positive result (sensitivity of 90.1%), 9 of those with the disease will get a false negative result (false negative rate of 9.9%), 827 of those without the disease will get a true negative result (specificity of 91.0%), and 82 of those without the disease will get a false positive result (false positive rate of 9.0%). Before taking any test, the patient's odds for having the disease is 91:909. After receiving a positive result, the patient's odds for having the disease is $\\frac{91}{909}\\times\\frac{90.1\\%}{9.0\\%}=\\frac{91\\times90.1\\%}{909\\times9.0\\%}=1:1$ which is consistent with the fact that there are 82 true positives and 82 false positives in the group of 1,000."
            },
            {
                "text": "Correspondence to other mathematical frameworks Propositional logic Where the conditional probability $P(A \\vert B)$ is defined, it can be seen to capture the implication $B \\to A$. The probabilistic calculus then mirrors or even generalizes various logical inference rules. Beyond, for example, assigning binary truth values, here one assigns probability values to statements. The assertion $B \\to A$ is captured by the assertion $P(A\\vert B) = 1$, i.e. that the conditional probability take the extremal probability value $1$. Likewise, the assertion of a negation of an implication is captured by the assignment of $0$.Audun Jøsang, 2016, Subjective Logic; A formalism for Reasoning Under Uncertainty. Springer, Cham, So, for example, if $P(A) = 1$, then (if it is defined) $P(A\\vert B) = 1$, which entails $A\\to (B \\to A)$, the implication introduction in logic. Similarly, as the product of two probabilities equaling $1$ necessitates that both factors are also $1$, one finds that Bayes' theorem $P(A)\\, P(B \\vert A) = P(B)\\, P(A\\vert B)$ entails $\\big(A\\land(A\\to B)\\big) \\leftrightarrow \\big(B\\land(B\\to A)\\big)$, which now also includes modus ponens."
            },
            {
                "text": "For positive values $P(A)$, if it equals $P(B)$, then the two conditional probabilities are equal as well, and vice versa. Note that this mirrors the generally valid $(A\\leftrightarrow B)\\leftrightarrow\\big((A\\to B)\\leftrightarrow(B\\to A)\\big)$. On the other hand, reasoning about either of the probabilities equalling $0$ classically entails the following contrapositive form of the above: $\\big(\\neg B\\lor\\neg(B\\to A)\\big)\\leftrightarrow\\big(\\neg A\\lor\\neg(A\\to B)\\big)$. Bayes' theorem with negated $A$ gives $P(B \\vert \\neg A) \\big(1-P(A)\\big) = \\big(1-P(A\\vert B)\\big)P(B)$. Ruling out the extremal case $P(\\neg A)=0$ (i.e. $P(A)=1$), one has $P(B\\vert \\neg A) = P(B)\\cdot\\tfrac{1-P(A\\vert B)}{1 - P(A)}$ and in particular $P(\\neg B\\vert \\neg A) = 1 - P(B)\\cdot\\frac{1-P(A\\vert B)}{1 - P(A)}$."
            },
            {
                "text": "Ruling out also the extremal case $P(B)=0$, one finds they attain the maximum $1$ simultaneously: $P(A\\vert B) = 1\\ \\leftrightarrow\\ P(\\neg B\\vert \\neg A) = 1$ which (at least when having ruled out explosive antecedents) captures the classical contraposition principle $(B \\to A) \\leftrightarrow (\\neg A \\to \\neg B)$. Subjective logic Bayes' theorem represents a special case of deriving inverted conditional opinions in subjective logic expressed as: $(\\omega^S_{A\\tilde{|}B},\\omega^S_{A\\tilde{|}\\lnot B}) = (\\omega^S_{B\\vert A}, \\omega^S_{B\\vert\\lnot A}) \\widetilde{\\phi} a_A,$ where $\\widetilde{\\phi}$ denotes the operator for inverting conditional opinions. The argument $(\\omega^S_{B\\vert A},\\omega^S_{B\\vert\\lnot A})$ denotes a pair of binomial conditional opinions given by source $S$, and the argument $a_{A}$ denotes the prior probability (aka."
            },
            {
                "text": "the base rate) of $A$. The pair of derivative inverted conditional opinions is denoted $(\\omega^S_{A\\tilde{|}B},\\omega^{S}_{A\\tilde{|}\\lnot B})$. The conditional opinion $\\omega^S_{A\\vert B}$ generalizes the probabilistic conditional $P(A \\vert B)$, i.e. in addition to assigning a probability the source $S$ can assign any subjective opinion to the conditional statement $(A\\vert B)$. A binomial subjective opinion $\\omega^{S}_{A}$ is the belief in the truth of statement $A$ with degrees of epistemic uncertainty, as expressed by source $S$. Every subjective opinion has a corresponding projected probability $P(\\omega^{S}_{A})$. The application of Bayes' theorem to projected probabilities of opinions is a homomorphism, meaning that Bayes' theorem can be expressed in terms of projected probabilities of opinions: $P(\\omega^S_{A \\tilde{|} B}) = \\frac{P(\\omega^S_{B \\vert A}) a(A)}{P(\\omega^S_{B\\vert A}) a(A) + P(\\omega^S_{B \\vert \\lnot A}) a(\\lnot A)}."
            },
            {
                "text": "$ Hence, the subjective Bayes' theorem represents a generalization of Bayes' theorem.Audun Jøsang, 2016, Generalising Bayes' Theorem in Subjective Logic. IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI 2016), Baden-Baden, September 2016 Generalizations Bayes' theorem for 3 events A version of Bayes' theorem for 3 events results from the addition of a third event $C$, with $P(C)>0,$ on which all probabilities are conditioned: $P(A \\vert B \\cap C) = \\frac{P(B \\vert A \\cap C) \\, P(A \\vert C)}{P(B \\vert C)} $ Derivation Using the chain rule $P(A \\cap B \\cap C) = P(A \\vert B \\cap C) \\, P(B \\vert C) \\, P(C)$ And, on the other hand $P(A \\cap B \\cap C) = P(B \\cap A \\cap C) = P(B \\vert A \\cap C) \\, P(A \\vert C) \\, P(C) $ The desired result is obtained by identifying both expressions and solving for $P(A \\vert B \\cap C)$."
            },
            {
                "text": "Use in genetics In genetics, Bayes' rule can be used to estimate the probability that someone has a specific genotype. Many people seek to assess their chances of being affected by a genetic disease or their likelihood of being a carrier for a recessive gene of interest. A Bayesian analysis can be done based on family history or genetic testing to predict whether someone will develop a disease or pass one on to their children. Genetic testing and prediction is common among couples who plan to have children but are concerned that they may both be carriers for a disease, especially in communities with low genetic variance. Using pedigree to calculate probabilities HypothesisHypothesis 1: Patient is a carrierHypothesis 2: Patient is not a carrierPrior Probability1/21/2Conditional Probability that all four offspring will be unaffected(1/2) ⋅ (1/2) ⋅ (1/2) ⋅ (1/2) = 1/16 About 1Joint Probability(1/2) ⋅ (1/16) = 1/32 (1/2) ⋅ 1 = 1/2Posterior Probability(1/32) / (1/32 + 1/2) = 1/17(1/2) / (1/32 + 1/2) = 16/17 Example of a Bayesian analysis table for a female's risk for a disease based on the knowledge that the disease is present in her siblings but not in her parents or any of her four children."
            },
            {
                "text": "Based solely on the status of the subject's siblings and parents, she is equally likely to be a carrier as to be a non-carrier (this likelihood is denoted by the Prior Hypothesis). The probability that the subject's four sons would all be unaffected is 1/16 (⋅⋅⋅) if she is a carrier and about 1 if she is a non-carrier (this is the Conditional Probability). The Joint Probability reconciles these two predictions by multiplying them together. The last line (the Posterior Probability) is calculated by dividing the Joint Probability for each hypothesis by the sum of both joint probabilities. Using genetic test results Parental genetic testing can detect around 90% of known disease alleles in parents that can lead to carrier or affected status in their children. Cystic fibrosis is a heritable disease caused by an autosomal recessive mutation on the CFTR gene,\"Types of CFTR Mutations\". Cystic Fibrosis Foundation, www.cff.org/What-is-CF/Genetics/Types-of-CFTR-Mutations/. located on the q arm of chromosome 7. \"CFTR Gene – Genetics Home Reference\". U.S. National Library of Medicine, National Institutes of Health, ghr.nlm.nih.gov/gene/CFTR#location."
            },
            {
                "text": "Here is a Bayesian analysis of a female patient with a family history of cystic fibrosis (CF) who has tested negative for CF, demonstrating how the method was used to determine her risk of having a child born with CF: because the patient is unaffected, she is either homozygous for the wild-type allele, or heterozygous. To establish prior probabilities, a Punnett square is used, based on the knowledge that neither parent was affected by the disease but both could have been carriers: W Homozygous for the wild-type allele (a non-carrier)M Heterozygous(a CF carrier)W Homozygous for the wild-type allele (a non-carrier)WWMWM Heterozygous (a CF carrier)MWMM (affected by cystic fibrosis) Given that the patient is unaffected, there are only three possibilities. Within these three, there are two scenarios in which the patient carries the mutant allele. Thus the prior probabilities are and . Next, the patient undergoes genetic testing and tests negative for cystic fibrosis. This test has a 90% detection rate, so the conditional probabilities of a negative test are 1/10 and 1."
            },
            {
                "text": "Finally, the joint and posterior probabilities are calculated as before. HypothesisHypothesis 1: Patient is a carrierHypothesis 2: Patient is not a carrierPrior Probability2/31/3Conditional Probability of a negative test1/101Joint Probability1/151/3Posterior Probability1/65/6 After carrying out the same analysis on the patient's male partner (with a negative test result), the chance that their child is affected is the product of the parents' respective posterior probabilities for being carriers times the chance that two carriers will produce an affected offspring (). Genetic testing done in parallel with other risk factor identification Bayesian analysis can be done using phenotypic information associated with a genetic condition. When combined with genetic testing, this analysis becomes much more complicated. Cystic fibrosis, for example, can be identified in a fetus with an ultrasound looking for an echogenic bowel, one that appears brighter than normal on a scan. This is not a foolproof test, as an echogenic bowel can be present in a perfectly healthy fetus. Parental genetic testing is very influential in this case, where a phenotypic facet can be overly influential in probability calculation. In the case of a fetus with an echogenic bowel, with a mother who has been tested and is known to be a CF carrier, the posterior probability that the fetus has the disease is very high (0.64). But once the father has tested negative for CF, the posterior probability drops significantly (to 0.16). Risk factor calculation is a powerful tool in genetic counseling and reproductive planning but cannot be treated as the only important factor."
            },
            {
                "text": "As above, incomplete testing can yield falsely high probability of carrier status, and testing can be financially inaccessible or unfeasible when a parent is not present. See also Bayesian epistemology Inductive probability Quantum Bayesianism Why Most Published Research Findings Are False, a 2005 essay in metascience by John Ioannidis Regular conditional probability Bayesian persuasion Notes References Bibliography Further reading External links Category:Bayesian statistics Category:Probability theorems Category:Theorems in statistics"
            }
        ],
        "latex_formulas": [
            "P(A\\vert B) = \\frac{P(B \\vert A) P(A)}{P(B)}",
            "A",
            "B",
            "P(B) \\neq 0",
            "P(A\\vert B)",
            "A",
            "B",
            "A",
            "B",
            "P(B\\vert A)",
            "B",
            "A",
            "A",
            "B",
            "P(B\\vert A)=L(A\\vert B)",
            "P(A)",
            "P(B)",
            "A",
            "B",
            "P(A\\vert B)=\\frac{P(A \\cap B)}{P(B)}, \\text{ if } P(B) \\neq 0,",
            "P(A \\cap B)",
            "P(B\\vert A)=\\frac{P(A \\cap B)}{P(A)}, \\text{ if } P(A) \\neq 0.",
            "P(A \\cap B)",
            "P(A\\vert B)",
            "P(A\\vert B) = \\frac{P(B\\vert A) P(A)}{P(B)}, \\text{ if } P(B) \\neq 0.",
            "f_{X \\vert  Y=y} (x) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}",
            "f_{Y \\vert  X=x}(y) = \\frac{f_{X,Y}(x,y)}{f_X(x)}",
            "f_{X \\vert Y=y}(x) = \\frac{f_{Y \\vert  X=x}(y) f_X(x)}{f_Y(y)}.",
            "x",
            "y",
            "f_X(x) > 0",
            "f_Y(y)>0",
            "P_Y^x",
            "Y",
            "X = x",
            "P_X",
            "X",
            "P_{X,Y} (dx,dy) = P_Y^x (dy) P_X (dx)",
            "P_X^y",
            "X",
            "Y=y",
            "P(\\text{User}\\vert \\text{Positive})",
            "\\begin{align}\nP(\\text{User}\\vert \\text{Positive}) &= \\frac{P(\\text{Positive}\\vert \\text{User}) P(\\text{User})}{P(\\text{Positive})} \\\\\n &= \\frac{P(\\text{Positive}\\vert\\text{User}) P(\\text{User})}{P(\\text{Positive}\\vert\\text{User}) P(\\text{User}) + P(\\text{Positive}\\vert\\text{Non-user}) P(\\text{Non-user})} \\\\[8pt] \n&= \\frac{0.90 \\times 0.05}{0.90 \\times 0.05 + 0.20 \\times 0.95}\n= \\frac{0.045}{0.045 + 0.19} \\approx 19\\%\n\\end{align}",
            "P(\\text{Positive}) = P(\\text{Positive}\\vert\\text{User}) P(\\text{User}) + P(\\text{Positive}\\vert\\text{Non-user}) P(\\text{Non-user})",
            "P(\\text{User}\\vert \\text{Positive})",
            "\\begin{align}\nP(\\text{Cancer}|\\text{Symptoms}) &= \\frac{P(\\text{Symptoms}|\\text{Cancer}) P(\\text{Cancer})}{P(\\text{Symptoms})} \\\\\n &= \\frac{P(\\text{Symptoms}|\\text{Cancer}) P(\\text{Cancer})}{P(\\text{Symptoms}|\\text{Cancer}) P(\\text{Cancer}) + P(\\text{Symptoms}|\\text{Non-Cancer}) P(\\text{Non-Cancer})} \\\\[8pt]\n&= \\frac{1 \\times 0.00001}{1 \\times 0.00001 + (10/99999) \\times 0.99999} = \\frac1{11} \\approx 9.1\\%\n\\end{align}",
            "P(X_A) = 0.2, \\quad P(X_B) = 0.3, \\quad  P(X_C) = 0.5.",
            "P(Y| X_A) = 0.05, \\quad  P(Y |X_B) = 0.03, \\quad  P(Y| X_C) = 0.01.",
            "P(Y) = \\sum_i  P(Y| X_i) P(X_i) = (0.05)(0.2) + (0.03)(0.3) + (0.01)(0.5) = 0.024.",
            "P(X_C|Y) = \\frac{P(Y | X_C) P(X_C)}{P(Y)} = \\frac{0.01 \\cdot 0.50}{0.024} = \\frac{5}{24}",
            "\\overline{P}",
            "P(A| B) = \\frac{P(B |  A) P(A)}{P(B)} .",
            "P(A| B) \\propto P(A) \\cdot P(B| A) .",
            "P(A| B) = c \\cdot P(A) \\cdot P(B| A) \\text{ and } P(\\neg A| B) = c \\cdot P(\\neg A) \\cdot P(B| \\neg A).",
            "1 = c \\cdot (P(B| A)\\cdot P(A) + P(B| \\neg A) \\cdot P(\\neg A)),",
            "c = \\frac{1}{P(B| A)\\cdot P(A) + P(B| \\neg A) \\cdot P(\\neg A)}  = \\frac 1 {P(B)}.",
            "P(B|A)\\cdot P(A)",
            "= P(A|B)\\cdot P(B)",
            "P(\\neg B|A)\\cdot P(A)",
            "= P(A|\\neg B)\\cdot P(\\neg B)",
            "P(B|\\neg A)\\cdot P(\\neg A)",
            "= P(\\neg A|B)\\cdot P(B)",
            "P(\\neg B|\\neg A)\\cdot P(\\neg A)",
            "= P(\\neg A|\\neg B)\\cdot P(\\neg B)",
            "P(\\neg A)",
            "1-P(A)",
            "P(\\neg B) = 1-P(B)",
            "P(A| B) = \\frac{P(B| A) P(A)}{ P(B| A) P(A) + P(B| \\neg A) P(\\neg A)}.",
            "P(A)",
            "P(\\neg A)",
            "P(\\neg A) =1-P(A)",
            "P(B| A)",
            "P(B|\\neg A)",
            "P(A| B)",
            "P(B)=\\sum_{j}P(B \\cap A_j),",
            "P(B) = {\\sum_j P(B| A_j) P(A_j)},",
            "\\Rightarrow P(A_i| B) = \\frac{P(B| A_i) P(A_i)}{\\sum\\limits_j P(B| A_j) P(A_j)}\\cdot",
            "P(A| B) = \\frac{P(B| A) P(A)}{ P(B| A) P(A) + P(B| \\neg A) P(\\neg A)}\\cdot",
            "P( X{=}x  | Y {=} y) = \\frac{P(Y{=}y | X{=}x) P(X{=}x)}{P(Y{=}y)}",
            "f_{X | Y{=}y}(x) = \\frac{P(Y{=}y| X{=}x) f_X(x)}{P(Y{=}y)}",
            "f",
            "P(X{=}x| Y{=}y) = \\frac{f_{Y | X{=}x}(y) P(X{=}x)}{f_Y(y)}.",
            "f_{X| Y{=}y}(x) = \\frac{f_{Y | X{=}x}(y) f_X(x)}{f_Y(y)}.",
            "f_Y(y) = \\int_{-\\infty}^\\infty f_{Y| X = \\xi}(y) f_X(\\xi)\\,d\\xi .",
            "O(A_1:A_2\\vert B) = O(A_1:A_2) \\cdot \\Lambda(A_1:A_2\\vert B)",
            "\\Lambda(A_1:A_2\\vert B) = \\frac{P(B\\vert A_1)}{P(B\\vert A_2)}",
            "O(A_1:A_2) = \\frac{P(A_1)}{P(A_2)},",
            "O(A_1:A_2\\vert  B) = \\frac{P(A_1\\vert  B)}{P(A_2\\vert  B)},",
            "A_1 = A",
            "A_2 = \\neg A",
            "O(A)=O(A:\\neg A) =P(A)/(1-P(A))",
            "A",
            "A",
            "O(A\\vert B) = O(A)  \\cdot \\Lambda(A\\vert B) ,",
            "A",
            "A",
            "A",
            "B",
            "\\Lambda_+ = P(\\text{True Positive})/P(\\text{False Positive}) = 90\\%/(100\\%-91\\%)=10",
            "\\frac{91}{909}\\times\\frac{90.1\\%}{9.0\\%}=\\frac{91\\times90.1\\%}{909\\times9.0\\%}=1:1",
            "P(A \\vert B)",
            "B \\to A",
            "B \\to A",
            "P(A\\vert B) = 1",
            "1",
            "0",
            "P(A) = 1",
            "P(A\\vert B) = 1",
            "A\\to (B \\to A)",
            "1",
            "1",
            "P(A)\\, P(B \\vert A) = P(B)\\, P(A\\vert B)",
            "\\big(A\\land(A\\to B)\\big) \\leftrightarrow \\big(B\\land(B\\to A)\\big)",
            "P(A)",
            "P(B)",
            "(A\\leftrightarrow B)\\leftrightarrow\\big((A\\to B)\\leftrightarrow(B\\to A)\\big)",
            "0",
            "\\big(\\neg B\\lor\\neg(B\\to A)\\big)\\leftrightarrow\\big(\\neg A\\lor\\neg(A\\to B)\\big)",
            "A",
            "P(B \\vert \\neg A) \\big(1-P(A)\\big) = \\big(1-P(A\\vert B)\\big)P(B)",
            "P(\\neg A)=0",
            "P(A)=1",
            "P(B\\vert \\neg A) = P(B)\\cdot\\tfrac{1-P(A\\vert B)}{1 - P(A)}",
            "P(\\neg B\\vert \\neg A) = 1 - P(B)\\cdot\\frac{1-P(A\\vert B)}{1 - P(A)}",
            "P(B)=0",
            "1",
            "P(A\\vert B) = 1\\ \\leftrightarrow\\ P(\\neg B\\vert \\neg A) = 1",
            "(B \\to A) \\leftrightarrow (\\neg A \\to \\neg B)",
            "(\\omega^S_{A\\tilde{|}B},\\omega^S_{A\\tilde{|}\\lnot B}) = (\\omega^S_{B\\vert A}, \\omega^S_{B\\vert\\lnot A}) \\widetilde{\\phi} a_A,",
            "\\widetilde{\\phi}",
            "(\\omega^S_{B\\vert A},\\omega^S_{B\\vert\\lnot A})",
            "S",
            "a_{A}",
            "A",
            "(\\omega^S_{A\\tilde{|}B},\\omega^{S}_{A\\tilde{|}\\lnot B})",
            "\\omega^S_{A\\vert B}",
            "P(A \\vert B)",
            "S",
            "(A\\vert B)",
            "\\omega^{S}_{A}",
            "A",
            "S",
            "P(\\omega^{S}_{A})",
            "P(\\omega^S_{A \\tilde{|} B}) = \\frac{P(\\omega^S_{B \\vert A}) a(A)}{P(\\omega^S_{B\\vert A})  a(A) + P(\\omega^S_{B \\vert \\lnot A}) a(\\lnot A)}.",
            "C",
            "P(C)>0,",
            "P(A \\vert B \\cap C) = \\frac{P(B \\vert A \\cap C) \\, P(A \\vert C)}{P(B \\vert C)}",
            "P(A \\cap B \\cap C) = P(A \\vert B \\cap C) \\, P(B \\vert C) \\, P(C)",
            "P(A \\cap B \\cap C) = P(B \\cap A \\cap C) = P(B \\vert A \\cap C) \\, P(A \\vert C) \\, P(C)",
            "P(A \\vert B \\cap C)"
        ]
    },
    "Decision_boundary": {
        "title": "Decision_boundary",
        "chunks": [
            {
                "text": "__NOTOC__ In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous. If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable. Decision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous. Decision boundaries can be approximations of optimal stopping boundaries. The decision boundary is the set of points of that hyperplane that pass through zero. https://cmci.colorado.edu/classes/INFO-4604/files/notes_svm.pdf For example, the angle between a vector and points in a set must be zero for points that are on or close to the decision boundary."
            },
            {
                "text": "Decision boundary instability can be incorporated with generalization error as a standard for selecting the most accurate and stable classifier. In neural networks and support vector models In the case of backpropagation based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has. If it has no hidden layers, then it can only learn linear problems. If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the universal approximation theorem, thus it can have an arbitrary decision boundary. In particular, support vector machines find a hyperplane that separates the feature space into two classes with the maximum margin. If the problem is not originally linearly separable, the kernel trick can be used to turn it into a linearly separable one, by increasing the number of dimensions. Thus a general hypersurface in a small dimension space is turned into a hyperplane in a space with much larger dimensions. Neural networks try to learn the decision boundary which minimizes the empirical error, while support vector machines try to learn the decision boundary which maximizes the empirical margin between the decision boundary and data points. See also Discriminant function Hyperplane separation theorem References Further reading Category:Classification algorithms Category:Statistical classification Category:Pattern recognition#Probabilistic classifiers"
            }
        ],
        "latex_formulas": []
    },
    "Probability_theory": {
        "title": "Probability_theory",
        "chunks": [
            {
                "text": "Probability theory or probability calculus is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of the sample space is called an event. Central subjects in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes (which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion). Although it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem. As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of data.Inferring From Data Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics or sequential estimation."
            },
            {
                "text": "A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics. History of probability The modern mathematical theory of probability has its roots in attempts to analyze games of chance by Gerolamo Cardano in the sixteenth century, and by Pierre de Fermat and Blaise Pascal in the seventeenth century (for example the \"problem of points\"). Christiaan Huygens published a book on the subject in 1657. In the 19th century, what is considered the classical definition of probability was completed by Pierre Laplace. Initially, probability theory mainly considered events, and its methods were mainly combinatorial. Eventually, analytical considerations compelled the incorporation of variables into the theory. This culminated in modern probability theory, on foundations laid by Andrey Nikolaevich Kolmogorov. Kolmogorov combined the notion of sample space, introduced by Richard von Mises, and measure theory and presented his axiom system for probability theory in 1933. This became the mostly undisputed axiomatic basis for modern probability theory; but, alternatives exist, such as the adoption of finite rather than countable additivity by Bruno de Finetti."
            },
            {
                "text": "Treatment Most introductions to probability theory treat discrete probability distributions and continuous probability distributions separately. The measure theory-based treatment of probability covers the discrete, continuous, a mix of the two, and more. Motivation Consider an experiment that can produce a number of outcomes. The set of all outcomes is called the sample space of the experiment. The power set of the sample space (or equivalently, the event space) is formed by considering all different collections of possible results. For example, rolling an honest die produces one of six possible results. One collection of possible results corresponds to getting an odd number. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called events. In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, that event is said to have occurred. Probability is a way of assigning every \"event\" a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) be assigned a value of one."
            },
            {
                "text": "To qualify as a probability distribution, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events that contain no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that any of these events occurs is given by the sum of the probabilities of the events. The probability that any one of the events {1,6}, {3}, or {2,4} will occur is 5/6. This is the same as saying that the probability of event {1,2,3,4,6} is 5/6. This event encompasses the possibility of any number except five being rolled. The mutually exclusive event {5} has a probability of 1/6, and the event {1,2,3,4,5,6} has a probability of 1, that is, absolute certainty. When doing calculations using the outcomes of an experiment, it is necessary that all those elementary events have a number assigned to them. This is done using a random variable. A random variable is a function that assigns to each elementary event in the sample space a real number."
            },
            {
                "text": "This function is usually denoted by a capital letter. In the case of a die, the assignment of a number to certain elementary events can be done using the identity function. This does not always work. For example, when flipping a coin the two possible outcomes are \"heads\" and \"tails\". In this example, the random variable X could assign to the outcome \"heads\" the number \"0\" () and to the outcome \"tails\" the number \"1\" ($X(\\text{tails})=1$). Discrete probability distributions The Poisson distribution, a discrete probability distribution deals with events that occur in countable sample spaces. Examples: Throwing dice, experiments with decks of cards, random walk, and tossing coins. : Initially the probability of an event to occur was defined as the number of cases favorable for the event, over the number of total outcomes possible in an equiprobable sample space: see Classical definition of probability. For example, if the event is \"occurrence of an even number when a dice is rolled\", the probability is given by $\\tfrac{3}{6}=\\tfrac{1}{2}$, since 3 faces out of the 6 have even numbers and each face has the same probability of appearing."
            },
            {
                "text": ": The modern definition starts with a finite or countable set called the sample space, which relates to the set of all possible outcomes in classical sense, denoted by $\\Omega$. It is then assumed that for each element $x \\in \\Omega\\,$, an intrinsic \"probability\" value $f(x)\\,$ is attached, which satisfies the following properties: $f(x)\\in[0,1]\\mbox{ for all }x\\in \\Omega\\,;$ $\\sum_{x\\in \\Omega} f(x) = 1\\,.$ That is, the probability function f(x) lies between zero and one for every value of x in the sample space Ω, and the sum of f(x) over all values x in the sample space Ω is equal to 1. An is defined as any subset $E\\,$ of the sample space $\\Omega\\,$. The of the event $E\\,$ is defined as $P(E)=\\sum_{x\\in E} f(x)\\,.$ So, the probability of the entire sample space is 1, and the probability of the null event is 0."
            },
            {
                "text": "The function $f(x)\\,$ mapping a point in the sample space to the \"probability\" value is called a abbreviated as . Continuous probability distributions The normal distribution, a continuous probability distribution deals with events that occur in a continuous sample space. : The classical definition breaks down when confronted with the continuous case. See Bertrand's paradox. : If the sample space of a random variable X is the set of real numbers ($\\mathbb{R}$) or a subset thereof, then a function called the () $F\\,$ exists, defined by $F(x) = P(X\\le x) \\,$. That is, F(x) returns the probability that X will be less than or equal to x. The CDF necessarily satisfies the following properties. $F\\,$ is a monotonically non-decreasing, right-continuous function; $\\lim_{x\\rightarrow -\\infty} F(x)=0\\,;$ $\\lim_{x\\rightarrow \\infty} F(x)=1\\,.$ The random variable $X$ is said to have a continuous probability distribution if the corresponding CDF $F$ is continuous."
            },
            {
                "text": "If $F\\,$ is absolutely continuous, then its derivative exists almost everywhere and integrating the derivative gives us the CDF back again. In this case, the random variable X is said to have a () or simply $f(x)=\\frac{dF(x)}{dx}\\,.$ For a set $E \\subseteq \\mathbb{R}$, the probability of the random variable X being in $E\\,$ is $P(X\\in E) = \\int_{x\\in E} dF(x)\\,.$ In case the PDF exists, this can be written as $P(X\\in E) = \\int_{x\\in E} f(x)\\,dx\\,.$ Whereas the PDF exists only for continuous random variables, the CDF exists for all random variables (including discrete random variables) that take values in $\\mathbb{R}\\,.$ These concepts can be generalized for multidimensional cases on $\\mathbb{R}^n$ and other continuous sample spaces. Measure-theoretic probability theory The utility of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used."
            },
            {
                "text": "Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two. An example of such distributions could be a mix of discrete and continuous distributions—for example, a random variable that is 0 with probability 1/2, and takes a random value from a normal distribution with probability 1/2. It can still be studied to some extent by considering it to have a PDF of $(\\delta[x] + \\varphi(x))/2$, where $\\delta[x]$ is the Dirac delta function. Other distributions may not even be a mix, for example, the Cantor distribution has no positive probability for any single point, neither does it have a density. The modern approach to probability theory solves these problems using measure theory to define the probability space: Given any set $\\Omega\\,$ (also called ) and a σ-algebra $\\mathcal{F}\\,$ on it, a measure $P\\,$ defined on $\\mathcal{F}\\,$ is called a if $P(\\Omega)=1.\\,$ If $\\mathcal{F}\\,$ is the Borel σ-algebra on the set of real numbers, then there is a unique probability measure on $\\mathcal{F}\\,$ for any CDF, and vice versa."
            },
            {
                "text": "The measure corresponding to a CDF is said to be by the CDF. This measure coincides with the pmf for discrete variables and PDF for continuous variables, making the measure-theoretic approach free of fallacies. The probability of a set $E\\,$ in the σ-algebra $\\mathcal{F}\\,$ is defined as $P(E) = \\int_{\\omega\\in E} \\mu_F(d\\omega)\\,$ where the integration is with respect to the measure $\\mu_F\\,$ induced by $F\\,.$ Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outside $\\mathbb{R}^n$, as in the theory of stochastic processes. For example, to study Brownian motion, probability is defined on a space of functions. When it is convenient to work with a dominating measure, the Radon-Nikodym theorem is used to define a density as the Radon-Nikodym derivative of the probability distribution of interest with respect to this dominating measure. Discrete densities are usually defined as this derivative with respect to a counting measure over the set of all possible outcomes."
            },
            {
                "text": "Densities for absolutely continuous distributions are usually defined as this derivative with respect to the Lebesgue measure. If a theorem can be proved in this general setting, it holds for both discrete and continuous distributions as well as others; separate proofs are not required for discrete and continuous distributions. Classical probability distributions Certain random variables occur very often in probability theory because they well describe many natural or physical processes. Their distributions, therefore, have gained special importance in probability theory. Some fundamental discrete distributions are the discrete uniform, Bernoulli, binomial, negative binomial, Poisson and geometric distributions. Important continuous distributions include the continuous uniform, normal, exponential, gamma and beta distributions. Convergence of random variables In probability theory, there are several notions of convergence for random variables. They are listed below in the order of strength, i.e., any subsequent notion of convergence in the list implies convergence according to all of the preceding notions. Weak convergence A sequence of random variables $X_1,X_2,\\dots,\\,$ converges to the random variable $X\\,$ if their respective CDF converges$F_1,F_2,\\dots\\,$ converges to the CDF $F\\,$ of $X\\,$, wherever $F\\,$ is continuous."
            },
            {
                "text": "Weak convergence is also called . Most common shorthand notation: $\\displaystyle X_n \\, \\xrightarrow{\\mathcal D} \\, X$ Convergence in probability The sequence of random variables $X_1,X_2,\\dots\\,$ is said to converge towards the random variable $X\\,$ if $\\lim_{n\\rightarrow\\infty}P\\left(\\left|X_n-X\\right|\\geq\\varepsilon\\right)=0$ for every ε > 0. Most common shorthand notation: $\\displaystyle X_n \\, \\xrightarrow{P} \\, X$ Strong convergence The sequence of random variables $X_1,X_2,\\dots\\,$ is said to converge towards the random variable $X\\,$ if $P(\\lim_{n\\rightarrow\\infty} X_n=X)=1$. Strong convergence is also known as . Most common shorthand notation: $\\displaystyle X_n \\, \\xrightarrow{\\mathrm{a.s.}} \\, X$ As the names indicate, weak convergence is weaker than strong convergence. In fact, strong convergence implies convergence in probability, and convergence in probability implies weak convergence. The reverse statements are not always true. Law of large numbers Common intuition suggests that if a fair coin is tossed many times, then roughly half of the time it will turn up heads, and the other half it will turn up tails."
            },
            {
                "text": "Furthermore, the more often the coin is tossed, the more likely it should be that the ratio of the number of heads to the number of tails will approach unity. Modern probability theory provides a formal version of this intuitive idea, known as the . This law is remarkable because it is not assumed in the foundations of probability theory, but instead emerges from these foundations as a theorem. Since it links theoretically derived probabilities to their actual frequency of occurrence in the real world, the law of large numbers is considered as a pillar in the history of statistical theory and has had widespread influence. The (LLN) states that the sample average $\\overline{X}_n=\\frac1n{\\sum_{k=1}^n X_k}$ of a sequence of independent and identically distributed random variables $X_k$ converges towards their common expectation (expected value) $\\mu$, provided that the expectation of $|X_k|$ is finite. It is in the different forms of convergence of random variables that separates the weak and the strong law of large numbers Weak law: $\\displaystyle \\overline{X}_n \\, \\xrightarrow{P} \\, \\mu$ for $n \\to \\infty$ Strong law: $\\displaystyle \\overline{X}_n \\, \\xrightarrow{\\mathrm{a.\\,s.}}"
            },
            {
                "text": "\\, \\mu $ for $ n \\to \\infty .$ It follows from the LLN that if an event of probability p is observed repeatedly during independent experiments, the ratio of the observed frequency of that event to the total number of repetitions converges towards p. For example, if $Y_1,Y_2,...\\,$ are independent Bernoulli random variables taking values 1 with probability p and 0 with probability 1-p, then $\\textrm{E}(Y_i)=p$ for all i, so that $\\bar Y_n$ converges to p almost surely. Central limit theorem The central limit theorem (CLT) explains the ubiquitous occurrence of the normal distribution in nature, and this theorem, according to David Williams, \"is one of the great results of mathematics. \"David Williams, \"Probability with martingales\", Cambridge 1991/2008 The theorem states that the average of many independent and identically distributed random variables with finite variance tends towards a normal distribution irrespective of the distribution followed by the original random variables. Formally, let $X_1,X_2,\\dots\\,$ be independent random variables with mean $\\mu$ and variance $\\sigma^2 > 0.\\,$ Then the sequence of random variables $Z_n=\\frac{\\sum_{i=1}^n (X_i - \\mu)}{\\sigma\\sqrt{n}}\\,$ converges in distribution to a standard normal random variable. For some classes of random variables, the classic central limit theorem works rather fast, as illustrated in the Berry–Esseen theorem."
            },
            {
                "text": "For example, the distributions with finite first, second, and third moment from the exponential family; on the other hand, for some random variables of the heavy tail and fat tail variety, it works very slowly or may not work at all: in such cases one may use the Generalized Central Limit Theorem (GCLT). See also Lists References Citations Sources The first major treatise blending calculus with probability theory, originally in French: Théorie Analytique des Probabilités. An English translation by Nathan Morrison appeared under the title Foundations of the Theory of Probability (Chelsea, New York) in 1950, with a second edition in 1956. Olav Kallenberg; Foundations of Modern Probability, 2nd ed. Springer Series in Statistics. (2002). 650 pp. A lively introduction to probability theory for the beginner. Olav Kallenberg; Probabilistic Symmetries and Invariance Principles. Springer -Verlag, New York (2005). 510 pp."
            }
        ],
        "latex_formulas": [
            "X(\\text{tails})=1",
            "\\tfrac{3}{6}=\\tfrac{1}{2}",
            "\\Omega",
            "x \\in \\Omega\\,",
            "f(x)\\,",
            "f(x)\\in[0,1]\\mbox{ for all }x\\in \\Omega\\,;",
            "\\sum_{x\\in \\Omega} f(x) = 1\\,.",
            "E\\,",
            "\\Omega\\,",
            "E\\,",
            "P(E)=\\sum_{x\\in E} f(x)\\,.",
            "f(x)\\,",
            "\\mathbb{R}",
            "F\\,",
            "F(x) = P(X\\le x) \\,",
            "F\\,",
            "\\lim_{x\\rightarrow -\\infty} F(x)=0\\,;",
            "\\lim_{x\\rightarrow \\infty} F(x)=1\\,.",
            "X",
            "F",
            "F\\,",
            "f(x)=\\frac{dF(x)}{dx}\\,.",
            "E \\subseteq \\mathbb{R}",
            "E\\,",
            "P(X\\in E) = \\int_{x\\in E} dF(x)\\,.",
            "P(X\\in E) = \\int_{x\\in E} f(x)\\,dx\\,.",
            "\\mathbb{R}\\,.",
            "\\mathbb{R}^n",
            "(\\delta[x] + \\varphi(x))/2",
            "\\delta[x]",
            "\\Omega\\,",
            "\\mathcal{F}\\,",
            "P\\,",
            "\\mathcal{F}\\,",
            "P(\\Omega)=1.\\,",
            "\\mathcal{F}\\,",
            "\\mathcal{F}\\,",
            "E\\,",
            "\\mathcal{F}\\,",
            "P(E) = \\int_{\\omega\\in E} \\mu_F(d\\omega)\\,",
            "\\mu_F\\,",
            "F\\,.",
            "\\mathbb{R}^n",
            "X_1,X_2,\\dots,\\,",
            "X\\,",
            "F_1,F_2,\\dots\\,",
            "F\\,",
            "X\\,",
            "F\\,",
            "\\displaystyle X_n \\, \\xrightarrow{\\mathcal D} \\, X",
            "X_1,X_2,\\dots\\,",
            "X\\,",
            "\\lim_{n\\rightarrow\\infty}P\\left(\\left|X_n-X\\right|\\geq\\varepsilon\\right)=0",
            "\\displaystyle X_n \\, \\xrightarrow{P} \\, X",
            "X_1,X_2,\\dots\\,",
            "X\\,",
            "P(\\lim_{n\\rightarrow\\infty} X_n=X)=1",
            "\\displaystyle X_n \\, \\xrightarrow{\\mathrm{a.s.}} \\, X",
            "\\overline{X}_n=\\frac1n{\\sum_{k=1}^n X_k}",
            "X_k",
            "\\mu",
            "|X_k|",
            "\\displaystyle \\overline{X}_n \\, \\xrightarrow{P} \\, \\mu",
            "n \\to \\infty",
            "\\displaystyle \\overline{X}_n \\, \\xrightarrow{\\mathrm{a.\\,s.}} \\, \\mu",
            "n \\to \\infty .",
            "Y_1,Y_2,...\\,",
            "\\textrm{E}(Y_i)=p",
            "\\bar Y_n",
            "X_1,X_2,\\dots\\,",
            "\\mu",
            "\\sigma^2 > 0.\\,",
            "Z_n=\\frac{\\sum_{i=1}^n (X_i - \\mu)}{\\sigma\\sqrt{n}}\\,"
        ]
    },
    "Linear_algebra": {
        "title": "Linear_algebra",
        "chunks": [
            {
                "text": "right|In three-dimensional Euclidean space, these three planes represent solutions to linear equations, and their intersection represents the set of common solutions: in this case, a unique point. The blue line is the common solution to two of these equations. Linear algebra is the branch of mathematics concerning linear equations such as $a_1x_1+\\cdots +a_nx_n=b,$ linear maps such as $(x_1, \\ldots, x_n) \\mapsto a_1x_1+\\cdots +a_nx_n,$ and their representations in vector spaces and through matrices. Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces. Linear algebra is also used in most sciences and fields of engineering because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point."
            },
            {
                "text": "History The procedure (using counting rods) for solving simultaneous linear equations now called Gaussian elimination appears in the ancient Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations. Systems of linear equations arose in Europe with the introduction in 1637 by René Descartes of coordinates in geometry. In fact, in this new geometry, now called Cartesian geometry, lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations. The first systematic methods for solving linear systems used determinants and were first considered by Leibniz in 1693. In 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's rule. Later, Gauss further described the method of elimination, which was initially listed as an advancement in geodesy. In 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for womb."
            },
            {
                "text": "Linear algebra grew with ideas noted in the complex plane. For instance, two numbers and in $\\mathbb{C}$ have a difference $w – z$, and the line segments $ and $ are of the same length and direction. The segments are equipollent. The four-dimensional system $\\mathbb{H}$ of quaternions was discovered by W.R. Hamilton in 1843.Koecher, M., Remmert, R. (1991). Hamilton’s Quaternions. In: Numbers. Graduate Texts in Mathematics, vol 123. Springer, New York, NY. https://doi.org/10.1007/978-1-4612-1005-4_10 The term vector was introduced as $v representing a point in space. The quaternion difference $p – q$ also produces a segment equipollent to $. Other hypercomplex number systems also used the idea of a linear space with a basis. Arthur Cayley introduced matrix multiplication and the inverse matrix in 1856, making possible the general linear group. The mechanism of group representation became available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object."
            },
            {
                "text": "He also realized the connection between matrices and determinants and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\". Benjamin Peirce published his Linear Associative Algebra (1872), and his son Charles Sanders Peirce extended the work later.Benjamin Peirce (1872) Linear Associative Algebra, lithograph, new edition with corrections, notes, and an added 1875 paper by Peirce, plus notes by his son Charles Sanders Peirce, published in the American Journal of Mathematics v. 4, 1881, Johns Hopkins University, pp. 221–226, Google Eprint and as an extract, D. Van Nostrand, 1882, Google Eprint. The telegraph required an explanatory system, and the 1873 publication by James Clerk Maxwell of A Treatise on Electricity and Magnetism instituted a field theory of forces and required differential geometry for expression. Linear algebra is flat differential geometry and serves in tangent spaces to manifolds. Electromagnetic symmetries of spacetime are expressed by the Lorentz transformations, and much of the history of linear algebra is the history of Lorentz transformations."
            },
            {
                "text": "The first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century when many ideas and methods of previous centuries were generalized as abstract algebra. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modeling and simulations. Vector spaces Until the 19th century, linear algebra was introduced through systems of linear equations and matrices. In modern mathematics, the presentation through vector spaces is generally preferred, since it is more synthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract. A vector space over a field $F$ (often the field of the real numbers) is a set $V$ equipped with two binary operations. Elements of $V$ are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors $v$ and $w$ and outputs a third vector $v + w$."
            },
            {
                "text": "The second operation, scalar multiplication, takes any scalar $a$ and any vector $v$ and outputs a new . The axioms that addition and scalar multiplication must satisfy are the following. (In the list below, $u, v$ and $w$ are arbitrary elements of $V$, and $a$ and $b$ are arbitrary scalars in the field $F$.) {| border=\"0\" style=\"width:100%;\" |- | Axiom ||Signification |- | Associativity of addition || $1=u + (v + w) = (u + v) + w$ |- style=\"background:#F8F4FF;\" | Commutativity of addition || $1=u + v = v + u$ |- | Identity element of addition || There exists an element $0$ in $V$, called the zero vector (or simply zero), such that $1=v + 0 = v$ for all $v$ in $V$."
            },
            {
                "text": "|- style=\"background:#F8F4FF;\" | Inverse elements of addition || For every $v$ in $V$, there exists an element $−v$ in $V$, called the additive inverse of $v$, such that $1=v + (−v) = 0$ |- | Distributivity of scalar multiplication with respect to vector addition || $1=a(u + v) = au + av$ |- style=\"background:#F8F4FF;\" | Distributivity of scalar multiplication with respect to field addition || $1=(a + b)v = av + bv$ |- | Compatibility of scalar multiplication with field multiplication || $1=a(bv) = (ab)v$ |- style=\"background:#F8F4FF;\" | Identity element of scalar multiplication || $1=1v = v$, where $1$ denotes the multiplicative identity of ."
            },
            {
                "text": "|} The first four axioms mean that $V$ is an abelian group under addition. An element of a specific vector space may have various natures; for example, it could be a sequence, a function, a polynomial, or a matrix. Linear algebra is concerned with the properties of such objects that are common to all vector spaces. Linear maps Linear maps are mappings between vector spaces that preserve the vector-space structure. Given two vector spaces $V$ and $W$ over a field , a linear map (also called, in some contexts, linear transformation or linear mapping) is a map $ T:V\\to W $ That is compatible with addition and scalar multiplication, that is $ T(\\mathbf u + \\mathbf v)=T(\\mathbf u)+T(\\mathbf v), \\quad T(a \\mathbf v)=aT(\\mathbf v) $ for any vectors $u,v$ in $V$ and scalar $a$ in . This implies that for any vectors $u, v$ in $V$ and scalars $a, b$ in , one has $T(a \\mathbf u + b \\mathbf v)= T(a \\mathbf u) + T(b \\mathbf v) = aT(\\mathbf u) + bT(\\mathbf v) $ When $1=V = W$ are the same vector space, a linear map $T : V → V$ is also known as a linear operator on ."
            },
            {
                "text": "A bijective linear map between two vector spaces (that is, every vector from the second space is associated with exactly one in the first) is an isomorphism. Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its range (or image) and the set of elements that are mapped to the zero vector, called the kernel of the map. All these questions can be solved by using Gaussian elimination or some variant of this algorithm. Subspaces, span, and basis The study of those subsets of vector spaces that are in themselves vector spaces under the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called linear subspaces. More precisely, a linear subspace of a vector space over a field is a subset of such that $u + v$ and $au$ are in , for every , in , and every in ."
            },
            {
                "text": "(These conditions suffice for implying that is a vector space.) For example, given a linear map $T : V → W$, the image $T(V)$ of , and the inverse image $T−1(0)$ of $0$ (called kernel or null space), are linear subspaces of and , respectively. Another important way of forming a subspace is to consider linear combinations of a set of vectors: the set of all sums $ a_1 \\mathbf v_1 + a_2 \\mathbf v_2 + \\cdots + a_k \\mathbf v_k,$ where $v1, v2, ..., vk$ are in , and $a1, a2, ..., ak$ are in form a linear subspace called the span of . The span of is also the intersection of all linear subspaces containing . In other words, it is the smallest (for the inclusion relation) linear subspace containing . A set of vectors is linearly independent if none is in the span of the others. Equivalently, a set of vectors is linearly independent if the only way to express the zero vector as a linear combination of elements of is to take zero for every coefficient ."
            },
            {
                "text": "A set of vectors that spans a vector space is called a spanning set or generating set. If a spanning set is linearly dependent (that is not linearly independent), then some element of is in the span of the other elements of , and the span would remain the same if one were to remove from . One may continue to remove elements of until getting a linearly independent spanning set. Such a linearly independent set that spans a vector space is called a basis of $V$. The importance of bases lies in the fact that they are simultaneously minimal-generating sets and maximal independent sets. More precisely, if is a linearly independent set, and is a spanning set such that $S ⊆ T$, then there is a basis such that $S ⊆ B ⊆ T$. Any two bases of a vector space $V$ have the same cardinality, which is called the dimension of $V$; this is the dimension theorem for vector spaces. Moreover, two vector spaces over the same field are isomorphic if and only if they have the same dimension."
            },
            {
                "text": "p. 82, §3.59 If any basis of $V$ (and therefore every basis) has a finite number of elements, $V$ is a finite-dimensional vector space. If $U$ is a subspace of $V$, then $dim U ≤ dim V$. In the case where $V$ is finite-dimensional, the equality of the dimensions implies $1=U = V$. If $U1$ and $U2$ are subspaces of $V$, then $\\dim(U_1 + U_2) = \\dim U_1 + \\dim U_2 - \\dim(U_1 \\cap U_2),$ where $U1 + U2$ denotes the span of $U1 ∪ U2$. p. 23, §1.45 Matrices Matrices allow explicit manipulation of finite-dimensional vector spaces and linear maps. Their theory is thus an essential part of linear algebra. Let be a finite-dimensional vector space over a field $F$, and $(v1, v2, ..., vm)$ be a basis of $V$ (thus is the dimension of $V$)."
            },
            {
                "text": "By definition of a basis, the map $\\begin{align} (a_1, \\ldots, a_m)&\\mapsto a_1 \\mathbf v_1+\\cdots a_m \\mathbf v_m\\\\ F^m &\\to V \\end{align}$ is a bijection from $Fm$, the set of the sequences of elements of , onto . This is an isomorphism of vector spaces, if $Fm$ is equipped with its standard structure of vector space, where vector addition and scalar multiplication are done component by component. This isomorphism allows representing a vector by its inverse image under this isomorphism, that is by the coordinate vector $(a1, ..., am)$ or by the column matrix $\\begin{bmatrix}a_1\\\\\\vdots\\\\a_m\\end{bmatrix}.$ If is another finite dimensional vector space (possibly the same), with a basis $(w1, ..., wn)$, a linear map from to is well defined by its values on the basis elements, that is $(f(w1), ..., f(wn))$. Thus, is well represented by the list of the corresponding column matrices."
            },
            {
                "text": "That is, if $f(w_j)=a_{1,j}v_1 + \\cdots+a_{m,j}v_m,$ for $1=j = 1, ..., n$, then is represented by the matrix $\\begin{bmatrix} a_{1,1}&\\cdots&a_{1,n}\\\\ \\vdots&\\ddots&\\vdots\\\\ a_{m,1}&\\cdots&a_{m,n} \\end{bmatrix},$ with rows and columns. Matrix multiplication is defined in such a way that the product of two matrices is the matrix of the composition of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing the same concepts. Two matrices that encode the same linear transformation in different bases are called similar. It can be proved that two matrices are similar if and only if one can transform one into the other by elementary row and column operations."
            },
            {
                "text": "For a matrix representing a linear map from to , the row operations correspond to change of bases in and the column operations correspond to change of bases in . Every matrix is similar to an identity matrix possibly bordered by zero rows and zero columns. In terms of vector spaces, this means that, for any linear map from to , there are bases such that a part of the basis of is mapped bijectively on a part of the basis of , and that the remaining basis elements of , if any, are mapped to zero. Gaussian elimination is the basic algorithm for finding these elementary operations, and proving these results. Linear systems A finite set of linear equations in a finite set of variables, for example, $x1, x2, ..., xn$, or $x, y, ..., z$ is called a system of linear equations or a linear system. Systems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory have been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems."
            },
            {
                "text": "For example, let be a linear system. To such a system, one may associate its matrix $M = \\left[\\begin{array}{rrr} 2 & 1 & -1\\\\ -3 & -1 & 2 \\\\ -2 & 1 & 2 \\end{array}\\right]. $ and its right member vector $\\mathbf{v} = \\begin{bmatrix} 8\\\\-11\\\\-3 \\end{bmatrix}. $ Let be the linear transformation associated with the matrix . A solution of the system () is a vector $\\mathbf{X}=\\begin{bmatrix} x\\\\y\\\\z \\end{bmatrix}$ such that $T(\\mathbf{X}) = \\mathbf{v},$ that is an element of the preimage of by . Let () be the associated homogeneous system, where the right-hand sides of the equations are put to zero: The solutions of () are exactly the elements of the kernel of or, equivalently, . The Gaussian-elimination consists of performing elementary row operations on the augmented matrix $\\left[\\!\\begin{array}{c|c}M&\\mathbf{v}\\end{array}\\!\\right] = \\left[\\begin{array}{rrr|r} 2 & 1 & -1&8\\\\ -3 & -1 & 2&-11 \\\\ -2 & 1 & 2&-3 \\end{array}\\right] $ for putting it in reduced row echelon form."
            },
            {
                "text": "These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is $\\left[\\!\\begin{array}{c|c}M&\\mathbf{v}\\end{array}\\!\\right] = \\left[\\begin{array}{rrr|r} 1 & 0 & 0&2\\\\ 0 & 1 & 0&3 \\\\ 0 & 0 & 1&-1 \\end{array}\\right], $ showing that the system () has the unique solution $\\begin{align}x&=2\\\\y&=3\\\\z&=-1.\\end{align}$ It follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the ranks, kernels, matrix inverses. Endomorphisms and square matrices A linear endomorphism is a linear map that maps a vector space to itself. If has a basis of elements, such an endomorphism is represented by a square matrix of size . Concerning general linear maps, linear endomorphisms, and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including geometric transformations, coordinate changes, quadratic forms, and many other parts of mathematics."
            },
            {
                "text": "Determinant The determinant of a square matrix is defined to be pp. 76–77, § 4.4.1–4.4.6 $\\sum_{\\sigma \\in S_n} (-1)^{\\sigma} a_{1\\sigma(1)} \\cdots a_{n\\sigma(n)}, $ where $Sn$ is the group of all permutations of elements, is a permutation, and $(−1)σ$ the parity of the permutation. A matrix is invertible if and only if the determinant is invertible (i.e., nonzero if the scalars belong to a field). Cramer's rule is a closed-form expression, in terms of determinants, of the solution of a system of linear equations in unknowns. Cramer's rule is useful for reasoning about the solution, but, except for $1=n = 2$ or $3$, it is rarely used for computing a solution, since Gaussian elimination is a faster algorithm. The determinant of an endomorphism is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense since this determinant is independent of the choice of the basis."
            },
            {
                "text": "Eigenvalues and eigenvectors If is a linear endomorphism of a vector space over a field , an eigenvector of is a nonzero vector of such that $1=f(v) = av$ for some scalar in . This scalar is an eigenvalue of . If the dimension of is finite, and a basis has been chosen, and may be represented, respectively, by a square matrix and a column matrix ; the equation defining eigenvectors and eigenvalues becomes $Mz=az.$ Using the identity matrix , whose entries are all zero, except those of the main diagonal, which are equal to one, this may be rewritten $(M-aI)z=0.$ As is supposed to be nonzero, this means that $M – aI$ is a singular matrix, and thus that its determinant $det (M − aI)$ equals zero. The eigenvalues are thus the roots of the polynomial $\\det(xI-M).$ If is of dimension , this is a monic polynomial of degree , called the characteristic polynomial of the matrix (or of the endomorphism), and there are, at most, eigenvalues."
            },
            {
                "text": "If a basis exists that consists only of eigenvectors, the matrix of on this basis has a very simple structure: it is a diagonal matrix such that the entries on the main diagonal are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said to be diagonalizable. More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after extending the field of scalars. In this extended sense, if the characteristic polynomial is square-free, then the matrix is diagonalizable. A symmetric matrix is always diagonalizable. There are non-diagonalizable matrices, the simplest being $\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}$ (it cannot be diagonalizable since its square is the zero matrix, and the square of a nonzero diagonal matrix is never zero). When an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The Frobenius normal form does not need to extend the field of scalars and makes the characteristic polynomial immediately readable on the matrix."
            },
            {
                "text": "The Jordan normal form requires to extension of the field of scalar for containing all eigenvalues and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1. Duality A linear form is a linear map from a vector space over a field to the field of scalars , viewed as a vector space over itself. Equipped by pointwise addition and multiplication by a scalar, the linear forms form a vector space, called the dual space of , and usually denoted p. 37 §2.1.3 or . p. 20, §13 p. 101, §3.94 If $v1, ..., vn$ is a basis of (this implies that is finite-dimensional), then one can define, for $1=i = 1, ..., n$, a linear map $vi*$ such that $vi*(vi) and $vi*(vj) if $j ≠ i$. These linear maps form a basis of $V*$, called the dual basis of $v1, ..., vn$. (If is not finite-dimensional, the $vi*$ may be defined similarly; they are linearly independent, but do not form a basis.)"
            },
            {
                "text": "For $v$ in , the map $f\\to f(\\mathbf v)$ is a linear form on . This defines the canonical linear map from into $(V*)*$, the dual of , called the double dual or bidual of . This canonical map is an isomorphism if is finite-dimensional, and this allows identifying with its bidual. (In the infinite-dimensional case, the canonical map is injective, but not surjective.) There is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the bra–ket notation $\\langle f, \\mathbf x\\rangle$ for denoting $f(x)$. Dual map Let $f:V\\to W$ be a linear map. For every linear form on , the composite function $h ∘ f$ is a linear form on . This defines a linear map $f^*:W^*\\to V^*$ between the dual spaces, which is called the dual or the transpose of . If and are finite-dimensional, and is the matrix of in terms of some ordered bases, then the matrix of over the dual bases is the transpose $MT$ of , obtained by exchanging rows and columns."
            },
            {
                "text": "If elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in bra–ket notation by $\\langle h^\\mathsf T , M \\mathbf v\\rangle = \\langle h^\\mathsf T M, \\mathbf v\\rangle.$ To highlight this symmetry, the two members of this equality are sometimes written $\\langle h^\\mathsf T \\mid M \\mid \\mathbf v\\rangle.$ Inner-product spaces Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a map. $ \\langle \\cdot, \\cdot \\rangle : V \\times V \\to F $ that satisfies the following three axioms for all vectors $u, v, w$ in $V$ and all scalars $a$ in $F$: Conjugate symmetry: $\\langle \\mathbf u, \\mathbf v\\rangle =\\overline{\\langle \\mathbf v, \\mathbf u\\rangle}.$ In $\\mathbb{R}$, it is symmetric."
            },
            {
                "text": "Linearity in the first argument: $\\begin{align} \\langle a \\mathbf u, \\mathbf v\\rangle &= a \\langle \\mathbf u, \\mathbf v\\rangle. \\\\ \\langle \\mathbf u + \\mathbf v, \\mathbf w\\rangle &= \\langle \\mathbf u, \\mathbf w\\rangle+ \\langle \\mathbf v, \\mathbf w\\rangle. \\end{align}$ Positive-definiteness: $\\langle \\mathbf v, \\mathbf v\\rangle \\geq 0$ with equality only for $v . We can define the length of a vector v in V by $\\|\\mathbf v\\|^2=\\langle \\mathbf v, \\mathbf v\\rangle,$ and we can prove the Cauchy–Schwarz inequality: $|\\langle \\mathbf u, \\mathbf v\\rangle| \\leq \\|\\mathbf u\\| \\cdot \\|\\mathbf v\\|.$ In particular, the quantity $\\frac{|\\langle \\mathbf u, \\mathbf v\\rangle|}{\\|\\mathbf u\\| \\cdot \\|\\mathbf v\\|} \\leq 1,$ and so we can call this quantity the cosine of the angle between the two vectors. Two vectors are orthogonal if $⟨u, v⟩ . An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure."
            },
            {
                "text": "Orthonormal bases are particularly easy to deal with, since if , then $a_i = \\langle \\mathbf v, \\mathbf v_i \\rangle.$ The inner product facilitates the construction of many useful concepts. For instance, given a transform $T$, we can define its Hermitian conjugate $T*$ as the linear transform satisfying $ \\langle T \\mathbf u, \\mathbf v \\rangle = \\langle \\mathbf u, T^* \\mathbf v\\rangle.$ If $T$ satisfies $TT* , we call $T$ normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span $V$. Relationship with geometry There is a strong relationship between linear algebra and geometry, which started with the introduction by René Descartes, in 1637, of Cartesian coordinates. In this new (at that time) geometry, now called Cartesian geometry, points are represented by Cartesian coordinates, which are sequences of three real numbers (in the case of the usual three-dimensional space). The basic objects of geometry, which are lines and planes are represented by linear equations."
            },
            {
                "text": "Thus, computing intersections of lines and planes amounts to solving systems of linear equations. This was one of the main motivations for developing linear algebra. Most geometric transformation, such as translations, rotations, reflections, rigid motions, isometries, and projections transform lines into lines. It follows that they can be defined, specified, and studied in terms of linear maps. This is also the case of homographies and Möbius transformations when considered as transformations of a projective space. Until the end of the 19th century, geometric spaces were defined by axioms relating points, lines, and planes (synthetic geometry). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example, Projective space and Affine space). It has been shown that the two approaches are essentially equivalent.Emil Artin (1957) Geometric Algebra Interscience Publishers In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, including finite fields."
            },
            {
                "text": "Presently, most textbooks introduce geometric spaces from linear algebra, and geometry is often presented, at the elementary level, as a subfield of linear algebra. Usage and applications Linear algebra is used in almost all areas of mathematics, thus making it relevant in almost all scientific domains that use mathematics. These applications may be divided into several wide categories. Functional analysis Functional analysis studies function spaces. These are vector spaces with additional structure, such as Hilbert spaces. Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular, quantum mechanics (wave functions) and Fourier analysis (orthogonal basis). Scientific computation Nearly all scientific computations involve linear algebra. Consequently, linear algebra algorithms have been highly optimized. BLAS and LAPACK are the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, to adapt them to the specificities of the computer (cache size, number of available cores, ...). Since the 1960s there have been processors with specialized instructions for optimizing the operations of linear algebra, optional array processors under the control of a conventional processor, supercomputers designed for array processing and conventional processors augmented with vector registers."
            },
            {
                "text": "Some contemporary processors, typically graphics processing units (GPU), are designed with a matrix structure, for optimizing the operations of linear algebra. Geometry of ambient space The modeling of ambient space is based on geometry. Sciences concerned with this space use geometry widely. This is the case with mechanics and robotics, for describing rigid body dynamics; geodesy for describing Earth shape; perspectivity, computer vision, and computer graphics, for describing the relationship between a scene and its plane representation; and many other scientific domains. In all these applications, synthetic geometry is often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute with coordinates. This requires the heavy use of linear algebra. Study of complex systems Most physical phenomena are modeled by partial differential equations. To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interacting cells. For linear systems this interaction involves linear functions. For nonlinear systems, this interaction is often approximated by linear functions.This is called a linear model or first-order approximation."
            },
            {
                "text": "Linear models are frequently used for complex nonlinear real-world systems because they make parametrization more manageable. In both cases, very large matrices are generally involved. Weather forecasting (or more specifically, parametrization for atmospheric modeling) is a typical example of a real-world application, where the whole Earth atmosphere is divided into cells of, say, 100 km of width and 100 km of height. Fluid mechanics, fluid dynamics, and thermal energy systems Linear algebra, a branch of mathematics dealing with vector spaces and linear mappings between these spaces, plays a critical role in various engineering disciplines, including fluid mechanics, fluid dynamics, and thermal energy systems. Its application in these fields is multifaceted and indispensable for solving complex problems. In fluid mechanics, linear algebra is integral to understanding and solving problems related to the behavior of fluids. It assists in the modeling and simulation of fluid flow, providing essential tools for the analysis of fluid dynamics problems. For instance, linear algebraic techniques are used to solve systems of differential equations that describe fluid motion. These equations, often complex and non-linear, can be linearized using linear algebra methods, allowing for simpler solutions and analyses."
            },
            {
                "text": "In the field of fluid dynamics, linear algebra finds its application in computational fluid dynamics (CFD), a branch that uses numerical analysis and data structures to solve and analyze problems involving fluid flows. CFD relies heavily on linear algebra for the computation of fluid flow and heat transfer in various applications. For example, the Navier–Stokes equations, fundamental in fluid dynamics, are often solved using techniques derived from linear algebra. This includes the use of matrices and vectors to represent and manipulate fluid flow fields. Furthermore, linear algebra plays a crucial role in thermal energy systems, particularly in power systems analysis. It is used to model and optimize the generation, transmission, and distribution of electric power. Linear algebraic concepts such as matrix operations and eigenvalue problems are employed to enhance the efficiency, reliability, and economic performance of power systems. The application of linear algebra in this context is vital for the design and operation of modern power systems, including renewable energy sources and smart grids. Overall, the application of linear algebra in fluid mechanics, fluid dynamics, and thermal energy systems is an example of the profound interconnection between mathematics and engineering."
            },
            {
                "text": "It provides engineers with the necessary tools to model, analyze, and solve complex problems in these domains, leading to advancements in technology and industry. Extensions and generalizations This section presents several related topics that do not appear generally in elementary textbooks on linear algebra but are commonly considered, in advanced mathematics, as parts of linear algebra. Module theory The existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by a ring , and this gives the structure called a module over , or -module. The concepts of linear independence, span, basis, and linear maps (also called module homomorphisms) are defined for modules exactly as for vector spaces, with the essential difference that, if is not a field, there are modules that do not have any basis. The modules that have a basis are the free modules, and those that are spanned by a finite set are the finitely generated modules. Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except that determinants exist only if the ring is commutative, and that a square matrix over a commutative ring is invertible only if its determinant has a multiplicative inverse in the ring."
            },
            {
                "text": "Vector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is a cokernel of a homomorphism of free modules. Modules over the integers can be identified with abelian groups, since the multiplication by an integer may be identified as a repeated addition. Most of the theory of abelian groups may be extended to modules over a principal ideal domain. In particular, over a principal ideal domain, every submodule of a free module is free, and the fundamental theorem of finitely generated abelian groups may be extended straightforwardly to finitely generated modules over a principal ring. There are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a computational complexity that is much higher than similar algorithms over a field. For more details, see Linear equation over a ring. Multilinear algebra and tensors In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of several different variables."
            },
            {
                "text": "This line of inquiry naturally leads to the idea of the dual space, the vector space $V*$ consisting of linear maps $f : V → F$ where F is the field of scalars. Multilinear maps $T : Vn → F$ can be described via tensor products of elements of $V*$. If, in addition to vector addition and scalar multiplication, there is a bilinear vector product $V × V → V$, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials). Topological vector spaces Vector spaces that are not finite-dimensional often require additional structure to be tractable. A normed vector space is a vector space along with a function called a norm, which measures the \"size\" of elements. The norm induces a metric, which measures the distance between elements, and induces a topology, which allows for a definition of continuous maps. The metric also allows for a definition of limits and completeness – a normed vector space that is complete is known as a Banach space."
            },
            {
                "text": "A complete metric space along with the additional structure of an inner product (a conjugate symmetric sesquilinear form) is known as a Hilbert space, which is in some sense a particularly well-behaved Banach space. Functional analysis applies the methods of linear algebra alongside those of mathematical analysis to study various function spaces; the central objects of study in functional analysis are spaces, which are Banach spaces, and especially the $L2$ space of square-integrable functions, which is the only Hilbert space among them. Functional analysis is of particular importance to quantum mechanics, the theory of partial differential equations, digital signal processing, and electrical engineering. It also provides the foundation and theoretical framework that underlies the Fourier transform and related methods. See also Fundamental matrix (computer vision) Geometric algebra Linear programming Linear regression, a statistical estimation method Numerical linear algebra Outline of linear algebra Transformation matrix Explanatory notes Citations General and cited sources Further reading History Fearnley-Sander, Desmond, \"Hermann Grassmann and the Creation of Linear Algebra\", American Mathematical Monthly 86 (1979), pp. 809–817."
            },
            {
                "text": "Introductory textbooks Murty, Katta G. (2014) Computational and Algorithmic Linear Algebra and n-Dimensional Geometry, World Scientific Publishing, . Chapter 1: Systems of Simultaneous Linear Equations Noble, B. & Daniel, J.W. (2nd Ed. 1977) , Pearson Higher Education, . The Manga Guide to Linear Algebra (2012), by Shin Takahashi, Iroha Inoue and Trend-Pro Co., Ltd., Advanced textbooks Study guides and outlines External links Online Resources MIT Linear Algebra Video Lectures, a series of 34 recorded lectures by Professor Gilbert Strang (Spring 2010) International Linear Algebra Society Linear Algebra on MathWorld Matrix and Linear Algebra Terms on Earliest Known Uses of Some of the Words of Mathematics Earliest Uses of Symbols for Matrices and Vectors on Earliest Uses of Various Mathematical Symbols Essence of linear algebra, a video presentation from 3Blue1Brown of the basics of linear algebra, with emphasis on the relationship between the geometric, the matrix and the abstract points of view Online books Sharipov, Ruslan, Course of linear algebra and multidimensional geometry Treil, Sergei, Linear Algebra Done Wrong Category:Numerical analysis"
            }
        ],
        "latex_formulas": [
            "''w'' – ''z''",
            "{{overline|''wz''}}",
            "{{overline|0(''w'' − ''z'')}}",
            "'''v''' {{=}} ''x'''''i''' + ''y'''''j''' + ''z'''''k'''",
            "''p'' – ''q''",
            "{{overline|''pq''}}",
            "''F''",
            "''V''",
            "''V''",
            "'''v'''",
            "'''w'''",
            "'''v''' + '''w'''",
            "''a''",
            "'''v'''",
            "''a'''''v'''",
            "'''u''', '''v'''",
            "'''w'''",
            "''V''",
            "''a''",
            "''b''",
            "''F''",
            "'''u''' + ('''v''' + '''w''') = ('''u''' + '''v''') + '''w'''",
            "'''u''' + '''v''' = '''v''' + '''u'''",
            "'''0'''",
            "''V''",
            "'''v''' + '''0''' = '''v'''",
            "'''v'''",
            "''V''",
            "'''v'''",
            "''V''",
            "−'''v'''",
            "''V''",
            "'''v'''",
            "'''v''' + (−'''v''') = '''0'''",
            "''a''('''u''' + '''v''') = ''a'''''u''' + ''a'''''v'''",
            "(''a'' + ''b'')'''v''' = ''a'''''v''' + ''b'''''v'''",
            "''a''(''b'''''v''') = (''ab'')'''v'''",
            "''b'''''v'''",
            "''ab''",
            "1'''v''' = '''v'''",
            "1",
            "''V''",
            "''V''",
            "''W''",
            "'''u''','''v'''",
            "''V''",
            "''a''",
            "'''u''', '''v'''",
            "''V''",
            "''a'', ''b''",
            "''V'' = ''W''",
            "''T'' : ''V'' → ''V''",
            "'''u''' + '''v'''",
            "''a'''''u'''",
            "'''u'''",
            "'''v'''",
            "''T'' : ''V'' → ''W''",
            "''T''(''V'')",
            "''T''<sup>−1</sup>('''0''')",
            "'''0'''",
            "'''v'''<sub>1</sub>, '''v'''<sub>2</sub>, ..., '''v'''<sub>''k''</sub>",
            "''a''<sub>1</sub>, ''a''<sub>2</sub>, ..., ''a''<sub>''k''</sub>",
            "'''w'''",
            "'''w'''",
            "''V''",
            "''S'' ⊆ ''T''",
            "''S'' ⊆ ''B'' ⊆ ''T''",
            "''V''",
            "''V''",
            "''V''",
            "''V''",
            "''U''",
            "''V''",
            "dim ''U'' ≤ dim ''V''",
            "''V''",
            "''U'' = ''V''",
            "''U''<sub>1</sub>",
            "''U''<sub>2</sub>",
            "''V''",
            "''U''<sub>1</sub> + ''U''<sub>2</sub>",
            "''U''<sub>1</sub> ∪ ''U''<sub>2</sub>",
            "''F''",
            "('''v'''<sub>1</sub>, '''v'''<sub>2</sub>, ..., '''v'''<sub>''m''</sub>)",
            "''V''",
            "''V''",
            "''F<sup>m</sup>''",
            "''F<sup>m</sup>''",
            "(''a''<sub>1</sub>, ..., ''a<sub>m</sub>'')",
            "('''w'''<sub>1</sub>, ..., '''w'''<sub>''n''</sub>)",
            "(''f''('''w'''<sub>1</sub>), ..., ''f''('''w'''<sub>''n''</sub>))",
            "''j'' = 1, ..., ''n''",
            "''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x<sub>n</sub>''",
            "''x'', ''y'', ..., ''z''",
            "''S<sub>n</sub>''",
            "(−1)<sup>''σ''</sup>",
            "''n'' = 2",
            "3",
            "''f''(''v'') = ''av''",
            "''M'' – ''aI''",
            "det (''M'' − ''aI'')",
            "'''v'''<sub>1</sub>, ..., '''v'''<sub>''n''</sub>",
            "''i'' = 1, ..., ''n''",
            "''v<sub>i</sub>''*",
            "''v<sub>i</sub>''*('''v'''<sub>''i''</sub>) {{=}} 1",
            "''v<sub>i</sub>''*('''v'''<sub>''j''</sub>) {{=}} 0",
            "''j'' ≠ ''i''",
            "''V''*",
            "'''v'''<sub>1</sub>, ..., '''v'''<sub>''n''</sub>",
            "''v<sub>i</sub>''*",
            "'''v'''",
            "(''V''*)*",
            "''f''('''x''')",
            "''h'' ∘ ''f''",
            "''M''<sup>T</sup>",
            "'''u''', '''v''', '''w'''",
            "''V''",
            "''a''",
            "''F''",
            "'''v''' {{=}} 0",
            "⟨'''u''', '''v'''⟩ {{=}} 0",
            "''T''",
            "''T*''",
            "''T''",
            "''TT*'' {{=}} ''T*T''",
            "''T''",
            "''V''",
            "''V*''",
            "''f'' : ''V'' → ''F''",
            "''T'' : ''V<sup>n</sup>'' → ''F''",
            "''V*''",
            "''V'' × ''V'' → ''V''",
            "''L''<sup>2</sup>",
            "a_1x_1+\\cdots +a_nx_n=b,",
            "(x_1, \\ldots, x_n) \\mapsto a_1x_1+\\cdots +a_nx_n,",
            "\\mathbb{C}",
            "\\mathbb{H}",
            "T:V\\to W",
            "T(\\mathbf u + \\mathbf v)=T(\\mathbf u)+T(\\mathbf v), \\quad T(a \\mathbf v)=aT(\\mathbf v)",
            "T(a \\mathbf u + b \\mathbf v)= T(a \\mathbf u) + T(b \\mathbf v) = aT(\\mathbf u) + bT(\\mathbf v)",
            "a_1 \\mathbf v_1 + a_2 \\mathbf v_2 + \\cdots + a_k \\mathbf v_k,",
            "\\dim(U_1 + U_2) = \\dim U_1 + \\dim U_2 - \\dim(U_1 \\cap U_2),",
            "\\begin{align}\n(a_1, \\ldots, a_m)&\\mapsto a_1 \\mathbf v_1+\\cdots a_m \\mathbf v_m\\\\\nF^m &\\to V\n\\end{align}",
            "\\begin{bmatrix}a_1\\\\\\vdots\\\\a_m\\end{bmatrix}.",
            "f(w_j)=a_{1,j}v_1 + \\cdots+a_{m,j}v_m,",
            "\\begin{bmatrix}\na_{1,1}&\\cdots&a_{1,n}\\\\\n\\vdots&\\ddots&\\vdots\\\\\na_{m,1}&\\cdots&a_{m,n}\n\\end{bmatrix},",
            "\\begin{alignat}{7}\n2x &&\\; + \\;&& y             &&\\; - \\;&& z  &&\\; = \\;&& 8  \\\\\n-3x &&\\; - \\;&& y             &&\\; + \\;&& 2z &&\\; = \\;&& -11 \\\\\n-2x &&\\; + \\;&& y &&\\; +\\;&& 2z  &&\\; = \\;&& -3 \n\\end{alignat}",
            "M = \\left[\\begin{array}{rrr}\n2 & 1 & -1\\\\\n-3 & -1 & 2  \\\\\n-2 & 1 & 2\n\\end{array}\\right].",
            "\\mathbf{v} = \\begin{bmatrix} 8\\\\-11\\\\-3 \\end{bmatrix}.",
            "\\mathbf{X}=\\begin{bmatrix} x\\\\y\\\\z \\end{bmatrix}",
            "T(\\mathbf{X}) = \\mathbf{v},",
            "\\begin{alignat}{7}\n2x &&\\; + \\;&& y             &&\\; - \\;&& z  &&\\; = \\;&& 0  \\\\\n-3x &&\\; - \\;&& y             &&\\; + \\;&& 2z &&\\; = \\;&& 0 \\\\\n-2x &&\\; + \\;&& y &&\\; +\\;&& 2z  &&\\; = \\;&& 0 \n\\end{alignat}",
            "\\left[\\!\\begin{array}{c|c}M&\\mathbf{v}\\end{array}\\!\\right] = \\left[\\begin{array}{rrr|r}\n2 & 1 & -1&8\\\\\n-3 & -1 & 2&-11  \\\\\n-2 & 1 & 2&-3\n\\end{array}\\right]",
            "\\left[\\!\\begin{array}{c|c}M&\\mathbf{v}\\end{array}\\!\\right] = \\left[\\begin{array}{rrr|r}\n1 & 0 & 0&2\\\\\n0 & 1 & 0&3  \\\\\n0 & 0 & 1&-1\n\\end{array}\\right],",
            "\\begin{align}x&=2\\\\y&=3\\\\z&=-1.\\end{align}",
            "\\sum_{\\sigma \\in S_n} (-1)^{\\sigma} a_{1\\sigma(1)} \\cdots a_{n\\sigma(n)},",
            "Mz=az.",
            "(M-aI)z=0.",
            "\\det(xI-M).",
            "\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}",
            "f\\to f(\\mathbf v)",
            "\\langle f, \\mathbf x\\rangle",
            "f:V\\to W",
            "f^*:W^*\\to V^*",
            "\\langle h^\\mathsf T , M \\mathbf v\\rangle = \\langle h^\\mathsf T M, \\mathbf v\\rangle.",
            "\\langle h^\\mathsf T \\mid M \\mid \\mathbf v\\rangle.",
            "\\langle \\cdot, \\cdot \\rangle : V \\times V \\to F",
            "\\langle \\mathbf u, \\mathbf v\\rangle =\\overline{\\langle \\mathbf v, \\mathbf u\\rangle}.",
            "\\mathbb{R}",
            "\\begin{align}\n\\langle a \\mathbf u, \\mathbf v\\rangle &= a \\langle \\mathbf u, \\mathbf v\\rangle. \\\\\n\\langle \\mathbf u + \\mathbf v, \\mathbf w\\rangle &= \\langle \\mathbf u, \\mathbf w\\rangle+ \\langle \\mathbf v, \\mathbf w\\rangle.\n\\end{align}",
            "\\langle \\mathbf v, \\mathbf v\\rangle \\geq 0",
            "\\|\\mathbf v\\|^2=\\langle \\mathbf v, \\mathbf v\\rangle,",
            "|\\langle \\mathbf u, \\mathbf v\\rangle| \\leq \\|\\mathbf u\\| \\cdot \\|\\mathbf v\\|.",
            "\\frac{|\\langle \\mathbf u, \\mathbf v\\rangle|}{\\|\\mathbf u\\| \\cdot \\|\\mathbf v\\|} \\leq 1,",
            "a_i = \\langle \\mathbf v, \\mathbf v_i \\rangle.",
            "\\langle T \\mathbf u, \\mathbf v \\rangle = \\langle \\mathbf u, T^* \\mathbf v\\rangle."
        ]
    },
    "Model_selection": {
        "title": "Model_selection",
        "chunks": [
            {
                "text": "Model selection is the task of selecting a model from among various candidates on the basis of performance criterion to choose the best one. In the context of machine learning and more generally statistical analysis, this may be the selection of a statistical model from a set of candidate models, given data. In the simplest cases, a pre-existing set of data is considered. However, the task can also involve the design of experiments such that the data collected is well-suited to the problem of model selection. Given candidate models of similar predictive or explanatory power, the simplest model is most likely to be the best choice (Occam's razor). state, \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling\". Relatedly, has said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\". Model selection may also refer to the problem of selecting a few representative models from a large set of computational models for the purpose of decision making or optimization under uncertainty."
            },
            {
                "text": "In machine learning, algorithmic approaches to model selection include feature selection, hyperparameter optimization, and statistical learning theory. Introduction right|thumb|The scientific observation cycle. In its most basic forms, model selection is one of the fundamental tasks of scientific inquiry. Determining the principle that explains a series of observations is often linked directly to a mathematical model predicting those observations. For example, when Galileo performed his inclined plane experiments, he demonstrated that the motion of the balls fitted the parabola predicted by his model . Of the countless number of possible mechanisms and processes that could have produced the data, how can one even begin to choose the best model? The mathematical approach commonly taken decides among a set of candidate models; this set must be chosen by the researcher. Often simple models such as polynomials are used, at least initially . emphasize throughout their book the importance of choosing models based on sound scientific principles, such as understanding of the phenomenological processes or mechanisms (e.g., chemical reactions) underlying the data. Once the set of candidate models has been chosen, the statistical analysis allows us to select the best of these models."
            },
            {
                "text": "What is meant by best is controversial. A good model selection technique will balance goodness of fit with simplicity. More complex models will be better able to adapt their shape to fit the data (for example, a fifth-order polynomial can exactly fit six points), but the additional parameters may not represent anything useful. (Perhaps those six points are really just randomly distributed about a straight line.) Goodness of fit is generally determined using a likelihood ratio approach, or an approximation of this, leading to a chi-squared test. The complexity is generally measured by counting the number of parameters in the model. Model selection techniques can be considered as estimators of some physical quantity, such as the probability of the model producing the given data. The bias and variance are both important measures of the quality of this estimator; efficiency is also often considered. A standard example of model selection is that of curve fitting, where, given a set of points and other background knowledge (e.g. points are a result of i.i.d. samples), we must select a curve that describes the function that generated the points."
            },
            {
                "text": "Two directions of model selection There are two main objectives in inference and learning from data. One is for scientific discovery, also called statistical inference, understanding of the underlying data-generating mechanism and interpretation of the nature of the data. Another objective of learning from data is for predicting future or unseen observations, also called Statistical Prediction. In the second objective, the data scientist does not necessarily concern an accurate probabilistic description of the data. Of course, one may also be interested in both directions. In line with the two different objectives, model selection can also have two directions: model selection for inference and model selection for prediction. The first direction is to identify the best model for the data, which will preferably provide a reliable characterization of the sources of uncertainty for scientific interpretation. For this goal, it is significantly important that the selected model is not too sensitive to the sample size. Accordingly, an appropriate notion for evaluating model selection is the selection consistency, meaning that the most robust candidate will be consistently selected given sufficiently many data samples."
            },
            {
                "text": "The second direction is to choose a model as machinery to offer excellent predictive performance. For the latter, however, the selected model may simply be the lucky winner among a few close competitors, yet the predictive performance can still be the best possible. If so, the model selection is fine for the second goal (prediction), but the use of the selected model for insight and interpretation may be severely unreliable and misleading. Moreover, for very complex models selected this way, even predictions may be unreasonable for data only slightly different from those on which the selection was made. Methods to assist in choosing the set of candidate models Data transformation (statistics) Exploratory data analysis Model specification Scientific method Criteria Below is a list of criteria for model selection. The most commonly used information criteria are (i) the Akaike information criterion and (ii) the Bayes factor and/or the Bayesian information criterion (which to some extent approximates the Bayes factor), see for a review. Akaike information criterion (AIC), a measure of the goodness fit of an estimated statistical model Bayes factor Bayesian information criterion (BIC), also known as the Schwarz information criterion, a statistical criterion for model selection Bridge criterion (BC), a statistical criterion that can attain the better performance of AIC and BIC despite the appropriateness of model specification."
            },
            {
                "text": "Cross-validation Deviance information criterion (DIC), another Bayesian oriented model selection criterion False discovery rate Focused information criterion (FIC), a selection criterion sorting statistical models by their effectiveness for a given focus parameter Hannan–Quinn information criterion, an alternative to the Akaike and Bayesian criteria Kashyap information criterion (KIC) is a powerful alternative to AIC and BIC, because KIC uses Fisher information matrix Likelihood-ratio test Mallows's Cp Minimum description length Minimum message length (MML) PRESS statistic, also known as the PRESS criterion Structural risk minimization Stepwise regression Watanabe–Akaike information criterion (WAIC), also called the widely applicable information criterion Extended Bayesian Information Criterion (EBIC) is an extension of ordinary Bayesian information criterion (BIC) for models with high parameter spaces. Extended Fisher Information Criterion (EFIC) is a model selection criterion for linear regression models. Constrained Minimum Criterion (CMC) is a frequentist criterion for selecting regression models with a geometric underpinning. Among these criteria, cross-validation is typically the most accurate, and computationally the most expensive, for supervised learning problems. say the following: See also All models are wrong Analysis of competing hypotheses Automated machine learning (AutoML) Bias-variance dilemma Feature selection Freedman's paradox Grid search Identifiability Analysis Log-linear analysis Model identification Occam's razor Optimal design Parameter identification problem Scientific modelling Statistical model validation Stein's paradox Notes References [this has over 38000 citations on Google Scholar] (reprinted 1965, Science 148: 754–759 ) Category:Regression variable selection Category:Mathematical and quantitative methods (economics) Category:Management science"
            }
        ],
        "latex_formulas": []
    },
    "Evaluation_of_binary_classifiers": {
        "title": "Evaluation_of_binary_classifiers",
        "chunks": [
            {
                "text": "right|From the confusion matrix you can derive four basic measures. Evaluation of a binary classifier typically assigns a numerical value, or values, to a classifier that represent its accuracy. An example is error rate, which measures how frequently the classifier makes a mistake. There are many metrics that can be used; different fields have different preferences. For example, in medicine sensitivity and specificity are often used, while in computer science precision and recall are preferred. An important distinction is between metrics that are independent of the prevalence or skew (how often each class occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties. Often, evaluation is used to compare two methods of classification, so that one can be adopted and the other discarded. Such comparisons are more directly achieved by a form of evaluation that results in a single unitary metric rather than a pair of metrics. Contingency table Given a data set, a classification (the output of a classifier on that set) gives two numbers: the number of positives and the number of negatives, which add up to the total size of the set."
            },
            {
                "text": "To evaluate a classifier, one compares its output to another reference classification – ideally a perfect classification, but in practice the output of another gold standard test – and cross tabulates the data into a 2×2 contingency table, comparing the two classifications. One then evaluates the classifier relative to the gold standard by computing summary statistics of these 4 numbers. Generally these statistics will be scale invariant (scaling all the numbers by the same factor does not change the output), to make them independent of population size, which is achieved by using ratios of homogeneous functions, most simply homogeneous linear or homogeneous quadratic functions. Say we test some people for the presence of a disease. Some of these people have the disease, and our test correctly says they are positive. They are called true positives (TP). Some have the disease, but the test incorrectly claims they don't. They are called false negatives (FN). Some don't have the disease, and the test says they don't – true negatives (TN). Finally, there might be healthy people who have a positive test result – false positives (FP)."
            },
            {
                "text": "These can be arranged into a 2×2 contingency table (confusion matrix), conventionally with the test result on the vertical axis and the actual condition on the horizontal axis. These numbers can then be totaled, yielding both a grand total and marginal totals. Totaling the entire table, the number of true positives, false negatives, true negatives, and false positives add up to 100% of the set. Totaling the columns (adding vertically) the number of true positives and false positives add up to 100% of the test positives, and likewise for negatives. Totaling the rows (adding horizontally), the number of true positives and false negatives add up to 100% of the condition positives (conversely for negatives). The basic marginal ratio statistics are obtained by dividing the 2×2=4 values in the table by the marginal totals (either rows or columns), yielding 2 auxiliary 2×2 tables, for a total of 8 ratios. These ratios come in 4 complementary pairs, each pair summing to 1, and so each of these derived 2×2 tables can be summarized as a pair of 2 numbers, together with their complements."
            },
            {
                "text": "Further statistics can be obtained by taking ratios of these ratios, ratios of ratios, or more complicated functions. The contingency table and the most common derived ratios are summarized below; see sequel for details. Note that the rows correspond to the condition actually being positive or negative (or classified as such by the gold standard), as indicated by the color-coding, and the associated statistics are prevalence-independent, while the columns correspond to the test being positive or negative, and the associated statistics are prevalence-dependent. There are analogous likelihood ratios for prediction values, but these are less commonly used, and not depicted above. Pairs of metrics Often accuracy is evaluated with a pair of metrics composed in a standard pattern. Sensitivity and specificity The fundamental prevalence-independent statistics are sensitivity and specificity. Sensitivity or True Positive Rate (TPR), also known as recall, is the proportion of people that tested positive and are positive (True Positive, TP) of all the people that actually are positive (Condition Positive, CP = TP + FN). It can be seen as the probability that the test is positive given that the patient is sick."
            },
            {
                "text": "With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of the factory quality control, fewer faulty products go to the market). Specificity (SPC) or True Negative Rate (TNR) is the proportion of people that tested negative and are negative (True Negative, TN) of all the people that actually are negative (Condition Negative, CN = TN + FP). As with sensitivity, it can be looked at as the probability that the test result is negative given that the patient is not sick. With higher specificity, fewer healthy people are labeled as sick (or, in the factory case, fewer good products are discarded). The relationship between sensitivity and specificity, as well as the performance of the classifier, can be visualized and studied using the Receiver Operating Characteristic (ROC) curve. In theory, sensitivity and specificity are independent in the sense that it is possible to achieve 100% in both (such as in the red/blue ball example given above). In more practical, less contrived instances, however, there is usually a trade-off, such that they are inversely proportional to one another to some extent."
            },
            {
                "text": "This is because we rarely measure the actual thing we would like to classify; rather, we generally measure an indicator of the thing we would like to classify, referred to as a surrogate marker. The reason why 100% is achievable in the ball example is because redness and blueness is determined by directly detecting redness and blueness. However, indicators are sometimes compromised, such as when non-indicators mimic indicators or when indicators are time-dependent, only becoming evident after a certain lag time. The following example of a pregnancy test will make use of such an indicator. Modern pregnancy tests do not use the pregnancy itself to determine pregnancy status; rather, human chorionic gonadotropin is used, or hCG, present in the urine of gravid females, as a surrogate marker to indicate that a woman is pregnant. Because hCG can also be produced by a tumor, the specificity of modern pregnancy tests cannot be 100% (because false positives are possible). Also, because hCG is present in the urine in such small concentrations after fertilization and early embryogenesis, the sensitivity of modern pregnancy tests cannot be 100% (because false negatives are possible)."
            },
            {
                "text": "Positive and negative predictive values In addition to sensitivity and specificity, the performance of a binary classification test can be measured with positive predictive value (PPV), also known as precision, and negative predictive value (NPV). The positive prediction value answers the question \"If the test result is positive, how well does that predict an actual presence of disease?\". It is calculated as TP/(TP + FP); that is, it is the proportion of true positives out of all positive results. The negative prediction value is the same, but for negatives, naturally. Impact of prevalence on predictive values Prevalence has a significant impact on prediction values. As an example, suppose there is a test for a disease with 99% sensitivity and 99% specificity. If 2000 people are tested and the prevalence (in the sample) is 50%, 1000 of them are sick and 1000 of them are healthy. Thus about 990 true positives and 990 true negatives are likely, with 10 false positives and 10 false negatives. The positive and negative prediction values would be 99%, so there can be high confidence in the result."
            },
            {
                "text": "However, if the prevalence is only 5%, so of the 2000 people only 100 are really sick, then the prediction values change significantly. The likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 19+99 people tested positive, only 99 really have the disease – that means, intuitively, that given that a patient's test result is positive, there is only 84% chance that they really have the disease. On the other hand, given that the patient's test result is negative, there is only 1 chance in 1882, or 0.05% probability, that the patient has the disease despite the test result. Precision and recall Precision and recall can be interpreted as (estimated) conditional probabilities: Precision is given by $P(C=P|\\hat{C}=P)$ while recall is given by $P(\\hat{C}=P|C=P)$, where $\\hat{C}$ is the predicted class and $C$ is the actual class. Both quantities are therefore connected by Bayes' theorem."
            },
            {
                "text": "Relationships There are various relationships between these ratios. If the prevalence, sensitivity, and specificity are known, the positive predictive value can be obtained from the following identity: $ \\text{PPV} = \\frac{(\\text{sensitivity}) (\\text{prevalence})}{(\\text{sensitivity}) (\\text{prevalence}) + (1 - \\text{specificity}) (1-\\text{prevalence})} $ If the prevalence, sensitivity, and specificity are known, the negative predictive value can be obtained from the following identity: $ \\text{NPV} = \\frac{(\\text{specificity}) (1 - \\text{prevalence})}{(\\text{specificity}) (1 - \\text{prevalence}) + (1 - \\text{sensitivity}) (\\text{prevalence})}. $ Unitary metrics In addition to the paired metrics, there are also unitary metrics that give a single number to evaluate the test. Perhaps the simplest statistic is accuracy or fraction correct (FC), which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications: (TP + TN)/total population = (TP + TN)/(TP + TN + FP + FN)."
            },
            {
                "text": "As such, it compares estimates of pre- and post-test probability. In total ignorance, one can compare a rule to flipping a coin (p0=0.5). This measure is prevalence-dependent. If 90% of people with COVID symptoms don't have COVID, the prior probability P(-) is 0.9, and the simple rule \"Classify all such patients as COVID-free.\" would be 90% accurate. Diagnosis should be better than that. One can construct a \"One-proportion z-test\" with p0 as max(priors) = max(P(-),P(+)) for a diagnostic method hoping to beat a simple rule using the most likely outcome. Here, the hypotheses are \"Ho: p ≤ 0.9 vs. Ha: p > 0.9\", rejecting Ho for large values of z. One diagnostic rule could be compared to another if the other's accuracy is known and substituted for p0 in calculating the z statistic. If not known and calculated from data, an accuracy comparison test could be made using \"Two-proportion z-test, pooled for Ho: p1 = p2\"."
            },
            {
                "text": "Not used very much is the complementary statistic, the fraction incorrect (FiC): FC + FiC = 1, or (FP + FN)/(TP + TN + FP + FN) – this is the sum of the antidiagonal, divided by the total population. Cost-weighted fractions incorrect could compare expected costs of misclassification for different methods. The diagnostic odds ratio (DOR) can be a more useful overall metric, which can be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN), or indirectly as a ratio of ratio of ratios (ratio of likelihood ratios, which are themselves ratios of true rates or prediction values). This has a useful interpretation – as an odds ratio – and is prevalence-independent. Likelihood ratio is generally considered to be prevalence-independent and is easily interpreted as the multiplier to turn prior probabilities into posterior probabilities. An F-score is a combination of the precision and the recall, providing a single score. There is a one-parameter family of statistics, with parameter β, which determines the relative weights of precision and recall."
            },
            {
                "text": "The traditional or balanced F-score (F1 score) is the harmonic mean of precision and recall: $F_1 = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}} $. F-scores do not take the true negative rate into account and, therefore, are more suited to information retrieval and information extraction evaluation where the true negatives are innumerable. Instead, measures such as the phi coefficient, Matthews correlation coefficient, informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are markedness (deltap) and informedness (Youden's J statistic or deltap'). Choosing the appropriate form of evaluation Hand has highlighted the importance of choosing an appropriate method of evaluation. However, of the many different methods for evaluating the accuracy of a classifier, there is no general method for determining which method should be used in which circumstances."
            },
            {
                "text": "Different fields have taken different approaches. Cullerne Bown has distinguished three basic approaches to evaluation: ° Mathematical - such as the Matthews Correlation Coefficient, in which both kinds of error are axiomatically treated as equally problematic; ° Cost-benefit - in which a currency is adopted (e.g. money or Quality Adjusted Life Years) and values assigned to errors and successes on the basis of empirical measurement; ° Judgemental - in which a human judgement is made about the relative importance of the two kinds of error; typically this starts by adopting a pair of indicators such as sensitivity and specificity, precision and recall or positive predictive value and negative predictive value. In the judgemental case, he has provided a flow chart for determining which pair of indicators should be used when, and consequently how to choose between the Receiver Operating Characteristic and the Precision-Recall Curve. Evaluation of underlying technologies Often, we want to evaluate not a specific classifier working in a specific way but an underlying technology. Typically, the technology can be adjusted through altering the threshold of a score function, the threshold determining whether the result is a positive or negative."
            },
            {
                "text": "For such evaluations a useful single measure is \"area under the ROC curve\", AUC. Accuracy aside Apart from accuracy, binary classifiers can be assessed in many other ways, for example in terms of their speed or cost. Evaluation of probabilistic classifiers Probabilistic classification models go beyond providing binary outputs and instead produce probability scores for each class. These models are designed to assess the likelihood or probability of an instance belonging to different classes. In the context of evaluating probabilistic classifiers, alternative evaluation metrics have been developed to properly assess the performance of these models. These metrics take into account the probabilistic nature of the classifier's output and provide a more comprehensive assessment of its effectiveness in assigning accurate probabilities to different classes. These evaluation metrics aim to capture the degree of calibration, discrimination, and overall accuracy of the probabilistic classifier's predictions. In information systems Information retrieval systems, such as databases and web search engines, are evaluated by many different metrics, some of which are derived from the confusion matrix, which divides results into true positives (documents correctly retrieved), true negatives (documents correctly not retrieved), false positives (documents incorrectly retrieved), and false negatives (documents incorrectly not retrieved)."
            },
            {
                "text": "Commonly used metrics include the notions of precision and recall. In this context, precision is defined as the fraction of documents correctly retrieved compared to the documents retrieved (true positives divided by true positives plus false positives), using a set of ground truth relevant results selected by humans. Recall is defined as the fraction of documents correctly retrieved compared to the relevant documents (true positives divided by true positives plus false negatives). Less commonly, the metric of accuracy is used, is defined as the fraction of documents correctly classified compared to the documents (true positives plus true negatives divided by true positives plus true negatives plus false positives plus false negatives). None of these metrics take into account the ranking of results. Ranking is very important for web search engines because readers seldom go past the first page of results, and there are too many documents on the web to manually classify all of them as to whether they should be included or excluded from a given search. Adding a cutoff at a particular number of results takes ranking into account to some degree. The measure precision at k, for example, is a measure of precision looking only at the top ten (k=10) search results. More sophisticated metrics, such as discounted cumulative gain, take into account each individual ranking, and are more commonly used where this is important. See also Population impact measures Attributable risk Attributable risk percent Scoring rule (for probability predictions) Pseudo-R-squared Likelihood ratios References External links Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules Category:Statistical classification Category:Machine learning"
            }
        ],
        "latex_formulas": [
            "P(C=P|\\hat{C}=P)",
            "P(\\hat{C}=P|C=P)",
            "\\hat{C}",
            "C",
            "\\text{PPV} = \\frac{(\\text{sensitivity}) (\\text{prevalence})}{(\\text{sensitivity}) (\\text{prevalence}) + (1 - \\text{specificity}) (1-\\text{prevalence})}",
            "\\text{NPV} = \\frac{(\\text{specificity}) (1 - \\text{prevalence})}{(\\text{specificity}) (1 - \\text{prevalence}) + (1 - \\text{sensitivity}) (\\text{prevalence})}.",
            "F_1 = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}"
        ]
    },
    "Multiclass_classification": {
        "title": "Multiclass_classification",
        "chunks": [
            {
                "text": "In machine learning and statistical classification, multiclass classification or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification). For example, deciding on whether an image is showing a banana, peach, orange, or an apple is a multiclass classification problem, with four possible classes (banana, peach, orange, apple), while deciding on whether an image contains an apple or not is a binary classification problem (with the two possible classes being: apple, no apple). While many classification algorithms (notably multinomial logistic regression) naturally permit the use of more than two classes, some are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies. Multiclass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance (e.g., predicting that an image contains both an apple and an orange, in the previous example). General strategies The existing multi-class classification techniques can be categorised into transformation to binary extension from binary hierarchical classification."
            },
            {
                "text": "Transformation to binary This section discusses strategies for reducing the problem of multiclass classification to multiple binary classification problems. It can be categorized into one vs rest and one vs one. The techniques developed based on reducing the multi-class problem into multiple binary problems can also be called problem transformation techniques. One-vs.-rest One-vs.-rest (OvR or one-vs.-all, OvA or one-against-all, OAA) strategy involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. This strategy requires the base classifiers to produce a real-valued score for its decision (see also scoring rule), rather than just a class label; discrete class labels alone can lead to ambiguities, where multiple classes are predicted for a single sample.In multi-label classification, OvR is known as binary relevance and the prediction of multiple classes is considered a feature, not a problem."
            },
            {
                "text": "In pseudocode, the training algorithm for an OvR learner constructed from a binary classification learner is as follows: Inputs: , a learner (training algorithm for binary classifiers) samples labels where ∈ {1, … } is the label for the sample Output: a list of classifiers for ∈ {1, …, } Procedure: For each in {1, …, } Construct a new label vector where $z $y if $y and $z otherwise Apply to , to obtain Making decisions means applying all classifiers to an unseen sample and predicting the label for which the corresponding classifier reports the highest confidence score: $\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\arg\\!\\max}\\; f_k(x)$ Although this strategy is popular, it is a heuristic that suffers from several problems."
            },
            {
                "text": "Firstly, the scale of the confidence values may differ between the binary classifiers. Second, even if the class distribution is balanced in the training set, the binary classification learners see unbalanced distributions because typically the set of negatives they see is much larger than the set of positives. One-vs.-one In the one-vs.-one (OvO) reduction, one trains $K (K − 1) / 2$ binary classifiers for a -way multiclass problem; each receives the samples of a pair of classes from the original training set, and must learn to distinguish these two classes. At prediction time, a voting scheme is applied: all $K (K − 1) / 2$ classifiers are applied to an unseen sample and the class that got the highest number of \"+1\" predictions gets predicted by the combined classifier. Like OvR, OvO suffers from ambiguities in that some regions of its input space may receive the same number of votes. Extension from binary This section discusses strategies of extending the existing binary classifiers to solve multi-class classification problems. Several algorithms have been developed based on neural networks, decision trees, k-nearest neighbors, naive Bayes, support vector machines and extreme learning machines to address multi-class classification problems."
            },
            {
                "text": "These types of techniques can also be called algorithm adaptation techniques. Neural networks Multiclass perceptrons provide a natural extension to the multi-class problem. Instead of just having one neuron in the output layer, with binary output, one could have N binary neurons leading to multi-class classification. In practice, the last layer of a neural network is usually a softmax function layer, which is the algebraic simplification of N logistic classifiers, normalized per class by the sum of the N-1 other logistic classifiers. Neural Network-based classification has brought significant improvements and scopes for thinking from different perspectives. Extreme learning machines Extreme learning machines (ELM) is a special case of single hidden layer feed-forward neural networks (SLFNs) wherein the input weights and the hidden node biases can be chosen at random. Many variants and developments are made to the ELM for multiclass classification. k-nearest neighbours k-nearest neighbors kNN is considered among the oldest non-parametric classification algorithms. To classify an unknown example, the distance from that example to every other training example is measured. The k smallest distances are identified, and the most represented class by these k nearest neighbours is considered the output class label."
            },
            {
                "text": "Naive Bayes Naive Bayes is a successful classifier based upon the principle of maximum a posteriori (MAP). This approach is naturally extensible to the case of having more than two classes, and was shown to perform well in spite of the underlying simplifying assumption of conditional independence. Decision trees Decision tree learning is a powerful classification technique. The tree tries to infer a split of the training data based on the values of the available features to produce a good generalization. The algorithm can naturally handle binary or multiclass classification problems. The leaf nodes can refer to any of the K classes concerned. Support vector machines Support vector machines are based upon the idea of maximizing the margin i.e. maximizing the minimum distance from the separating hyperplane to the nearest example. The basic SVM supports only binary classification, but extensions have been proposed to handle the multiclass classification case as well. In these extensions, additional parameters and constraints are added to the optimization problem to handle the separation of the different classes. Multi expression programming Multi expression programming (MEP) is an evolutionary algorithm for generating computer programs (that can be used for classification tasks too)."
            },
            {
                "text": "MEP has a unique feature: it encodes multiple programs into a single chromosome. Each of these programs can be used to generate the output for a class, thus making MEP naturally suitable for solving multi-class classification problems. Hierarchical classification Hierarchical classification tackles the multi-class classification problem by dividing the output space i.e. into a tree. Each parent node is divided into multiple child nodes and the process is continued until each child node represents only one class. Several methods have been proposed based on hierarchical classification. Learning paradigms Based on learning paradigms, the existing multi-class classification techniques can be classified into batch learning and online learning. Batch learning algorithms require all the data samples to be available beforehand. It trains the model using the entire training data and then predicts the test sample using the found relationship. The online learning algorithms, on the other hand, incrementally build their models in sequential iterations. In iteration t, an online algorithm receives a sample, xt and predicts its label ŷt using the current model; the algorithm then receives yt, the true label of xt and updates its model based on the sample-label pair: (xt, yt). Recently, a new learning paradigm called progressive learning technique has been developed. The progressive learning technique is capable of not only learning from new samples but also capable of learning new classes of data and yet retain the knowledge learnt thus far. Evaluation The performance of a multi-class classification system is often assessed by comparing the predictions of the system against reference labels with an evaluation metric. Common evaluation metrics are Accuracy or macro F1."
            },
            {
                "text": "See also Binary classification One-class classification Multi-label classification Multiclass perceptron Multi-task learning Notes References Category:Classification algorithms Category:Statistical classification"
            }
        ],
        "latex_formulas": [
            "''z''{{sub|''i''}}",
            "''y''{{sub|''i''}}",
            "''y''{{sub|''i''}} {{=}} ''k''",
            "''z''{{sub|''i''}} {{=}} 0",
            "''K'' (''K'' − 1) / 2",
            "''K'' (''K'' − 1) / 2",
            "\\hat{y} = \\underset{k \\in \\{1 \\ldots K\\}}{\\arg\\!\\max}\\; f_k(x)"
        ]
    },
    "Data_preprocessing": {
        "title": "Data_preprocessing",
        "chunks": [
            {
                "text": "Data preprocessing can refer to manipulation, filtration or augmentation of data before it is analyzed, and is often an important step in the data mining process. Data collection methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, and missing values, amongst other issues. Preprocessing is the process by which unstructured data is transformed into intelligible representations suitable for machine-learning models. This phase of model deals with noise in order to arrive at better and improved results from the original data set which was noisy. This dataset also has some level of missing value present in it. The preprocessing pipeline used can often have large effects on the conclusions drawn from the downstream analysis. Thus, representation and quality of data is necessary before running any analysis.Pyle, D., 1999. Data Preparation for Data Mining. Morgan Kaufmann Publishers, Los Altos, California. Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology. If there is a high proportion of irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase may be more difficult."
            },
            {
                "text": "Data preparation and filtering steps can take a considerable amount of processing time. Examples of methods used in data preprocessing include cleaning, instance selection, normalization, one-hot encoding, data transformation, feature extraction and feature selection. Applications Data mining Data preprocessing allows for the removal of unwanted data with the use of data cleaning, this allows the user to have a dataset to contain more valuable information after the preprocessing stage for data manipulation later in the data mining process. Editing such dataset to either correct data corruption or human error is a crucial step to get accurate quantifiers like true positives, true negatives, false positives and false negatives found in a confusion matrix that are commonly used for a medical diagnosis. Users are able to join data files together and use preprocessing to filter any unnecessary noise from the data which can allow for higher accuracy. Users use Python programming scripts accompanied by the pandas library which gives them the ability to import data from a comma-separated values as a data-frame. The data-frame is then used to manipulate data that can be challenging otherwise to do in Excel."
            },
            {
                "text": "Pandas (software) which is a powerful tool that allows for data analysis and manipulation; which makes data visualizations, statistical operations and much more, a lot easier. Many also use the R programming language to do such tasks as well. The reason why a user transforms existing files into a new one is because of many reasons. Aspects of data preprocessing may include imputing missing values, aggregating numerical quantities and transforming continuous data into categories (data binning). More advanced techniques like principal component analysis and feature selection are working with statistical formulas and are applied to complex datasets which are recorded by GPS trackers and motion capture devices. Semantic data preprocessing Semantic data mining is a subset of data mining that specifically seeks to incorporate domain knowledge, such as formal semantics, into the data mining process. Domain knowledge is the knowledge of the environment the data was processed in. Domain knowledge can have a positive influence on many aspects of data mining, such as filtering out redundant or inconsistent data during the preprocessing phase. Domain knowledge also works as constraint. It does this by using working as set of prior knowledge to reduce the space required for searching and acting as a guide to the data."
            },
            {
                "text": "Simply put, semantic preprocessing seeks to filter data using the original environment of said data more correctly and efficiently. There are increasingly complex problems which are asking to be solved by more elaborate techniques to better analyze existing information. Instead of creating a simple script for aggregating different numerical values into a single value, it make sense to focus on semantic based data preprocessing. The idea is to build a dedicated ontology, which explains on a higher level what the problem is about. In regards to semantic data mining and semantic pre-processing, ontologies are a way to conceptualize and formally define semantic knowledge and data. The Protégé (software) is the standard tool for constructing an ontology. In general, the use of ontologies bridges the gaps between data, applications, algorithms, and results that occur from semantic mismatches. As a result, semantic data mining combined with ontology has many applications where semantic ambiguity can impact the usefulness and efficiency of data systems. Applications include the medical field, language processing, banking, and even tutoring, among many more. There are various strengths to using a semantic data mining and ontological based approach."
            },
            {
                "text": "As previously mentioned, these tools can help during the per-processing phase by filtering out non-desirable data from the data set. Additionally, well-structured formal semantics integrated into well designed ontologies can return powerful data that can be easily read and processed by machines. A specifically useful example of this exists in the medical use of semantic data processing. As an example, a patient is having a medical emergency and is being rushed to hospital. The emergency responders are trying to figure out the best medicine to administer to help the patient. Under normal data processing, scouring all the patient’s medical data to ensure they are getting the best treatment could take too long and risk the patients’ health or even life. However, using semantically processed ontologies, the first responders could save the patient’s life. Tools like a semantic reasoner can use ontology to infer the what best medicine to administer to the patient is based on their medical history, such as if they have a certain cancer or other conditions, simply by examining the natural language used in the patient's medical records."
            },
            {
                "text": "This would allow the first responders to quickly and efficiently search for medicine without having worry about the patient’s medical history themselves, as the semantic reasoner would already have analyzed this data and found solutions. In general, this illustrates the incredible strength of using semantic data mining and ontologies. They allow for quicker and more efficient data extraction on the user side, as the user has fewer variables to account for, since the semantically pre-processed data and ontology built for the data have already accounted for many of these variables. However, there are some drawbacks to this approach. Namely, it requires a high amount of computational power and complexity, even with relatively small data sets. This could result in higher costs and increased difficulties in building and maintaining semantic data processing systems. This can be mitigated somewhat if the data set is already well organized and formatted, but even then, the complexity is still higher when compared to standard data processing. Below is a simple a diagram combining some of the processes, in particular semantic data mining and their use in ontology."
            },
            {
                "text": "SimpleSemanticDataMiningDiagram The diagram depicts a data set being broken up into two parts: the characteristics of its domain, or domain knowledge, and then the actual acquired data. The domain characteristics are then processed to become user understood domain knowledge that can be applied to the data. Meanwhile, the data set is processed and stored so that the domain knowledge can applied to it, so that the process may continue. This application forms the ontology. From there, the ontology can be used to analyze data and process results. Fuzzy preprocessing is another, more advanced technique for solving complex problems. Fuzzy preprocessing and fuzzy data mining make use of fuzzy sets. These data sets are composed of two elements: a set and a membership function for the set which comprises 0 and 1. Fuzzy preprocessing uses this fuzzy data set to ground numerical values with linguistic information. Raw data is then transformed into natural language. Ultimately, fuzzy data mining's goal is to help deal with inexact information, such as an incomplete database. Currently fuzzy preprocessing, as well as other fuzzy based data mining techniques see frequent use with neural networks and artificial intelligence. References External links Online Data Processing Compendium Data preprocessing in predictive data mining. Knowledge Eng. Review 34: e1 (2019) Category:Machine learning Category:Data mining"
            }
        ],
        "latex_formulas": []
    },
    "Nonlinear_system": {
        "title": "Nonlinear_system",
        "chunks": [
            {
                "text": "In mathematics and science, a nonlinear system (or a non-linear system) is a system in which the change of the output is not proportional to the change of the input. Nonlinear problems are of interest to engineers, biologists, physicists, mathematicians, and many other scientists since most systems are inherently nonlinear in nature. Nonlinear dynamical systems, describing changes in variables over time, may appear chaotic, unpredictable, or counterintuitive, contrasting with much simpler linear systems. Typically, the behavior of a nonlinear system is described in mathematics by a nonlinear system of equations, which is a set of simultaneous equations in which the unknowns (or the unknown functions in the case of differential equations) appear as variables of a polynomial of degree higher than one or in the argument of a function which is not a polynomial of degree one. In other words, in a nonlinear system of equations, the equation(s) to be solved cannot be written as a linear combination of the unknown variables or functions that appear in them. Systems can be defined as nonlinear, regardless of whether known linear functions appear in the equations."
            },
            {
                "text": "In particular, a differential equation is linear if it is linear in terms of the unknown function and its derivatives, even if nonlinear in terms of the other variables appearing in it. As nonlinear dynamical equations are difficult to solve, nonlinear systems are commonly approximated by linear equations (linearization). This works well up to some accuracy and some range for the input values, but some interesting phenomena such as solitons, chaos,Nonlinear Dynamics I: Chaos at MIT's OpenCourseWare and singularities are hidden by linearization. It follows that some aspects of the dynamic behavior of a nonlinear system can appear to be counterintuitive, unpredictable or even chaotic. Although such chaotic behavior may resemble random behavior, it is in fact not random. For example, some aspects of the weather are seen to be chaotic, where simple changes in one part of the system produce complex effects throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology. Some authors use the term nonlinear science for the study of nonlinear systems. This term is disputed by others: Definition In mathematics, a linear map (or linear function) $f(x)$ is one which satisfies both of the following properties: Additivity or superposition principle: $\\textstyle f(x + y) = f(x) + f(y);$ Homogeneity: $\\textstyle f(\\alpha x) = \\alpha f(x).$ Additivity implies homogeneity for any rational α, and, for continuous functions, for any real α."
            },
            {
                "text": "For a complex α, homogeneity does not follow from additivity. For example, an antilinear map is additive but not homogeneous. The conditions of additivity and homogeneity are often combined in the superposition principle $f(\\alpha x + \\beta y) = \\alpha f(x) + \\beta f(y)$ An equation written as $f(x) = C$ is called linear if $f(x)$ is a linear map (as defined above) and nonlinear otherwise. The equation is called homogeneous if $C = 0$ and $f(x)$ is a homogeneous function. The definition $f(x) = C$ is very general in that $x$ can be any sensible mathematical object (number, vector, function, etc. ), and the function $f(x)$ can literally be any mapping, including integration or differentiation with associated constraints (such as boundary values). If $f(x)$ contains differentiation with respect to $x$, the result will be a differential equation."
            },
            {
                "text": "Nonlinear systems of equations A nonlinear system of equations consists of a set of equations in several variables such that at least one of them is not a linear equation. For a single equation of the form $f(x)=0,$ many methods have been designed; see Root-finding algorithm. In the case where is a polynomial, one has a polynomial equation such as $x^2 + x - 1 = 0.$ The general root-finding algorithms apply to polynomial roots, but, generally they do not find all the roots, and when they fail to find a root, this does not imply that there is no roots. Specific methods for polynomials allow finding all roots or the real roots; see real-root isolation. Solving systems of polynomial equations, that is finding the common zeros of a set of several polynomials in several variables is a difficult problem for which elaborate algorithms have been designed, such as Gröbner base algorithms. For the general case of system of equations formed by equating to zero several differentiable functions, the main method is Newton's method and its variants."
            },
            {
                "text": "Generally they may provide a solution, but do not provide any information on the number of solutions. Nonlinear recurrence relations A nonlinear recurrence relation defines successive terms of a sequence as a nonlinear function of preceding terms. Examples of nonlinear recurrence relations are the logistic map and the relations that define the various Hofstadter sequences. Nonlinear discrete models that represent a wide class of nonlinear recurrence relationships include the NARMAX (Nonlinear Autoregressive Moving Average with eXogenous inputs) model and the related nonlinear system identification and analysis procedures.Billings S.A. \"Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains\". Wiley, 2013 These approaches can be used to study a wide class of complex nonlinear behaviors in the time, frequency, and spatio-temporal domains. Nonlinear differential equations A system of differential equations is said to be nonlinear if it is not a system of linear equations. Problems involving nonlinear differential equations are extremely diverse, and methods of solution or analysis are problem dependent. Examples of nonlinear differential equations are the Navier–Stokes equations in fluid dynamics and the Lotka–Volterra equations in biology."
            },
            {
                "text": "One of the greatest difficulties of nonlinear problems is that it is not generally possible to combine known solutions into new solutions. In linear problems, for example, a family of linearly independent solutions can be used to construct general solutions through the superposition principle. A good example of this is one-dimensional heat transport with Dirichlet boundary conditions, the solution of which can be written as a time-dependent linear combination of sinusoids of differing frequencies; this makes solutions very flexible. It is often possible to find several very specific solutions to nonlinear equations, however the lack of a superposition principle prevents the construction of new solutions. Ordinary differential equations First order ordinary differential equations are often exactly solvable by separation of variables, especially for autonomous equations. For example, the nonlinear equation $\\frac{d u}{d x} = -u^2$ has $u=\\frac{1}{x+C}$ as a general solution (and also the special solution $u = 0,$ corresponding to the limit of the general solution when C tends to infinity). The equation is nonlinear because it may be written as $\\frac{du}{d x} + u^2=0$ and the left-hand side of the equation is not a linear function of $u$ and its derivatives."
            },
            {
                "text": "Note that if the $u^2$ term were replaced with $u$, the problem would be linear (the exponential decay problem). Second and higher order ordinary differential equations (more generally, systems of nonlinear equations) rarely yield closed-form solutions, though implicit solutions and solutions involving nonelementary integrals are encountered. Common methods for the qualitative analysis of nonlinear ordinary differential equations include: Examination of any conserved quantities, especially in Hamiltonian systems Examination of dissipative quantities (see Lyapunov function) analogous to conserved quantities Linearization via Taylor expansion Change of variables into something easier to study Bifurcation theory Perturbation methods (can be applied to algebraic equations too) Existence of solutions of Finite-Duration, which can happen under specific conditions for some non-linear ordinary differential equations. Partial differential equations The most common basic approach to studying nonlinear partial differential equations is to change the variables (or otherwise transform the problem) so that the resulting problem is simpler (possibly linear). Sometimes, the equation may be transformed into one or more ordinary differential equations, as seen in separation of variables, which is always useful whether or not the resulting ordinary differential equation(s) is solvable."
            },
            {
                "text": "Another common (though less mathematical) tactic, often exploited in fluid and heat mechanics, is to use scale analysis to simplify a general, natural equation in a certain specific boundary value problem. For example, the (very) nonlinear Navier-Stokes equations can be simplified into one linear partial differential equation in the case of transient, laminar, one dimensional flow in a circular pipe; the scale analysis provides conditions under which the flow is laminar and one dimensional and also yields the simplified equation. Other methods include examining the characteristics and using the methods outlined above for ordinary differential equations. Pendula right|200px right|200px A classic, extensively studied nonlinear problem is the dynamics of a frictionless pendulum under the influence of gravity. Using Lagrangian mechanics, it may be shownDavid Tong: Lectures on Classical Dynamics that the motion of a pendulum can be described by the dimensionless nonlinear equation $\\frac{d^2 \\theta}{d t^2} + \\sin(\\theta) = 0$ where gravity points \"downwards\" and $\\theta$ is the angle the pendulum forms with its rest position, as shown in the figure at right."
            },
            {
                "text": "One approach to \"solving\" this equation is to use $d\\theta/dt$ as an integrating factor, which would eventually yield $\\int{\\frac{d \\theta}{\\sqrt{C_0 + 2 \\cos(\\theta)}}} = t + C_1$ which is an implicit solution involving an elliptic integral. This \"solution\" generally does not have many uses because most of the nature of the solution is hidden in the nonelementary integral (nonelementary unless $C_0 = 2$). Another way to approach the problem is to linearize any nonlinearity (the sine function term in this case) at the various points of interest through Taylor expansions. For example, the linearization at $\\theta = 0$, called the small angle approximation, is $\\frac{d^2 \\theta}{d t^2} + \\theta = 0$ since $\\sin(\\theta) \\approx \\theta$ for $\\theta \\approx 0$. This is a simple harmonic oscillator corresponding to oscillations of the pendulum near the bottom of its path. Another linearization would be at $\\theta = \\pi$, corresponding to the pendulum being straight up: $\\frac{d^2 \\theta}{d t^2} + \\pi - \\theta = 0$ since $\\sin(\\theta) \\approx \\pi - \\theta$ for $\\theta \\approx \\pi$."
            },
            {
                "text": "The solution to this problem involves hyperbolic sinusoids, and note that unlike the small angle approximation, this approximation is unstable, meaning that $|\\theta|$ will usually grow without limit, though bounded solutions are possible. This corresponds to the difficulty of balancing a pendulum upright, it is literally an unstable state. One more interesting linearization is possible around $\\theta = \\pi/2$, around which $\\sin(\\theta) \\approx 1$: $\\frac{d^2 \\theta}{d t^2} + 1 = 0.$ This corresponds to a free fall problem. A very useful qualitative picture of the pendulum's dynamics may be obtained by piecing together such linearizations, as seen in the figure at right. Other techniques may be used to find (exact) phase portraits and approximate periods. Types of nonlinear dynamic behaviors Amplitude death – any oscillations present in the system cease due to some kind of interaction with other system or feedback by the same system Chaos – values of a system cannot be predicted indefinitely far into the future, and fluctuations are aperiodic Multistability – the presence of two or more stable states Solitons – self-reinforcing solitary waves Limit cycles – asymptotic periodic orbits to which destabilized fixed points are attracted. Self-oscillations – feedback oscillations taking place in open dissipative physical systems."
            },
            {
                "text": "Examples of nonlinear equations Algebraic Riccati equation Ball and beam system Bellman equation for optimal policy Boltzmann equation Colebrook equation General relativity Ginzburg–Landau theory Ishimori equation Kadomtsev–Petviashvili equation Korteweg–de Vries equation Landau–Lifshitz–Gilbert equation Liénard equation Navier–Stokes equations of fluid dynamics Nonlinear optics Nonlinear Schrödinger equation Power-flow study Richards equation for unsaturated water flow Self-balancing unicycle Sine-Gordon equation Van der Pol oscillator Vlasov equation See also Aleksandr Mikhailovich Lyapunov Dynamical system Feedback Initial condition Linear system Mode coupling Vector soliton Volterra series References Further reading External links Command and Control Research Program (CCRP) New England Complex Systems Institute: Concepts in Complex Systems Nonlinear Dynamics I: Chaos at MIT's OpenCourseWare Nonlinear Model Library (in MATLAB) a Database of Physical Systems The Center for Nonlinear Studies at Los Alamos National Laboratory Category:Dynamical systems Category:Concepts in physics"
            }
        ],
        "latex_formulas": [
            "f(x)",
            "\\textstyle f(x + y) = f(x) + f(y);",
            "\\textstyle f(\\alpha x) = \\alpha f(x).",
            "f(\\alpha x + \\beta y) = \\alpha f(x) + \\beta f(y)",
            "f(x) = C",
            "f(x)",
            "C = 0",
            "f(x)",
            "f(x) = C",
            "x",
            "f(x)",
            "f(x)",
            "x",
            "f(x)=0,",
            "x^2 + x - 1 = 0.",
            "\\frac{d u}{d x} = -u^2",
            "u=\\frac{1}{x+C}",
            "u = 0,",
            "\\frac{du}{d x} + u^2=0",
            "u",
            "u^2",
            "u",
            "\\frac{d^2 \\theta}{d t^2} + \\sin(\\theta) = 0",
            "\\theta",
            "d\\theta/dt",
            "\\int{\\frac{d \\theta}{\\sqrt{C_0 + 2 \\cos(\\theta)}}} = t + C_1",
            "C_0 = 2",
            "\\theta = 0",
            "\\frac{d^2 \\theta}{d t^2} + \\theta = 0",
            "\\sin(\\theta) \\approx \\theta",
            "\\theta \\approx 0",
            "\\theta = \\pi",
            "\\frac{d^2 \\theta}{d t^2} + \\pi - \\theta = 0",
            "\\sin(\\theta) \\approx \\pi - \\theta",
            "\\theta \\approx \\pi",
            "|\\theta|",
            "\\theta = \\pi/2",
            "\\sin(\\theta) \\approx 1",
            "\\frac{d^2 \\theta}{d t^2} + 1 = 0."
        ]
    },
    "Linear_regression": {
        "title": "Linear_regression",
        "chunks": [
            {
                "text": "In statistics, linear regression is a model that estimates the linear relationship between a scalar response (dependent variable) and one or more explanatory variables (regressor or independent variable). A model with exactly one explanatory variable is a simple linear regression; a model with two or more explanatory variables is a multiple linear regression. This term is distinct from multivariate linear regression, which predicts multiple correlated dependent variables rather than a single dependent variable.. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis. Linear regression is also a type of machine learning algorithm, more specifically a supervised algorithm, that learns from the labelled datasets and maps the data points to the most optimized linear functions that can be used for prediction on new datasets."
            },
            {
                "text": "Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine. Linear regression has many practical uses. Most applications fall into one of the following two broad categories: If the goal is error i.e. variance reduction in prediction or forecasting, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response. If the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response."
            },
            {
                "text": "Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Use of the Mean Squared Error (MSE) as the cost on a dataset that has many large outliers, can result in a model that fits the outliers more than the true data due to the higher importance assigned by MSE to large errors. So, cost functions that are robust to outliers should be used if the dataset has many large outliers. Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous. Formulation thumb|In linear regression, the observations (red) are assumed to be the result of random deviations (green) from an underlying relationship (blue) between a dependent variable (y) and an independent variable (x)."
            },
            {
                "text": "Given a data set $\\{y_i,\\, x_{i1}, \\ldots, x_{ip}\\}_{i=1}^n$ of n statistical units, a linear regression model assumes that the relationship between the dependent variable y and the vector of regressors x is linear. This relationship is modeled through a disturbance term or error variable ε—an unobserved random variable that adds \"noise\" to the linear relationship between the dependent variable and regressors. Thus the model takes the formwhere T denotes the transpose, so that xiTβ is the inner product between vectors xi and β."
            },
            {
                "text": "Often these n equations are stacked together and written in matrix notation as $ \\mathbf{y} = \\mathbf{X} \\boldsymbol\\beta + \\boldsymbol\\varepsilon, \\, $ where $ \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\quad $ $ \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}^\\mathsf{T}_1 \\\\ \\mathbf{x}^\\mathsf{T}_2 \\\\ \\vdots \\\\ \\mathbf{x}^\\mathsf{T}_n \\end{bmatrix} = \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1p} \\\\ 1 & x_{21} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & \\cdots & x_{np} \\end{bmatrix}, $ $ \\boldsymbol\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}, \\quad \\boldsymbol\\varepsilon = \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix}."
            },
            {
                "text": "$ Notation and terminology $\\mathbf{y}$ is a vector of observed values $y_i\\ (i=1,\\ldots,n)$ of the variable called the regressand, endogenous variable, response variable, target variable, measured variable, criterion variable, or dependent variable. This variable is also sometimes known as the predicted variable, but this should not be confused with predicted values, which are denoted $\\hat{y}$. The decision as to which variable in a data set is modeled as the dependent variable and which are modeled as the independent variables may be based on a presumption that the value of one of the variables is caused by, or directly influenced by the other variables. Alternatively, there may be an operational reason to model one of the variables in terms of the others, in which case there need be no presumption of causality. $\\mathbf{X}$ may be seen as a matrix of row-vectors $\\mathbf{x}_{i\\cdot}$ or of n-dimensional column-vectors $\\mathbf{x}_{\\cdot j}$, which are known as regressors, exogenous variables, explanatory variables, covariates, input variables, predictor variables, or independent variables (not to be confused with the concept of independent random variables)."
            },
            {
                "text": "The matrix $\\mathbf{X}$ is sometimes called the design matrix. Usually a constant is included as one of the regressors. In particular, $x_{i0}=1$ for $i=1,\\ldots,n$. The corresponding element of β is called the intercept. Many statistical inference procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero. Sometimes one of the regressors can be a non-linear function of another regressor or of the data values, as in polynomial regression and segmented regression. The model remains linear as long as it is linear in the parameter vector β. The values xij may be viewed as either observed values of random variables Xj or as fixed values chosen prior to observing the dependent variable. Both interpretations may be appropriate in different cases, and they generally lead to the same estimation procedures; however different approaches to asymptotic analysis are used in these two situations. $\\boldsymbol\\beta$ is a $(p+1)$-dimensional parameter vector, where $\\beta_0$ is the intercept term (if one is included in the model—otherwise $\\boldsymbol\\beta$ is p-dimensional)."
            },
            {
                "text": "Its elements are known as effects or regression coefficients (although the latter term is sometimes reserved for the estimated effects). In simple linear regression, p=1, and the coefficient is known as regression slope. Statistical estimation and inference in linear regression focuses on β. The elements of this parameter vector are interpreted as the partial derivatives of the dependent variable with respect to the various independent variables. $\\boldsymbol\\varepsilon$ is a vector of values $\\varepsilon_i$. This part of the model is called the error term, disturbance term, or sometimes noise (in contrast with the \"signal\" provided by the rest of the model). This variable captures all other factors which influence the dependent variable y other than the regressors x. The relationship between the error term and the regressors, for example their correlation, is a crucial consideration in formulating a linear regression model, as it will determine the appropriate estimation method. Fitting a linear model to a given data set usually requires estimating the regression coefficients $\\boldsymbol\\beta$ such that the error term $\\boldsymbol\\varepsilon=\\mathbf{y}- \\mathbf{X}\\boldsymbol\\beta $ is minimized."
            },
            {
                "text": "For example, it is common to use the sum of squared errors $\\|\\boldsymbol\\varepsilon\\|_2^2$ as a measure of $\\boldsymbol\\varepsilon$ for minimization. Example Consider a situation where a small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti. Physics tells us that, ignoring the drag, the relationship can be modeled as $ h_i = \\beta_1 t_i + \\beta_2 t_i^2 + \\varepsilon_i, $ where β1 determines the initial velocity of the ball, β2 is proportional to the standard gravity, and εi is due to measurement errors. Linear regression can be used to estimate the values of β1 and β2 from the measured data. This model is non-linear in the time variable, but it is linear in the parameters β1 and β2; if we take regressors xi = (xi1, xi2) = (ti, ti2), the model takes on the standard form $ h_i = \\mathbf{x}^\\mathsf{T}_i\\boldsymbol\\beta + \\varepsilon_i. $ Assumptions Standard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variable and their relationship."
            },
            {
                "text": "Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model. Example of a cubic polynomial regression, which is a type of linear regression. Although polynomial regression fits a curve model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y | x) is linear in the unknown parameters that are estimated from the data. For this reason, polynomial regression is considered to be a special case of multiple linear regression. The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares): Weak exogeneity. This essentially means that the predictor variables x can be treated as fixed values, rather than random variables. This means, for example, that the predictor variables are assumed to be error-free—that is, not contaminated with measurement errors."
            },
            {
                "text": "Although this assumption is not realistic in many settings, dropping it leads to significantly more difficult errors-in-variables models. Linearity. This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables. Note that this assumption is much less restrictive than it may at first seem. Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters. The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently. This technique is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given degree) of a predictor variable. With this much flexibility, models such as polynomial regression often have \"too much power\", in that they tend to overfit the data. As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process."
            },
            {
                "text": "Common examples are ridge regression and lasso regression. Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, ridge regression and lasso regression can both be viewed as special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.) Visualization of heteroscedasticity in a scatter plot against 100 random fitted values using MatlabConstant variance (a.k.a. homoscedasticity). This means that the variance of the errors does not depend on the values of the predictor variables. Thus the variability of the responses for given fixed values of the predictors is the same regardless of how large or small the responses are. This is often not the case, as a variable whose mean is large will typically have a greater variance than one whose mean is small. For example, a person whose income is predicted to be $100,000 may easily have an actual income of $80,000 or $120,000—i.e., a standard deviation of around $20,000—while another person with a predicted income of $10,000 is unlikely to have the same $20,000 standard deviation, since that would imply their actual income could vary anywhere between −$10,000 and $30,000."
            },
            {
                "text": "(In fact, as this shows, in many cases—often the same cases where the assumption of normally distributed errors fails—the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) The absence of homoscedasticity is called heteroscedasticity. In order to check this assumption, a plot of residuals versus predicted values (or the values of each individual predictor) can be examined for a \"fanning effect\" (i.e., increasing or decreasing vertical spread as one moves left to right on the plot). A plot of the absolute or squared residuals versus the predicted values (or each predictor) can also be examined for a trend or curvature. Formal tests can also be used; see Heteroscedasticity. The presence of heteroscedasticity will result in an overall \"average\" estimate of variance being used instead of one that takes into account the true variance structure. This leads to less precise (but in the case of ordinary least squares, not biased) parameter estimates and biased standard errors, resulting in misleading tests and interval estimates. The mean squared error for the model will also be wrong."
            },
            {
                "text": "Various estimation techniques including weighted least squares and the use of heteroscedasticity-consistent standard errors can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g., fitting the logarithm of the response variable using a linear regression model, which implies that the response variable itself has a log-normal distribution rather than a normal distribution). To check for violations of the assumptions of linearity, constant variance, and independence of errors within a linear regression model, the residuals are typically plotted against the predicted values (or each of the individual predictors). An apparently random scatter of points about the horizontal midline at 0 is ideal, but cannot rule out certain kinds of violations such as autocorrelation in the errors or their correlation with one or more covariates. Independence of errors. This assumes that the errors of the response variables are uncorrelated with each other. (Actual statistical independence is a stronger condition than mere lack of correlation and is often not needed, although it can be exploited if it is known to hold.)"
            },
            {
                "text": "Some methods such as generalized least squares are capable of handling correlated errors, although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors. Bayesian linear regression is a general way of handling this issue. Lack of perfect multicollinearity in the predictors. For standard least squares estimation methods, the design matrix X must have full column rank p; otherwise perfect multicollinearity exists in the predictor variables, meaning a linear relationship exists between two or more predictor variables. This can be caused by accidentally duplicating a variable in the data, using a linear transformation of a variable along with the original (e.g., the same temperature measurements expressed in Fahrenheit and Celsius), or including a linear combination of multiple variables in the model, such as their mean. It can also happen if there is too little data available compared to the number of parameters to be estimated (e.g., fewer data points than regression coefficients). Near violations of this assumption, where predictors are highly but not perfectly correlated, can reduce the precision of parameter estimates (see Variance inflation factor)."
            },
            {
                "text": "In the case of perfect multicollinearity, the parameter vector β will be non-identifiable—it has no unique solution. In such a case, only some of the parameters can be identified (i.e., their values can only be estimated within some linear subspace of the full parameter space Rp). See partial least squares regression. Methods for fitting linear models with multicollinearity have been developed, some of which require additional assumptions such as \"effect sparsity\"—that a large fraction of the effects are exactly zero. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem. Violations of these assumptions can result in biased estimations of β, biased standard errors, untrustworthy confidence intervals and significance tests. Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods: The statistical relationship between the error terms and the regressors plays an important role in determining whether an estimation procedure has desirable sampling properties such as being unbiased and consistent. The arrangement, or probability distribution of the predictor variables x has a major influence on the precision of estimates of β."
            },
            {
                "text": "Sampling and design of experiments are highly developed subfields of statistics that provide guidance for collecting data in such a way to achieve a precise estimate of β. Interpretation The data sets in the Anscombe's quartet are designed to have approximately the same linear regression line (as well as nearly identical means, standard deviations, and correlations) but are graphically very different. This illustrates the pitfalls of relying solely on a fitted model to understand the relationship between variables. A fitted linear regression model can be used to identify the relationship between a single predictor variable xj and the response variable y when all the other predictor variables in the model are \"held fixed\". Specifically, the interpretation of βj is the expected change in y for a one-unit change in xj when the other covariates are held fixed—that is, the expected value of the partial derivative of y with respect to xj. This is sometimes called the unique effect of xj on y. In contrast, the marginal effect of xj on y can be assessed using a correlation coefficient or simple linear regression model relating only xj to y; this effect is the total derivative of y with respect to xj."
            },
            {
                "text": "Care must be taken when interpreting regression results, as some of the regressors may not allow for marginal changes (such as dummy variables, or the intercept term), while others cannot be held fixed (recall the example from the introduction: it would be impossible to \"hold ti fixed\" and at the same time change the value of ti2). It is possible that the unique effect be nearly zero even when the marginal effect is large. This may imply that some other covariate captures all the information in xj, so that once that variable is in the model, there is no contribution of xj to the variation in y. Conversely, the unique effect of xj can be large while its marginal effect is nearly zero. This would happen if the other covariates explained a great deal of the variation of y, but they mainly explain variation in a way that is complementary to what is captured by xj. In this case, including the other variables in the model reduces the part of the variability of y that is unrelated to xj, thereby strengthening the apparent relationship with xj."
            },
            {
                "text": "The meaning of the expression \"held fixed\" may depend on how the values of the predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been \"held fixed\" by the experimenter. Alternatively, the expression \"held fixed\" can refer to a selection that takes place in the context of data analysis. In this case, we \"hold a variable fixed\" by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of \"held fixed\" that can be used in an observational study. The notion of a \"unique effect\" is appealing when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that is linked to the value of a predictor variable. However, it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design."
            },
            {
                "text": "Extensions Numerous extensions of linear regression have been developed, which allow some or all of the assumptions underlying the basic model to be relaxed. Simple and multiple linear regression Example of simple linear regression, which has one independent variable The simplest case of a single scalar predictor variable x and a single scalar response variable y is known as simple linear regression. The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression (not to be confused with multivariate linear regression). Multiple linear regression is a generalization of simple linear regression to the case of more than one independent variable, and a special case of general linear models, restricted to one dependent variable. The basic model for multiple linear regression is $ Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + \\epsilon_i$ for each observation . In the formula above we consider n observations of one dependent variable and p independent variables. Thus, Yi is the ith observation of the dependent variable, Xij is ith observation of the jth independent variable, j = 1, 2, ..., p. The values βj represent parameters to be estimated, and εi is the ith independent identically distributed normal error."
            },
            {
                "text": "In the more general multivariate linear regression, there is one equation of the above form for each of m > 1 dependent variables that share the same set of explanatory variables and hence are estimated simultaneously with each other: $ Y_{ij} = \\beta_{0j} + \\beta_{1j} X_{i1} + \\beta_{2j}X_{i2} + \\ldots + \\beta_{pj} X_{ip} + \\epsilon_{ij}$ for all observations indexed as i = 1, ... , n and for all dependent variables indexed as j = 1, ... , m. Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased in terms of the multiple regression model. Note, however, that in these cases the response variable y is still a scalar. Another term, multivariate linear regression, refers to cases where y is a vector, i.e., the same as general linear regression. General linear models The general linear model considers the situation when the response variable is not a scalar (for each observation) but a vector, yi."
            },
            {
                "text": "Conditional linearity of $E(\\mathbf{y}\\mid\\mathbf{x}_i)=\\mathbf{x}_i^\\mathsf{T}B$ is still assumed, with a matrix B replacing the vector β of the classical linear regression model. Multivariate analogues of ordinary least squares (OLS) and generalized least squares (GLS) have been developed. \"General linear models\" are also called \"multivariate linear models\". These are not the same as multivariable linear models (also called \"multiple linear models\"). Heteroscedastic models Various models have been created that allow for heteroscedasticity, i.e. the errors for different response variables may have different variances. For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors. Generalized linear models The Generalized linear model (GLM) is a framework for modeling response variables that are bounded or discrete."
            },
            {
                "text": "This is used, for example: when modeling positive quantities (e.g. prices or populations) that vary over a large scale—which are better described using a skewed distribution such as the log-normal distribution or Poisson distribution (although GLMs are not used for log-normal data, instead the response variable is simply transformed using the logarithm function); when modeling categorical data, such as the choice of a given candidate in an election (which is better described using a Bernoulli distribution/binomial distribution for binary choices, or a categorical distribution/multinomial distribution for multi-way choices), where there are a fixed number of choices that cannot be meaningfully ordered; when modeling ordinal data, e.g. ratings on a scale from 0 to 5, where the different outcomes can be ordered but where the quantity itself may not have any absolute meaning (e.g. a rating of 4 may not be \"twice as good\" in any objective sense as a rating of 2, but simply indicates that it is better than 2 or 3 but not as good as 5). Generalized linear models allow for an arbitrary link function, g, that relates the mean of the response variable(s) to the predictors: $E(Y) = g^{-1}(XB)$."
            },
            {
                "text": "The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the $(-\\infty,\\infty)$ range of the linear predictor and the range of the response variable. Some common examples of GLMs are: Poisson regression for count data. Logistic regression and probit regression for binary data. Multinomial logistic regression and multinomial probit regression for categorical data. Ordered logit and ordered probit regression for ordinal data. Single index models allow some degree of nonlinearity in the relationship between x and y, while preserving the central role of the linear predictor β′x as in the classical linear regression model. Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate β up to a proportionality constant. Hierarchical linear models Hierarchical linear models (or multilevel regression) organizes the data into a hierarchy of regressions, for example where A is regressed on B, and B is regressed on C. It is often used where the variables of interest have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district."
            },
            {
                "text": "The response variable might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels. Errors-in-variables Errors-in-variables models (or \"measurement error models\") extend the traditional linear regression model to allow the predictor variables X to be observed with error. This error causes standard estimators of β to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero. Group effects In a multiple linear regression model $ y= \\beta_{0} + \\beta_{1} x_{1} + \\cdots + \\beta_{p} x_{p} + \\varepsilon, $ parameter $\\beta_j$ of predictor variable $x_j$ represents the individual effect of $x_j$. It has an interpretation as the expected change in the response variable $y$ when $x_j$ increases by one unit with other predictor variables held constant. When $x_j$ is strongly correlated with other predictor variables, it is improbable that $x_j$ can increase by one unit with other variables held constant."
            },
            {
                "text": "In this case, the interpretation of $\\beta_j$ becomes problematic as it is based on an improbable condition, and the effect of $x_j$ cannot be evaluated in isolation. For a group of predictor variables, say, $\\{x_1, x_2, \\dots, x_q\\}$, a group effect $\\xi(\\mathbf{w})$ is defined as a linear combination of their parameters $ \\xi(\\mathbf{w}) = w_1\\beta_1+w_2\\beta_2+\\dots+w_q\\beta_q, $ where $\\mathbf{w}=(w_1,w_2,\\dots,w_q)^\\intercal$ is a weight vector satisfying . Because of the constraint on $ {w_j}$, $\\xi(\\mathbf{w})$ is also referred to as a normalized group effect. A group effect $\\xi(\\mathbf{w})$ has an interpretation as the expected change in $y$ when variables in the group $x_1, x_2,\\dots,x_q$ change by the amount $w_1, w_2, \\dots, w_q$, respectively, at the same time with other variables (not in the group) held constant."
            },
            {
                "text": "It generalizes the individual effect of a variable to a group of variables in that ($i$) if $q=1$, then the group effect reduces to an individual effect, and ($ii$) if $ w_i=1$ and $w_j=0$ for $j\\neq i$, then the group effect also reduces to an individual effect. A group effect $\\xi(\\mathbf{w})$ is said to be meaningful if the underlying simultaneous changes of the $q$ variables $(x_1,x_2,\\dots, x_q)^\\intercal$ is probable. Group effects provide a means to study the collective impact of strongly correlated predictor variables in linear regression models. Individual effects of such variables are not well-defined as their parameters do not have good interpretations. Furthermore, when the sample size is not large, none of their parameters can be accurately estimated by the least squares regression due to the multicollinearity problem. Nevertheless, there are meaningful group effects that have good interpretations and can be accurately estimated by the least squares regression. A simple way to identify these meaningful group effects is to use an all positive correlations (APC) arrangement of the strongly correlated variables under which pairwise correlations among these variables are all positive, and standardize all $p$ predictor variables in the model so that they all have mean zero and length one."
            },
            {
                "text": "To illustrate this, suppose that $\\{x_1, x_2, \\dots, x_q\\}$ is a group of strongly correlated variables in an APC arrangement and that they are not strongly correlated with predictor variables outside the group. Let $y'$ be the centred $y$ and $x_j'$ be the standardized $x_j$. Then, the standardized linear regression model is $ y'= \\beta_{1}' x_{1}' + \\cdots + \\beta_{p}' x_{p}' + \\varepsilon . $ Parameters $\\beta_j$ in the original model, including $\\beta_0$, are simple functions of $\\beta_j'$ in the standardized model. The standardization of variables does not change their correlations, so $\\{x_1', x_2', \\dots, x_q'\\}$ is a group of strongly correlated variables in an APC arrangement and they are not strongly correlated with other predictor variables in the standardized model. A group effect of $\\{x_1', x_2', \\dots, x_q'\\}$ is $ \\xi'(\\mathbf{w})=w_1\\beta_1'+w_2\\beta_2'+\\dots+w_q\\beta_q', $ and its minimum-variance unbiased linear estimator is $ \\hat{\\xi}'(\\mathbf{w})=w_1\\hat{\\beta}_1'+w_2\\hat{\\beta}_2'+\\dots+w_q\\hat{\\beta}_q', $ where $\\hat{\\beta}_j'$ is the least squares estimator of $\\beta_j'$."
            },
            {
                "text": "In particular, the average group effect of the $q$ standardized variables is $ \\xi_A=\\frac{1}{q}(\\beta_1'+\\beta_2'+\\dots+\\beta_q'), $ which has an interpretation as the expected change in $y'$ when all $x_j'$ in the strongly correlated group increase by $(1/q)$th of a unit at the same time with variables outside the group held constant. With strong positive correlations and in standardized units, variables in the group are approximately equal, so they are likely to increase at the same time and in similar amount. Thus, the average group effect $\\xi_A$ is a meaningful effect. It can be accurately estimated by its minimum-variance unbiased linear estimator , even when individually none of the $\\beta_j'$ can be accurately estimated by $\\hat{\\beta}_j'$. Not all group effects are meaningful or can be accurately estimated. For example, $\\beta_1'$ is a special group effect with weights $w_1=1$ and $w_j=0$ for $j\\neq 1$, but it cannot be accurately estimated by $\\hat{\\beta}'_1$."
            },
            {
                "text": "It is also not a meaningful effect. In general, for a group of $q$ strongly correlated predictor variables in an APC arrangement in the standardized model, group effects whose weight vectors $\\mathbf{w}$ are at or near the centre of the simplex ($w_j\\geq 0$) are meaningful and can be accurately estimated by their minimum-variance unbiased linear estimators. Effects with weight vectors far away from the centre are not meaningful as such weight vectors represent simultaneous changes of the variables that violate the strong positive correlations of the standardized variables in an APC arrangement. As such, they are not probable. These effects also cannot be accurately estimated. Applications of the group effects include (1) estimation and inference for meaningful group effects on the response variable, (2) testing for \"group significance\" of the $q$ variables via testing $H_0: \\xi_A=0$ versus $H_1: \\xi_A\\neq 0$, and (3) characterizing the region of the predictor variable space over which predictions by the least squares estimated model are accurate."
            },
            {
                "text": "A group effect of the original variables $\\{x_1, x_2, \\dots, x_q\\}$ can be expressed as a constant times a group effect of the standardized variables $\\{x_1', x_2',\\dots, x_q'\\}$. The former is meaningful when the latter is. Thus meaningful group effects of the original variables can be found through meaningful group effects of the standardized variables. Others In Dempster–Shafer theory, or a linear belief function in particular, a linear regression model may be represented as a partially swept matrix, which can be combined with similar matrices representing observations and other assumed normal distributions and state equations. The combination of swept or unswept matrices provides an alternative method for estimating linear regression models. Estimation methods A large number of procedures have been developed for parameter estimation and inference in linear regression. These methods differ in computational simplicity of algorithms, presence of a closed-form solution, robustness with respect to heavy-tailed distributions, and theoretical assumptions needed to validate desirable statistical properties such as consistency and asymptotic efficiency. Some of the more common estimation techniques for linear regression are summarized below."
            },
            {
                "text": "Least-squares estimation and related techniques Francis Galton's 1886 illustration of the correlation between the heights of adults and their parents. The observation that adult children's heights tended to deviate less from the mean height than their parents suggested the concept of \"regression toward the mean\", giving regression its name. The \"locus of horizontal tangential points\" passing through the leftmost and rightmost points on the ellipse (which is a level curve of the bivariate normal distribution estimated from the data) is the OLS estimate of the regression of parents' heights on children's heights, while the \"locus of vertical tangential points\" is the OLS estimate of the regression of children's heights on parent's heights. The major axis of the ellipse is the TLS estimate. Assuming that the independent variables are $\\vec{x_i} = \\left[x_1^i, x_2^i, \\ldots, x_m^i\\right]$ and the model's parameters are $\\vec{\\beta} = \\left[\\beta_0, \\beta_1, \\ldots, \\beta_m\\right]$, then the model's prediction would be $y_i \\approx \\beta_0 + \\sum_{j=1}^m \\beta_j\\times x_j^i$."
            },
            {
                "text": "If $\\vec{x_i}$ is extended to $\\vec{x_i} = \\left[1, x_1^i, x_2^i, \\ldots, x_m^i\\right]$ then $y_i$ would become a dot product of the parameter and the independent vectors, i.e. $y_i \\approx \\sum_{j=0}^ m \\beta_j \\times x_j^i = \\vec{\\beta} \\cdot \\vec{x_i}$."
            },
            {
                "text": "In the least-squares setting, the optimum parameter vector is defined as such that minimizes the sum of mean squared loss: $\\vec{\\hat{\\beta}} = \\underset{\\vec{\\beta}} \\mbox{arg min}\\,L\\left(D, \\vec{\\beta}\\right) = \\underset{\\vec{\\beta}}\\mbox{arg min} \\sum_{i=1}^{n} \\left(\\vec{\\beta} \\cdot \\vec{x_i} - y_i\\right)^2$ Now putting the independent and dependent variables in matrices $X$ and $Y$ respectively, the loss function can be rewritten as: $ \\begin{align} L\\left(D, \\vec{\\beta}\\right) &= \\|X\\vec{\\beta} - Y\\|^2 \\\\ &= \\left(X\\vec{\\beta} - Y\\right)^\\textsf{T} \\left(X\\vec{\\beta} - Y\\right) \\\\ &= Y^\\textsf{T}Y - Y^\\textsf{T}X\\vec{\\beta} - \\vec{\\beta}^\\textsf{T}X^\\textsf{T} Y + \\vec{\\beta}^\\textsf{T}X^\\textsf{T} X\\vec{\\beta} \\end{align} $ As the loss function is convex, the optimum solution lies at gradient zero."
            },
            {
                "text": "The gradient of the loss function is (using Denominator layout convention): $ \\begin{align} \\frac{\\partial L\\left(D, \\vec{\\beta}\\right)}{\\partial\\vec{\\beta}} &= \\frac{\\partial \\left(Y^\\textsf{T}Y - Y^\\textsf{T}X\\vec{\\beta} - \\vec{\\beta}^\\textsf{T}X^\\textsf{T}Y + \\vec{\\beta}^\\textsf{T}X^\\textsf{T}X\\vec{\\beta}\\right)}{\\partial \\vec{\\beta}} \\\\ &= -2X^\\textsf{T}Y + 2X^\\textsf{T}X\\vec{\\beta} \\end{align} $ Setting the gradient to zero produces the optimum parameter: $ \\begin{align} -2X^\\textsf{T}Y + 2X^\\textsf{T}X\\vec{\\beta} &= 0 \\\\ \\Rightarrow X^\\textsf{T}X\\vec{\\beta} &= X^\\textsf{T}Y \\\\ \\Rightarrow \\vec{\\hat{\\beta}} &= \\left(X^\\textsf{T}X\\right)^{-1}X^\\textsf{T}Y \\end{align} $ Note: The $\\hat{\\beta}$ obtained may indeed be the local minimum, one needs to differentiate once more to obtain the Hessian matrix and show that it is positive definite."
            },
            {
                "text": "This is provided by the Gauss–Markov theorem. Linear least squares methods include mainly: Ordinary least squares Weighted least squares Generalized least squares Linear Template Fit Maximum-likelihood estimation and related techniques Maximum likelihood estimation Maximum likelihood estimation can be performed when the distribution of the error terms is known to belong to a certain parametric family ƒθ of probability distributions. When fθ is a normal distribution with zero mean and variance θ, the resulting estimate is identical to the OLS estimate. GLS estimates are maximum likelihood estimates when ε follows a multivariate normal distribution with a known covariance matrix. Let's denote each data point by $ (\\vec{x_i}, y_i) $ and the regression parameters as $ \\vec{\\beta} $, and the set of all data by $ D $ and the cost function by $ L(D, \\vec{\\beta}) = \\sum_i(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i})^2 $. As shown below the same optimal parameter that minimizes $ L(D, \\vec{\\beta}) $ achieves maximum likelihood too."
            },
            {
                "text": "Here the assumption is that the dependent variable $ y $ is a random variable that follows a Gaussian distribution, where the standard deviation is fixed and the mean is a linear combination of $ \\vec{x} $:$ \\begin{align} H(D, \\vec{\\beta}) &= \\prod_{i=1}^{n}Pr(y_i|\\vec{x_i}\\,\\,;\\vec{\\beta}, \\sigma) \\\\ &= \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2}{2\\sigma^2}\\right) \\end{align} $ Now, we need to look for a parameter that maximizes this likelihood function. Since the logarithmic function is strictly increasing, instead of maximizing this function, we can also maximize its logarithm and find the optimal parameter that way."
            },
            {
                "text": "$ \\begin{align} I(D, \\vec{\\beta}) &= \\log\\prod_{i=1}^{n}Pr(y_i|\\vec{x_i}\\,\\,;\\vec{\\beta}, \\sigma) \\\\ &= \\log\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2}{2\\sigma^2}\\right) \\\\ &= n\\log\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2 \\end{align} $ The optimal parameter is thus equal to: $ \\begin{align} \\underset{\\vec{\\beta}}\\mbox{arg max}\\, I(D, \\vec{\\beta}) &= \\underset{\\vec{\\beta}}\\mbox{arg max} \\left(n\\log\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2\\right) \\\\ &= \\underset{\\vec{\\beta}}\\mbox{arg min} \\sum_{i=1}^n \\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2 \\\\ &= \\underset{\\vec{\\beta}}\\mbox{arg min} \\,L(D, \\vec{\\beta}) \\\\ &= \\vec{\\hat{\\beta}} \\end{align} $ In this way, the parameter that maximizes $ H(D, \\vec{\\beta}) $ is the same as the one that minimizes $ L(D, \\vec{\\beta}) $."
            },
            {
                "text": "This means that in linear regression, the result of the least squares method is the same as the result of the maximum likelihood estimation method.Machine learning: a probabilistic perspective , Kevin P Murphy, 2012, p. 217, Cambridge, MA Regularized Regression Ridge regression and other forms of penalized estimation, such as Lasso regression, deliberately introduce bias into the estimation of β in order to reduce the variability of the estimate. The resulting estimates generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias. Least Absolute Deviation Least absolute deviation (LAD) regression is a robust estimation technique in that it is less sensitive to the presence of outliers than OLS (but is less efficient than OLS when no outliers are present). It is equivalent to maximum likelihood estimation under a Laplace distribution model for ε. Adaptive Estimation If we assume that error terms are independent of the regressors, $\\varepsilon_i \\perp \\mathbf{x}_i$, then the optimal estimator is the 2-step MLE, where the first step is used to non-parametrically estimate the distribution of the error term."
            },
            {
                "text": "Other estimation techniques thumb|Comparison of the Theil–Sen estimator (black) and simple linear regression (blue) for a set of points with outliers Bayesian linear regression applies the framework of Bayesian statistics to linear regression. (See also Bayesian multivariate linear regression.) In particular, the regression coefficients β are assumed to be random variables with a specified prior distribution. The prior distribution can bias the solutions for the regression coefficients, in a way similar to (but more general than) ridge regression or lasso regression. In addition, the Bayesian estimation process produces not a single point estimate for the \"best\" values of the regression coefficients but an entire posterior distribution, completely describing the uncertainty surrounding the quantity. This can be used to estimate the \"best\" coefficients using the mean, mode, median, any quantile (see quantile regression), or any other function of the posterior distribution. Quantile regression focuses on the conditional quantiles of y given X rather than the conditional mean of y given X. Linear quantile regression models a particular conditional quantile, for example the conditional median, as a linear function βTx of the predictors."
            },
            {
                "text": "Mixed models are widely used to analyze linear regression relationships involving dependent data when the dependencies have a known structure. Common applications of mixed models include analysis of data involving repeated measurements, such as longitudinal data, or data obtained from cluster sampling. They are generally fit as parametric models, using maximum likelihood or Bayesian estimation. In the case where the errors are modeled as normal random variables, there is a close connection between mixed models and generalized least squares. Fixed effects estimation is an alternative approach to analyzing this type of data. Principal component regression (PCR) is used when the number of predictor variables is large, or when strong correlations exist among the predictor variables. This two-stage procedure first reduces the predictor variables using principal component analysis, and then uses the reduced variables in an OLS regression fit. While it often works well in practice, there is no general theoretical reason that the most informative linear function of the predictor variables should lie among the dominant principal components of the multivariate distribution of the predictor variables. The partial least squares regression is the extension of the PCR method which does not suffer from the mentioned deficiency."
            },
            {
                "text": "Least-angle regression is an estimation procedure for linear regression models that was developed to handle high-dimensional covariate vectors, potentially with more covariates than observations. The Theil–Sen estimator is a simple robust estimation technique that chooses the slope of the fit line to be the median of the slopes of the lines through pairs of sample points. It has similar statistical efficiency properties to simple linear regression but is much less sensitive to outliers. ; . Other robust estimation techniques, including the α-trimmed mean approach, and L-, M-, S-, and R-estimators have been introduced. Applications Linear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. It ranks as one of the most important tools used in these disciplines. Trend line A trend line represents a trend, the long-term movement in time series data after other components have been accounted for. It tells whether a particular data set (say GDP, oil prices or stock prices) have increased or decreased over the period of time. A trend line could simply be drawn by eye through a set of data points, but more properly their position and slope is calculated using statistical techniques like linear regression."
            },
            {
                "text": "Trend lines typically are straight lines, although some variations use higher degree polynomials depending on the degree of curvature desired in the line. Trend lines are sometimes used in business analytics to show changes in data over time. This has the advantage of being simple. Trend lines are often used to argue that a particular action or event (such as training, or an advertising campaign) caused observed changes at a point in time. This is a simple technique, and does not require a control group, experimental design, or a sophisticated analysis technique. However, it suffers from a lack of scientific validity in cases where other potential changes can affect the data. Epidemiology Early evidence relating tobacco smoking to mortality and morbidity came from observational studies employing regression analysis. In order to reduce spurious correlations when analyzing observational data, researchers usually include several variables in their regression models in addition to the variable of primary interest. For example, in a regression model in which cigarette smoking is the independent variable of primary interest and the dependent variable is lifespan measured in years, researchers might include education and income as additional independent variables, to ensure that any observed effect of smoking on lifespan is not due to those other socio-economic factors."
            },
            {
                "text": "However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data. Finance The capital asset pricing model uses linear regression as well as the concept of beta for analyzing and quantifying the systematic risk of an investment. This comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets. Economics Linear regression is the predominant empirical tool in economics. For example, it is used to predict consumption spending, fixed investment spending, inventory investment, purchases of a country's exports, spending on imports, the demand to hold liquid assets, labor demand, and labor supply."
            },
            {
                "text": "Environmental science Linear regression finds application in a wide range of environmental science applications such as land use, infectious diseases, and air pollution. For example, linear regression can be used to predict the changing effects of car pollution. One notable example of this application in infectious diseases is the flattening the curve strategy emphasized early in the COVID-19 pandemic, where public health officials dealt with sparse data on infected individuals and sophisticated models of disease transmission to characterize the spread of COVID-19. Building science Linear regression is commonly used in building science field studies to derive characteristics of building occupants. In a thermal comfort field study, building scientists usually ask occupants' thermal sensation votes, which range from -3 (feeling cold) to 0 (neutral) to +3 (feeling hot), and measure occupants' surrounding temperature data. A neutral or comfort temperature can be calculated based on a linear regression between the thermal sensation vote and indoor temperature, and setting the thermal sensation vote as zero. However, there has been a debate on the regression direction: regressing thermal sensation votes (y-axis) against indoor temperature (x-axis) or the opposite: regressing indoor temperature (y-axis) against thermal sensation votes (x-axis)."
            },
            {
                "text": "Machine learning Linear regression plays an important role in the subfield of artificial intelligence known as machine learning. The linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and well-known properties. History Isaac Newton is credited with inventing \"a certain technique known today as linear regression analysis\" in his work on equinoxes in 1700, and wrote down the first of the two normal equations of the ordinary least squares method. The Least squares linear regression, as a means of finding a good rough linear fit to a set of points was performed by Legendre (1805) and Gauss (1809) for the prediction of planetary movement. Quetelet was responsible for making the procedure well-known and for using it extensively in the social sciences. See also Analysis of variance Blinder–Oaxaca decomposition Censored regression model Cross-sectional regression Curve fitting Empirical Bayes method Errors and residuals Lack-of-fit sum of squares Line fitting Linear classifier Linear equation Logistic regression M-estimator Multivariate adaptive regression spline Nonlinear regression Nonparametric regression Normal equations Projection pursuit regression Response modeling methodology Segmented linear regression Standard deviation line Stepwise regression Structural break Support vector machine Truncated regression model Deming regression References Citations Sources Cohen, J., Cohen P., West, S. G., & Aiken, L. S. (2003)."
            },
            {
                "text": "Applied multiple regression/correlation analysis for the behavioral sciences . (2nd ed.) Hillsdale, New Jersey: Lawrence Erlbaum Associates Charles Darwin. The Variation of Animals and Plants under Domestication. (1868) (Chapter XIII describes what was known about reversion in Galton's time. Darwin uses the term \"reversion\".) Francis Galton. \"Regression Towards Mediocrity in Hereditary Stature,\" Journal of the Anthropological Institute, 15:246–263 (1886). (Facsimile at: ) Robert S. Pindyck and Daniel L. Rubinfeld (1998, 4th ed.). Econometric Models and Economic Forecasts'', ch. 1 (Intro, including appendices on Σ operators & derivation of parameter est.) & Appendix 4.3 (mult. regression in matrix form). Further reading Mathieu Rouaud, 2013: Probability, Statistics and Estimation Chapter 2: Linear Regression, Linear Regression with Error Bars and Nonlinear Regression. External links Least-Squares Regression, PhET Interactive simulations, University of Colorado at Boulder DIY Linear Fit Category:Curve fitting Category:Estimation theory Category:Parametric statistics Category:Single-equation methods (econometrics)"
            }
        ],
        "latex_formulas": [
            "\\{y_i,\\, x_{i1}, \\ldots, x_{ip}\\}_{i=1}^n",
            "\\mathbf{y} = \\mathbf{X} \\boldsymbol\\beta + \\boldsymbol\\varepsilon, \\,",
            "\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\quad",
            "\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}^\\mathsf{T}_1 \\\\ \\mathbf{x}^\\mathsf{T}_2 \\\\ \\vdots \\\\ \\mathbf{x}^\\mathsf{T}_n \\end{bmatrix}\n = \\begin{bmatrix} 1 &  x_{11} & \\cdots & x_{1p} \\\\\n 1 & x_{21} & \\cdots & x_{2p} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n 1 & x_{n1} & \\cdots & x_{np}\n \\end{bmatrix},",
            "\\boldsymbol\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}, \\quad\n \\boldsymbol\\varepsilon = \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix}.",
            "\\mathbf{y}",
            "y_i\\ (i=1,\\ldots,n)",
            "\\hat{y}",
            "\\mathbf{X}",
            "\\mathbf{x}_{i\\cdot}",
            "\\mathbf{x}_{\\cdot j}",
            "\\mathbf{X}",
            "x_{i0}=1",
            "i=1,\\ldots,n",
            "\\boldsymbol\\beta",
            "(p+1)",
            "\\beta_0",
            "\\boldsymbol\\beta",
            "\\boldsymbol\\varepsilon",
            "\\varepsilon_i",
            "\\boldsymbol\\beta",
            "\\boldsymbol\\varepsilon=\\mathbf{y}- \\mathbf{X}\\boldsymbol\\beta",
            "\\|\\boldsymbol\\varepsilon\\|_2^2",
            "\\boldsymbol\\varepsilon",
            "h_i = \\beta_1 t_i + \\beta_2 t_i^2 + \\varepsilon_i,",
            "h_i = \\mathbf{x}^\\mathsf{T}_i\\boldsymbol\\beta + \\varepsilon_i.",
            "Y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ip} + \\epsilon_i",
            "Y_{ij} = \\beta_{0j} + \\beta_{1j} X_{i1} + \\beta_{2j}X_{i2} + \\ldots + \\beta_{pj} X_{ip} + \\epsilon_{ij}",
            "E(\\mathbf{y}\\mid\\mathbf{x}_i)=\\mathbf{x}_i^\\mathsf{T}B",
            "E(Y) = g^{-1}(XB)",
            "(-\\infty,\\infty)",
            "y= \\beta_{0} + \\beta_{1} x_{1} + \\cdots + \\beta_{p} x_{p} + \\varepsilon,",
            "\\beta_j",
            "x_j",
            "x_j",
            "y",
            "x_j",
            "x_j",
            "x_j",
            "\\beta_j",
            "x_j",
            "\\{x_1, x_2, \\dots, x_q\\}",
            "\\xi(\\mathbf{w})",
            "\\xi(\\mathbf{w}) = w_1\\beta_1+w_2\\beta_2+\\dots+w_q\\beta_q,",
            "\\mathbf{w}=(w_1,w_2,\\dots,w_q)^\\intercal",
            "{w_j}",
            "\\xi(\\mathbf{w})",
            "\\xi(\\mathbf{w})",
            "y",
            "x_1, x_2,\\dots,x_q",
            "w_1, w_2, \\dots, w_q",
            "i",
            "q=1",
            "ii",
            "w_i=1",
            "w_j=0",
            "j\\neq i",
            "\\xi(\\mathbf{w})",
            "q",
            "(x_1,x_2,\\dots, x_q)^\\intercal",
            "p",
            "\\{x_1, x_2, \\dots, x_q\\}",
            "y'",
            "y",
            "x_j'",
            "x_j",
            "y'= \\beta_{1}' x_{1}' + \\cdots + \\beta_{p}' x_{p}' + \\varepsilon .",
            "\\beta_j",
            "\\beta_0",
            "\\beta_j'",
            "\\{x_1', x_2', \\dots, x_q'\\}",
            "\\{x_1', x_2', \\dots, x_q'\\}",
            "\\xi'(\\mathbf{w})=w_1\\beta_1'+w_2\\beta_2'+\\dots+w_q\\beta_q',",
            "\\hat{\\xi}'(\\mathbf{w})=w_1\\hat{\\beta}_1'+w_2\\hat{\\beta}_2'+\\dots+w_q\\hat{\\beta}_q',",
            "\\hat{\\beta}_j'",
            "\\beta_j'",
            "q",
            "\\xi_A=\\frac{1}{q}(\\beta_1'+\\beta_2'+\\dots+\\beta_q'),",
            "y'",
            "x_j'",
            "(1/q)",
            "\\xi_A",
            "\\beta_j'",
            "\\hat{\\beta}_j'",
            "\\beta_1'",
            "w_1=1",
            "w_j=0",
            "j\\neq 1",
            "\\hat{\\beta}'_1",
            "q",
            "\\mathbf{w}",
            "w_j\\geq 0",
            "q",
            "H_0: \\xi_A=0",
            "H_1: \\xi_A\\neq 0",
            "\\{x_1, x_2, \\dots, x_q\\}",
            "\\{x_1', x_2',\\dots, x_q'\\}",
            "\\vec{x_i} = \\left[x_1^i, x_2^i, \\ldots, x_m^i\\right]",
            "\\vec{\\beta} = \\left[\\beta_0, \\beta_1, \\ldots, \\beta_m\\right]",
            "y_i \\approx \\beta_0 + \\sum_{j=1}^m \\beta_j\\times x_j^i",
            "\\vec{x_i}",
            "\\vec{x_i} = \\left[1, x_1^i, x_2^i, \\ldots, x_m^i\\right]",
            "y_i",
            "y_i \\approx \\sum_{j=0}^ m \\beta_j \\times x_j^i = \\vec{\\beta} \\cdot \\vec{x_i}",
            "\\vec{\\hat{\\beta}} = \\underset{\\vec{\\beta}} \\mbox{arg min}\\,L\\left(D, \\vec{\\beta}\\right) = \\underset{\\vec{\\beta}}\\mbox{arg min} \\sum_{i=1}^{n} \\left(\\vec{\\beta} \\cdot \\vec{x_i} - y_i\\right)^2",
            "X",
            "Y",
            "\\begin{align}\n  L\\left(D, \\vec{\\beta}\\right)\n    &= \\|X\\vec{\\beta} - Y\\|^2 \\\\\n    &= \\left(X\\vec{\\beta} - Y\\right)^\\textsf{T} \\left(X\\vec{\\beta} - Y\\right) \\\\\n    &= Y^\\textsf{T}Y - Y^\\textsf{T}X\\vec{\\beta} - \\vec{\\beta}^\\textsf{T}X^\\textsf{T} Y + \\vec{\\beta}^\\textsf{T}X^\\textsf{T} X\\vec{\\beta}\n\\end{align}",
            "\\begin{align}\n  \\frac{\\partial L\\left(D, \\vec{\\beta}\\right)}{\\partial\\vec{\\beta}}\n    &= \\frac{\\partial \\left(Y^\\textsf{T}Y - Y^\\textsf{T}X\\vec{\\beta} - \\vec{\\beta}^\\textsf{T}X^\\textsf{T}Y + \\vec{\\beta}^\\textsf{T}X^\\textsf{T}X\\vec{\\beta}\\right)}{\\partial \\vec{\\beta}} \\\\\n    &= -2X^\\textsf{T}Y + 2X^\\textsf{T}X\\vec{\\beta}\n\\end{align}",
            "\\begin{align}\n  -2X^\\textsf{T}Y + 2X^\\textsf{T}X\\vec{\\beta} &= 0 \\\\\n  \\Rightarrow X^\\textsf{T}X\\vec{\\beta} &= X^\\textsf{T}Y \\\\\n  \\Rightarrow \\vec{\\hat{\\beta}} &= \\left(X^\\textsf{T}X\\right)^{-1}X^\\textsf{T}Y\n\\end{align}",
            "\\hat{\\beta}",
            "(\\vec{x_i}, y_i)",
            "\\vec{\\beta}",
            "D",
            "L(D, \\vec{\\beta}) = \\sum_i(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i})^2",
            "L(D, \\vec{\\beta})",
            "y",
            "\\vec{x}",
            "\\begin{align}\nH(D, \\vec{\\beta}) &= \\prod_{i=1}^{n}Pr(y_i|\\vec{x_i}\\,\\,;\\vec{\\beta}, \\sigma) \\\\ &= \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2}{2\\sigma^2}\\right)\n\\end{align}",
            "\\begin{align}\nI(D, \\vec{\\beta}) &= \\log\\prod_{i=1}^{n}Pr(y_i|\\vec{x_i}\\,\\,;\\vec{\\beta}, \\sigma) \\\\ &= \\log\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2}{2\\sigma^2}\\right) \\\\ &= n\\log\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2\n\\end{align}",
            "\\begin{align}\n\\underset{\\vec{\\beta}}\\mbox{arg max}\\, I(D, \\vec{\\beta}) &= \\underset{\\vec{\\beta}}\\mbox{arg max} \\left(n\\log\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2\\right) \\\\ &= \\underset{\\vec{\\beta}}\\mbox{arg min} \\sum_{i=1}^n \\left(y_i - \\vec{\\beta} \\, \\cdot \\, \\vec{x_i} \\right)^2 \\\\ &= \\underset{\\vec{\\beta}}\\mbox{arg min} \\,L(D, \\vec{\\beta}) \\\\ &= \\vec{\\hat{\\beta}}\n\\end{align}",
            "H(D, \\vec{\\beta})",
            "L(D, \\vec{\\beta})",
            "\\varepsilon_i \\perp \\mathbf{x}_i"
        ]
    },
    "Perceptron": {
        "title": "Perceptron",
        "chunks": [
            {
                "text": "In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function that can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. History alt= thumb|The Mark 1 Perceptron, being adjusted by Charles Wightman (Mark I Perceptron project engineer). Sensory units at left, association units in center, and control panel and response units at far right. The sensory-to-association plugboard is behind the closed panel to the right of the operator. The letter \"C\" on the front panel is a display of the current state of the sensory input. The artificial neuron network was invented in 1943 by Warren McCulloch and Walter Pitts in A logical calculus of the ideas immanent in nervous activity. In 1957, Frank Rosenblatt was at the Cornell Aeronautical Laboratory. He simulated the perceptron on an IBM 704."
            },
            {
                "text": "Later, he obtained funding by the Information Systems Branch of the United States Office of Naval Research and the Rome Air Development Center, to build a custom-made computer, the Mark I Perceptron. It was first publicly demonstrated on 23 June 1960. The machine was \"part of a previously secret four-year NPIC [the US' National Photographic Interpretation Center] effort from 1963 through 1966 to develop this algorithm into a useful tool for photo-interpreters\". Rosenblatt described the details of the perceptron in a 1958 paper. His organization of a perceptron is constructed of three kinds of cells (\"units\"): AI, AII, R, which stand for \"projection\", \"association\" and \"response\". He presented at the first international symposium on AI, Mechanisation of Thought Processes, which took place in 1958 November.Frank Rosenblatt, ‘Two Theorems of Statistical Separability in the Perceptron’, Symposium on the Mechanization of Thought, National Physical Laboratory, Teddington, UK, November 1958, vol. 1, H. M. Stationery Office, London, 1959. Rosenblatt's project was funded under Contract Nonr-401(40) \"Cognitive Systems Research Program\", which lasted from 1959 to 1970,Rosenblatt, Frank, and CORNELL UNIV ITHACA NY."
            },
            {
                "text": "Cognitive Systems Research Program. Technical report, Cornell University, 72, 1971. and Contract Nonr-2381(00) \"Project PARA\" (\"PARA\" means \"Perceiving and Recognition Automata\"), which lasted from 1957 to 1963.Muerle, John Ludwig, and CORNELL AERONAUTICAL LAB INC BUFFALO NY. Project Para, Perceiving and Recognition Automata. Cornell Aeronautical Laboratory, Incorporated, 1963. In 1959, the Institute for Defense Analysis awarded his group a $10,000 contract. By September 1961, the ONR awarded further $153,000 worth of contracts, with $108,000 committed for 1962. The ONR research manager, Marvin Denicoff, stated that ONR, instead of ARPA, funded the Perceptron project, because the project was unlikely to produce technological results in the near or medium term. Funding from ARPA go up to the order of millions dollars, while from ONR are on the order of 10,000 dollars. Meanwhile, the head of IPTO at ARPA, J.C.R. Licklider, was interested in 'self-organizing', 'adaptive' and other biologically-inspired methods in the 1950s; but by the mid-1960s he was openly critical of these, including the perceptron."
            },
            {
                "text": "Instead he strongly favored the logical AI approach of Simon and Newell. Mark I Perceptron machine Organization of a biological brain and a perceptron. The perceptron was intended to be a machine, rather than a program, and while its first implementation was in software for the IBM 704, it was subsequently implemented in custom-built hardware as the Mark I Perceptron with the project name \"Project PARA\", designed for image recognition. The machine is currently in Smithsonian National Museum of American History. The Mark I Perceptron had 3 layers. One version was implemented as follows: An array of 400 photocells arranged in a 20x20 grid, named \"sensory units\" (S-units), or \"input retina\". Each S-unit can connect to up to 40 A-units. A hidden layer of 512 perceptrons, named \"association units\" (A-units). An output layer of 8 perceptrons, named \"response units\" (R-units). Rosenblatt called this three-layered perceptron network the alpha-perceptron, to distinguish it from other perceptron models he experimented with. The S-units are connected to the A-units randomly (according to a table of random numbers) via a plugboard (see photo), to \"eliminate any particular intentional bias in the perceptron\"."
            },
            {
                "text": "The connection weights are fixed, not learned. Rosenblatt was adamant about the random connections, as he believed the retina was randomly connected to the visual cortex, and he wanted his perceptron machine to resemble human visual perception. The A-units are connected to the R-units, with adjustable weights encoded in potentiometers, and weight updates during learning were performed by electric motors.The hardware details are in an operators' manual. thumb|Components of the Mark I Perceptron. From the operator's manual. In a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling AI community; based on Rosenblatt's statements, The New York Times reported the perceptron to be \"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\" The Photo Division of Central Intelligence Agency, from 1960 to 1964, studied the use of Mark I Perceptron machine for recognizing militarily interesting silhouetted targets (such as planes and ships) in aerial photos."
            },
            {
                "text": "Principles of Neurodynamics (1962) Rosenblatt described his experiments with many variants of the Perceptron machine in a book Principles of Neurodynamics (1962). The book is a published version of the 1961 report.Principles of neurodynamics: Perceptrons and the theory of brain mechanisms, by Frank Rosenblatt, Report Number VG-1196-G-8, Cornell Aeronautical Laboratory, published on 15 March 1961. The work reported in this volume has been carried out under Contract Nonr-2381 (00) (Project PARA) at C.A.L. and Contract Nonr-401(40), at Cornell Univensity. Among the variants are: \"cross-coupling\" (connections between units within the same layer) with possibly closed loops, \"back-coupling\" (connections from units in a later layer to units in a previous layer), four-layer perceptrons where the last two layers have adjustible weights (and thus a proper multilayer perceptron), incorporating time-delays to perceptron units, to allow for processing sequential data, analyzing audio (instead of images). The machine was shipped from Cornell to Smithsonian in 1967, under a government transfer administered by the Office of Naval Research."
            },
            {
                "text": "Perceptrons (1969) Although the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised that a feedforward neural network with two or more layers (also called a multilayer perceptron) had greater processing power than perceptrons with one layer (also called a single-layer perceptron). Single-layer perceptrons are only capable of learning linearly separable patterns. For a classification task with some step activation function, a single node will have a single line dividing the data points forming the patterns. More nodes can create more dividing lines, but those lines must somehow be combined to form more complex classifications. A second layer of perceptrons, or even linear nodes, are sufficient to solve many otherwise non-separable problems. In 1969, a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function. It is often incorrectly believed that they also conjectured that a similar result would hold for a multi-layer perceptron network."
            },
            {
                "text": "However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on Perceptrons (book) for more information.) Nevertheless, the often-miscited Minsky and Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until neural network research experienced a resurgence in the 1980s. This text was reprinted in 1987 as \"Perceptrons - Expanded Edition\" where some errors in the original text are shown and corrected. Subsequent work Rosenblatt continued working on perceptrons despite diminishing funding. The last attempt was Tobermory, built between 1961 and 1967, built for speech recognition.Rosenblatt, Frank (1962). “A Description of the Tobermory Perceptron.” Cognitive Research Program. Report No. 4. Collected Technical Papers, Vol. 2. Edited by Frank Rosenblatt. Ithaca, NY: Cornell University. It occupied an entire room.Nagy, George. 1963. System and circuit designs for the Tobermory perceptron. Technical report number 5, Cognitive Systems Research Program, Cornell University, Ithaca New York."
            },
            {
                "text": "It had 4 layers with 12,000 weights implemented by toroidal magnetic cores. By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines.Nagy, George. \"Neural networks-then and now.\" IEEE Transactions on Neural Networks 2.2 (1991): 316-318. He died in a boating accident in 1971. thumb|Isometric view of Tobermory Phase I. The kernel perceptron algorithm was already introduced in 1964 by Aizerman et al. Margin bounds guarantees were given for the Perceptron algorithm in the general non-separable case first by Freund and Schapire (1998), and more recently by Mohri and Rostamizadeh (2013) who extend previous results and give new and more favorable L1 bounds. Foundations of Machine Learning, MIT Press (Chapter 8). The perceptron is a simplified model of a biological neuron. While the complexity of biological neuron models is often required to fully understand neural behavior, research suggests a perceptron-like linear model can produce some behavior seen in real neurons. The solution spaces of decision boundaries for all binary functions and learning behaviors are studied in."
            },
            {
                "text": "Definition right|The appropriate weights are applied to the inputs, and the resulting weighted sum passed to a function that produces the output o.In the modern sense, the perceptron is an algorithm for learning a binary classifier called a threshold function: a function that maps its input $\\mathbf{x}$ (a real-valued vector) to an output value $f(\\mathbf{x})$ (a single binary value): where $h$ is the Heaviside step-function (where an input of outputs 1; otherwise 0 is the output ), $\\mathbf{w}$ is a vector of real-valued weights, $\\mathbf{w} \\cdot \\mathbf{x}$ is the dot product , where is the number of inputs to the perceptron, and is the bias."
            },
            {
                "text": "The bias shifts the decision boundary away from the origin and does not depend on any input value. Equivalently, since $\\mathbf{w}\\cdot \\mathbf{x} + b = (\\mathbf{w}, b) \\cdot (\\mathbf{x}, 1)$, we can add the bias term $b$ as another weight $\\mathbf{w}_{m+1}$ and add a coordinate $1$ to each input $\\mathbf{x}$, and then write it as a linear classifier that passes the origin: The binary value of $f(\\mathbf{x})$ (0 or 1) is used to perform binary classification on $\\mathbf{x}$ as either a positive or a negative instance. Spatially, the bias shifts the position (though not the orientation) of the planar decision boundary. In the context of neural networks, a perceptron is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network."
            },
            {
                "text": "As a linear classifier, the single-layer perceptron is the simplest feedforward neural network. Power of representation Information theory From an information theory point of view, a single perceptron with K inputs has a capacity of 2K bits of information. This result is due to Thomas Cover. Specifically let $T(N, K)$ be the number of ways to linearly separate N points in K dimensions, thenWhen K is large, $T(N, K)/2^N$ is very close to one when $N \\leq 2K$, but very close to zero when $N> 2K$. In words, one perceptron unit can almost certainly memorize a random assignment of binary labels on N points when $N \\leq 2K$, but almost certainly not when $N> 2K$. Boolean function When operating on only binary inputs, a perceptron is called a linearly separable Boolean function, or threshold Boolean function. The sequence of numbers of threshold Boolean functions on n inputs is OEIS A000609. The value is only known exactly up to $n=9$ case, but the order of magnitude is known quite exactly: it has upper bound $2^{n^2 - n \\log_2 n + O(n)}$ and lower bound $2^{n^2 - n \\log_2 n - O(n)}$."
            },
            {
                "text": "Any Boolean linear threshold function can be implemented with only integer weights. Furthermore, the number of bits necessary and sufficient for representing a single integer weight parameter is $\\Theta(n \\ln n)$. Universal approximation theorem A single perceptron can learn to classify any half-space. It cannot solve any linearly nonseparable vectors, such as the Boolean exclusive-or problem (the famous \"XOR problem\"). A perceptron network with one hidden layer can learn to classify any compact subset arbitrarily closely. Similarly, it can also approximate any compactly-supported continuous function arbitrarily closely. This is essentially a special case of the theorems by George Cybenko and Kurt Hornik. Conjunctively local perceptron Perceptrons (Minsky and Papert, 1969) studied the kind of perceptron networks necessary to learn various Boolean functions. Consider a perceptron network with $n$ input units, one hidden layer, and one output, similar to the Mark I Perceptron machine. It computes a Boolean function of type $f: 2^n \\to 2 $. They call a function conjunctively local of order $k$, iff there exists a perceptron network such that each unit in the hidden layer connects to at most $k$ input units."
            },
            {
                "text": "Theorem. (Theorem 3.1.1): The parity function is conjunctively local of order $n$. Theorem. (Section 5.5): The connectedness function is conjunctively local of order $\\Omega(n^{1/2})$. Learning algorithm for a single-layer perceptron right|A diagram showing a perceptron updating its linear boundary as more training examples are added Below is an example of a learning algorithm for a single-layer perceptron with a single output unit. For a single-layer perceptron with multiple output units, since the weights of one output unit are completely separate from all the others', the same algorithm can be run for each output unit. For multilayer perceptrons, where a hidden layer exists, more sophisticated algorithms such as backpropagation must be used. If the activation function or the underlying process being modeled by the perceptron is nonlinear, alternative learning algorithms such as the delta rule can be used as long as the activation function is differentiable. Nonetheless, the learning algorithm described in the steps below will often work, even for multilayer perceptrons with nonlinear activation functions. When multiple perceptrons are combined in an artificial neural network, each output neuron operates independently of all the others; thus, learning each output can be considered in isolation."
            },
            {
                "text": "Definitions We first define some variables: $r$ is the learning rate of the perceptron. Learning rate is a positive number usually chosen to be less than 1. The larger the value, the greater the chance for volatility in the weight changes. $y = f(\\mathbf{z}) $ denotes the output from the perceptron for an input vector $\\mathbf{z}$. $D = \\{(\\mathbf{x}_1,d_1),\\dots,(\\mathbf{x}_s,d_s)\\} $ is the training set of $s$ samples, where: $\\mathbf{x}_j$ is the $n$-dimensional input vector. $d_j $ is the desired output value of the perceptron for that input. We show the values of the features as follows: $x_{j,i} $ is the value of the $i$th feature of the $j$th training input vector. $x_{j,0} = 1 $. To represent the weights: $w_i $ is the $i$th value in the weight vector, to be multiplied by the value of the $i$th input feature."
            },
            {
                "text": "Because $x_{j,0} = 1 $, the $w_0 $ is effectively a bias that we use instead of the bias constant $b$. To show the time-dependence of $\\mathbf{w}$, we use: $w_i(t) $ is the weight $i$ at time $t$. Steps The algorithm updates the weights after every training sample in step 2b. Convergence of one perceptron on a linearly separable dataset Illustration of the perceptron convergence. In the picture, $\\gamma = 0.01, R = 1, r = 1 $. All data points have $y = +1$, since the negative samples are equivalent to $y = +1$ after reflection through the origin. As the learning proceeds, the weight vector performs a somewhat random walk in the space of weights. Each step is at least 90 degrees away from its current direction, thus increasing its norm-square by at most $R$. Each step adds to $w$ by a point in the samples, and since all the samples have $x_1 \\geq 0.01$, the weight vector must move along $x_1$ by at least $0.01$."
            },
            {
                "text": "Since the norm grows like $\\sqrt t$ but the $x_1$-component grows like $t$, this would eventually force the weight vector to point almost entirely in the $x_1$ direction, and thus achieve convergence. A single perceptron is a linear classifier. It can only reach a stable state if all input vectors are classified correctly. In case the training set is not linearly separable, i.e. if the positive examples cannot be separated from the negative examples by a hyperplane, then the algorithm would not converge since there is no solution. Hence, if linear separability of the training set is not known a priori, one of the training variants below should be used. Detailed analysis and extensions to the convergence theorem are in Chapter 11 of Perceptrons (1969). Linear separability is testable in time $\\min(O(n^{d/2}), O(d^{2n}), O(n^{d-1} \\ln n)) $, where $n$ is the number of data points, and $d$ is the dimension of each point."
            },
            {
                "text": "If the training set is linearly separable, then the perceptron is guaranteed to converge after making finitely many mistakes. The theorem is proved by Rosenblatt et al. The following simple proof is due to Novikoff (1962). The idea of the proof is that the weight vector is always adjusted by a bounded amount in a direction with which it has a negative dot product, and thus can be bounded above by $O(, where is the number of changes to the weight vector. However, it can also be bounded below by $O(t)$ because if there exists an (unknown) satisfactory weight vector, then every change makes progress in this (unknown) direction by a positive amount that depends only on the input vector. Two classes of points, and two of the infinitely many linear boundaries that separate them. Even though the boundaries are at nearly right angles to one another, the perceptron algorithm has no way of choosing between them. While the perceptron algorithm is guaranteed to converge on some solution in the case of a linearly separable training set, it may still pick any solution and problems may admit many solutions of varying quality."
            },
            {
                "text": "The perceptron of optimal stability, nowadays better known as the linear support-vector machine, was designed to solve this problem (Krauth and Mezard, 1987). Perceptron cycling theorem When the dataset is not linearly separable, then there is no way for a single perceptron to converge. However, we still have This is proved first by Bradley Efron.Efron, Bradley. \"The perceptron correction procedure in nonseparable situations.\" Rome Air Dev. Center Tech. Doc. Rept (1964). Learning a Boolean function Consider a dataset where the $x$ are from $\\{-1, +1\\}^n$, that is, the vertices of an n-dimensional hypercube centered at origin, and $y = \\theta(x_i)$. That is, all data points with positive $x_i$ have $y=1$, and vice versa. By the perceptron convergence theorem, a perceptron would converge after making at most $n$ mistakes. If we were to write a logical program to perform the same task, each positive example shows that one of the coordinates is the right one, and each negative example shows that its complement is a positive example."
            },
            {
                "text": "By collecting all the known positive examples, we eventually eliminate all but one coordinate, at which point the dataset is learned. This bound is asymptotically tight in terms of the worst-case. In the worst-case, the first presented example is entirely new, and gives $n$ bits of information, but each subsequent example would differ minimally from previous examples, and gives 1 bit each. After $n+1$ examples, there are $2n$ bits of information, which is sufficient for the perceptron (with $2n$ bits of information). However, it is not tight in terms of expectation if the examples are presented uniformly at random, since the first would give $n$ bits, the second $n/2$ bits, and so on, taking $O(\\ln n)$ examples in total. Variants The pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far \"in its pocket\". The pocket algorithm then returns the solution in the pocket, rather than the last solution."
            },
            {
                "text": "It can be used also for non-separable data sets, where the aim is to find a perceptron with a small number of misclassifications. However, these solutions appear purely stochastically and hence the pocket algorithm neither approaches them gradually in the course of learning, nor are they guaranteed to show up within a given number of learning steps. The Maxover algorithm (Wendemuth, 1995) is \"robust\" in the sense that it will converge regardless of (prior) knowledge of linear separability of the data set. In the linearly separable case, it will solve the training problem – if desired, even with optimal stability (maximum margin between the classes). For non-separable data sets, it will return a solution with a computable small number of misclassifications. In all cases, the algorithm gradually approaches the solution in the course of learning, without memorizing previous states and without stochastic jumps. Convergence is to global optimality for separable data sets and to local optimality for non-separable data sets. The Voted Perceptron (Freund and Schapire, 1999), is a variant using multiple weighted perceptrons."
            },
            {
                "text": "The algorithm starts a new perceptron every time an example is wrongly classified, initializing the weights vector with the final weights of the last perceptron. Each perceptron will also be given another weight corresponding to how many examples do they correctly classify before wrongly classifying one, and at the end the output will be a weighted vote on all perceptrons. In separable problems, perceptron training can also aim at finding the largest separating margin between the classes. The so-called perceptron of optimal stability can be determined by means of iterative training and optimization schemes, such as the Min-Over algorithm (Krauth and Mezard, 1987) or the AdaTron (Anlauf and Biehl, 1989)). AdaTron uses the fact that the corresponding quadratic optimization problem is convex. The perceptron of optimal stability, together with the kernel trick, are the conceptual foundations of the support-vector machine. The $\\alpha$-perceptron further used a pre-processing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify analogue patterns, by projecting them into a binary space. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable."
            },
            {
                "text": "Another way to solve nonlinear problems without using multiple layers is to use higher order networks (sigma-pi unit). In this type of network, each element in the input vector is extended with each pairwise combination of multiplied inputs (second order). This can be extended to an n-order network. It should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal, and the nonlinear solution is overfitted. Other linear classification algorithms include Winnow, support-vector machine, and logistic regression. Multiclass perceptron Like most other techniques for training linear classifiers, the perceptron generalizes naturally to multiclass classification. Here, the input $x$ and the output $y$ are drawn from arbitrary sets. A feature representation function $f(x,y)$ maps each possible input/output pair to a finite-dimensional real-valued feature vector. As before, the feature vector is multiplied by a weight vector $w$, but now the resulting score is used to choose among many possible outputs: $\\hat y = \\operatorname{argmax}_y f(x,y) \\cdot w.$ Learning again iterates over the examples, predicting an output for each, leaving the weights unchanged when the predicted output matches the target, and changing them when it does not."
            },
            {
                "text": "The update becomes: $ w_{t+1} = w_t + f(x, y) - f(x,\\hat y).$ This multiclass feedback formulation reduces to the original perceptron when $x$ is a real-valued vector, $y$ is chosen from $\\{0,1\\}$, and $f(x,y) = y x$. For certain problems, input/output representations and features can be chosen so that $\\mathrm{argmax}_y f(x,y) \\cdot w$ can be found efficiently even though $y$ is chosen from a very large or even infinite set. Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002). It has also been applied to large-scale machine learning problems in a distributed computing setting. References Further reading Aizerman, M. A. and Braverman, E. M. and Lev I. Rozonoer. Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control, 25:821–837, 1964."
            },
            {
                "text": "Rosenblatt, Frank (1958), The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, Cornell Aeronautical Laboratory, Psychological Review, v65, No. 6, pp. 386–408. . Rosenblatt, Frank (1962), Principles of Neurodynamics. Washington, DC: Spartan Books. Minsky, M. L. and Papert, S. A. 1969. Perceptrons. Cambridge, MA: MIT Press. Gallant, S. I. (1990). Perceptron-based learning algorithms. IEEE Transactions on Neural Networks, vol. 1, no. 2, pp. 179–191. Olazaran Rodriguez, Jose Miguel. A historical sociology of neural network research. PhD Dissertation. University of Edinburgh, 1991. Mohri, Mehryar and Rostamizadeh, Afshin (2013). Perceptron Mistake Bounds arXiv:1305.0208, 2013. Novikoff, A. B. (1962). On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12, 615–622. Polytechnic Institute of Brooklyn. Widrow, B., Lehr, M.A., \"30 years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation,\" Proc. IEEE, vol 78, no 9, pp. 1415–1442, (1990). Collins, M. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with the perceptron algorithm in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP '02)."
            },
            {
                "text": "Yin, Hongfeng (1996), Perceptron-Based Algorithms and Analysis, Spectrum Library, Concordia University, Canada External links A Perceptron implemented in MATLAB to learn binary NAND function Chapter 3 Weighted networks - the perceptron and chapter 4 Perceptron learning of Neural Networks - A Systematic Introduction by Raúl Rojas () History of perceptrons Mathematics of multilayer perceptrons Applying a perceptron model using scikit-learn - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html Category:Classification algorithms Category:Artificial neural networks"
            }
        ],
        "latex_formulas": [
            "''O''({{sqrt|''t''}})",
            "''O''(''t'')",
            "\\mathbf{x}",
            "f(\\mathbf{x})",
            "h",
            "\\mathbf{w}",
            "\\mathbf{w} \\cdot \\mathbf{x}",
            "\\mathbf{w}\\cdot \\mathbf{x} + b = (\\mathbf{w}, b) \\cdot (\\mathbf{x}, 1)",
            "b",
            "\\mathbf{w}_{m+1}",
            "1",
            "\\mathbf{x}",
            "f(\\mathbf{x})",
            "\\mathbf{x}",
            "T(N, K)",
            "T(N, K)/2^N",
            "N \\leq 2K",
            "N> 2K",
            "N \\leq 2K",
            "N> 2K",
            "n=9",
            "2^{n^2 - n \\log_2 n + O(n)}",
            "2^{n^2 - n \\log_2 n - O(n)}",
            "\\Theta(n \\ln n)",
            "n",
            "f: 2^n \\to 2",
            "k",
            "k",
            "n",
            "\\Omega(n^{1/2})",
            "r",
            "y = f(\\mathbf{z})",
            "\\mathbf{z}",
            "D = \\{(\\mathbf{x}_1,d_1),\\dots,(\\mathbf{x}_s,d_s)\\}",
            "s",
            "\\mathbf{x}_j",
            "n",
            "d_j",
            "x_{j,i}",
            "i",
            "j",
            "x_{j,0} = 1",
            "w_i",
            "i",
            "i",
            "x_{j,0} = 1",
            "w_0",
            "b",
            "\\mathbf{w}",
            "w_i(t)",
            "i",
            "t",
            "\\mathbf{x}_j",
            "d_j",
            "\\begin{align}\ny_j(t) &= f[\\mathbf{w}(t)\\cdot\\mathbf{x}_j] \\\\\n&= f[w_0(t)x_{j,0} + w_1(t)x_{j,1} + w_2(t)x_{j,2} + \\dotsb + w_n(t)x_{j,n}]\n\\end{align}",
            "w_i(t+1) = w_i(t) \\; \\boldsymbol{+} \\; r\\cdot(d_j - y_j(t)) x_{j,i}",
            "0 \\leq i \\leq n",
            "r",
            "\\frac{1}{s} \\sum_{j=1}^s |d_j - y_j(t)|",
            "\\gamma",
            "\\gamma = 0.01, R = 1, r = 1",
            "y = +1",
            "y = +1",
            "R",
            "w",
            "x_1 \\geq 0.01",
            "x_1",
            "0.01",
            "\\sqrt t",
            "x_1",
            "t",
            "x_1",
            "\\min(O(n^{d/2}), O(d^{2n}), O(n^{d-1} \\ln n))",
            "n",
            "d",
            "D",
            "M",
            "w_0",
            "w_t",
            "\\|w_t\\| \\leq \\|w_0\\|+M",
            "x",
            "\\{-1, +1\\}^n",
            "y = \\theta(x_i)",
            "x_i",
            "y=1",
            "n",
            "n",
            "n+1",
            "2n",
            "2n",
            "n",
            "n/2",
            "O(\\ln n)",
            "\\alpha",
            "x",
            "y",
            "f(x,y)",
            "w",
            "\\hat y = \\operatorname{argmax}_y f(x,y) \\cdot w.",
            "w_{t+1} = w_t + f(x, y) - f(x,\\hat y).",
            "x",
            "y",
            "\\{0,1\\}",
            "f(x,y) = y x",
            "\\mathrm{argmax}_y f(x,y) \\cdot w",
            "y"
        ]
    },
    "Weight": {
        "title": "Weight",
        "chunks": [
            {
                "text": "In science and engineering, the weight of an object is a quantity associated with the gravitational force exerted on the object by other objects in its environment, although there is some variation and debate as to the exact definition. Some standard textbooks define weight as a vector quantity, the gravitational force acting on the object. Others define weight as a scalar quantity, the magnitude of the gravitational force. Yet others define it as the magnitude of the reaction force exerted on a body by mechanisms that counteract the effects of gravity: the weight is the quantity that is measured by, for example, a spring scale. Thus, in a state of free fall, the weight would be zero. In this sense of weight, terrestrial objects can be weightless: so if one ignores air resistance, one could say the legendary apple falling from the tree, on its way to meet the ground near Isaac Newton, was weightless. The unit of measurement for weight is that of force, which in the International System of Units (SI) is the newton."
            },
            {
                "text": "For example, an object with a mass of one kilogram has a weight of about 9.8 newtons on the surface of the Earth, and about one-sixth as much on the Moon. Although weight and mass are scientifically distinct quantities, the terms are often confused with each other in everyday use (e.g. comparing and converting force weight in pounds to mass in kilograms and vice versa).The National Standard of Canada, CAN/CSA-Z234.1-89 Canadian Metric Practice Guide, January 1989: 5.7.3 Considerable confusion exists in the use of the term \"weight\". In commercial and everyday use, the term \"weight\" nearly always means mass. In science and technology \"weight\" has primarily meant a force due to gravity. In scientific and technical work, the term \"weight\" should be replaced by the term \"mass\" or \"force\", depending on the application. 5.7.4 The use of the verb \"to weigh\" meaning \"to determine the mass of\", e.g., \"I weighed this object and determined its mass to be 5kg,\" is correct."
            },
            {
                "text": "Further complications in elucidating the various concepts of weight have to do with the theory of relativity according to which gravity is modeled as a consequence of the curvature of spacetime. In the teaching community, a considerable debate has existed for over half a century on how to define weight for their students. The current situation is that a multiple set of concepts co-exist and find use in their various contexts. History Discussion of the concepts of heaviness (weight) and lightness (levity) date back to the ancient Greek philosophers. These were typically viewed as inherent properties of objects. Plato described weight as the natural tendency of objects to seek their kin. To Aristotle, weight and levity represented the tendency to restore the natural order of the basic elements: air, earth, fire and water. He ascribed absolute weight to earth and absolute levity to fire. Archimedes saw weight as a quality opposed to buoyancy, with the conflict between the two determining if an object sinks or floats. The first operational definition of weight was given by Euclid, who defined weight as: \"the heaviness or lightness of one thing, compared to another, as measured by a balance.\""
            },
            {
                "text": "Operational balances (rather than definitions) had, however, been around much longer.http://www.averyweigh-tronix.com/museum accessed 29 March 2013. According to Aristotle, weight was the direct cause of the falling motion of an object, the speed of the falling object was supposed to be directly proportionate to the weight of the object. As medieval scholars discovered that in practice the speed of a falling object increased with time, this prompted a change to the concept of weight to maintain this cause-effect relationship. Weight was split into a \"still weight\" or , which remained constant, and the actual gravity or , which changed as the object fell. The concept of was eventually replaced by Jean Buridan's impetus, a precursor to momentum. The rise of the Copernican view of the world led to the resurgence of the Platonic idea that like objects attract but in the context of heavenly bodies. In the 17th century, Galileo made significant advances in the concept of weight. He proposed a way to measure the difference between the weight of a moving object and an object at rest."
            },
            {
                "text": "Ultimately, he concluded weight was proportionate to the amount of matter of an object, not the speed of motion as supposed by the Aristotelean view of physics. Newton The introduction of Newton's laws of motion and the development of Newton's law of universal gravitation led to considerable further development of the concept of weight. Weight became fundamentally separate from mass. Mass was identified as a fundamental property of objects connected to their inertia, while weight became identified with the force of gravity on an object and therefore dependent on the context of the object. In particular, Newton considered weight to be relative to another object causing the gravitational pull, e.g. the weight of the Earth towards the Sun. Newton considered time and space to be absolute. This allowed him to consider concepts as true position and true velocity. Newton also recognized that weight as measured by the action of weighing was affected by environmental factors such as buoyancy. He considered this a false weight induced by imperfect measurement conditions, for which he introduced the term apparent weight as compared to the true weight defined by gravity."
            },
            {
                "text": "Although Newtonian physics made a clear distinction between weight and mass, the term weight continued to be commonly used when people meant mass. This led the 3rd General Conference on Weights and Measures (CGPM) of 1901 to officially declare \"The word weight denotes a quantity of the same nature as a force: the weight of a body is the product of its mass and the acceleration due to gravity\", thus distinguishing it from mass for official usage. Relativity In the 20th century, the Newtonian concepts of absolute time and space were challenged by relativity. Einstein's equivalence principle put all observers, moving or accelerating, on the same footing. This led to an ambiguity as to what exactly is meant by the force of gravity and weight. A scale in an accelerating elevator cannot be distinguished from a scale in a gravitational field. Gravitational force and weight thereby became essentially frame-dependent quantities. This prompted the abandonment of the concept as superfluous in the fundamental sciences such as physics and chemistry. Nonetheless, the concept remained important in the teaching of physics."
            },
            {
                "text": "The ambiguities introduced by relativity led, starting in the 1960s, to considerable debate in the teaching community as how to define weight for their students, choosing between a nominal definition of weight as the force due to gravity or an operational definition defined by the act of weighing. Definitions Several definitions exist for weight, not all of which are equivalent. Gravitational definition The most common definition of weight found in introductory physics textbooks defines weight as the force exerted on a body by gravity. This is often expressed in the formula , where W is the weight, m the mass of the object, and g gravitational acceleration. In 1901, the 3rd General Conference on Weights and Measures (CGPM) established this as their official definition of weight: This resolution defines weight as a vector, since force is a vector quantity. However, some textbooks also take weight to be a scalar by defining: The gravitational acceleration varies from place to place. Sometimes, it is simply taken to have a standard value of , which gives the standard weight. The force whose magnitude is equal to mg newtons is also known as the m kilogram weight (which term is abbreviated to kg-wt)Chester, W. Mechanics."
            },
            {
                "text": "George Allen & Unwin. London. 1979. . Section 3.2 at page 83. Operational definition In the operational definition, the weight of an object is the force measured by the operation of weighing it, which is the force it exerts on its support. Since W is the downward force on the body by the centre of Earth and there is no acceleration in the body, there exists an opposite and equal force by the support on the body. It is equal to the force exerted by the body on its support because action and reaction have same numerical value and opposite direction. This can make a considerable difference, depending on the details; for example, an object in free fall exerts little if any force on its support, a situation that is commonly referred to as weightlessness. However, being in free fall does not affect the weight according to the gravitational definition. Therefore, the operational definition is sometimes refined by requiring that the object be at rest. However, this raises the issue of defining \"at rest\" (usually being at rest with respect to the Earth is implied by using standard gravity)."
            },
            {
                "text": "In the operational definition, the weight of an object at rest on the surface of the Earth is lessened by the effect of the centrifugal force from the Earth's rotation. The operational definition, as usually given, does not explicitly exclude the effects of buoyancy, which reduces the measured weight of an object when it is immersed in a fluid such as air or water. As a result, a floating balloon or an object floating in water might be said to have zero weight. ISO definition In the ISO International standard ISO 80000-4:2006,ISO 80000-4:2006, Quantities and units - Part 4: Mechanics describing the basic physical quantities and units in mechanics as a part of the International standard ISO/IEC 80000, the definition of weight is given as: The definition is dependent on the chosen frame of reference. When the chosen frame is co-moving with the object in question then this definition precisely agrees with the operational definition. If the specified frame is the surface of the Earth, the weight according to the ISO and gravitational definitions differ only by the centrifugal effects due to the rotation of the Earth."
            },
            {
                "text": "Apparent weight In many real world situations the act of weighing may produce a result that differs from the ideal value provided by the definition used. This is usually referred to as the apparent weight of the object. A common example of this is the effect of buoyancy, when an object is immersed in a fluid the displacement of the fluid will cause an upward force on the object, making it appear lighter when weighed on a scale. The apparent weight may be similarly affected by levitation and mechanical suspension. When the gravitational definition of weight is used, the operational weight measured by an accelerating scale is often also referred to as the apparent weight. Mass An object with mass m resting on a surface and the corresponding free body diagram of just the object showing the forces acting on it. The magnitude of force that the table is pushing upward on the object (the N vector) is equal to the downward force of the object's weight (shown here as mg, as weight is equal to the object's mass multiplied with the acceleration due to gravity): because these forces are equal, the object is in a state of equilibrium (all the forces and moments acting on it sum to zero)."
            },
            {
                "text": "In modern scientific usage, weight and mass are fundamentally different quantities: mass is an intrinsic property of matter, whereas weight is a force that results from the action of gravity on matter: it measures how strongly the force of gravity pulls on that matter. However, in most practical everyday situations the word \"weight\" is used when, strictly, \"mass\" is meant. For example, most people would say that an object \"weighs one kilogram\", even though the kilogram is a unit of mass. The distinction between mass and weight is unimportant for many practical purposes because the strength of gravity does not vary too much on the surface of the Earth. In a uniform gravitational field, the gravitational force exerted on an object (its weight) is directly proportional to its mass. For example, object A weighs 10 times as much as object B, so therefore the mass of object A is 10 times greater than that of object B. This means that an object's mass can be measured indirectly by its weight, and so, for everyday purposes, weighing (using a weighing scale) is an entirely acceptable way of measuring mass."
            },
            {
                "text": "Similarly, a balance measures mass indirectly by comparing the weight of the measured item to that of an object(s) of known mass. Since the measured item and the comparison mass are in virtually the same location, so experiencing the same gravitational field, the effect of varying gravity does not affect the comparison or the resulting measurement. The Earth's gravitational field is not uniform but can vary by as much as 0.5% at different locations on Earth (see Earth's gravity). These variations alter the relationship between weight and mass, and must be taken into account in high-precision weight measurements that are intended to indirectly measure mass. Spring scales, which measure local weight, must be calibrated at the location at which the objects will be used to show this standard weight, to be legal for commerce. This table shows the variation of acceleration due to gravity (and hence the variation of weight) at various locations on the Earth's surface. Location Latitude m/s2Absolute difference from equatorPercentage difference from equator Equator 0° 9.78030.00000% Sydney 33°52′ S 9.79680.01650.17% Aberdeen 57°9′ N 9.81680.03650.37% North Pole 90° N 9.83220.05190.53% The historical use of \"weight\" for \"mass\" also persists in some scientific terminology – for example, the chemical terms \"atomic weight\", \"molecular weight\", and \"formula weight\", can still be found rather than the preferred \"atomic mass\", etc."
            },
            {
                "text": "In a different gravitational field, for example, on the surface of the Moon, an object can have a significantly different weight than on Earth. The gravity on the surface of the Moon is only about one-sixth as strong as on the surface of the Earth. A one-kilogram mass is still a one-kilogram mass (as mass is an intrinsic property of the object) but the downward force due to gravity, and therefore its weight, is only one-sixth of what the object would have on Earth. So a man of mass 180 pounds weighs only about 30 pounds-force when visiting the Moon. SI units In most modern scientific work, physical quantities are measured in SI units. The SI unit of weight is the same as that of force: the newton (N) – a derived unit which can also be expressed in SI base units as kg⋅m/s2 (kilograms times metres per second squared). In commercial and everyday use, the term \"weight\" is usually used to mean mass, and the verb \"to weigh\" means \"to determine the mass of\" or \"to have a mass of\"."
            },
            {
                "text": "Used in this sense, the proper SI unit is the kilogram (kg). Pound and other non-SI units In United States customary units, the pound can be either a unit of force or a unit of mass. Related units used in some distinct, separate subsystems of units include the poundal and the slug. The poundal is defined as the force necessary to accelerate an object of one-pound mass at 1ft/s2, and is equivalent to about 1/32.2 of a pound-force. The slug is defined as the amount of mass that accelerates at 1ft/s2 when one pound-force is exerted on it, and is equivalent to about 32.2 pounds (mass). The kilogram-force is a non-SI unit of force, defined as the force exerted by a one-kilogram mass in standard Earth gravity (equal to 9.80665 newtons exactly). The dyne is the cgs unit of force and is not a part of SI, while weights measured in the cgs unit of mass, the gram, remain a part of SI. Sensation The sensation of weight is caused by the force exerted by fluids in the vestibular system, a three-dimensional set of tubes in the inner ear."
            },
            {
                "text": "It is actually the sensation of g-force, regardless of whether this is due to being stationary in the presence of gravity, or, if the person is in motion, the result of any other forces acting on the body such as in the case of acceleration or deceleration of a lift, or centrifugal forces when turning sharply. Measuring thumb|A weighbridge, used for weighing trucks Weight is commonly measured using one of two methods. A spring scale or hydraulic or pneumatic scale measures local weight, the local force of gravity on the object (strictly apparent weight force). Since the local force of gravity can vary by up to 0.5% at different locations, spring scales will measure slightly different weights for the same object (the same mass) at different locations. To standardize weights, scales are always calibrated to read the weight an object would have at a nominal standard gravity of 9.80665m/s2 (approx. 32.174ft/s2). However, this calibration is done at the factory. When the scale is moved to another location on Earth, the force of gravity will be different, causing a slight error."
            },
            {
                "text": "So to be highly accurate and legal for commerce, spring scales must be re-calibrated at the location at which they will be used. A balance on the other hand, compares the weight of an unknown object in one scale pan to the weight of standard masses in the other, using a lever mechanism – a lever-balance. The standard masses are often referred to, non-technically, as \"weights\". Since any variations in gravity will act equally on the unknown and the known weights, a lever-balance will indicate the same value at any location on Earth. Therefore, balance \"weights\" are usually calibrated and marked in mass units, so the lever-balance measures mass by comparing the Earth's attraction on the unknown object and standard masses in the scale pans. In the absence of a gravitational field, away from planetary bodies (e.g. space), a lever-balance would not work, but on the Moon, for example, it would give the same reading as on Earth. Some balances are marked in weight units, but since the weights are calibrated at the factory for standard gravity, the balance will measure standard weight, i.e."
            },
            {
                "text": "what the object would weigh at standard gravity, not the actual local force of gravity on the object. If the actual force of gravity on the object is needed, this can be calculated by multiplying the mass measured by the balance by the acceleration due to gravity – either standard gravity (for everyday work) or the precise local gravity (for precision work). Tables of the gravitational acceleration at different locations can be found on the web. Gross weight is a term that is generally found in commerce or trade applications, and refers to the total weight of a product and its packaging. Conversely, net weight refers to the weight of the product alone, discounting the weight of its container or packaging; and tare weight is the weight of the packaging alone. Relative weights on the Earth and other celestial bodies The table below shows comparative gravitational accelerations at the surface of the Sun, the Moon, and at each of the planets in the Solar System. The \"surface\" is taken to mean the cloud tops of the giant planets (Jupiter, Saturn, Uranus, and Neptune). For the Sun, the surface is taken to mean the photosphere. The values in the table have not been de-rated for the centrifugal effect of planet rotation (and cloud-top wind speeds for the giant planets) and therefore, generally speaking, are similar to the actual gravity that would be experienced near the poles."
            },
            {
                "text": "Body Multiple ofEarth gravity Surface gravitym/s2 Sun 27.90 274.1 Mercury 0.3770 3.703 Venus 0.9032 8.872 Earth 1 (by definition) 9.8226This value excludes the adjustment for centrifugal force due to Earth’s rotation and is therefore greater than the 9.80665m/s2 value of standard gravity. Moon 0.1655 1.625 Mars 0.3895 3.728 Jupiter 2.640 25.93 Saturn 1.139 11.19 Uranus 0.917 9.01 Neptune 1.148 11.28 See also Tare weight the English unit List of weights Notes References *"
            }
        ],
        "latex_formulas": [
            "W",
            "\\mathsf{MLT}^{-2}",
            "W = mg",
            "W = ma",
            "F_g = m g \\,"
        ]
    },
    "Bias_(statistics)": {
        "title": "Bias_(statistics)",
        "chunks": [
            {
                "text": "Statistical bias, in the mathematical field of statistics, is a systematic tendency in which the methods used to gather data and generate statistics present an inaccurate, skewed or biased depiction of reality. Statistical bias exists in numerous stages of the data collection and analysis process, including: the source of the data, the methods used to collect the data, the estimator chosen, and the methods used to analyze the data. Data analysts can take various measures at each stage of the process to reduce the impact of statistical bias in their work. Understanding the source of statistical bias can help to assess whether the observed results are close to actuality. Issues of statistical bias has been argued to be closely linked to issues of statistical validity. Statistical bias can have significant real world implications as data is used to inform decision making across a wide variety of processes in society. Data is used to inform lawmaking, industry regulation, corporate marketing and distribution tactics, and institutional policies in organizations and workplaces. Therefore, there can be significant implications if statistical bias is not accounted for and controlled."
            },
            {
                "text": "For example, if a pharmaceutical company wishes to explore the effect of a medication on the common cold but the data sample only includes men, any conclusions made from that data will be biased towards how the medication affects men rather than people in general. That means the information would be incomplete and not useful for deciding if the medication is ready for release in the general public. In this scenario, the bias can be addressed by broadening the sample. This sampling error is only one of the ways in which data can be biased. Bias can be differentiated from other statistical mistakes such as accuracy (instrument failure/inadequacy), lack of data, or mistakes in transcription (typos). Bias implies that the data selection may have been skewed by the collection criteria. Other forms of human-based bias emerge in data collection as well such as response bias, in which participants give inaccurate responses to a question. Bias does not preclude the existence of any other mistakes. One may have a poorly designed sample, an inaccurate measurement device, and typos in recording data simultaneously."
            },
            {
                "text": "Ideally, all factors are controlled and accounted for. Also it is useful to recognize that the term “error” specifically refers to the outcome rather than the process (errors of rejection or acceptance of the hypothesis being tested), or from the phenomenon of random errors. The terms flaw or mistake are recommended to differentiate procedural errors from these specifically defined outcome-based terms. Bias of an estimator Statistical bias is a feature of a statistical technique or of its results whereby the expected value of the results differs from the true underlying quantitative parameter being estimated. The bias of an estimator of a parameter should not be confused with its degree of precision, as the degree of precision is a measure of the sampling error. The bias is defined as follows: let $T$ be a statistic used to estimate a parameter $\\theta$, and let $\\operatorname E(T)$ denote the expected value of $T$. Then, $\\operatorname{bias}(T, \\theta) = \\operatorname{bias}(T) = \\operatorname E(T) - \\theta$ is called the bias of the statistic $T$ (with respect to $\\theta$)."
            },
            {
                "text": "If $\\operatorname{bias}(T, \\theta)=0$, then $T$ is said to be an unbiased estimator of $\\theta$; otherwise, it is said to be a biased estimator of $\\theta$. The bias of a statistic $T$ is always relative to the parameter $\\theta$ it is used to estimate, but the parameter $\\theta$ is often omitted when it is clear from the context what is being estimated. Types Statistical bias comes from all stages of data analysis. The following sources of bias will be listed in each stage separately. Data selection Selection bias involves individuals being more likely to be selected for study than others, biasing the sample. This can also be termed selection effect, sampling bias and Berksonian bias. Spectrum bias arises from evaluating diagnostic tests on biased patient samples, leading to an overestimate of the sensitivity and specificity of the test. For example, a high prevalence of disease in a study population increases positive predictive values, which will cause a bias between the prediction values and the real ones."
            },
            {
                "text": "Observer selection bias occurs when the evidence presented has been pre-filtered by observers, which is so-called anthropic principle. The data collected is not only filtered by the design of experiment, but also by the necessary precondition that there must be someone doing a study. An example is the impact of the Earth in the past. The impact event may cause the extinction of intelligent animals, or there were no intelligent animals at that time. Therefore, some impact events have not been observed, but they may have occurred in the past. Volunteer bias occurs when volunteers have intrinsically different characteristics from the target population of the study. Research has shown that volunteers tend to come from families with higher socioeconomic status. Furthermore, another study shows that women are more probable to volunteer for studies than men. Funding bias may lead to the selection of outcomes, test samples, or test procedures that favor a study's financial sponsor. Attrition bias arises due to a loss of participants, e.g., loss of follow up during a study. Recall bias arises due to differences in the accuracy or completeness of participant recollections of past events; for example, patients cannot recall how many cigarettes they smoked last week exactly, leading to over-estimation or under-estimation."
            },
            {
                "text": "Hypothesis testing Type I and type II errors in statistical hypothesis testing leads to wrong results. Type I error happens when the null hypothesis is correct but is rejected. For instance, suppose that the null hypothesis is that if the average driving speed limit ranges from 75 to 85 km/h, it is not considered as speeding. On the other hand, if the average speed is not in that range, it is considered speeding. If someone receives a ticket with an average driving speed of 7 km/h, the decision maker has committed a Type I error. In other words, the average driving speed meets the null hypothesis but is rejected. On the contrary, Type II error happens when the null hypothesis is not correct but is accepted. Bias in hypothesis testing occurs when the power (the complement of the type II error rate) at some alternative is lower than the supremum of the Type I error rate (which is usually the significance level, $\\alpha$). Equivalently, if no rejection rate at any alternative is lower than the rejection rate at any point in the null hypothesis set, the test is said to be unbiased.Casella, George; Berger, Roger L. (2002), Statistical Inference, 2nd Ed., p387 Estimator selection The bias of an estimator is the difference between an estimator's expected value and the true value of the parameter being estimated."
            },
            {
                "text": "Although an unbiased estimator is theoretically preferable to a biased estimator, in practice, biased estimators with small biases are frequently used. A biased estimator may be more useful for several reasons. First, an unbiased estimator may not exist without further assumptions. Second, sometimes an unbiased estimator is hard to compute. Third, a biased estimator may have a lower value of mean squared error. A biased estimator is better than any unbiased estimator arising from the Poisson distribution. The value of a biased estimator is always positive and the mean squared error of it is smaller than the unbiased one, which makes the biased estimator be more accurate. Omitted-variable bias is the bias that appears in estimates of parameters in regression analysis when the assumed specification omits an independent variable that should be in the model. Analysis methods Detection bias occurs when a phenomenon is more likely to be observed for a particular set of study subjects. For instance, the syndemic involving obesity and diabetes may mean doctors are more likely to look for diabetes in obese patients than in thinner patients, leading to an inflation in diabetes among obese patients because of skewed detection efforts."
            },
            {
                "text": "In educational measurement, bias is defined as \"Systematic errors in test content, test administration, and/or scoring procedures that can cause some test takers to get either lower or higher scores than their true ability would merit.\" The source of the bias is irrelevant to the trait the test is intended to measure. Observer bias arises when the researcher subconsciously influences the experiment due to cognitive bias where judgment may alter how an experiment is carried out / how results are recorded. Interpretation Reporting bias involves a skew in the availability of data, such that observations of a certain kind are more likely to be reported. Addressing statistical bias Depending on the type of bias present, researchers and analysts can take different steps to reduce bias on a data set. All types of bias mentioned above have corresponding measures which can be taken to reduce or eliminate their impacts. Bias should be accounted for at every step of the data collection process, beginning with clearly defined research parameters and consideration of the team who will be conducting the research. Observer bias may be reduced by implementing a blind or double-blind technique. Avoidance of p-hacking is essential to the process of accurate data collection. One way to check for bias in results after is rerunning analyses with different independent variables to observe whether a given phenomenon still occurs in dependent variables. Careful use of language in reporting can reduce misleading phrases, such as discussion of a result \"approaching\" statistical significant as compared to actually achieving it."
            },
            {
                "text": "See also Trueness Systematic error References External links The Catalogue of Bias is a project at the Centre for Evidence-Based Medicine cataloguing biases that affect health evidence. Statistics Category:Accuracy and precision"
            }
        ],
        "latex_formulas": [
            "T",
            "\\theta",
            "\\operatorname E(T)",
            "T",
            "\\operatorname{bias}(T, \\theta) = \\operatorname{bias}(T) = \\operatorname E(T) - \\theta",
            "T",
            "\\theta",
            "\\operatorname{bias}(T, \\theta)=0",
            "T",
            "\\theta",
            "\\theta",
            "T",
            "\\theta",
            "\\theta",
            "\\alpha"
        ]
    },
    "Learning_rate": {
        "title": "Learning_rate",
        "chunks": [
            {
                "text": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model \"learns\". In the adaptive control literature, the learning rate is commonly referred to as gain. In setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the descent direction is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum. In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate."
            },
            {
                "text": "The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method. The learning rate is related to the step length determined by inexact line search in quasi-Newton methods and related optimization algorithms. Learning rate schedule Initial rate can be left as system default or can be selected using a range of techniques. A learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations. This is mainly done with two parameters: decay and momentum. There are many different learning rate schedules but the most common are time-based, step-based and exponential. Decay serves to settle the learning in a nice place and avoid oscillations, a situation that may arise when a too high constant learning rate makes the learning jump back and forth over a minimum, and is controlled by a hyperparameter. Momentum is analogous to a ball rolling down a hill; we want the ball to settle at the lowest point of the hill (corresponding to the lowest error)."
            },
            {
                "text": "Momentum both speeds up the learning (increasing the learning rate) when the error cost gradient is heading in the same direction for a long time and also avoids local minima by 'rolling over' small bumps. Momentum is controlled by a hyperparameter analogous to a ball's mass which must be chosen manually—too high and the ball will roll over minima which we wish to find, too low and it will not fulfil its purpose. The formula for factoring in the momentum is more complex than for decay but is most often built in with deep learning libraries such as Keras. Time-based learning schedules alter the learning rate depending on the learning rate of the previous time iteration. Factoring in the decay the mathematical formula for the learning rate is: $\\eta_{n+1} = \\frac{\\eta_n }{1+dn}$ where $\\eta$ is the learning rate, $d$ is a decay parameter and $n$ is the iteration step. Step-based learning schedules changes the learning rate according to some predefined steps. The decay application formula is here defined as: $\\eta_{n} = \\eta_0d^{\\left\\lfloor\\frac{1+n}{r}\\right\\rfloor}$ where $\\eta_{n}$ is the learning rate at iteration $n$, $\\eta_0$ is the initial learning rate, $d$ is how much the learning rate should change at each drop (0.5 corresponds to a halving) and $r$ corresponds to the drop rate, or how often the rate should be dropped (10 corresponds to a drop every 10 iterations)."
            },
            {
                "text": "The floor function ($\\lfloor\\dots\\rfloor$) here drops the value of its input to 0 for all values smaller than 1. Exponential learning schedules are similar to step-based, but instead of steps, a decreasing exponential function is used. The mathematical formula for factoring in the decay is: $\\eta_{n} = \\eta_0e^{-dn}$ where $d$ is a decay parameter. Adaptive learning rate The issue with learning rate schedules is that they all depend on hyperparameters that must be manually chosen for each given learning session and may vary greatly depending on the problem at hand or the model used. To combat this, there are many different types of adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, and Adam which are generally built into deep learning libraries such as Keras. See also Hyperparameter (machine learning) Hyperparameter optimization Stochastic gradient descent Variable metric methods Overfitting Backpropagation AutoML Model selection Self-tuning References Further reading External links Category:Machine learning Category:Model selection Category:Optimization algorithms and methods"
            }
        ],
        "latex_formulas": [
            "\\eta_{n+1} = \\frac{\\eta_n }{1+dn}",
            "\\eta",
            "d",
            "n",
            "\\eta_{n} = \\eta_0d^{\\left\\lfloor\\frac{1+n}{r}\\right\\rfloor}",
            "\\eta_{n}",
            "n",
            "\\eta_0",
            "d",
            "r",
            "\\lfloor\\dots\\rfloor",
            "\\eta_{n} = \\eta_0e^{-dn}",
            "d"
        ]
    },
    "Outlier": {
        "title": "Outlier",
        "chunks": [
            {
                "text": "thumb|Figure 1. Box plot of data from the Michelson–Morley experiment displaying four outliers in the middle column, as well as one outlier in the first column. In statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to a variability in the measurement, an indication of novel data, or it may be the result of experimental error; the latter are sometimes excluded from the data set.Pimentel, M. A., Clifton, D. A., Clifton, L., & Tarassenko, L. (2014). A review of novelty detection. Signal Processing, 99, 215-249. stating \"An outlying observation may be merely an extreme manifestation of the random variability inherent in the data. ... On the other hand, an outlying observation may be the result of gross deviation from prescribed experimental procedure or an error in calculating or recording the numerical value.\" An outlier can be an indication of exciting possibility, but can also cause serious problems in statistical analyses. Outliers can occur by chance in any distribution, but they can indicate novel behaviour or structures in the data-set, measurement error, or that the population has a heavy-tailed distribution."
            },
            {
                "text": "In the case of measurement error, one wishes to discard them or use statistics that are robust to outliers, while in the case of heavy-tailed distributions, they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate 'correct trial' versus 'measurement error'; this is modeled by a mixture model. In most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. This can be due to incidental systematic error or flaws in the theory that generated an assumed family of probability distributions, or it may be that some observations are far from the center of the data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition)."
            },
            {
                "text": "Outliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations. Naive interpretation of statistics derived from data sets that include outliers may be misleading. For example, if one is calculating the average temperature of 10 objects in a room, and nine of them are between 20 and 25 degrees Celsius, but an oven is at 175 °C, the median of the data will be between 20 and 25 °C but the mean temperature will be between 35.5 and 40 °C. In this case, the median better reflects the temperature of a randomly sampled object (but not the temperature in the room) than the mean; naively interpreting the mean as \"a typical sample\", equivalent to the median, is incorrect. As illustrated in this case, outliers may indicate data points that belong to a different population than the rest of the sample set."
            },
            {
                "text": "Estimators capable of coping with outliers are said to be robust: the median is a robust statistic of central tendency, while the mean is not.Ripley, Brian D. 2004. Robust statistics Occurrence and causes Relative probabilities in a normal distribution In the case of normally distributed data, the three sigma rule means that roughly 1 in 22 observations will differ by twice the standard deviation or more from the mean, and 1 in 370 will deviate by three times the standard deviation. In a sample of 1000 observations, the presence of up to five observations deviating from the mean by more than three times the standard deviation is within the range of what can be expected, being less than twice the expected number and hence within 1 standard deviation of the expected number – see Poisson distribution – and not indicate an anomaly. If the sample size is only 100, however, just three such outliers are already reason for concern, being more than 11 times the expected number. In general, if the nature of the population distribution is known a priori, it is possible to test if the number of outliers deviate significantly from what can be expected: for a given cutoff (so samples fall beyond the cutoff with probability p) of a given distribution, the number of outliers will follow a binomial distribution with parameter p, which can generally be well-approximated by the Poisson distribution with λ = pn."
            },
            {
                "text": "Thus if one takes a normal distribution with cutoff 3 standard deviations from the mean, p is approximately 0.3%, and thus for 1000 trials one can approximate the number of samples whose deviation exceeds 3 sigmas by a Poisson distribution with λ = 3. Causes Outliers can have many anomalous causes. A physical apparatus for taking measurements may have suffered a transient malfunction. There may have been an error in data transmission or transcription. Outliers arise due to changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. A sample may have been contaminated with elements from outside the population being examined. Alternatively, an outlier could be the result of a flaw in the assumed theory, calling for further investigation by the researcher. Additionally, the pathological appearance of outliers of a certain form appears in a variety of datasets, indicating that the causative mechanism for the data might differ at the extreme end (King effect). Definitions and detection There is no rigid mathematical definition of what constitutes an outlier; determining whether or not an observation is an outlier is ultimately a subjective exercise."
            },
            {
                "text": "There are various methods of outlier detection, some of which are treated as synonymous with novelty detection.Pimentel, M. A., Clifton, D. A., Clifton, L., & Tarassenko, L. (2014). A review of novelty detection. Signal Processing, 99, 215-249. Some are graphical such as normal probability plots. Others are model-based. Box plots are a hybrid. Model-based methods which are commonly used for identification assume that the data are from a normal distribution, and identify observations which are deemed \"unlikely\" based on mean and standard deviation: Chauvenet's criterion Grubbs's test for outliers Dixon's Q test ASTM E178: Standard Practice for Dealing With Outlying ObservationsE178: Standard Practice for Dealing With Outlying Observations Mahalanobis distance and leverage are often used to detect outliers, especially in the development of linear regression models. Subspace and correlation based techniques for high-dimensional numerical data Peirce's criterion It is proposed to determine in a series of $m$ observations the limit of error, beyond which all observations involving so great an error may be rejected, provided there are as many as $n$ such observations."
            },
            {
                "text": "The principle upon which it is proposed to solve this problem is, that the proposed observations should be rejected when the probability of the system of errors obtained by retaining them is less than that of the system of errors obtained by their rejection multiplied by the probability of making so many, and no more, abnormal observations. (Quoted in the editorial note on page 516 to Peirce (1982 edition) from A Manual of Astronomy 2:558 by Chauvenet.) Benjamin Peirce, \"Criterion for the Rejection of Doubtful Observations\", Astronomical Journal II 45 (1852) and Errata to the original paper.. NOAA PDF Eprint (goes to Report p. 200, PDF's p. 215). – Appendix 21, according to the editorial note on page 515 Tukey's fences Other methods flag observations based on measures such as the interquartile range. For example, if $Q_1$ and $Q_3$ are the lower and upper quartiles respectively, then one could define an outlier to be any observation outside the range: $ \\big[ Q_1 - k (Q_3 - Q_1 ) , Q_3 + k (Q_3 - Q_1 ) \\big]$ for some nonnegative constant $k$."
            },
            {
                "text": "John Tukey proposed this test, where $k=1.5$ indicates an \"outlier\", and $k=3$ indicates data that is \"far out\". In anomaly detection In various domains such as, but not limited to, statistics, signal processing, finance, econometrics, manufacturing, networking and data mining, the task of anomaly detection may take other approaches. Some of these may be distance-based and density-based such as Local Outlier Factor (LOF). Some approaches may use the distance to the k-nearest neighbors to label observations as outliers or non-outliers. Modified Thompson Tau test The modified Thompson Tau test is a method used to determine if an outlier exists in a data set. The strength of this method lies in the fact that it takes into account a data set's standard deviation, average and provides a statistically determined rejection zone; thus providing an objective method to determine if a data point is an outlier.Thompson .R. (1985). \"A Note on Restricted Maximum Likelihood Estimation with an Alternative Outlier Model\".Journal of the Royal Statistical Society. Series B (Methodological), Vol."
            },
            {
                "text": "47, No. 1, pp. 53-55 How it works: First, a data set's average is determined. Next the absolute deviation between each data point and the average are determined. Thirdly, a rejection region is determined using the formula: $\\text{Rejection Region} \\frac{\\left ( n-1 \\right )}}{\\sqrt{n}\\sqrt{n-2+{t_{\\alpha/2}^2}}} $; where $\\scriptstyle{t_{\\alpha/2}}$ is the critical value from the Student distribution with n-2 degrees of freedom, n is the sample size, and s is the sample standard deviation. To determine if a value is an outlier: Calculate $\\scriptstyle \\delta = |(X - mean(X)) / s|$. If δ > Rejection Region, the data point is an outlier. If δ ≤ Rejection Region, the data point is not an outlier. The modified Thompson Tau test is used to find one outlier at a time (largest value of δ is removed if it is an outlier)."
            },
            {
                "text": "Meaning, if a data point is found to be an outlier, it is removed from the data set and the test is applied again with a new average and rejection region. This process is continued until no outliers remain in a data set. Some work has also examined outliers for nominal (or categorical) data. In the context of a set of examples (or instances) in a data set, instance hardness measures the probability that an instance will be misclassified ( $1-p(y|x)$ where is the assigned class label and represent the input attribute value for an instance in the training set ).Smith, M.R. ; Martinez, T.; Giraud-Carrier, C. (2014). \"An Instance Level Analysis of Data Complexity\". Machine Learning, 95(2): 225-256. Ideally, instance hardness would be calculated by summing over the set of all possible hypotheses : $\\begin{align}IH(\\langle x, y\\rangle) &= \\sum_H (1 - p(y, x, h))p(h|t)\\\\ &= \\sum_H p(h|t) - p(y, x, h)p(h|t)\\\\ &= 1- \\sum_H p(y, x, h)p(h|t).\\end{align}$ Practically, this formulation is unfeasible as is potentially infinite and calculating $p(h|t)$ is unknown for many algorithms."
            },
            {
                "text": "Thus, instance hardness can be approximated using a diverse subset $L \\subset H$: $IH_L (\\langle x,y\\rangle) = 1 - \\frac{1}{|L|} \\sum_{j=1}^{|L|} p(y|x, g_j(t, \\alpha))$ where $g_j(t, \\alpha)$ is the hypothesis induced by learning algorithm $g_j$ trained on training set with hyperparameters $\\alpha$. Instance hardness provides a continuous value for determining if an instance is an outlier instance. Working with outliers The choice of how to deal with an outlier should depend on the cause. Some estimators are highly sensitive to outliers, notably estimation of covariance matrices. Retention Even when a normal distribution model is appropriate to the data being analyzed, outliers are expected for large sample sizes and should not automatically be discarded if that is the case. Instead, one should use a method that is robust to outliers to model or analyze data with naturally occurring outliers. Exclusion When deciding whether to remove an outlier, the cause has to be considered."
            },
            {
                "text": "As mentioned earlier, if the outlier's origin can be attributed to an experimental error, or if it can be otherwise determined that the outlying data point is erroneous, it is generally recommended to remove it. However, it is more desirable to correct the erroneous value, if possible. Removing a data point solely because it is an outlier, on the other hand, is a controversial practice, often frowned upon by many scientists and science instructors, as it typically invalidates statistical results. While mathematical criteria provide an objective and quantitative method for data rejection, they do not make the practice more scientifically or methodologically sound, especially in small sets or where a normal distribution cannot be assumed. Rejection of outliers is more acceptable in areas of practice where the underlying model of the process being measured and the usual distribution of measurement error are confidently known. The two common approaches to exclude outliers are truncation (or trimming) and Winsorising. Trimming discards the outliers whereas Winsorising replaces the outliers with the nearest \"nonsuspect\" data. Exclusion can also be a consequence of the measurement process, such as when an experiment is not entirely capable of measuring such extreme values, resulting in censored data."
            },
            {
                "text": "In regression problems, an alternative approach may be to only exclude points which exhibit a large degree of influence on the estimated coefficients, using a measure such as Cook's distance.Cook, R. Dennis (Feb 1977). \"Detection of Influential Observations in Linear Regression\". Technometrics (American Statistical Association) 19 (1): 15–18. If a data point (or points) is excluded from the data analysis, this should be clearly stated on any subsequent report. Non-normal distributions The possibility should be considered that the underlying distribution of the data is not approximately normal, having \"fat tails\". For instance, when sampling from a Cauchy distribution,Weisstein, Eric W. Cauchy Distribution. From MathWorld--A Wolfram Web Resource the sample variance increases with the sample size, the sample mean fails to converge as the sample size increases, and outliers are expected at far larger rates than for a normal distribution. Even a slight difference in the fatness of the tails can make a large difference in the expected number of extreme values. Set-membership uncertainties A set membership approach considers that the uncertainty corresponding to the ith measurement of an unknown random vector x is represented by a set Xi (instead of a probability density function)."
            },
            {
                "text": "If no outliers occur, x should belong to the intersection of all Xi's. When outliers occur, this intersection could be empty, and we should relax a small number of the sets Xi (as small as possible) in order to avoid any inconsistency. This can be done using the notion of q-relaxed intersection. As illustrated by the figure, the q-relaxed intersection corresponds to the set of all x which belong to all sets except q of them. Sets Xi that do not intersect the q-relaxed intersection could be suspected to be outliers. thumb|Figure 5. q-relaxed intersection of 6 sets for q=2 (red), q=3 (green), q= 4 (blue), q= 5 (yellow). Alternative models In cases where the cause of the outliers is known, it may be possible to incorporate this effect into the model structure, for example by using a hierarchical Bayes model, or a mixture model.Roberts, S. and Tarassenko, L.: 1995, A probabilistic resource allocating network for novelty detection. Neural Computation 6, 270–284. See also Anomaly (natural sciences) Novelty detection Anscombe's quartet Data transformation (statistics) Extreme value theory Influential observation Random sample consensus Robust regression Studentized residual Winsorizing References External links Grubbs test described by NIST manual Category:Statistical charts and diagrams Category:Robust statistics"
            }
        ],
        "latex_formulas": [
            "m",
            "n",
            "Q_1",
            "Q_3",
            "\\big[ Q_1 - k (Q_3 - Q_1 ) , Q_3 + k (Q_3 - Q_1 ) \\big]",
            "k",
            "k=1.5",
            "k=3",
            "\\text{Rejection Region}{{=}} \\frac{{t_{\\alpha/2}}{\\left ( n-1 \\right )}}{\\sqrt{n}\\sqrt{n-2+{t_{\\alpha/2}^2}}}",
            "\\scriptstyle{t_{\\alpha/2}}",
            "\\scriptstyle \\delta  = |(X - mean(X)) / s|",
            "1-p(y|x)",
            "\\begin{align}IH(\\langle x, y\\rangle) &= \\sum_H (1 - p(y, x, h))p(h|t)\\\\\n&= \\sum_H p(h|t) - p(y, x, h)p(h|t)\\\\\n&= 1- \\sum_H p(y, x, h)p(h|t).\\end{align}",
            "p(h|t)",
            "L \\subset H",
            "IH_L (\\langle x,y\\rangle) = 1 - \\frac{1}{|L|} \\sum_{j=1}^{|L|} p(y|x, g_j(t, \\alpha))",
            "g_j(t, \\alpha)",
            "g_j",
            "\\alpha"
        ]
    },
    "Noise_(signal_processing)": {
        "title": "Noise_(signal_processing)",
        "chunks": [
            {
                "text": "In signal processing, noise is a general term for unwanted (and, in general, unknown) modifications that a signal may suffer during capture, storage, transmission, processing, or conversion. Vyacheslav Tuzlukov (2010), Signal Processing Noise, Electrical Engineering and Applied Signal Processing Series, CRC Press. 688 pages. Sometimes the word is also used to mean signals that are random (unpredictable) and carry no useful information; even if they are not interfering with other signals or may have been introduced intentionally, as in comfort noise. Noise reduction, the recovery of the original signal from the noise-corrupted one, is a very common goal in the design of signal processing systems, especially filters. The mathematical limits for noise removal are set by information theory. Types of noise Signal processing noise can be classified by its statistical properties (sometimes called the \"color\" of the noise) and by how it modifies the intended signal: Additive noise, gets added to the intended signal White noise Additive white Gaussian noise Black noise Gaussian noise Pink noise or flicker noise, with 1/f power spectrum Brownian noise, with 1/f2 power spectrum Contaminated Gaussian noise, whose PDF is a linear mixture of Gaussian PDFs Power-law noise Cauchy noise Multiplicative noise, multiplies or modulates the intended signal Quantization error, due to conversion from continuous to discrete values Poisson noise, typical of signals that are rates of discrete events Shot noise, e.g."
            },
            {
                "text": "caused by static electricity discharge Transient noise, a short pulse followed by decaying oscillations Burst noise, powerful but only during short intervals Phase noise, random time shifts in a signal Noise in specific kinds of signals Noise may arise in signals of interest to various scientific and technical fields, often with specific features: Noise (audio), such as \"hiss\" or \"hum\", in audio signals Background noise, due to spurious sounds during signal capture Comfort noise, added to voice communications to fill silent gaps Electromagnetically induced noise, audible noise due to electromagnetic vibrations in systems involving electromagnetic fields Noise (video), such as \"snow\" Noise (radio), such as \"static\", in radio transmissions Image noise, affects images, usually digital ones Salt and pepper noise or spike noise, scattered very dark or very light pixels Fixed pattern noise, that is tied to pixel sensors Shadow noise, made visible by increasing brightness or contrast Speckle noise, typical of radar imaging and interferograms Film grain in analog photography Compression artifacts or \"mosquito noise\" around edges in JPEG and other formats Noise (electronics) in electrical signals Johnson–Nyquist noise, in semiconductors Quantum noise Quantum 1/f noise, a disputed theory about quantum systems Generation-recombination noise, in semiconductor devices Oscillator phase noise, random fluctuations of the phase of an oscillator Barkhausen effect or Barkhausen noise, in the strength of a ferromagnet Spectral splatter or switch noise, caused by on/off transmitter switching Ground noise, appearing at the ground terminal of audio equipment Synaptic noise, observed in neuroscience Neuronal noise, observed in neuroscience Transcriptional noise in the transcription of genes to proteins Cosmic noise, in radioastronomy Phonon noise in materials science Internet background noise, packets sent to unassigned or inactive IP addresses Fano noise, in particle detectors Mode partition noise in optical cables Seismic noise, spurious ground vibrations in seismology Cosmic microwave background, microwave noise left over from the Big Bang Measures of noise in signals A long list of noise measures have been defined to measure noise in signal processing: in absolute terms, relative to some standard noise level, or relative to the desired signal level."
            },
            {
                "text": "They include: Dynamic range, often defined by inherent noise level Signal-to-noise ratio (SNR), ratio of noise power to signal power Peak signal-to-noise ratio, maximum SNR in a system Signal to noise ratio (imaging), for images Carrier-to-noise ratio, the signal-to-noise ratio of a modulated signal Noise power Noise figure Noise-equivalent flux density, a measure of noise in astronomy Noise floor Noise margin, by how much a signal exceeds the noise level Reference noise, a reference level for electronic noise Noise spectral density, noise power per unit of bandwidth Noise temperature Effective input noise temperature Noise-equivalent power, a measure of sensitivity for photodetectors Relative intensity noise, in a laser beam Antenna noise temperature, measure of noise in telecommunications antenna Received noise power, noise at a telecommunications receiver Circuit noise level, ratio of circuit noise to some reference level Channel noise level, some measure of noise in a communication channel Noise-equivalent target, intensity of a target when the signal-to-noise level is 1 Equivalent noise resistance, a measure of noise based on equivalent resistor Carrier-to-receiver noise density, ratio of received carrier power to receiver noise Carrier-to-noise-density ratio, Spectral signal-to-noise ratio Antenna gain-to-noise temperature, a measure of antenna performance Contrast-to-noise ratio, a measure of image quality Noise print, statistical signature of ambient noise for its suppression Equivalent pulse code modulation noise, measure of noise by comparing to PCM quantization noise Technology for noise in signals Almost every technique and device for signal processing has some connection to noise."
            },
            {
                "text": "Some random examples are: Noise shaping Antenna analyzer or noise bridge, used to measure the efficiency of antennas Noise gate Noise generator, a circuit that produces a random electrical signal Radio noise source used to calibrate radiotelescopes Friis formulas for the noise in telecommunications Noise-domain reflectometry, uses existing signals to find cable faults Noise-immune cavity-enhanced optical heterodyne molecular spectroscopy See also Anti-information Noise (electronics) Signal-to-noise statistic, a mathematical formula to measure the difference of two values relative to their standard deviations References Category:Signal processing"
            }
        ],
        "latex_formulas": []
    },
    "Accuracy_and_precision": {
        "title": "Accuracy_and_precision",
        "chunks": [
            {
                "text": "Accuracy is the proximity of measurement results to the accepted value; precision is the degree to which repeated (or reproducible) measurements under unchanged conditions show the same results. Accuracy and precision are two measures of observational error. Accuracy is how close a given set of measurements (observations or readings) are to their true value. Precision is how close the measurements are to each other. The International Organization for Standardization (ISO) defines a related measure: trueness, \"the closeness of agreement between the arithmetic mean of a large number of test results and the true or accepted reference value.\" According to ISO 5725-1, accuracy consists of trueness (proximity of the mean of measurement results to the true value) and precision (repeatability or reproducibility of the measurement). While precision is a description of random errors (a measure of statistical variability), accuracy has two different definitions: More commonly, a description of systematic errors (a measure of statistical bias of a given measure of central tendency, such as the mean). In this definition of \"accuracy\", the concept is independent of \"precision\", so a particular set of data can be said to be accurate, precise, both, or neither."
            },
            {
                "text": "This concept corresponds to ISO's trueness. A combination of both precision and trueness, accounting for the two types of observational error (random and systematic), so that high accuracy requires both high precision and high trueness. This usage corresponds to ISO's definition of accuracy (trueness and precision). Common technical definition In simpler terms, given a statistical sample or set of data points from repeated measurements of the same quantity, the sample or set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if their standard deviation is relatively small. In the fields of science and engineering, the accuracy of a measurement system is the degree of closeness of measurements of a quantity to that quantity's true value.JCGM 200:2008 International vocabulary of metrology — Basic and general concepts and associated terms (VIM) The precision of a measurement system, related to reproducibility and repeatability, is the degree to which repeated measurements under unchanged conditions show the same results. Although the two words precision and accuracy can be synonymous in colloquial use, they are deliberately contrasted in the context of the scientific method."
            },
            {
                "text": "The field of statistics, where the interpretation of measurements plays a central role, prefers to use the terms bias and variability instead of accuracy and precision: bias is the amount of inaccuracy and variability is the amount of imprecision. A measurement system can be accurate but not precise, precise but not accurate, neither, or both. For example, if an experiment contains a systematic error, then increasing the sample size generally increases precision but does not improve accuracy. The result would be a consistent yet inaccurate string of results from the flawed experiment. Eliminating the systematic error improves accuracy but does not change precision. A measurement system is considered valid if it is both accurate and precise. Related terms include bias (non-random or directed effects caused by a factor or factors unrelated to the independent variable) and error (random variability). The terminology is also applied to indirect measurements—that is, values obtained by a computational procedure from observed data. In addition to accuracy and precision, measurements may also have a measurement resolution, which is the smallest change in the underlying physical quantity that produces a response in the measurement."
            },
            {
                "text": "In numerical analysis, accuracy is also the nearness of a calculation to the true value; while precision is the resolution of the representation, typically defined by the number of decimal or binary digits. In military terms, accuracy refers primarily to the accuracy of fire (justesse de tir), the precision of fire expressed by the closeness of a grouping of shots at and around the centre of the target.North Atlantic Treaty Organization, NATO Standardization Agency AAP-6 – Glossary of terms and definitions, p 43. A shift in the meaning of these terms appeared with the publication of the ISO 5725 series of standards in 1994, which is also reflected in the 2008 issue of the BIPM International Vocabulary of Metrology (VIM), items 2.13 and 2.14. According to ISO 5725-1,BS ISO 5725-1: \"Accuracy (trueness and precision) of measurement methods and results - Part 1: General principles and definitions. \", p.1 (1994) the general term \"accuracy\" is used to describe the closeness of a measurement to the true value. When the term is applied to sets of measurements of the same measurand, it involves a component of random error and a component of systematic error."
            },
            {
                "text": "In this case trueness is the closeness of the mean of a set of measurement results to the actual (true) value, that is the systematic error, and precision is the closeness of agreement among a set of results, that is the random error. ISO 5725-1 and VIM also avoid the use of the term \"bias\", previously specified in BS 5497-1,BS 5497-1: \"Precision of test methods. Guide for the determination of repeatability and reproducibility for a standard test method.\" (1979) because it has different connotations outside the fields of science and engineering, as in medicine and law. Quantification and applications In industrial instrumentation, accuracy is the measurement tolerance, or transmission of the instrument and defines the limits of the errors made when the instrument is used in normal operating conditions.Creus, Antonio. Instrumentación Industrial Ideally a measurement device is both accurate and precise, with measurements all close to and tightly clustered around the true value. The accuracy and precision of a measurement process is usually established by repeatedly measuring some traceable reference standard. Such standards are defined in the International System of Units (abbreviated SI from French: Système international d'unités) and maintained by national standards organizations such as the National Institute of Standards and Technology in the United States."
            },
            {
                "text": "This also applies when measurements are repeated and averaged. In that case, the term standard error is properly applied: the precision of the average is equal to the known standard deviation of the process divided by the square root of the number of measurements averaged. Further, the central limit theorem shows that the probability distribution of the averaged measurements will be closer to a normal distribution than that of individual measurements. With regard to accuracy we can distinguish: the difference between the mean of the measurements and the reference value, the bias. Establishing and correcting for bias is necessary for calibration. the combined effect of that and precision. A common convention in science and engineering is to express accuracy and/or precision implicitly by means of significant figures. Where not explicitly stated, the margin of error is understood to be one-half the value of the last significant place. For instance, a recording of 843.6 m, or 843.0 m, or 800.0 m would imply a margin of 0.05 m (the last significant place is the tenths place), while a recording of 843 m would imply a margin of error of 0.5 m (the last significant digits are the units)."
            },
            {
                "text": "A reading of 8,000 m, with trailing zeros and no decimal point, is ambiguous; the trailing zeros may or may not be intended as significant figures. To avoid this ambiguity, the number could be represented in scientific notation: 8.0 × 103 m indicates that the first zero is significant (hence a margin of 50 m) while 8.000 × 103 m indicates that all three zeros are significant, giving a margin of 0.5 m. Similarly, one can use a multiple of the basic measurement unit: 8.0 km is equivalent to 8.0 × 103 m. It indicates a margin of 0.05 km (50 m). However, reliance on this convention can lead to false precision errors when accepting data from sources that do not obey it. For example, a source reporting a number like 153,753 with precision +/- 5,000 looks like it has precision +/- 0.5. Under the convention it would have been rounded to 150,000. Alternatively, in a scientific context, if it is desired to indicate the margin of error with more precision, one can use a notation such as 7.54398(23) × 10−10 m, meaning a range of between 7.54375 and 7.54421 × 10−10 m. Precision includes: repeatability — the variation arising when all efforts are made to keep conditions constant by using the same instrument and operator, and repeating during a short time period; and reproducibility — the variation arising using the same measurement process among different instruments and operators, and over longer time periods."
            },
            {
                "text": "In engineering, precision is often taken as three times Standard Deviation of measurements taken, representing the range that 99.73% of measurements can occur within. For example, an ergonomist measuring the human body can be confident that 99.73% of their extracted measurements fall within ± 0.7 cm - if using the GRYPHON processing system - or ± 13 cm - if using unprocessed data. In classification In binary classification Accuracy is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition. That is, the accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. As such, it compares estimates of pre- and post-test probability. To make the context clear by the semantics, it is often referred to as the \"Rand accuracy\" or \"Rand index\". It is a parameter of the test. The formula for quantifying binary accuracy is: where $1=TP = True positive$; $1=FP = False positive$; $1=TN = True negative$; $1=FN = False negative$ In this context, the concepts of trueness and precision as defined by ISO 5725-1 are not applicable."
            },
            {
                "text": "One reason is that there is not a single “true value” of a quantity, but rather two possible true values for every case, while accuracy is an average across all cases and therefore takes into account both values. However, the term precision is used in this context to mean a different metric originating from the field of information retrieval (see below). In multiclass classification When computing accuracy in multiclass classification, accuracy is simply the fraction of correct classifications: This is usually expressed as a percentage. For example, if a classifier makes ten predictions and nine of them are correct, the accuracy is 90%. Accuracy is sometimes also viewed as a micro metric, to underline that it tends to be greatly affected by the particular class prevalence in a dataset and the classifier's biases. Furthermore, it is also called top-1 accuracy to distinguish it from top-5 accuracy, common in convolutional neural network evaluation. To evaluate top-5 accuracy, the classifier must provide relative likelihoods for each class. When these are sorted, a classification is considered correct if the correct classification falls anywhere within the top 5 predictions made by the network."
            },
            {
                "text": "Top-5 accuracy was popularized by the ImageNet challenge. It is usually higher than top-1 accuracy, as any correct predictions in the 2nd through 5th positions will not improve the top-1 score, but do improve the top-5 score. In psychometrics and psychophysics In psychometrics and psychophysics, the term accuracy is interchangeably used with validity and constant error. Precision is a synonym for reliability and variable error. The validity of a measurement instrument or psychological test is established through experiment or correlation with behavior. Reliability is established with a variety of statistical techniques, classically through an internal consistency test like Cronbach's alpha to ensure sets of related questions have related responses, and then comparison of those related question between reference and target population. In logic simulation thumb|Comparative waveforms for logic values, circuit voltages, and measure voltages In logic simulation, a common mistake in evaluation of accurate models is to compare a logic simulation model to a transistor circuit simulation model. This is a comparison of differences in precision, not accuracy. Precision is measured with respect to detail and accuracy is measured with respect to reality."
            },
            {
                "text": "In information systems Information retrieval systems, such as databases and web search engines, are evaluated by many different metrics, some of which are derived from the confusion matrix, which divides results into true positives (documents correctly retrieved), true negatives (documents correctly not retrieved), false positives (documents incorrectly retrieved), and false negatives (documents incorrectly not retrieved). Commonly used metrics include the notions of precision and recall. In this context, precision is defined as the fraction of documents correctly retrieved compared to the documents retrieved (true positives divided by true positives plus false positives), using a set of ground truth relevant results selected by humans. Recall is defined as the fraction of documents correctly retrieved compared to the relevant documents (true positives divided by true positives plus false negatives). Less commonly, the metric of accuracy is used, is defined as the fraction of documents correctly classified compared to the documents (true positives plus true negatives divided by true positives plus true negatives plus false positives plus false negatives). None of these metrics take into account the ranking of results."
            },
            {
                "text": "Ranking is very important for web search engines because readers seldom go past the first page of results, and there are too many documents on the web to manually classify all of them as to whether they should be included or excluded from a given search. Adding a cutoff at a particular number of results takes ranking into account to some degree. The measure precision at k, for example, is a measure of precision looking only at the top ten (k=10) search results. More sophisticated metrics, such as discounted cumulative gain, take into account each individual ranking, and are more commonly used where this is important. In cognitive systems In cognitive systems, accuracy and precision is used to characterize and measure results of a cognitive process performed by biological or artificial entities where a cognitive process is a transformation of data, information, knowledge, or wisdom to a higher-valued form. (DIKW Pyramid) Sometimes, a cognitive process produces exactly the intended or desired output but sometimes produces output far from the intended or desired. Furthermore, repetitions of a cognitive process do not always produce the same output."
            },
            {
                "text": "Cognitive accuracy (CA) is the propensity of a cognitive process to produce the intended or desired output. Cognitive precision (CP) is the propensity of a cognitive process to produce the same output. To measure augmented cognition in human/cog ensembles, where one or more humans work collaboratively with one or more cognitive systems (cogs), increases in cognitive accuracy and cognitive precision assist in measuring the degree of cognitive augmentation. See also Bias–variance tradeoff in statistics and machine learning Accepted and experimental value Data quality Engineering tolerance Exactness (disambiguation) Experimental uncertainty analysis F-score Floating-point arithmetic (section Accuracy problems) Hypothesis tests for accuracy Information quality Measurement uncertainty Precision (statistics) Probability Random and systematic errors Sensitivity and specificity Significant figures Statistical significance References External links BIPM - Guides in metrology, Guide to the Expression of Uncertainty in Measurement (GUM) and International Vocabulary of Metrology (VIM) \"Beyond NIST Traceability: What really creates accuracy\", Controlled Environments magazine Precision and Accuracy with Three Psychophysical Methods Appendix D.1: Terminology, Guidelines for Evaluating and Expressing the Uncertainty of NIST Measurement Results Accuracy and Precision Accuracy vs Precision — a brief video by Matt Parker What's the difference between accuracy and precision? by Matt Anticole at TED-Ed Category:Biostatistics Category:Metrology Category:Psychometrics Category:ISO standards Category:Software quality"
            }
        ],
        "latex_formulas": [
            "TP = True positive",
            "FP = False positive",
            "TN = True negative",
            "FN = False negative"
        ]
    },
    "Time_complexity": {
        "title": "Time_complexity",
        "chunks": [
            {
                "text": "thumb|Graphs of functions commonly used in the analysis of algorithms, showing the number of operations N as the result of input size n for each function In theoretical computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a constant factor. Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the worst-case time complexity, which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the average-case complexity, which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a function of the size of the input."
            },
            {
                "text": "Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the asymptotic behavior of the complexity. Therefore, the time complexity is commonly expressed using big O notation, typically etc., where is the size in units of bits needed to represent the input. Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity $O(n)$ is a linear time algorithm and an algorithm with time complexity $O(n^\\alpha)$ for some constant $\\alpha > 0$ is a polynomial time algorithm. Table of common time complexities The following table summarizes some classes of commonly encountered time complexities. In the table, , i.e., polynomial in x. Name Complexity class Time complexity Examples of running times Example algorithms constant time $O(1)$ 10 Finding the median value in a sorted array of numbers. Calculating $(−1)."
            },
            {
                "text": "inverse Ackermann time $O\\bigl(\\alpha(n)\\bigr)$ Amortized time per operation using a disjoint set iterated logarithmic time $O(\\log^*n)$ Distributed coloring of cycles log-logarithmic $O(\\log\\log n)$ Amortized time per operation using a bounded priority queue logarithmic time DLOGTIME $O(\\log n)$ $\\log n$, $\\log (n^2)$ Binary search polylogarithmic time $\\text{poly}(\\log n)$ $(\\log n)^2$ fractional power $O(n^c)$ where $0<c<1$ $n^{\\frac{1}{2}}$, $n^{\\frac{2}{3}}$ Range searching in a k-d tree linear time $O(n)$ , $2n+5$ Finding the smallest or largest item in an unsorted array. Kadane's algorithm. Linear search. \"n log-star n\" time $O(n\\log^*n)$ Seidel's polygon triangulation algorithm. linearithmic time $O(n\\log n)$ $n\\log n$, $\\log n!$ Fastest possible comparison sort."
            },
            {
                "text": "Fast Fourier transform. quasilinear time $n\\text{poly}(\\log n)$ $n\\log^2 n$ Multipoint polynomial evaluation quadratic time $O(n^2)$ $n^2$ Bubble sort. Insertion sort. Direct convolution cubic time $O(n^3)$ $n^3$ Naive multiplication of two $n\\times n$ matrices. Calculating partial correlation. polynomial time P $2^{O(\\log n)}=\\text{poly}(n)$ $n^2+n$, $n^{10}$ Karmarkar's algorithm for linear programming. AKS primality test quasi-polynomial time QP $2^{\\text{poly}(\\log n)}$ $n^{\\log \\log n}$, $n^{\\log n}$ Best-known $O(log-approximation algorithm for the directed Steiner tree problem, best known parity game solver, best known graph isomorphism algorithm sub-exponential time(first definition) SUBEXP $O(2^{n^\\epsilon})$ for all $0<\\epsilon <1$ Contains BPP unless EXPTIME (see below) equals MA. sub-exponential time(second definition) $2^{o(n)}$ $2^{\\sqrt[3]{n}}$ Best classical algorithm for integer factorization formerly-best algorithm for graph isomorphism exponential time(with linear exponent) E $2^{O(n)}$ $1.1^n$, $10^n$ Solving the traveling salesman problem using dynamic programming factorial time $O(n)!"
            },
            {
                "text": "= 2^{O(n \\log n)} $ || $n!, n^n, 2^{n \\log n}$ || Solving the traveling salesman problem via brute-force search exponential time EXPTIME $2^{\\text{poly}(n)}$ $2^n$, $2^{n^2}$ Solving matrix chain multiplication via brute-force search double exponential time 2-EXPTIME $2^{2^{\\text{poly}(n)}}$ $2^{2^n}$ Deciding the truth of a given statement in Presburger arithmetic Constant time An algorithm is said to be constant time (also written as time) if the value of (the complexity of the algorithm) is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an array takes constant time as only one operation has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each element in the array is needed in order to determine the minimal value."
            },
            {
                "text": "Hence it is a linear time operation, taking time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time. Despite the name \"constant time\", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be independent of the problem size. For example, the task \"exchange the values of and if necessary so that \" is called constant time even though the time may depend on whether or not it is already true that . However, there is some constant such that the time required is always at most . Logarithmic time An algorithm is said to take logarithmic time when $T(n) = O(\\log n)$. Since $\\log_a n$ and $\\log_b n$ are related by a constant multiplier, and such a multiplier is irrelevant to big O classification, the standard usage for logarithmic-time algorithms is $O(\\log n)$ regardless of the base of the logarithm appearing in the expression of ."
            },
            {
                "text": "Algorithms taking logarithmic time are commonly found in operations on binary trees or when using binary search. An $O(\\log n)$ algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size is of the order of . An example of logarithmic time is given by dictionary search. Consider a dictionary $D$ which contains entries, sorted in alphabetical order. We suppose that, for $1 \\le k \\le n$, one may access the th entry of the dictionary in a constant time. Let $D(k)$ denote this th entry. Under these hypotheses, the test to see if a word is in the dictionary may be done in logarithmic time: consider $D\\left(\\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right)$, where $\\lfloor\\;\\rfloor$ denotes the floor function."
            },
            {
                "text": "If $w = D\\left(\\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right)$--that is to say, the word is exactly in the middle of the dictionary--then we are done. Else, if $w < D\\left(\\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right)$--i.e., if the word comes earlier in alphabetical order than the middle word of the whole dictionary--we continue the search in the same way in the left (i.e. earlier) half of the dictionary, and then again repeatedly until the correct word is found. Otherwise, if it comes after the middle word, continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary. As a result, the search space within the dictionary decreases as the algorithm gets closer to the target word. Polylogarithmic time An algorithm is said to run in polylogarithmic time if its time $T(n)$ is $O\\bigl((\\log n)^k\\bigr)$ for some constant ."
            },
            {
                "text": "Another way to write this is $O(\\log^kn)$. For example, matrix chain ordering can be solved in polylogarithmic time on a parallel random-access machine, and a graph can be determined to be planar in a fully dynamic way in $O(\\log^3n)$ time per insert/delete operation. Sub-linear time An algorithm is said to run in sub-linear time (often spelled sublinear time) if $T(n)=o(n)$. In particular this includes algorithms with the time complexities defined above. The specific term sublinear time algorithm commonly refers to randomized algorithms that sample a small fraction of their inputs and process them efficiently to approximately infer properties of the entire instance. This type of sublinear time algorithm is closely related to property testing and statistics. Other settings where algorithms can run in sublinear time include: Parallel algorithms that have linear or greater total work (allowing them to read the entire input), but sub-linear depth. Algorithms that have guaranteed assumptions on the input structure. An important example are operations on data structures, e.g."
            },
            {
                "text": "binary search in a sorted array. Algorithms that search for local structure in the input, for example finding a local minimum in a 1-D array (can be solved in $O(\\log(n))$ time using a variant of binary search). A closely related notion is that of Local Computation Algorithms (LCA) where the algorithm receives a large input and queries to local information about some valid large output. Linear time An algorithm is said to take linear time, or $O(n)$ time, if its time complexity is $O(n)$. Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant such that the running time is at most $cn$ for every input of size . For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant. Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input."
            },
            {
                "text": "Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit parallelism to provide this. An example is content-addressable memory. This concept of linear time is used in string matching algorithms such as the Boyer–Moore string-search algorithm and Ukkonen's algorithm. Quasilinear time An algorithm is said to run in quasilinear time (also referred to as log-linear time) if $T(n)=O(n\\log^kn)$ for some positive constant ; linearithmic time is the case $k=1$. Using soft O notation these algorithms are $\\tilde{O}(n)$. Quasilinear time algorithms are also $O(n^{1+\\varepsilon })$ for every constant $\\varepsilon >0$ and thus run faster than any polynomial time algorithm whose time bound includes a term $n^c$ for any $c>1$. Algorithms which run in quasilinear time include: In-place merge sort, $O(n\\log^2n)$ Quicksort, $O(n\\log n)$, in its randomized version, has a running time that is $O(n\\log n)$ in expectation on the worst-case input."
            },
            {
                "text": "Its non-randomized version has an $O(n\\log n)$ running time only when considering average case complexity. Heapsort, $O(n\\log n)$, merge sort, introsort, binary tree sort, smoothsort, patience sorting, etc. in the worst case Fast Fourier transforms, $O(n\\log n)$ Monge array calculation, $O(n\\log n)$ In many cases, the $O(n\\log n)$ running time is simply the result of performing a $\\Theta (\\log n)$ operation times (for the notation, see ). For example, binary tree sort creates a binary tree by inserting each element of the -sized array one by one. Since the insert operation on a self-balancing binary search tree takes $O(\\log n)$ time, the entire algorithm takes $O(n\\log n)$ time. Comparison sorts require at least $\\Omega (n\\log n)$ comparisons in the worst case because $\\log (n! )=\\Theta (n\\log n)$, by Stirling's approximation."
            },
            {
                "text": "They also frequently arise from the recurrence relation . Sub-quadratic time An algorithm is said to be subquadratic time if $T(n)=o(n^2)$. For example, simple, comparison-based sorting algorithms are quadratic (e.g. insertion sort), but more advanced algorithms can be found that are subquadratic (e.g. shell sort). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance. Polynomial time An algorithm is said to be of polynomial time if its running time is upper bounded by a polynomial expression in the size of the input for the algorithm, that is, for some positive constant k. Problems for which a deterministic polynomial-time algorithm exists belong to the complexity class P, which is central in the field of computational complexity theory. Cobham's thesis states that polynomial time is a synonym for \"tractable\", \"feasible\", \"efficient\", or \"fast\". Some examples of polynomial-time algorithms: The selection sort sorting algorithm on n integers performs $An^2$ operations for some constant A."
            },
            {
                "text": "Thus it runs in time $O(n^2)$ and is a polynomial-time algorithm. All the basic arithmetic operations (addition, subtraction, multiplication, division, and comparison) can be done in polynomial time. Maximum matchings in graphs can be found in polynomial time. In some contexts, especially in optimization, one differentiates between strongly polynomial time and weakly polynomial time algorithms. These two concepts are only relevant if the inputs to the algorithms consist of integers. Complexity classes The concept of polynomial time leads to several complexity classes in computational complexity theory. Some important classes defined using polynomial time are the following. P: The complexity class of decision problems that can be solved on a deterministic Turing machine in polynomial time NP: The complexity class of decision problems that can be solved on a non-deterministic Turing machine in polynomial time ZPP: The complexity class of decision problems that can be solved with zero error on a probabilistic Turing machine in polynomial time RP: The complexity class of decision problems that can be solved with 1-sided error on a probabilistic Turing machine in polynomial time."
            },
            {
                "text": "BPP: The complexity class of decision problems that can be solved with 2-sided error on a probabilistic Turing machine in polynomial time BQP: The complexity class of decision problems that can be solved with 2-sided error on a quantum Turing machine in polynomial time P is the smallest time-complexity class on a deterministic machine which is robust in terms of machine model changes. (For example, a change from a single-tape Turing machine to a multi-tape machine can lead to a quadratic speedup, but any algorithm that runs in polynomial time under one model also does so on the other.) Any given abstract machine will have a complexity class corresponding to the problems which can be solved in polynomial time on that machine. Superpolynomial time An algorithm is defined to take superpolynomial time if T(n) is not bounded above by any polynomial. Using little omega notation, it is ω(nc) time for all constants c, where n is the input parameter, typically the number of bits in the input. For example, an algorithm that runs for 2n steps on an input of size n requires superpolynomial time (more specifically, exponential time)."
            },
            {
                "text": "An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the Adleman–Pomerance–Rumely primality test runs for time on n-bit inputs; this grows faster than any polynomial for large enough n, but the input size must become impractically large before it cannot be dominated by a polynomial with small degree. An algorithm that requires superpolynomial time lies outside the complexity class P. Cobham's thesis posits that these algorithms are impractical, and in many cases they are. Since the P versus NP problem is unresolved, it is unknown whether NP-complete problems require superpolynomial time. Quasi-polynomial time Quasi-polynomial time algorithms are algorithms whose running time exhibits quasi-polynomial growth, a type of behavior that may be slower than polynomial time but yet is significantly faster than exponential time. The worst case running time of a quasi-polynomial time algorithm is $2^{O(\\log^c n)}$ for some fixed When $c=1$ this gives polynomial time, and for $c < 1$ it gives sub-linear time. There are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known."
            },
            {
                "text": "Such problems arise in approximation algorithms; a famous example is the directed Steiner tree problem, for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of $O(\\log^3 n)$ (n being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem. Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the planted clique problem in which the goal is to find a large clique in the union of a clique and a random graph. Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a computational hardness assumption to prove the difficulty of several other problems in computational game theory, property testing, and machine learning. The complexity class QP consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of DTIME as follows. $\\mbox{QP} = \\bigcup_{c \\in \\mathbb{N}} \\mbox{DTIME} \\left(2^{\\log^c n}\\right)$ Relation to NP-complete problems In complexity theory, the unsolved P versus NP problem asks if all problems in NP have polynomial-time algorithms."
            },
            {
                "text": "All the best-known algorithms for NP-complete problems like 3SAT etc. take exponential time. Indeed, it is conjectured for many natural NP-complete problems that they do not have sub-exponential time algorithms. Here \"sub-exponential time\" is taken to mean the second definition presented below. (On the other hand, many graph problems represented in the natural way by adjacency matrices are solvable in subexponential time simply because the size of the input is the square of the number of vertices.) This conjecture (for the k-SAT problem) is known as the exponential time hypothesis. Since it is conjectured that NP-complete problems do not have quasi-polynomial time algorithms, some inapproximability results in the field of approximation algorithms make the assumption that NP-complete problems do not have quasi-polynomial time algorithms. For example, see the known inapproximability results for the set cover problem. Sub-exponential time The term sub-exponential time is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms."
            },
            {
                "text": "The precise definition of \"sub-exponential\" is not generally agreed upon, however the two most widely used are below. First definition A problem is said to be sub-exponential time solvable if it can be solved in running times whose logarithms grow smaller than any given polynomial. More precisely, a problem is in sub-exponential time if for every there exists an algorithm which solves the problem in time O(2nε). The set of all such problems is the complexity class SUBEXP which can be defined in terms of DTIME as follows. $\\text{SUBEXP}=\\bigcap_{\\varepsilon>0} \\text{DTIME}\\left(2^{n^\\varepsilon}\\right)$ This notion of sub-exponential is non-uniform in terms of ε in the sense that ε is not part of the input and each ε may have its own algorithm for the problem. Second definition Some authors define sub-exponential time as running times in $2^{o(n)}$. This definition allows larger running times than the first definition of sub-exponential time. An example of such a sub-exponential time algorithm is the best-known classical algorithm for integer factorization, the general number field sieve, which runs in time about where the length of the input is ."
            },
            {
                "text": "Another example was the graph isomorphism problem, which the best known algorithm from 1982 to 2016 solved in However, at STOC 2016 a quasi-polynomial time algorithm was presented. It makes a difference whether the algorithm is allowed to be sub-exponential in the size of the instance, the number of vertices, or the number of edges. In parameterized complexity, this difference is made explicit by considering pairs $(L,k)$ of decision problems and parameters k. SUBEPT is the class of all parameterized problems that run in time sub-exponential in k and polynomial in the input size n: $\\text{SUBEPT}=\\text{DTIME}\\left(2^{o(k)} \\cdot \\text{poly}(n)\\right).$ More precisely, SUBEPT is the class of all parameterized problems $(L,k)$ for which there is a computable function $f : \\N \\to \\N$ with $f \\in o(k)$ and an algorithm that decides L in time $2^{f(k)} \\cdot \\text{poly}(n)$."
            },
            {
                "text": "Exponential time hypothesis The exponential time hypothesis (ETH) is that 3SAT, the satisfiability problem of Boolean formulas in conjunctive normal form with at most three literals per clause and with n variables, cannot be solved in time 2o(n). More precisely, the hypothesis is that there is some absolute constant $c > 0$ such that 3SAT cannot be decided in time 2cn by any deterministic Turing machine. With m denoting the number of clauses, ETH is equivalent to the hypothesis that kSAT cannot be solved in time 2o(m) for any integer $k ≥ 3$. The exponential time hypothesis implies P ≠ NP. Exponential time An algorithm is said to be exponential time, if T(n) is upper bounded by 2poly(n), where poly(n) is some polynomial in n. More formally, an algorithm is exponential time if T(n) is bounded by O(2nk) for some constant k. Problems which admit exponential time algorithms on a deterministic Turing machine form the complexity class known as EXP."
            },
            {
                "text": "$\\text{EXP} = \\bigcup_{c \\in \\mathbb{R_+}} \\text{DTIME}\\left(2^{n^c}\\right)$ Sometimes, exponential time is used to refer to algorithms that have T(n) = 2O(n), where the exponent is at most a linear function of n. This gives rise to the complexity class E. $\\text{E} = \\bigcup_{c \\in \\mathbb{N}} \\text{DTIME}\\left(2^{cn}\\right)$ Factorial time An algorithm is said to be factorial time if T(n) is upper bounded by the factorial function n!. Factorial time is a subset of exponential time (EXP) because $ n! \\leq n^n = 2^{n \\log n}= O \\left(2^{n^{1 + \\epsilon}} \\right)$ for all $\\epsilon > 0$. However, it is not a subset of E. An example of an algorithm that runs in factorial time is bogosort, a notoriously inefficient sorting algorithm based on trial and error."
            },
            {
                "text": "Bogosort sorts a list of n items by repeatedly shuffling the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the n! orderings of the n items. If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the infinite monkey theorem. Double exponential time An algorithm is said to be double exponential time if T(n) is upper bounded by 22poly(n), where poly(n) is some polynomial in n. Such algorithms belong to the complexity class 2-EXPTIME. $\\mbox{2-EXPTIME} = \\bigcup_{c \\in \\N} \\mbox{DTIME}\\left( 2^{2^{n^c}}\\right)$ Well-known double exponential time algorithms include: Decision procedures for Presburger arithmetic Computing a Gröbner basis (in the worst case) Quantifier elimination on real closed fields takes at least double exponential time, and can be done in this time. See also L-notation Space complexity References Category:Analysis of algorithms Category:Computational complexity theory Category:Computational resources Category:Time"
            }
        ],
        "latex_formulas": [
            "''O''(''n'')",
            "(−1){{sup|''n''}}",
            "''O''(log log ''N'')",
            "''O''(''n'')",
            "''O''(log{{sup|2}}''n'')",
            "''D''",
            "''c'' > 0",
            "''k'' ≥ 3",
            "O(n)",
            "O(n\\log n)",
            "O(n^\\alpha)",
            "O(2^n)",
            "O(n)",
            "O(n^\\alpha)",
            "\\alpha > 0",
            "O(1)",
            "O\\bigl(\\alpha(n)\\bigr)",
            "O(\\log^*n)",
            "O(\\log\\log n)",
            "O(\\log n)",
            "\\log n",
            "\\log (n^2)",
            "\\text{poly}(\\log n)",
            "(\\log n)^2",
            "O(n^c)",
            "0<c<1",
            "n^{\\frac{1}{2}}",
            "n^{\\frac{2}{3}}",
            "O(n)",
            "2n+5",
            "O(n\\log^*n)",
            "O(n\\log n)",
            "n\\log n",
            "\\log n!",
            "n\\text{poly}(\\log n)",
            "n\\log^2 n",
            "O(n^2)",
            "n^2",
            "O(n^3)",
            "n^3",
            "n\\times n",
            "2^{O(\\log n)}=\\text{poly}(n)",
            "n^2+n",
            "n^{10}",
            "2^{\\text{poly}(\\log n)}",
            "n^{\\log \\log n}",
            "n^{\\log n}",
            "O(2^{n^\\epsilon})",
            "0<\\epsilon <1",
            "2^{o(n)}",
            "2^{\\sqrt[3]{n}}",
            "2^{O(n)}",
            "1.1^n",
            "10^n",
            "O(n)! = 2^{O(n \\log n)}",
            "n!, n^n, 2^{n \\log n}",
            "2^{\\text{poly}(n)}",
            "2^n",
            "2^{n^2}",
            "2^{2^{\\text{poly}(n)}}",
            "2^{2^n}",
            "T(n) = O(\\log n)",
            "\\log_a n",
            "\\log_b n",
            "O(\\log n)",
            "O(\\log n)",
            "1 \\le k \\le n",
            "D(k)",
            "D\\left(\\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right)",
            "\\lfloor\\;\\rfloor",
            "w = D\\left(\\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right)",
            "w < D\\left(\\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right)",
            "T(n)",
            "O\\bigl((\\log n)^k\\bigr)",
            "O(\\log^kn)",
            "O(\\log^3n)",
            "T(n)=o(n)",
            "O(\\log(n))",
            "O(n)",
            "O(n)",
            "cn",
            "T(n)=O(n\\log^kn)",
            "k=1",
            "\\tilde{O}(n)",
            "O(n^{1+\\varepsilon })",
            "\\varepsilon >0",
            "n^c",
            "c>1",
            "O(n\\log^2n)",
            "O(n\\log n)",
            "O(n\\log n)",
            "O(n\\log n)",
            "O(n\\log n)",
            "O(n\\log n)",
            "O(n\\log n)",
            "O(n\\log n)",
            "\\Theta (\\log n)",
            "O(\\log n)",
            "O(n\\log n)",
            "\\Omega (n\\log n)",
            "\\log (n!)=\\Theta (n\\log n)",
            "T(n)=o(n^2)",
            "An^2",
            "O(n^2)",
            "2^{O(\\log^c n)}",
            "c > 0",
            "c=1",
            "c < 1",
            "O(\\log^3 n)",
            "\\mbox{QP} = \\bigcup_{c \\in \\mathbb{N}} \\mbox{DTIME} \\left(2^{\\log^c n}\\right)",
            "\\text{SUBEXP}=\\bigcap_{\\varepsilon>0} \\text{DTIME}\\left(2^{n^\\varepsilon}\\right)",
            "2^{o(n)}",
            "2^{\\tilde{O}(n^{1/3})}",
            "2^{O\\left(\\sqrt{n \\log n}\\right)}",
            "(L,k)",
            "\\text{SUBEPT}=\\text{DTIME}\\left(2^{o(k)} \\cdot \\text{poly}(n)\\right).",
            "(L,k)",
            "f : \\N \\to \\N",
            "f \\in o(k)",
            "2^{f(k)} \\cdot \\text{poly}(n)",
            "\\text{EXP} = \\bigcup_{c \\in \\mathbb{R_+}} \\text{DTIME}\\left(2^{n^c}\\right)",
            "\\text{E} = \\bigcup_{c \\in \\mathbb{N}} \\text{DTIME}\\left(2^{cn}\\right)",
            "n! \\leq n^n = 2^{n \\log n}= O \\left(2^{n^{1 + \\epsilon}} \\right)",
            "\\epsilon > 0",
            "\\mbox{2-EXPTIME} = \\bigcup_{c \\in \\N} \\mbox{DTIME}\\left( 2^{2^{n^c}}\\right)"
        ]
    },
    "Logit": {
        "title": "Logit",
        "chunks": [
            {
                "text": "thumbnail|upright=1.3|Plot of logit(x) in the domain of 0 to 1, where the base of the logarithm is e. In statistics, the logit ( ) function is the quantile function associated with the standard logistic distribution. It has many uses in data analysis and machine learning, especially in data transformations. Mathematically, the logit is the inverse of the standard logistic function $\\sigma(x) = 1/(1+e^{-x})$, so the logit is defined as $\\operatorname{logit} p = \\sigma^{-1}(p) = \\ln \\frac{p}{1-p} \\quad \\text{for} \\quad p \\in (0,1).$ Because of this, the logit is also called the log-odds since it is equal to the logarithm of the odds $\\frac{p}{1-p}$ where is a probability. Thus, the logit is a type of function that maps probability values from $(0, 1)$ to real numbers in $(-\\infty, +\\infty)$, akin to the probit function."
            },
            {
                "text": "Definition If is a probability, then $ is the corresponding odds; the $logit$ of the probability is the logarithm of the odds, i.e. : $\\operatorname{logit}(p)=\\ln\\left( \\frac{p}{1-p} \\right) =\\ln(p)-\\ln(1-p)=-\\ln\\left( \\frac{1}{p}-1\\right)=2\\operatorname{atanh}(2p-1). $ The base of the logarithm function used is of little importance in the present article, as long as it is greater than 1, but the natural logarithm with base is the one most often used. The choice of base corresponds to the choice of logarithmic unit for the value: base 2 corresponds to a shannon, base to a nat, and base 10 to a hartley; these units are particularly used in information-theoretic interpretations. For each choice of base, the logit function takes values between negative and positive infinity."
            },
            {
                "text": "The “logistic” function of any number $\\alpha$ is given by the inverse-$logit$: $\\operatorname{logit}^{-1}(\\alpha) = \\operatorname{logistic}(\\alpha) = \\frac{1}{1 + \\exp(-\\alpha)} = \\frac{\\exp(\\alpha)}{ \\exp(\\alpha) + 1} = \\frac{\\tanh(\\frac{\\alpha}{2})+1}{2}$ The difference between the $logit$s of two probabilities is the logarithm of the odds ratio (), thus providing a shorthand for writing the correct combination of odds ratios only by adding and subtracting: $\\ln(R)=\\ln\\left( \\frac{p_1/(1-p_1)}{p_2/(1-p_2)} \\right) =\\ln\\left( \\frac{p_1}{1-p_1} \\right) - \\ln\\left(\\frac{p_2}{1-p_2}\\right) = \\operatorname{logit}(p_1)-\\operatorname{logit}(p_2)\\,.$ The Taylor series for the logit function is given by: $\\operatorname{logit}(x)=2\\sum_{n=0}^\\infty \\frac{(2x-1)^{2n+1}}{2n+1}.$ History Several approaches have been explored to adapt linear regression methods to a domain where the output is a probability value $(0, 1)$, instead of any real number $(-\\infty, +\\infty)$."
            },
            {
                "text": "In many cases, such efforts have focused on modeling this problem by mapping the range $(0, 1)$ to $(-\\infty, +\\infty)$ and then running the linear regression on these transformed values. In 1934, Chester Ittner Bliss used the cumulative normal distribution function to perform this mapping and called his model probit, an abbreviation for \"probability unit\". This is, however, computationally more expensive. In 1944, Joseph Berkson used log of odds and called this function logit, an abbreviation for \"logistic unit\", following the analogy for probit: Log odds was used extensively by Charles Sanders Peirce (late 19th century). G. A. Barnard in 1949 coined the commonly used term log-odds;. the log-odds of an event is the logit of the probability of the event.. Barnard also coined the term lods as an abstract form of \"log-odds\", but suggested that \"in practice the term 'odds' should normally be used, since this is more familiar in everyday life\". Uses and properties The logit in logistic regression is a special case of a link function in a generalized linear model: it is the canonical link function for the Bernoulli distribution."
            },
            {
                "text": "More abstractly, the logit is the natural parameter for the binomial distribution; see . The logit function is the negative of the derivative of the binary entropy function. The logit is also central to the probabilistic Rasch model for measurement, which has applications in psychological and educational assessment, among other areas. The inverse-logit function (i.e., the logistic function) is also sometimes referred to as the expit function. In plant disease epidemiology, the logistic, Gompertz, and monomolecular models are collectively known as the Richards family models. The log-odds function of probabilities is often used in state estimation algorithms because of its numerical advantages in the case of small probabilities. Instead of multiplying very small floating point numbers, log-odds probabilities can just be summed up to calculate the (log-odds) joint probability. Comparison with probit right|300px|thumb|Comparison of the logit function with a scaled probit (i.e. the inverse CDF of the normal distribution), comparing $\\operatorname{logit}(x)$ vs. $\\tfrac{\\Phi^{-1}(x)}{\\,\\sqrt{\\pi/8\\,}\\,}$, which makes the slopes the same at the -origin."
            },
            {
                "text": "Closely related to the $logit$ function (and logit model) are the probit function and probit model. The $logit$ and $probit$ are both sigmoid functions with a domain between 0 and 1, which makes them both quantile functions – i.e., inverses of the cumulative distribution function (CDF) of a probability distribution. In fact, the $logit$ is the quantile function of the logistic distribution, while the $probit$ is the quantile function of the normal distribution. The $probit$ function is denoted $\\Phi^{-1}(x)$, where $\\Phi(x)$ is the CDF of the standard normal distribution, as just mentioned: $\\Phi(x) = \\frac 1 {\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-y^2/2} dy.$ As shown in the graph on the right, the $logit$ and $probit$ functions are extremely similar when the $probit$ function is scaled, so that its slope at $y matches the slope of the $logit$. As a result, probit models are sometimes used in place of logit models because for certain applications (e.g., in item response theory) the implementation is easier."
            },
            {
                "text": "See also Sigmoid function, inverse of the logit function Discrete choice on binary logit, multinomial logit, conditional logit, nested logit, mixed logit, exploded logit, and ordered logit Limited dependent variable Logit analysis in marketing Multinomial logit Ogee, curve with similar shape Perceptron Probit, another function with the same domain and range as the logit Ridit scoring Data transformation (statistics) Arcsin (transformation) Rasch model References External links Which Link Function — Logit, Probit, or Cloglog? 12.04.2023 Further reading Category:Logarithms Category:Special functions"
            }
        ],
        "latex_formulas": [
            "{{nowrap|''p''/(1 &minus; ''p'')}}",
            "logit",
            "logit",
            "logit",
            "logit",
            "logit",
            "probit",
            "logit",
            "probit",
            "probit",
            "logit",
            "probit",
            "probit",
            "''y'' {{=}} 0",
            "logit",
            "\\sigma(x) = 1/(1+e^{-x})",
            "\\operatorname{logit} p = \\sigma^{-1}(p) = \\ln \\frac{p}{1-p} \\quad \\text{for} \\quad p \\in (0,1).",
            "\\frac{p}{1-p}",
            "(0, 1)",
            "(-\\infty, +\\infty)",
            "\\operatorname{logit}(p)=\\ln\\left( \\frac{p}{1-p} \\right) =\\ln(p)-\\ln(1-p)=-\\ln\\left( \\frac{1}{p}-1\\right)=2\\operatorname{atanh}(2p-1).",
            "\\alpha",
            "\\operatorname{logit}^{-1}(\\alpha) = \\operatorname{logistic}(\\alpha) = \\frac{1}{1 + \\exp(-\\alpha)} = \\frac{\\exp(\\alpha)}{ \\exp(\\alpha) + 1} = \\frac{\\tanh(\\frac{\\alpha}{2})+1}{2}",
            "\\ln(R)=\\ln\\left( \\frac{p_1/(1-p_1)}{p_2/(1-p_2)} \\right) =\\ln\\left( \\frac{p_1}{1-p_1} \\right) - \\ln\\left(\\frac{p_2}{1-p_2}\\right) = \\operatorname{logit}(p_1)-\\operatorname{logit}(p_2)\\,.",
            "\\operatorname{logit}(x)=2\\sum_{n=0}^\\infty \\frac{(2x-1)^{2n+1}}{2n+1}.",
            "(0, 1)",
            "(-\\infty, +\\infty)",
            "(0, 1)",
            "(-\\infty, +\\infty)",
            "\\ln p/q",
            "\\operatorname{logit}(x)",
            "\\tfrac{\\Phi^{-1}(x)}{\\,\\sqrt{\\pi/8\\,}\\,}",
            "\\Phi^{-1}(x)",
            "\\Phi(x)",
            "\\Phi(x) = \\frac 1 {\\sqrt{2\\pi}}\\int_{-\\infty}^x  e^{-y^2/2} dy."
        ]
    },
    "Cross_entropy": {
        "title": "Cross_entropy",
        "chunks": [
            {
                "text": "REDIRECT Cross-entropy"
            }
        ],
        "latex_formulas": []
    },
    "Overfitting": {
        "title": "Overfitting",
        "chunks": [
            {
                "text": "Figure 1. The green line represents an overfitted model and the black line represents a regularized model. While the green line best follows the training data, it is too dependent on that data and is likely to have a higher error rate on new unseen data, illustrated by black-outlined dots, compared to the black line. Figure 2. Noisy (roughly linear) data is fitted to a linear function and a polynomial function. Although the polynomial function is a perfect fit, the linear function can be expected to generalize better: If the two functions were used to extrapolate beyond the fitted data, the linear function should make better predictions. Figure 3. The blue dashed line represents an underfitted model. A straight line can never fit a parabola. This model is too simple. In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\".Definition of \"overfitting\" at OxfordDictionaries.com: this definition is specifically for statistics."
            },
            {
                "text": "An overfitted model is a mathematical model that contains more parameters than can be justified by the data. In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure. Underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Underfitting would occur, for example, when fitting a linear model to nonlinear data. Such a model will tend to have poor predictive performance. The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; overfitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend."
            },
            {
                "text": "As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. Overfitting is directly related to approximation error of the selected function class and the optimization error of the optimization procedure. A function class that is too large, in a suitable sense, relative to the dataset size is likely to overfit. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new dataset than on the dataset used for fitting (a phenomenon sometimes known as shrinkage).Everitt B.S., Skrondal A. (2010), Cambridge Dictionary of Statistics, Cambridge University Press. In particular, the value of the coefficient of determination will shrink relative to the original data. To lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout)."
            },
            {
                "text": "The basis of some techniques is to either (1) explicitly penalize overly complex models or (2) test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter. Statistical inference In statistics, an inference is drawn from a statistical model, which has been selected via some procedure. Burnham & Anderson, in their much-cited text on model selection, argue that to avoid overfitting, we should adhere to the \"Principle of Parsimony\".. The authors also state the following. Overfitting is more likely to be a serious concern when there is little theory available to guide the analysis, in part because then there tend to be a large number of models to select from. The book Model Selection and Model Averaging (2008) puts it this way.. Regression In regression analysis, overfitting occurs frequently.. As an extreme example, if there are p variables in a linear regression with p data points, the fitted line can go exactly through every point."
            },
            {
                "text": "For logistic regression or Cox proportional hazards models, there are a variety of rules of thumb (e.g. 5–9, 10 and 10–15 — the guideline of 10 observations per independent variable is known as the \"one in ten rule\"). In the process of regression model selection, the mean squared error of the random regression function can be split into random noise, approximation bias, and variance in the estimate of the regression function. The bias–variance tradeoff is often used to overcome overfit models. With a large set of explanatory variables that actually have no relation to the dependent variable being predicted, some variables will in general be falsely found to be statistically significant and the researcher may thus retain them in the model, thereby overfitting the model. This is known as Freedman's paradox. Machine learning Figure 4. Overfitting/overtraining in supervised learning (e.g., a neural network). Training error is shown in blue, and validation error in red, both as a function of the number of training cycles. If the validation error increases (positive slope) while the training error steadily decreases (negative slope), then a situation of overfitting may have occurred."
            },
            {
                "text": "The best predictive and fitted model would be where the validation error has its global minimum. Usually, a learning algorithm is trained using some set of \"training data\": exemplary situations for which the desired output is known. The goal is that the algorithm will also perform well on predicting the output when fed \"validation data\" that was not encountered during its training. Overfitting is the use of models or procedures that violate Occam's razor, for example by including more adjustable parameters than are ultimately optimal, or by using a more complicated approach than is ultimately optimal. For an example where there are too many adjustable parameters, consider a dataset where training data for can be adequately predicted by a linear function of two independent variables. Such a function requires only three parameters (the intercept and two slopes). Replacing this simple function with a new, more complex quadratic function, or with a new, more complex linear function on more than two independent variables, carries a risk: Occam's razor implies that any given complex function is a priori less probable than any given simple function."
            },
            {
                "text": "If the new, more complicated function is selected instead of the simple function, and if there was not a large enough gain in training data fit to offset the complexity increase, then the new complex function \"overfits\" the data and the complex overfitted function will likely perform worse than the simpler function on validation data outside the training dataset, even though the complex function performed as well, or perhaps even better, on the training dataset. When comparing different types of models, complexity cannot be measured solely by counting how many parameters exist in each model; the expressivity of each parameter must be considered as well. For example, it is nontrivial to directly compare the complexity of a neural net (which can track curvilinear relationships) with parameters to a regression model with parameters. Overfitting is especially likely in cases where learning was performed too long or where training examples are rare, causing the learner to adjust to very specific random features of the training data that have no causal relation to the target function. In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse."
            },
            {
                "text": "As a simple example, consider a database of retail purchases that includes the item bought, the purchaser, and the date and time of purchase. It's easy to construct a model that will fit the training set perfectly by using the date and time of purchase to predict the other attributes, but this model will not generalize at all to new data because those past times will never occur again. Generally, a learning algorithm is said to overfit relative to a simpler one if it is more accurate in fitting known data (hindsight) but less accurate in predicting new data (foresight). One can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups: information that is relevant for the future, and irrelevant information (\"noise\"). Everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise exists in past information that needs to be ignored. The problem is determining which part to ignore. A learning algorithm that can reduce the risk of fitting noise is called \"robust.\""
            },
            {
                "text": "Consequences The most obvious consequence of overfitting is poor performance on the validation dataset. Other negative consequences include: A function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function; gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data entry. A more complex, overfitted function is likely to be less portable than a simple one. At one extreme, a one-variable linear regression is so portable that, if necessary, it could even be done by hand. At the other extreme are models that can be reproduced only by exactly duplicating the original modeler's entire setup, making reuse or scientific reproduction difficult. It may be possible to reconstruct details of individual training instances from an overfitted machine learning model's training set. This may be undesirable if, for example, the training data includes sensitive personally identifiable information (PII). This phenomenon also presents problems in the area of artificial intelligence and copyright, with the developers of some generative deep learning models such as Stable Diffusion and GitHub Copilot being sued for copyright infringement because these models have been found to be capable of reproducing certain copyrighted items from their training data."
            },
            {
                "text": "Remedy The optimal function usually needs verification on bigger or completely new datasets. There are, however, methods like minimum spanning tree or life-time of correlation that applies the dependence between correlation coefficients and time-series (window width). Whenever the window width is big enough, the correlation coefficients are stable and don't depend on the window width size anymore. Therefore, a correlation matrix can be created by calculating a coefficient of correlation between investigated variables. This matrix can be represented topologically as a complex network where direct and indirect influences between variables are visualized. Dropout regularisation (random removal of training set data) can also improve robustness and therefore reduce over-fitting by probabilistically removing inputs to a layer. Underfitting Figure 5. The red line represents an underfitted model of the data points represented in blue. We would expect to see a parabola shaped line to represent the curvature of the data points. Figure 6. The blue line represents a fitted model of the data points represented in green. Underfitting is the inverse of overfitting, meaning that the statistical model or machine learning algorithm is too simplistic to accurately capture the patterns in the data."
            },
            {
                "text": "A sign of underfitting is that there is a high bias and low variance detected in the current model or algorithm used (the inverse of overfitting: low bias and high variance). This can be gathered from the Bias-variance tradeoff, which is the method of analyzing a model or algorithm for bias error, variance error, and irreducible error. With a high bias and low variance, the result of the model is that it will inaccurately represent the data points and thus insufficiently be able to predict future data results (see Generalization error). As shown in Figure 5, the linear line could not represent all the given data points due to the line not resembling the curvature of the points. We would expect to see a parabola-shaped line as shown in Figure 6 and Figure 1. If we were to use Figure 5 for analysis, we would get false predictive results contrary to the results if we analyzed Figure 6. Burnham & Anderson state the following. Resolving underfitting There are multiple ways to deal with underfitting: Increase the complexity of the model: If the model is too simple, it may be necessary to increase its complexity by adding more features, increasing the number of parameters, or using a more flexible model."
            },
            {
                "text": "However, this should be done carefully to avoid overfitting. Use a different algorithm: If the current algorithm is not able to capture the patterns in the data, it may be necessary to try a different one. For example, a neural network may be more effective than a linear regression model for some types of data. Increase the amount of training data: If the model is underfitting due to a lack of data, increasing the amount of training data may help. This will allow the model to better capture the underlying patterns in the data. Regularization: Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that discourages large parameter values. It can also be used to prevent underfitting by controlling the complexity of the model. Ensemble Methods: Ensemble methods combine multiple models to create a more accurate prediction. This can help reduce underfitting by allowing multiple models to work together to capture the underlying patterns in the data. Feature engineering: Feature engineering involves creating new model features from the existing ones that may be more relevant to the problem at hand."
            },
            {
                "text": "This can help improve the accuracy of the model and prevent underfitting. Benign overfitting Benign overfitting describes the phenomenon of a statistical model that seems to generalize well to unseen data, even when it has been fit perfectly on noisy training data (i.e., obtains perfect predictive accuracy on the training set). The phenomenon is of particular interest in deep neural networks, but is studied from a theoretical perspective in the context of much simpler models, such as linear regression. In particular, it has been shown that overparameterization is essential for benign overfitting in this setting. In other words, the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size.Bartlett, P.L., Long, P.M., Lugosi, G., & Tsigler, A. (2019). Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117, 30063 - 30070. See also Bias–variance tradeoff Curve fitting Data dredging Feature selection Feature engineering Freedman's paradox Generalization error Goodness of fit Life-time of correlation Model selection Researcher degrees of freedom Occam's razor Primary model Vapnik–Chervonenkis dimension – larger VC dimension implies larger risk of overfitting Notes References Tip 7: Minimize overfitting. Further reading External links The Problem of Overfitting Data – Stony Brook University What is \"overfitting,\" exactly? – Andrew Gelman blog CSE546: Linear Regression Bias / Variance Tradeoff – University of Washington What is Underfitting – IBM Category:Curve fitting Category:Applied mathematics Category:Mathematical modeling Category:Statistical inference Category:Machine learning"
            }
        ],
        "latex_formulas": []
    },
    "Softmax_function": {
        "title": "Softmax_function",
        "chunks": [
            {
                "text": "The softmax function, also known as softargmax or normalized exponential function, converts a vector of real numbers into a probability distribution of possible outcomes. It is a generalization of the logistic function to multiple dimensions, and is used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes. Definition The softmax function takes as input a vector of real numbers, and normalizes it into a probability distribution consisting of probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval $(0, 1)$, and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities."
            },
            {
                "text": "Formally, the standard (unit) softmax function $\\sigma\\colon \\R^K \\to (0, 1)^K$, where $K > 1$, takes a vector $\\mathbf{z} = (z_1, \\dotsc, z_K) \\in \\R^K$ and computes each component of vector $\\sigma(\\mathbf{z}) \\in (0, 1)^K$ with In words, the softmax applies the standard exponential function to each element $z_i$ of the input vector $\\mathbf z$ (consisting of $K$ real numbers), and normalizes these values by dividing by the sum of all these exponentials."
            },
            {
                "text": "The normalization ensures that the sum of the components of the output vector $\\sigma(\\mathbf z)$ is 1. The term \"softmax\" derives from the amplifying effects of the exponential on any maxima in the input vector. For example, the standard softmax of $(1,2,8)$ is approximately $(0.001,0.002,0.997)$, which amounts to assigning almost all of the total unit weight in the result to the position of the vector's maximal element (of 8). In general, instead of a different base $b > 0$ can be used. As above, if $b > 1$ then larger input components will result in larger output probabilities, and increasing the value of will create probability distributions that are more concentrated around the positions of the largest input values. Conversely, if $0 < b < 1$ then smaller input components will result in larger output probabilities, and decreasing the value of will create probability distributions that are more concentrated around the positions of the smallest input values. Writing $b = e^\\beta$ or $b = e^{-\\beta}$ (for real ) yields the expressions: A value proportional to the reciprocal of is sometimes referred to as the temperature: , where is typically 1 or the Boltzmann constant and is the temperature."
            },
            {
                "text": "A higher temperature results in a more uniform output distribution (i.e. with higher entropy; it is \"more random\"), while a lower temperature results in a sharper output distribution, with one value dominating. In some fields, the base is fixed, corresponding to a fixed scale, while in others the parameter (or ) is varied. Interpretations Smooth arg max The Softmax function is a smooth approximation to the arg max function: the function whose value is the index of a vector's largest element. The name \"softmax\" may be misleading. Softmax is not a smooth maximum (that is, a smooth approximation to the maximum function). The term \"softmax\" is also used for the closely related LogSumExp function, which is a smooth maximum. For this reason, some prefer the more accurate term \"softargmax\", though the term \"softmax\" is conventional in machine learning. This section uses the term \"softargmax\" for clarity. Formally, instead of considering the arg max as a function with categorical output $1, \\dots, n$ (corresponding to the index), consider the arg max function with one-hot representation of the output (assuming there is a unique maximum arg): where the output coordinate $y_i = 1$ if and only if $i$ is the arg max of $(z_1, \\dots, z_n)$, meaning $z_i$ is the unique maximum value of $(z_1,\\, \\dots,\\, z_n)$."
            },
            {
                "text": "For example, in this encoding $\\operatorname{arg\\,max}(1, 5, 10) = (0, 0, 1),$ since the third argument is the maximum. This can be generalized to multiple arg max values (multiple equal $z_i$ being the maximum) by dividing the 1 between all max args; formally $1/k$ where is the number of arguments assuming the maximum. For example, $\\operatorname{arg\\,max}(1,\\, 5,\\, 5) = (0,\\, 1/2,\\, 1/2),$ since the second and third argument are both the maximum. In case all arguments are equal, this is simply $\\operatorname{arg\\,max}(z, \\dots, z) = (1/n, \\dots, 1/n).$ Points with multiple arg max values are singular points (or singularities, and form the singular set) – these are the points where arg max is discontinuous (with a jump discontinuity) – while points with a single arg max are known as non-singular or regular points."
            },
            {
                "text": "With the last expression given in the introduction, softargmax is now a smooth approximation of arg max: as , softargmax converges to arg max. There are various notions of convergence of a function; softargmax converges to arg max pointwise, meaning for each fixed input $z$ as , $\\sigma_\\beta(\\mathbf{z}) \\to \\operatorname{arg\\,max}(\\mathbf{z}).$ However, softargmax does not converge uniformly to arg max, meaning intuitively that different points converge at different rates, and may converge arbitrarily slowly. In fact, softargmax is continuous, but arg max is not continuous at the singular set where two coordinates are equal, while the uniform limit of continuous functions is continuous. The reason it fails to converge uniformly is that for inputs where two coordinates are almost equal (and one is the maximum), the arg max is the index of one or the other, so a small change in input yields a large change in output. For example, $\\sigma_\\beta(1,\\, 1.0001) \\to (0, 1),$ but $\\sigma_\\beta(1,\\, 0.9999) \\to (1,\\, 0),$ and $\\sigma_\\beta(1,\\, 1) = 1/2$ for all inputs: the closer the points are to the singular set $(x, x)$, the slower they converge."
            },
            {
                "text": "However, softargmax does converge compactly on the non-singular set. Conversely, as , softargmax converges to arg min in the same way, where here the singular set is points with two arg min values. In the language of tropical analysis, the softmax is a deformation or \"quantization\" of arg max and arg min, corresponding to using the log semiring instead of the max-plus semiring (respectively min-plus semiring), and recovering the arg max or arg min by taking the limit is called \"tropicalization\" or \"dequantization\". It is also the case that, for any fixed , if one input is much larger than the others relative to the temperature, $T = 1/\\beta$, the output is approximately the arg max. For example, a difference of 10 is large relative to a temperature of 1: However, if the difference is small relative to the temperature, the value is not close to the arg max. For example, a difference of 10 is small relative to a temperature of 100: As , temperature goes to zero, $T = 1/\\beta \\to 0$, so eventually all differences become large (relative to a shrinking temperature), which gives another interpretation for the limit behavior."
            },
            {
                "text": "Statistical mechanics In statistical mechanics, the softargmax function is known as the Boltzmann distribution (or Gibbs distribution): the index set ${1,\\, \\dots,\\, k}$ are the microstates of the system; the inputs $z_i$ are the energies of that state; the denominator is known as the partition function, often denoted by ; and the factor is called the coldness (or thermodynamic beta, or inverse temperature). Applications The softmax function is used in various multiclass classification methods, such as multinomial logistic regression (also known as softmax regression), multiclass linear discriminant analysis, naive Bayes classifiers, and artificial neural networks.ai-faq What is a softmax activation function? Specifically, in multinomial logistic regression and linear discriminant analysis, the input to the function is the result of distinct linear functions, and the predicted probability for the th class given a sample vector $x$ and a weighting vector $w$ is: This can be seen as the composition of linear functions $\\mathbf{x} \\mapsto \\mathbf{x}^\\mathsf{T}\\mathbf{w}_1, \\ldots, \\mathbf{x} \\mapsto \\mathbf{x}^\\mathsf{T}\\mathbf{w}_K$ and the softmax function (where $\\mathbf{x}^\\mathsf{T}\\mathbf{w}$ denotes the inner product of $\\mathbf{x}$ and $\\mathbf{w}$)."
            },
            {
                "text": "The operation is equivalent to applying a linear operator defined by $\\mathbf{w}$ to vectors $\\mathbf{x}$, thus transforming the original, probably highly-dimensional, input to vectors in a -dimensional space $\\mathbb{R}^K$. Neural networks The standard softmax function is often used in the final layer of a neural network-based classifier. Such networks are commonly trained under a log loss (or cross-entropy) regime, giving a non-linear variant of multinomial logistic regression. Since the function maps a vector and a specific index $i$ to a real value, the derivative needs to take the index into account: This expression is symmetrical in the indexes $i, k$ and thus may also be expressed as Here, the Kronecker delta is used for simplicity (cf. the derivative of a sigmoid function, being expressed via the function itself). To ensure stable numerical computations subtracting the maximum value from the input vector is common. This approach, while not altering the output or the derivative theoretically, enhances stability by directly controlling the maximum exponent value computed."
            },
            {
                "text": "If the function is scaled with the parameter $\\beta$, then these expressions must be multiplied by $\\beta$. See multinomial logit for a probability model which uses the softmax activation function. Reinforcement learning In the field of reinforcement learning, a softmax function can be used to convert values into action probabilities. The function commonly used is:Sutton, R. S. and Barto A. G. Reinforcement Learning: An Introduction. The MIT Press, Cambridge, MA, 1998. Softmax Action Selection where the action value $q_t(a)$ corresponds to the expected reward of following action a and $\\tau$ is called a temperature parameter (in allusion to statistical mechanics). For high temperatures ($\\tau \\to \\infty$), all actions have nearly the same probability and the lower the temperature, the more expected rewards affect the probability. For a low temperature ($\\tau \\to 0^+$), the probability of the action with the highest expected reward tends to 1. Computational complexity and remedies In neural network applications, the number of possible outcomes is often large, e.g."
            },
            {
                "text": "in case of neural language models that predict the most likely outcome out of a vocabulary which might contain millions of possible words. This can make the calculations for the softmax layer (i.e. the matrix multiplications to determine the $z_i$, followed by the application of the softmax function itself) computationally expensive. What's more, the gradient descent backpropagation method for training such a neural network involves calculating the softmax for every training example, and the number of training examples can also become large. The computational effort for the softmax became a major limiting factor in the development of larger neural language models, motivating various remedies to reduce training times. Approaches that reorganize the softmax layer for more efficient calculation include the hierarchical softmax and the differentiated softmax. The hierarchical softmax (introduced by Morin and Bengio in 2005) uses a binary tree structure where the outcomes (vocabulary words) are the leaves and the intermediate nodes are suitably selected \"classes\" of outcomes, forming latent variables. The desired probability (softmax value) of a leaf (outcome) can then be calculated as the product of the probabilities of all nodes on the path from the root to that leaf."
            },
            {
                "text": "Ideally, when the tree is balanced, this would reduce the computational complexity from $O(K)$ to $O(\\log_2 K)$. In practice, results depend on choosing a good strategy for clustering the outcomes into classes. A Huffman tree was used for this in Google's word2vec models (introduced in 2013) to achieve scalability. A second kind of remedies is based on approximating the softmax (during training) with modified loss functions that avoid the calculation of the full normalization factor. These include methods that restrict the normalization sum to a sample of outcomes (e.g. Importance Sampling, Target Sampling). Numerical algorithms The standard softmax is numerically unstable because of large exponentiations. The safe softmax method calculates insteadwhere $m = \\max_i z_i $ is the largest factor involved. Subtracting by it guarantees that the exponentiations result in at most 1. The attention mechanism in Transformers takes three arguments: a \"query vector\" $q$, a list of \"key vectors\" $k_1, \\dots, k_N$, and a list of \"value vectors\" $v_1, \\dots, v_N$, and outputs a softmax-weighted sum over value vectors:The standard softmax method involves several loops over the inputs, which would be bottlenecked by memory bandwidth."
            },
            {
                "text": "The FlashAttention method is a communication-avoiding algorithm that fuses these operations into a single loop, increasing the arithmetic intensity. It is an online algorithm that computes the following quantities:and returns $o_N/l_N$. In practice, FlashAttention operates over multiple queries and keys per loop iteration, in a similar way as blocked matrix multiplication. If backpropagation is needed, then the output vectors and the intermediate arrays $[m_1, \\dots, m_N], [l_1, \\dots, l_N]$ are cached, and during the backward pass, attention matrices are rematerialized from these, making it a form of gradient checkpointing. Mathematical properties Geometrically the softmax function maps the vector space $\\mathbb{R}^K$ to the boundary of the standard $(K-1)$-simplex, cutting the dimension by one (the range is a $(K - 1)$-dimensional simplex in $K$-dimensional space), due to the linear constraint that all output sum to 1 meaning it lies on a hyperplane. Along the main diagonal $(x,\\, x,\\, \\dots,\\, x),$ softmax is just the uniform distribution on outputs, $(1/n, \\dots, 1/n)$: equal scores yield equal probabilities."
            },
            {
                "text": "More generally, softmax is invariant under translation by the same value in each coordinate: adding $\\mathbf{c} = (c,\\, \\dots,\\, c)$ to the inputs $\\mathbf{z}$ yields $\\sigma(\\mathbf{z} + \\mathbf{c}) = \\sigma(\\mathbf{z})$, because it multiplies each exponent by the same factor, $e^c$ (because $e^{z_i + c} = e^{z_i} \\cdot e^c$), so the ratios do not change: Geometrically, softmax is constant along diagonals: this is the dimension that is eliminated, and corresponds to the softmax output being independent of a translation in the input scores (a choice of 0 score). One can normalize input scores by assuming that the sum is zero (subtract the average: $\\mathbf{c}$ where ), and then the softmax takes the hyperplane of points that sum to zero, , to the open simplex of positive values that sum to 1, analogously to how the exponent takes 0 to 1, $e^0 = 1$ and is positive."
            },
            {
                "text": "By contrast, softmax is not invariant under scaling. For instance, $\\sigma\\bigl((0,\\, 1)\\bigr) = \\bigl(1/(1 + e),\\, e/(1 + e)\\bigr)$ but $\\sigma\\bigl((0, 2)\\bigr) = \\bigl(1/\\left(1 + e^2\\right),\\, e^2/\\left(1 + e^2\\right)\\bigr).$ The standard logistic function is the special case for a 1-dimensional axis in 2-dimensional space, say the x-axis in the $(x, y)$ plane. One variable is fixed at 0 (say $z_2 = 0$), so $e^0 = 1$, and the other variable can vary, denote it $z_1 = x$, so the standard logistic function, and its complement (meaning they add up to 1). The 1-dimensional input could alternatively be expressed as the line $(x/2,\\, -x/2)$, with outputs $e^{x/2}/\\left(e^{x/2} + e^{-x/2}\\right) = e^x/\\left(e^x + 1\\right)$ and $e^{-x/2}/\\left(e^{x/2} + e^{-x/2}\\right) = 1/\\left(e^x + 1\\right).$ Gradients The softmax function is also the gradient of the LogSumExp function:where the LogSumExp function is defined as $\\operatorname{LSE}(z_1,\\, \\dots,\\, z_n) = \\log\\left(\\exp(z_1) + \\cdots + \\exp(z_n)\\right)$."
            },
            {
                "text": "The gradient of softmax is thus $\\partial_{z_j} \\sigma_i = \\sigma_i (\\delta_{ij} - \\sigma_j)$. History The softmax function was used in statistical mechanics as the Boltzmann distribution in the foundational paper , formalized and popularized in the influential textbook . The use of the softmax in decision theory is credited to R. Duncan Luce, who used the axiom of independence of irrelevant alternatives in rational choice theory to deduce the softmax in Luce's choice axiom for relative preferences. In machine learning, the term \"softmax\" is credited to John S. Bridle in two 1989 conference papers, : and : Example With an input of $(1, 2, 3, 4, 1, 2, 3)$, the softmax is approximately $(0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175)$. The output has most of its weight where the \"4\" was in the original input. This is what the function is normally used for: to highlight the largest values and suppress values which are significantly below the maximum value."
            },
            {
                "text": "But note: a change of temperature changes the output. When the temperature is multiplied by 10, the inputs are effectively $(0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3)$ and the softmax is approximately $(0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153)$. This shows that high temperatures de-emphasize the maximum value. Computation of this example using Python code: >>> import numpy as np >>> z = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]) >>> beta = 1.0 >>> np.exp(beta * z) / np.sum(np.exp(beta * z)) array([0.02364054, 0.06426166, 0.1746813, 0.474833, 0.02364054, 0.06426166, 0.1746813]) Alternatives The softmax function generates probability predictions densely distributed over its support. Other functions like sparsemax or α-entmax can be used when sparse probability predictions are desired. \"Speeding Up Entmax\" by Maxat Tezekbayev, Vassilina Nikoulina, Matthias Gallé, Zhenisbek Assylbekov, https://arxiv.org/abs/2111.06832v3 Also the Gumbel-softmax reparametrization trick can be used when sampling from a discrete-discrete distribution needs to be mimicked in a differentiable manner."
            },
            {
                "text": "See also Softplus Multinomial logistic regression Dirichlet distribution – an alternative way to sample categorical distributions Partition function Exponential tilting – a generalization of Softmax to more general probability distributions Notes References Category:Computational neuroscience Category:Logistic regression Category:Artificial neural networks Category:Functions and mappings Category:Articles with example Python (programming language) code Category:Exponentials Category:Articles with example Julia code Category:Articles with example R code"
            }
        ],
        "latex_formulas": [
            "b > 0",
            "b > 1",
            "0 < b < 1",
            "−β",
            "1/k",
            "'''z'''",
            "'''x'''",
            "'''w'''",
            "(x, y)",
            "(1, 2, 3, 4, 1, 2, 3)",
            "(0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175)",
            "(0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3)",
            "(0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153)",
            "(0, 1)",
            "\\sigma\\colon \\R^K \\to (0, 1)^K",
            "K > 1",
            "\\mathbf{z} = (z_1, \\dotsc, z_K) \\in \\R^K",
            "\\sigma(\\mathbf{z}) \\in (0, 1)^K",
            "z_i",
            "\\mathbf z",
            "K",
            "\\sigma(\\mathbf z)",
            "(1,2,8)",
            "(0.001,0.002,0.997)",
            "b = e^\\beta",
            "b = e^{-\\beta}",
            "\\beta = 1/T",
            "T = 1/\\beta.",
            "\\beta = 0",
            "b = e^\\beta = e^0 = 1",
            "\\operatorname{softmax}(\\mathbf{z})^\\top \\mathbf{z}",
            "1, \\dots, n",
            "y_i = 1",
            "i",
            "(z_1, \\dots, z_n)",
            "z_i",
            "(z_1,\\, \\dots,\\, z_n)",
            "\\operatorname{arg\\,max}(1, 5, 10) = (0, 0, 1),",
            "z_i",
            "\\operatorname{arg\\,max}(1,\\, 5,\\, 5) = (0,\\, 1/2,\\, 1/2),",
            "\\operatorname{arg\\,max}(z, \\dots, z) = (1/n, \\dots, 1/n).",
            "\\sigma_\\beta(\\mathbf{z}) \\to \\operatorname{arg\\,max}(\\mathbf{z}).",
            "\\sigma_\\beta(1,\\, 1.0001) \\to (0, 1),",
            "\\sigma_\\beta(1,\\, 0.9999) \\to (1,\\, 0),",
            "\\sigma_\\beta(1,\\, 1) = 1/2",
            "(x, x)",
            "T = 1/\\beta",
            "T = 1/\\beta \\to 0",
            "{1,\\, \\dots,\\, k}",
            "z_i",
            "\\mathbf{x} \\mapsto \\mathbf{x}^\\mathsf{T}\\mathbf{w}_1, \\ldots, \\mathbf{x} \\mapsto \\mathbf{x}^\\mathsf{T}\\mathbf{w}_K",
            "\\mathbf{x}^\\mathsf{T}\\mathbf{w}",
            "\\mathbf{x}",
            "\\mathbf{w}",
            "\\mathbf{w}",
            "\\mathbf{x}",
            "\\mathbb{R}^K",
            "i",
            "i, k",
            "\\beta",
            "\\beta",
            "q_t(a)",
            "\\tau",
            "\\tau \\to \\infty",
            "\\tau \\to 0^+",
            "z_i",
            "O(K)",
            "O(\\log_2 K)",
            "m = \\max_i z_i",
            "q",
            "k_1, \\dots, k_N",
            "v_1, \\dots, v_N",
            "o_N/l_N",
            "[m_1, \\dots, m_N], [l_1, \\dots, l_N]",
            "\\mathbb{R}^K",
            "(K-1)",
            "(K - 1)",
            "K",
            "(x,\\, x,\\, \\dots,\\, x),",
            "(1/n, \\dots, 1/n)",
            "\\mathbf{c} = (c,\\, \\dots,\\, c)",
            "\\mathbf{z}",
            "\\sigma(\\mathbf{z} + \\mathbf{c}) = \\sigma(\\mathbf{z})",
            "e^c",
            "e^{z_i + c} = e^{z_i} \\cdot e^c",
            "\\mathbf{c}",
            "e^0 = 1",
            "\\sigma\\bigl((0,\\, 1)\\bigr) = \\bigl(1/(1 + e),\\, e/(1 + e)\\bigr)",
            "\\sigma\\bigl((0, 2)\\bigr) = \\bigl(1/\\left(1 + e^2\\right),\\, e^2/\\left(1 + e^2\\right)\\bigr).",
            "z_2 = 0",
            "e^0 = 1",
            "z_1 = x",
            "(x/2,\\, -x/2)",
            "e^{x/2}/\\left(e^{x/2} + e^{-x/2}\\right) = e^x/\\left(e^x + 1\\right)",
            "e^{-x/2}/\\left(e^{x/2} + e^{-x/2}\\right) = 1/\\left(e^x + 1\\right).",
            "\\operatorname{LSE}(z_1,\\, \\dots,\\, z_n) = \\log\\left(\\exp(z_1) + \\cdots + \\exp(z_n)\\right)",
            "\\partial_{z_j} \\sigma_i = \\sigma_i (\\delta_{ij} - \\sigma_j)"
        ]
    },
    "Convex_optimization": {
        "title": "Convex_optimization",
        "chunks": [
            {
                "text": "Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets (or, equivalently, maximizing concave functions over convex sets). Many classes of convex optimization problems admit polynomial-time algorithms, whereas mathematical optimization is in general NP-hard.Sahni, S. \"Computationally related problems,\" in SIAM Journal on Computing, 3, 262--279, 1974. Definition Abstract form A convex optimization problem is defined by two ingredients: The objective function, which is a real-valued convex function of n variables, $f :\\mathcal D \\subseteq \\mathbb{R}^n \\to \\mathbb{R}$; The feasible set, which is a convex subset $C\\subseteq \\mathbb{R}^n$. The goal of the problem is to find some $\\mathbf{x^\\ast} \\in C$ attaining $\\inf \\{ f(\\mathbf{x}) : \\mathbf{x} \\in C \\}$. In general, there are three options regarding the existence of a solution: If such a point x* exists, it is referred to as an optimal point or solution; the set of all optimal points is called the optimal set; and the problem is called solvable."
            },
            {
                "text": "If $f$ is unbounded below over $C$, or the infimum is not attained, then the optimization problem is said to be unbounded. Otherwise, if $C$ is the empty set, then the problem is said to be infeasible."
            },
            {
                "text": "Standard form A convex optimization problem is in standard form if it is written as $\\begin{align} &\\underset{\\mathbf{x}}{\\operatorname{minimize}}& & f(\\mathbf{x}) \\\\ &\\operatorname{subject\\ to} & &g_i(\\mathbf{x}) \\leq 0, \\quad i = 1, \\dots, m \\\\ &&&h_i(\\mathbf{x}) = 0, \\quad i = 1, \\dots, p, \\end{align}$ where: $\\mathbf{x} \\in \\mathbb{R}^n$ is the vector of optimization variables; The objective function $f: \\mathcal D \\subseteq \\mathbb{R}^n \\to \\mathbb{R}$ is a convex function; The inequality constraint functions $g_i : \\mathbb{R}^n \\to \\mathbb{R}$, $i=1, \\ldots, m$, are convex functions; The equality constraint functions $h_i : \\mathbb{R}^n \\to \\mathbb{R}$, $i=1, \\ldots, p$, are affine transformations, that is, of the form: $h_i(\\mathbf{x}) = \\mathbf{a_i}\\cdot \\mathbf{x} - b_i$, where $\\mathbf{a_i}$ is a vector and $b_i$ is a scalar."
            },
            {
                "text": "The feasible set $C$ of the optimization problem consists of all points $\\mathbf{x} \\in \\mathcal{D}$ satisfying the inequality and the equality constraints. This set is convex because $\\mathcal{D}$ is convex, the sublevel sets of convex functions are convex, affine sets are convex, and the intersection of convex sets is convex. Many optimization problems can be equivalently formulated in this standard form. For example, the problem of maximizing a concave function $f$ can be re-formulated equivalently as the problem of minimizing the convex function $-f$. The problem of maximizing a concave function over a convex set is commonly called a convex optimization problem. Epigraph form (standard form with linear objective) In the standard form it is possible to assume, without loss of generality, that the objective function f is a linear function."
            },
            {
                "text": "This is because any program with a general objective can be transformed into a program with a linear objective by adding a single variable t and a single constraint, as follows: $\\begin{align} &\\underset{\\mathbf{x},t}{\\operatorname{minimize}}& & t \\\\ &\\operatorname{subject\\ to} & &f(\\mathbf{x}) - t \\leq 0 \\\\ && &g_i(\\mathbf{x}) \\leq 0, \\quad i = 1, \\dots, m \\\\ &&&h_i(\\mathbf{x}) = 0, \\quad i = 1, \\dots, p, \\end{align}$ Conic form Every convex program can be presented in a conic form, which means minimizing a linear objective over the intersection of an affine plane and a convex cone: $\\begin{align} &\\underset{\\mathbf{x}}{\\operatorname{minimize}}& & c^T x \\\\ &\\operatorname{subject\\ to} & &x \\in (b+L)\\cap K \\end{align}$ where K is a closed pointed convex cone, L is a linear subspace of Rn, and b is a vector in Rn."
            },
            {
                "text": "A linear program in standard form is the special case in which K is the nonnegative orthant of Rn. Eliminating linear equality constraints It is possible to convert a convex program in standard form, to a convex program with no equality constraints. Denote the equality constraints hi(x)=0 as Ax=b, where A has n columns. If Ax=b is infeasible, then of course the original problem is infeasible. Otherwise, it has some solution x0 , and the set of all solutions can be presented as: Fz+x0, where z is in Rk, k=n-rank(A), and F is an n-by-k matrix. Substituting x = Fz+x0 in the original problem gives: $\\begin{align} &\\underset{\\mathbf{x}}{\\operatorname{minimize}}& & f(\\mathbf{F \\mathbf{z} + \\mathbf{x}_0}) \\\\ &\\operatorname{subject\\ to} & &g_i(\\mathbf{F \\mathbf{z} + \\mathbf{x}_0}) \\leq 0, \\quad i = 1, \\dots, m \\\\ \\end{align}$where the variables are z."
            },
            {
                "text": "Note that there are rank(A) fewer variables. This means that, in principle, one can restrict attention to convex optimization problems without equality constraints. In practice, however, it is often preferred to retain the equality constraints, since they might make some algorithms more efficient, and also make the problem easier to understand and analyze. Special cases The following problem classes are all convex optimization problems, or can be reduced to convex optimization problems via simple transformations: thumb|A hierarchy of convex optimization problems. (LP: linear programming, QP: quadratic programming, SOCP second-order cone program, SDP: semidefinite programming, CP: conic optimization.) Linear programming problems are the simplest convex programs. In LP, the objective and constraint functions are all linear. Quadratic programming are the next-simplest. In QP, the constraints are all linear, but the objective may be a convex quadratic function. Second order cone programming are more general. Semidefinite programming are more general. Conic optimization are even more general - see figure to the right, Other special cases include; Least squares Quadratic minimization with convex quadratic constraints Geometric programming Entropy maximization with appropriate constraints."
            },
            {
                "text": "Properties The following are useful properties of convex optimization problems: every local minimum is a global minimum; the optimal set is convex; if the objective function is strictly convex, then the problem has at most one optimal point. These results are used by the theory of convex minimization along with geometric notions from functional analysis (in Hilbert spaces) such as the Hilbert projection theorem, the separating hyperplane theorem, and Farkas' lemma. Algorithms Unconstrained and equality-constrained problems The convex programs easiest to solve are the unconstrained problems, or the problems with only equality constraints. As the equality constraints are all linear, they can be eliminated with linear algebra and integrated into the objective, thus converting an equality-constrained problem into an unconstrained one. In the class of unconstrained (or equality-constrained) problems, the simplest ones are those in which the objective is quadratic. For these problems, the KKT conditions (which are necessary for optimality) are all linear, so they can be solved analytically. For unconstrained (or equality-constrained) problems with a general convex objective that is twice-differentiable, Newton's method can be used."
            },
            {
                "text": "It can be seen as reducing a general unconstrained convex problem, to a sequence of quadratic problems.Newton's method can be combined with line search for an appropriate step size, and it can be mathematically proven to converge quickly. Other efficient algorithms for unconstrained minimization are gradient descent (a special case of steepest descent). General problems The more challenging problems are those with inequality constraints. A common way to solve them is to reduce them to unconstrained problems by adding a barrier function, enforcing the inequality constraints, to the objective function. Such methods are called interior point methods.They have to be initialized by finding a feasible interior point using by so-called phase I methods, which either find a feasible point or show that none exist. Phase I methods generally consist of reducing the search in question to a simpler convex optimization problem. Convex optimization problems can also be solved by the following contemporary methods:For methods for convex minimization, see the volumes by Hiriart-Urruty and Lemaréchal (bundle) and the textbooks by Ruszczyński, Bertsekas, and Boyd and Vandenberghe (interior point)."
            },
            {
                "text": "Bundle methods (Wolfe, Lemaréchal, Kiwiel), and Subgradient projection methods (Polyak), Interior-point methods, which make use of self-concordant barrier functions and self-regular barrier functions. Cutting-plane methods Ellipsoid method Subgradient method Dual subgradients and the drift-plus-penalty method Subgradient methods can be implemented simply and so are widely used. Dual subgradient methods are subgradient methods applied to a dual problem. The drift-plus-penalty method is similar to the dual subgradient method, but takes a time average of the primal variables. Lagrange multipliers Consider a convex minimization problem given in standard form by a cost function $f(x)$ and inequality constraints $g_i(x)\\leq 0$ for $ 1 \\leq i \\leq m$."
            },
            {
                "text": "Then the domain $\\mathcal{X}$ is: $\\mathcal{X} = \\left\\{x\\in X \\vert g_1(x), \\ldots, g_m(x)\\leq 0\\right\\}.$ The Lagrangian function for the problem is $L(x,\\lambda_{0},\\lambda_1, \\ldots ,\\lambda_{m})=\\lambda_{0} f(x) + \\lambda_{1} g_{1} (x)+\\cdots + \\lambda_{m} g_{m} (x).$ For each point $x$ in $X$ that minimizes $f$ over $X$, there exist real numbers $\\lambda_{0},\\lambda_1, \\ldots, \\lambda_{m},$ called Lagrange multipliers, that satisfy these conditions simultaneously: $x$ minimizes $L(y,\\lambda_{0},\\lambda_{1},\\ldots ,\\lambda_{m})$ over all $y \\in X,$ $\\lambda_{0},\\lambda_{1},\\ldots ,\\lambda_{m} \\geq 0,$ with at least one $\\lambda_{k} > 0,$ $\\lambda_{1}g_{1}(x)=\\cdots = \\lambda_{m}g_{m}(x) = 0$ (complementary slackness)."
            },
            {
                "text": "If there exists a \"strictly feasible point\", that is, a point $z$ satisfying $g_{1}(z), \\ldots, g_{m}(z)<0,$ then the statement above can be strengthened to require that $\\lambda_{0}=1$. Conversely, if some $x$ in $X$ satisfies (1)–(3) for scalars $\\lambda_{0},\\ldots,\\lambda_{m} $ with $\\lambda_{0}=1$ then $x$ is certain to minimize $f$ over $X$. Software There is a large software ecosystem for convex optimization. This ecosystem has two main categories: solvers on the one hand and modeling tools (or interfaces) on the other hand. Solvers implement the algorithms themselves and are usually written in C. They require users to specify optimization problems in very specific formats which may not be natural from a modeling perspective. Modeling tools are separate pieces of software that let the user specify an optimization in higher-level syntax."
            },
            {
                "text": "They manage all transformations to and from the user's high-level model and the solver's input/output format. The table below shows a mix of modeling tools (such as CVXPY and Convex.jl) and solvers (such as CVXOPT and MOSEK). This table is by no means exhaustive. +ProgramLanguageDescriptionFOSS?RefCVXMATLABInterfaces with SeDuMi and SDPT3 solvers; designed to only express convex optimization problems.CVXMODPythonInterfaces with the CVXOPT solver.CVXPYPythonConvex.jlJuliaDisciplined convex programming, supports many solvers.CVXRRYALMIPMATLAB, OctaveInterfaces with CPLEX, GUROBI, MOSEK, SDPT3, SEDUMI, CSDP, SDPA, PENNON solvers; also supports integer and nonlinear optimization, and some nonconvex optimization. Can perform robust optimization with uncertainty in LP/SOCP/SDP constraints.LMI labMATLABExpresses and solves semidefinite programming problems (called \"linear matrix inequalities\")LMIlab translatorTransforms LMI lab problems into SDP problems.xLMIMATLABSimilar to LMI lab, but uses the SeDuMi solver.AIMMSCan do robust optimization on linear programming (with MOSEK to solve second-order cone programming) and mixed integer linear programming. Modeling package for LP + SDP and robust versions.ROMEModeling system for robust optimization. Supports distributionally robust optimization and uncertainty sets.GloptiPoly 3MATLAB, OctaveModeling system for polynomial optimization.SOSTOOLSModeling system for polynomial optimization."
            },
            {
                "text": "Uses SDPT3 and SeDuMi. Requires Symbolic Computation Toolbox.SparsePOPModeling system for polynomial optimization. Uses the SDPA or SeDuMi solvers.CPLEXSupports primal-dual methods for LP + SOCP. Can solve LP, QP, SOCP, and mixed integer linear programming problems.CSDPCSupports primal-dual methods for LP + SDP. Interfaces available for MATLAB, R, and Python. Parallel version available. SDP solver.CVXOPTPythonSupports primal-dual methods for LP + SOCP + SDP. Uses Nesterov-Todd scaling. Interfaces to MOSEK and DSDP. MOSEKSupports primal-dual methods for LP + SOCP.SeDuMiMATLAB, Octave, MEXSolves LP + SOCP + SDP. Supports primal-dual methods for LP + SOCP + SDP.SDPAC++Solves LP + SDP. Supports primal-dual methods for LP + SDP. Parallelized and extended precision versions are available.SDPT3MATLAB, Octave, MEXSolves LP + SOCP + SDP. Supports primal-dual methods for LP + SOCP + SDP.ConicBundleSupports general-purpose codes for LP + SOCP + SDP. Uses a bundle method. Special support for SDP and SOCP constraints.DSDPSupports general-purpose codes for LP + SDP. Uses a dual interior point method.LOQOSupports general-purpose codes for SOCP, which it treats as a nonlinear programming problem.PENNONSupports general-purpose codes. Uses an augmented Lagrangian method, especially for problems with SDP constraints.SDPLRSupports general-purpose codes."
            },
            {
                "text": "Uses low-rank factorization with an augmented Lagrangian method.GAMSModeling system for linear, nonlinear, mixed integer linear/nonlinear, and second-order cone programming problems.Optimization ServicesXML standard for encoding optimization problems and solutions. Applications Convex optimization can be used to model problems in a wide range of disciplines, such as automatic control systems, estimation and signal processing, communications and networks, electronic circuit design, data analysis and modeling, finance, statistics (optimal experimental design),Chritensen/Klarbring, chpt. 4. and structural optimization, where the approximation concept has proven to be efficient.Schmit, L.A.; Fleury, C. 1980: Structural synthesis by combining approximation concepts and dual methods. J. Amer. Inst. Aeronaut. Astronaut 18, 1252-1260 Convex optimization can be used to model problems in the following fields: Portfolio optimization. Worst-case risk analysis. Optimal advertising. Variations of statistical regression (including regularization and quantile regression). Model fitting (particularly multiclass classification). Electricity generation optimization. Combinatorial optimization. Non-probabilistic modelling of uncertainty.Ben Haim Y. and Elishakoff I., Convex Models of Uncertainty in Applied Mechanics, Elsevier Science Publishers, Amsterdam, 1990 Localization using wireless signals Ahmad Bazzi, Dirk TM Slock, and Lisa Meilhac."
            },
            {
                "text": "\"Online angle of arrival estimation in the presence of mutual coupling.\" 2016 IEEE Statistical Signal Processing Workshop (SSP). IEEE, 2016. Extensions Extensions of convex optimization include the optimization of biconvex, pseudo-convex, and quasiconvex functions. Extensions of the theory of convex analysis and iterative methods for approximately solving non-convex minimization problems occur in the field of generalized convexity, also known as abstract convex analysis. See also Duality Karush–Kuhn–Tucker conditions Optimization problem Proximal gradient method Algorithmic problems on convex sets Notes References Hiriart-Urruty, Jean-Baptiste, and Lemaréchal, Claude. (2004). Fundamentals of Convex analysis. Berlin: Springer. Nesterov, Yurii. (2004). Introductory Lectures on Convex Optimization, Kluwer Academic Publishers Schmit, L.A.; Fleury, C. 1980: Structural synthesis by combining approximation concepts and dual methods. J. Amer. Inst. Aeronaut. Astronaut 18, 1252-1260 External links EE364a: Convex Optimization I and EE364b: Convex Optimization II, Stanford course homepages 6.253: Convex Analysis and Optimization, an MIT OCW course homepage Brian Borchers, An overview of software for convex optimization Convex Optimization Book by Lieven Vandenberghe and Stephen P. Boyd Category:Convex analysis Category:Mathematical optimization"
            }
        ],
        "latex_formulas": [
            "f :\\mathcal D \\subseteq \\mathbb{R}^n \\to \\mathbb{R}",
            "C\\subseteq \\mathbb{R}^n",
            "\\mathbf{x^\\ast} \\in C",
            "\\inf \\{ f(\\mathbf{x}) : \\mathbf{x} \\in C \\}",
            "f",
            "C",
            "C",
            "\\begin{align}\n&\\underset{\\mathbf{x}}{\\operatorname{minimize}}& & f(\\mathbf{x}) \\\\\n&\\operatorname{subject\\ to}\n& &g_i(\\mathbf{x}) \\leq 0, \\quad i = 1, \\dots, m \\\\\n&&&h_i(\\mathbf{x}) = 0, \\quad i = 1, \\dots, p,\n\\end{align}",
            "\\mathbf{x} \\in \\mathbb{R}^n",
            "f: \\mathcal D \\subseteq \\mathbb{R}^n \\to \\mathbb{R}",
            "g_i : \\mathbb{R}^n \\to \\mathbb{R}",
            "i=1, \\ldots, m",
            "h_i : \\mathbb{R}^n \\to \\mathbb{R}",
            "i=1, \\ldots, p",
            "h_i(\\mathbf{x}) = \\mathbf{a_i}\\cdot \\mathbf{x} - b_i",
            "\\mathbf{a_i}",
            "b_i",
            "C",
            "\\mathbf{x} \\in \\mathcal{D}",
            "\\mathcal{D}",
            "f",
            "-f",
            "\\begin{align}\n&\\underset{\\mathbf{x},t}{\\operatorname{minimize}}& & t \\\\\n&\\operatorname{subject\\ to}\n& &f(\\mathbf{x}) - t \\leq 0 \\\\\n&& &g_i(\\mathbf{x}) \\leq 0, \\quad i = 1, \\dots, m \\\\\n&&&h_i(\\mathbf{x}) = 0, \\quad i = 1, \\dots, p,\n\\end{align}",
            "\\begin{align}\n&\\underset{\\mathbf{x}}{\\operatorname{minimize}}& & c^T x \\\\\n&\\operatorname{subject\\ to}\n& &x \\in (b+L)\\cap K\n\\end{align}",
            "\\begin{align}\n&\\underset{\\mathbf{x}}{\\operatorname{minimize}}& & f(\\mathbf{F \\mathbf{z} + \\mathbf{x}_0}) \\\\\n&\\operatorname{subject\\ to}\n& &g_i(\\mathbf{F \\mathbf{z} + \\mathbf{x}_0}) \\leq 0, \\quad i = 1, \\dots, m \\\\\n\\end{align}",
            "f(x)",
            "g_i(x)\\leq 0",
            "1 \\leq i \\leq m",
            "\\mathcal{X}",
            "\\mathcal{X} = \\left\\{x\\in X \\vert g_1(x), \\ldots, g_m(x)\\leq 0\\right\\}.",
            "L(x,\\lambda_{0},\\lambda_1, \\ldots ,\\lambda_{m})=\\lambda_{0} f(x) + \\lambda_{1} g_{1} (x)+\\cdots + \\lambda_{m} g_{m} (x).",
            "x",
            "X",
            "f",
            "X",
            "\\lambda_{0},\\lambda_1, \\ldots, \\lambda_{m},",
            "x",
            "L(y,\\lambda_{0},\\lambda_{1},\\ldots ,\\lambda_{m})",
            "y \\in X,",
            "\\lambda_{0},\\lambda_{1},\\ldots ,\\lambda_{m} \\geq 0,",
            "\\lambda_{k} > 0,",
            "\\lambda_{1}g_{1}(x)=\\cdots = \\lambda_{m}g_{m}(x) = 0",
            "z",
            "g_{1}(z), \\ldots, g_{m}(z)<0,",
            "\\lambda_{0}=1",
            "x",
            "X",
            "\\lambda_{0},\\ldots,\\lambda_{m}",
            "\\lambda_{0}=1",
            "x",
            "f",
            "X"
        ]
    },
    "Scikit-learn": {
        "title": "Scikit-learn",
        "chunks": [
            {
                "text": "scikit-learn (formerly scikits.learn and also known as sklearn) is a free and open-source machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project. Overview The scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project stems from the notion that it is a \"SciKit\" (SciPy Toolkit), a separately developed and distributed third-party extension to SciPy. The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010. In November 2012, scikit-learn as well as scikit-image were described as two of the \"well-maintained and popular\" ."
            },
            {
                "text": "In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub. Features Large catalogue of well-established machine learning algorithms and data pre-processing methods (i.e. feature engineering) Utility methods for common data-science tasks, such as splitting data into train and test sets, cross-validation and grid search Consistent way of running machine learning models ( and ), which libraries can implement Declarative way of structuring a data science process (the ), including data pre-processing and model fitting Examples Fitting a random forest classifier:>>> from sklearn.ensemble import RandomForestClassifier >>> classifier = RandomForestClassifier(random_state=0) >>> X = [[ 1, 2, 3], # 2 samples, 3 features ... [11, 12, 13]] >>> y = [0, 1] # classes of each sample >>> classifier.fit(X, y) RandomForestClassifier(random_state=0) Implementation scikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance."
            },
            {
                "text": "Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible. scikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more. Version history scikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010. August 2013. scikit-learn 0.14 July 2014. scikit-learn 0.15.0 March 2015. scikit-learn 0.16.0 November 2015. scikit-learn 0.17.0 September 2016. scikit-learn 0.18.0 July 2017. scikit-learn 0.19.0 September 2018. scikit-learn 0.20.0 May 2019. scikit-learn 0.21.0 December 2019. scikit-learn 0.22 May 2020. scikit-learn 0.23.0 Jan 2021. scikit-learn 0.24 September 2021. scikit-learn 1.0.0 October 2021. scikit-learn 1.0.1 December 2021. scikit-learn 1.0.2 May 2022. scikit-learn 1.1.0 May 2022. scikit-learn 1.1.1 August 2022. scikit-learn 1.1.2 October 2022. scikit-learn 1.1.3 December 2022. scikit-learn 1.2.0 January 2023. scikit-learn 1.2.1 March 2023. scikit-learn 1.2.2 Awards 2019 Inria-French Academy of Sciences-Dassault Systèmes Innovation Prize 2022 Open Science Award for Open Source Research Software scikit-learn alternatives mlpy SpaCy NLTK Orange PyTorch TensorFlow JAX Infer.NET List of numerical analysis software References External links Category:Data mining and machine learning software Category:Free statistical software Category:Python (programming language) scientific libraries Category:Software using the BSD license"
            }
        ],
        "latex_formulas": []
    },
    "Convergence_(mathematics)": {
        "title": "Convergence_(mathematics)",
        "chunks": [
            {
                "text": "REDIRECT Convergent series"
            }
        ],
        "latex_formulas": []
    },
    "Derivative": {
        "title": "Derivative",
        "chunks": [
            {
                "text": "In mathematics, the derivative is a fundamental tool that quantifies the sensitivity to change of a function's output with respect to its input. The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value. For this reason, the derivative is often described as the instantaneous rate of change, the ratio of the instantaneous change in the dependent variable to that of the independent variable. The process of finding a derivative is called differentiation. There are multiple different notations for differentiation. Leibniz notation, named after Gottfried Wilhelm Leibniz, is represented as the ratio of two differentials, whereas prime notation is written by adding a prime mark. Higher order notations represent repeated differentiation, and they are usually denoted in Leibniz notation by adding superscripts to the differentials, and in prime notation by adding additional prime marks. The higher order derivatives can be applied in physics; for example, while the first derivative of the position of a moving object with respect to time is the object's velocity, how the position changes as time advances, the second derivative is the object's acceleration, how the velocity changes as time advances."
            },
            {
                "text": "Derivatives can be generalized to functions of several real variables. In this case, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. It can be calculated in terms of the partial derivatives with respect to the independent variables. For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector. Definition As a limit A function of a real variable $ f(x) $ is differentiable at a point $ a $ of its domain, if its domain contains an open interval containing , and the limit exists. This means that, for every positive real number , there exists a positive real number $\\delta$ such that, for every $ h $ such that $|h| < \\delta$ and $h\\ne 0$ then $f(a+h)$ is defined, and where the vertical bars denote the absolute value."
            },
            {
                "text": "This is an example of the (ε, δ)-definition of limit. If the function $ f $ is differentiable at , that is if the limit $ L $ exists, then this limit is called the derivative of $ f $ at $ a $. Multiple notations for the derivative exist. The derivative of $ f $ at $ a $ can be denoted , read as \" prime of \"; or it can be denoted , read as \"the derivative of $ f $ with respect to $ x $ at \" or \" by (or over) $ dx $ at \". See below. If $ f $ is a function that has a derivative at every point in its domain, then a function can be defined by mapping every point $ x $ to the value of the derivative of $ f $ at $ x $. This function is written $ f' $ and is called the derivative function or the derivative of . The function $ f $ sometimes has a derivative at most, but not all, points of its domain."
            },
            {
                "text": "The function whose value at $ a $ equals $ f'(a) $ whenever $ f'(a) $ is defined and elsewhere is undefined is also called the derivative of . It is still a function, but its domain may be smaller than the domain of $ f $. For example, let $f$ be the squaring function: $f(x) = x^2$. Then the quotient in the definition of the derivative is The division in the last step is valid as long as $h \\neq 0$. The closer $h$ is to , the closer this expression becomes to the value $2a$. The limit exists, and for every input $a$ the limit is $2a$. So, the derivative of the squaring function is the doubling function: . The ratio in the definition of the derivative is the slope of the line through two points on the graph of the function , specifically the points $(a,f(a))$ and $(a+h, f(a+h))$."
            },
            {
                "text": "As $h$ is made smaller, these points grow closer together, and the slope of this line approaches the limiting value, the slope of the tangent to the graph of $f$ at $a$. In other words, the derivative is the slope of the tangent. Using infinitesimals One way to think of the derivative is as the ratio of an infinitesimal change in the output of the function $f$ to an infinitesimal change in its input. In order to make this intuition rigorous, a system of rules for manipulating infinitesimal quantities is required. The system of hyperreal numbers is a way of treating infinite and infinitesimal quantities. The hyperreals are an extension of the real numbers that contain numbers greater than anything of the form $1 + 1 + \\cdots + 1 $ for any finite number of terms. Such numbers are infinite, and their reciprocals are infinitesimals. The application of hyperreal numbers to the foundations of calculus is called nonstandard analysis. This provides a way to define the basic concepts of calculus such as the derivative and integral in terms of infinitesimals, thereby giving a precise meaning to the $d$ in the Leibniz notation."
            },
            {
                "text": "Thus, the derivative of $f(x)$ becomes for an arbitrary infinitesimal , where $\\operatorname{st}$ denotes the standard part function, which \"rounds off\" each finite hyperreal to the nearest real. Taking the squaring function $f(x) = x^2$ as an example again, Continuity and differentiability If $ f $ is differentiable at , then $ f $ must also be continuous at $ a $. As an example, choose a point $ a $ and let $ f $ be the step function that returns the value 1 for all $ x $ less than , and returns a different value 10 for all $ x $ greater than or equal to $ a $. The function $ f $ cannot have a derivative at $ a $. If $ h $ is negative, then $ a + h $ is on the low part of the step, so the secant line from $ a $ to $ a + h $ is very steep; as $ h $ tends to zero, the slope tends to infinity."
            },
            {
                "text": "If $ h $ is positive, then $ a + h $ is on the high part of the step, so the secant line from $ a $ to $ a + h $ has slope zero. Consequently, the secant lines do not approach any single slope, so the limit of the difference quotient does not exist. However, even if a function is continuous at a point, it may not be differentiable there. For example, the absolute value function given by $ f(x) = |x| $ is continuous at , but it is not differentiable there. If $ h $ is positive, then the slope of the secant line from 0 to $ h $ is one; if $ h $ is negative, then the slope of the secant line from $ 0 $ to $ h $ is . This can be seen graphically as a \"kink\" or a \"cusp\" in the graph at $x=0$. Even a function with a smooth graph is not differentiable at a point where its tangent is vertical: For instance, the function given by $ f(x) = x^{1/3} $ is not differentiable at $ x = 0 $."
            },
            {
                "text": "In summary, a function that has a derivative is continuous, but there are continuous functions that do not have a derivative. Most functions that occur in practice have derivatives at all points or almost every point. Early in the history of calculus, many mathematicians assumed that a continuous function was differentiable at most points. Under mild conditions (for example, if the function is a monotone or a Lipschitz function), this is true. However, in 1872, Weierstrass found the first example of a function that is continuous everywhere but differentiable nowhere. This example is now known as the Weierstrass function. In 1931, Stefan Banach proved that the set of functions that have a derivative at some point is a meager set in the space of all continuous functions. Informally, this means that hardly any random continuous functions have a derivative at even one point., cited in . Notation One common way of writing the derivative of a function is Leibniz notation, introduced by Gottfried Wilhelm Leibniz in 1675, which denotes a derivative as the quotient of two differentials, such as $ dy $ and ."
            },
            {
                "text": "It is still commonly used when the equation $y=f(x)$ is viewed as a functional relationship between dependent and independent variables. The first derivative is denoted by , read as \"the derivative of $ y $ with respect to \". This derivative can alternately be treated as the application of a differential operator to a function, Higher derivatives are expressed using the notation for the $n$-th derivative of $y = f(x)$. These are abbreviations for multiple applications of the derivative operator; for example, Unlike some alternatives, Leibniz notation involves explicit specification of the variable for differentiation, in the denominator, which removes ambiguity when working with multiple interrelated quantities. The derivative of a composed function can be expressed using the chain rule: if $u = g(x)$ and $y = f(g(x))$ then In the formulation of calculus in terms of limits, various authors have assigned the $ du $ symbol various meanings. Some authors such as , p. 119 and , p. 177 do not assign a meaning to $ du $ by itself, but only as part of the symbol ."
            },
            {
                "text": "Others define $ dx $ as an independent variable, and define $ du $ by . In non-standard analysis $ du $ is defined as an infinitesimal. It is also interpreted as the exterior derivative of a function . See differential (infinitesimal) for further information. Another common notation for differentiation is by using the prime mark in the symbol of a function . This notation, due to Joseph-Louis Lagrange, is now known as prime notation. The first derivative is written as , read as \" prime of , or , read as \" prime\". Similarly, the second and the third derivatives can be written as $ f $ and , respectively. For denoting the number of higher derivatives beyond this point, some authors use Roman numerals in superscript, whereas others place the number in parentheses, such as $f^{\\mathrm{iv}}$ or . The latter notation generalizes to yield the notation $f^{(n)}$ for the th derivative of . In Newton's notation or the dot notation, a dot is placed over a symbol to represent a time derivative."
            },
            {
                "text": "If $ y $ is a function of , then the first and second derivatives can be written as $\\dot{y}$ and , respectively. This notation is used exclusively for derivatives with respect to time or arc length. It is typically used in differential equations in physics and differential geometry. However, the dot notation becomes unmanageable for high-order derivatives (of order 4 or more) and cannot deal with multiple independent variables. Another notation is D-notation, which represents the differential operator by the symbol . The first derivative is written $D f(x)$ and higher derivatives are written with a superscript, so the $n$-th derivative is . This notation is sometimes called Euler notation, although it seems that Leonhard Euler did not use it, and the notation was introduced by Louis François Antoine Arbogast. To indicate a partial derivative, the variable differentiated by is indicated with a subscript, for example given the function , its partial derivative with respect to $x$ can be written $D_x u$ or . Higher partial derivatives can be indicated by superscripts or multiple subscripts, e.g."
            },
            {
                "text": "and . Rules of computation In principle, the derivative of a function can be computed from the definition by considering the difference quotient and computing its limit. Once the derivatives of a few simple functions are known, the derivatives of other functions are more easily computed using rules for obtaining derivatives of more complicated functions from simpler ones. This process of finding a derivative is known as differentiation. Rules for basic functions The following are the rules for the derivatives of the most common basic functions. Here, $ a $ is a real number, and $ e $ is the base of the natural logarithm, approximately .. See p. 133 for the power rule, pp. 115–116 for the trigonometric functions, p. 326 for the natural logarithm, pp. 338–339 for exponential with base , p. 343 for the exponential with base , p. 344 for the logarithm with base , and p. 369 for the inverse of trigonometric functions."
            },
            {
                "text": "Derivatives of powers: $ \\frac{d}{dx}x^a = ax^{a-1} $ Functions of exponential, natural logarithm, and logarithm with general base: $ \\frac{d}{dx}e^x = e^x $ $ \\frac{d}{dx}a^x = a^x\\ln(a) $, for $ a > 0 $ $ \\frac{d}{dx}\\ln(x) = \\frac{1}{x} $, for $ x > 0 $ $ \\frac{d}{dx}\\log_a(x) = \\frac{1}{x\\ln(a)} $, for $ x, a > 0 $ Trigonometric functions: $ \\frac{d}{dx}\\sin(x) = \\cos(x) $ $ \\frac{d}{dx}\\cos(x) = -\\sin(x) $ $ \\frac{d}{dx}\\tan(x) = \\sec^2(x) = \\frac{1}{\\cos^2(x)} = 1 + \\tan^2(x) $ Inverse trigonometric functions: $ \\frac{d}{dx}\\arcsin(x) = \\frac{1}{\\sqrt{1-x^2}} $, for $ -1 < x < 1 $ $ \\frac{d}{dx}\\arccos(x)= -\\frac{1}{\\sqrt{1-x^2}} $, for $ -1 < x < 1 $ $ \\frac{d}{dx}\\arctan(x)= \\frac{1} $ Rules for combined functions Given that the $ f $ and $ g $ are the functions."
            },
            {
                "text": "The following are some of the most basic rules for deducing the derivative of functions from derivatives of basic functions. For constant rule and sum rule, see , respectively. For the product rule, quotient rule, and chain rule, see , respectively. For the special case of the product rule, that is, the product of a constant and a function, see . Constant rule: if $f$ is constant, then for all , $f'(x) = 0. $ Sum rule: $(\\alpha f + \\beta g)' = \\alpha f' + \\beta g' $ for all functions $f$ and $g$ and all real numbers $\\alpha$ and . Product rule: $(fg)' = f 'g + fg' $ for all functions $f$ and . As a special case, this rule includes the fact $(\\alpha f)' = \\alpha f'$ whenever $\\alpha$ is a constant because $\\alpha' f = 0 \\cdot f = 0$ by the constant rule."
            },
            {
                "text": "Quotient rule: $\\left(\\frac{f}{g} \\right)' = \\frac{f'g - fg'}{g^2}$ for all functions $f$ and $g$ at all inputs where . Chain rule for composite functions: If , then $f'(x) = h'(g(x)) \\cdot g'(x). $ Computation example The derivative of the function given by $f(x) = x^4 + \\sin \\left(x^2\\right) - \\ln(x) e^x + 7$ is Here the second term was computed using the chain rule and the third term using the product rule. The known derivatives of the elementary functions $ x^2 $, $ x^4 $, $ \\sin (x) $, $ \\ln (x) $, and $ \\exp(x) = e^x $, as well as the constant $ 7 $, were also used. Higher-order derivatives Higher order derivatives are the result of differentiating a function repeatedly. Given that $ f $ is a differentiable function, the derivative of $ f $ is the first derivative, denoted as ."
            },
            {
                "text": "The derivative of $ f' $ is the second derivative, denoted as , and the derivative of $ f $ is the third derivative, denoted as . By continuing this process, if it exists, the th derivative is the derivative of the th derivative or the derivative of order . As has been discussed above, the generalization of derivative of a function $ f $ may be denoted as . A function that has $ k $ successive derivatives is called $ k $ times differentiable. If the th derivative is continuous, then the function is said to be of differentiability class . A function that has infinitely many derivatives is called infinitely differentiable or smooth. Any polynomial function is infinitely differentiable; taking derivatives repeatedly will eventually result in a constant function, and all subsequent derivatives of that function are zero. One application of higher-order derivatives is in physics. Suppose that a function represents the position of an object at the time. The first derivative of that function is the velocity of an object with respect to time, the second derivative of the function is the acceleration of an object with respect to time, and the third derivative is the jerk."
            },
            {
                "text": "In other dimensions Vector-valued functions A vector-valued function $ \\mathbf{y} $ of a real variable sends real numbers to vectors in some vector space $ \\R^n $. A vector-valued function can be split up into its coordinate functions $ y_1(t), y_2(t), \\dots, y_n(t) $, meaning that $ \\mathbf{y} = (y_1(t), y_2(t), \\dots, y_n(t))$. This includes, for example, parametric curves in $ \\R^2 $ or $ \\R^3 $. The coordinate functions are real-valued functions, so the above definition of derivative applies to them. The derivative of $ \\mathbf{y}(t) $ is defined to be the vector, called the tangent vector, whose coordinates are the derivatives of the coordinate functions. That is, if the limit exists. The subtraction in the numerator is the subtraction of vectors, not scalars. If the derivative of $ \\mathbf{y} $ exists for every value of , then $ \\mathbf{y}' $ is another vector-valued function."
            },
            {
                "text": "Partial derivatives Functions can depend upon more than one variable. A partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant. Partial derivatives are used in vector calculus and differential geometry. As with ordinary derivatives, multiple notations exist: the partial derivative of a function $f(x, y, \\dots)$ with respect to the variable $x$ is variously denoted by among other possibilities. It can be thought of as the rate of change of the function in the $x$-direction. Here ∂ is a rounded d called the partial derivative symbol. To distinguish it from the letter d, ∂ is sometimes pronounced \"der\", \"del\", or \"partial\" instead of \"dee\". For example, let , then the partial derivative of function $ f $ with respect to both variables $ x $ and $ y $ are, respectively: In general, the partial derivative of a function $ f(x_1, \\dots, x_n) $ in the direction $ x_i $ at the point $(a_1, \\dots, a_n) $ is defined to be: This is fundamental for the study of the functions of several real variables."
            },
            {
                "text": "Let $ f(x_1, \\dots, x_n) $ be such a real-valued function. If all partial derivatives $ f $ with respect to $ x_j $ are defined at the point , these partial derivatives define the vector which is called the gradient of $ f $ at $ a $. If $ f $ is differentiable at every point in some domain, then the gradient is a vector-valued function $ \\nabla f $ that maps the point $ (a_1, \\dots, a_n) $ to the vector $ \\nabla f(a_1, \\dots, a_n) $. Consequently, the gradient determines a vector field. Directional derivatives If $ f $ is a real-valued function on , then the partial derivatives of $ f $ measure its variation in the direction of the coordinate axes. For example, if $ f $ is a function of $ x $ and , then its partial derivatives measure the variation in $ f $ in the $ x $ and $ y $ direction. However, they do not directly measure the variation of $ f $ in any other direction, such as along the diagonal line ."
            },
            {
                "text": "These are measured using directional derivatives. Given a vector , then the directional derivative of $ f $ in the direction of $ \\mathbf{v} $ at the point $ \\mathbf{x} $ is: If all the partial derivatives of $ f $ exist and are continuous at , then they determine the directional derivative of $ f $ in the direction $ \\mathbf{v} $ by the formula: Total derivative, total differential and Jacobian matrix When $ f $ is a function from an open subset of $ \\R^n $ to , then the directional derivative of $ f $ in a chosen direction is the best linear approximation to $ f $ at that point and in that direction. However, when , no single directional derivative can give a complete picture of the behavior of $ f $. The total derivative gives a complete picture by considering all directions at once. That is, for any vector $ \\mathbf{v} $ starting at , the linear approximation formula holds: Similarly with the single-variable derivative, $ f'(\\mathbf{a}) $ is chosen so that the error in this approximation is as small as possible."
            },
            {
                "text": "The total derivative of $ f $ at $ \\mathbf{a} $ is the unique linear transformation $ f'(\\mathbf{a}) \\colon \\R^n \\to \\R^m $ such that Here $ \\mathbf{h} $ is a vector in , so the norm in the denominator is the standard length on $ \\R^n $. However, $ f'(\\mathbf{a}) \\mathbf{h} $ is a vector in , and the norm in the numerator is the standard length on $ \\R^m $. If $ v $ is a vector starting at , then $ f'(\\mathbf{a}) \\mathbf{v} $ is called the pushforward of $ \\mathbf{v} $ by $ f $. If the total derivative exists at , then all the partial derivatives and directional derivatives of $ f $ exist at , and for all , $ f'(\\mathbf{a})\\mathbf{v} $ is the directional derivative of $ f $ in the direction . If $ f $ is written using coordinate functions, so that , then the total derivative can be expressed using the partial derivatives as a matrix."
            },
            {
                "text": "This matrix is called the Jacobian matrix of $ f $ at $ \\mathbf{a} $: Generalizations The concept of a derivative can be extended to many other settings. The common thread is that the derivative of a function at a point serves as a linear approximation of the function at that point. An important generalization of the derivative concerns complex functions of complex variables, such as functions from (a domain in) the complex numbers $\\C$ to . The notion of the derivative of such a function is obtained by replacing real variables with complex variables in the definition. If $\\C$ is identified with $\\R^2$ by writing a complex number $z$ as then a differentiable function from $\\C$ to $\\C$ is certainly differentiable as a function from $\\R^2$ to $\\R^2$ (in the sense that its partial derivatives all exist), but the converse is not true in general: the complex derivative only exists if the real derivative is complex linear and this imposes relations between the partial derivatives called the Cauchy–Riemann equations – see holomorphic functions."
            },
            {
                "text": "Another generalization concerns functions between differentiable or smooth manifolds. Intuitively speaking such a manifold $M$ is a space that can be approximated near each point $x$ by a vector space called its tangent space: the prototypical example is a smooth surface in . The derivative (or differential) of a (differentiable) map $f:M\\to N$ between manifolds, at a point $x$ in , is then a linear map from the tangent space of $M$ at $x$ to the tangent space of $N$ at . The derivative function becomes a map between the tangent bundles of $M$ and . This definition is used in differential geometry. Differentiation can also be defined for maps between vector space, such as Banach space, in which those generalizations are the Gateaux derivative and the Fréchet derivative.. See p. 209 for the Gateaux derivative, and p. 211 for the Fréchet derivative. One deficiency of the classical derivative is that very many functions are not differentiable. Nevertheless, there is a way of extending the notion of the derivative so that all continuous functions and many other functions can be differentiated using a concept known as the weak derivative."
            },
            {
                "text": "The idea is to embed the continuous functions in a larger space called the space of distributions and only require that a function is differentiable \"on average\". Properties of the derivative have inspired the introduction and study of many similar objects in algebra and topology; an example is differential algebra. Here, it consists of the derivation of some topics in abstract algebra, such as rings, ideals, field, and so on. The discrete equivalent of differentiation is finite differences. The study of differential calculus is unified with the calculus of finite differences in time scale calculus. The arithmetic derivative involves the function that is defined for the integers by the prime factorization. This is an analogy with the product rule. See also Covariant derivative Derivation Exterior derivative Functional derivative Integral Lie derivative Notes References . See the English version here. External links Khan Academy: \"Newton, Leibniz, and Usain Bolt\" Online Derivative Calculator from Wolfram Alpha. Category:Mathematical analysis Category:Differential calculus Category:Functions and mappings Category:Linear operators in calculus Category:Rates Category:Change"
            }
        ],
        "latex_formulas": [
            "''x'' {{=}} 0",
            "\\sin \\left(x^2\\right) + 2x^2 \\cos\\left(x^2\\right)",
            "f(x)",
            "a",
            "\\delta",
            "h",
            "|h| < \\delta",
            "h\\ne 0",
            "f(a+h)",
            "f",
            "L",
            "f",
            "a",
            "f",
            "a",
            "f",
            "x",
            "dx",
            "f",
            "x",
            "f",
            "x",
            "f'",
            "f",
            "a",
            "f'(a)",
            "f'(a)",
            "f",
            "f",
            "f(x) = x^2",
            "h \\neq 0",
            "h",
            "2a",
            "a",
            "2a",
            "(a,f(a))",
            "(a+h, f(a+h))",
            "h",
            "f",
            "a",
            "f",
            "1 + 1 + \\cdots + 1",
            "d",
            "f(x)",
            "\\operatorname{st}",
            "f(x) = x^2",
            "f",
            "f",
            "a",
            "a",
            "f",
            "x",
            "x",
            "a",
            "f",
            "a",
            "h",
            "a + h",
            "a",
            "a + h",
            "h",
            "h",
            "a + h",
            "a",
            "a + h",
            "f(x) = |x|",
            "h",
            "h",
            "h",
            "0",
            "h",
            "x=0",
            "f(x) = x^{1/3}",
            "x = 0",
            "dy",
            "y=f(x)",
            "y",
            "n",
            "y = f(x)",
            "u = g(x)",
            "y = f(g(x))",
            "du",
            "du",
            "dx",
            "du",
            "du",
            "f''",
            "f^{\\mathrm{iv}}",
            "f^{(n)}",
            "y",
            "\\dot{y}",
            "D f(x)",
            "n",
            "x",
            "D_x u",
            "a",
            "e",
            "\\frac{d}{dx}x^a = ax^{a-1}",
            "\\frac{d}{dx}e^x = e^x",
            "\\frac{d}{dx}a^x = a^x\\ln(a)",
            "a > 0",
            "\\frac{d}{dx}\\ln(x) = \\frac{1}{x}",
            "x > 0",
            "\\frac{d}{dx}\\log_a(x) = \\frac{1}{x\\ln(a)}",
            "x, a > 0",
            "\\frac{d}{dx}\\sin(x) = \\cos(x)",
            "\\frac{d}{dx}\\cos(x) = -\\sin(x)",
            "\\frac{d}{dx}\\tan(x) = \\sec^2(x) = \\frac{1}{\\cos^2(x)} = 1 + \\tan^2(x)",
            "\\frac{d}{dx}\\arcsin(x) = \\frac{1}{\\sqrt{1-x^2}}",
            "-1 < x < 1",
            "\\frac{d}{dx}\\arccos(x)= -\\frac{1}{\\sqrt{1-x^2}}",
            "-1 < x < 1",
            "\\frac{d}{dx}\\arctan(x)= \\frac{1}{{1+x^2}}",
            "f",
            "g",
            "f",
            "f'(x) = 0.",
            "(\\alpha f + \\beta g)' = \\alpha f' + \\beta g'",
            "f",
            "g",
            "\\alpha",
            "(fg)' = f 'g + fg'",
            "f",
            "(\\alpha f)' = \\alpha f'",
            "\\alpha",
            "\\alpha' f = 0 \\cdot f = 0",
            "\\left(\\frac{f}{g} \\right)' = \\frac{f'g - fg'}{g^2}",
            "f",
            "g",
            "f'(x) = h'(g(x)) \\cdot g'(x).",
            "f(x) = x^4 + \\sin \\left(x^2\\right) - \\ln(x) e^x + 7",
            "x^2",
            "x^4",
            "\\sin (x)",
            "\\ln (x)",
            "\\exp(x) = e^x",
            "7",
            "f",
            "f",
            "f'",
            "f''",
            "f",
            "k",
            "k",
            "k",
            "\\mathbf{y}",
            "\\R^n",
            "y_1(t), y_2(t), \\dots, y_n(t)",
            "\\mathbf{y} = (y_1(t), y_2(t), \\dots, y_n(t))",
            "\\R^2",
            "\\R^3",
            "\\mathbf{y}(t)",
            "\\mathbf{y}",
            "\\mathbf{y}'",
            "f(x, y, \\dots)",
            "x",
            "f_x",
            "f'_x",
            "\\partial_x f",
            "\\frac{\\partial}{\\partial x}f",
            "\\frac{\\partial f}{\\partial x}",
            "x",
            "f",
            "x",
            "y",
            "f(x_1, \\dots, x_n)",
            "x_i",
            "(a_1, \\dots, a_n)",
            "f(x_1, \\dots, x_n)",
            "f",
            "x_j",
            "f",
            "a",
            "f",
            "\\nabla f",
            "(a_1, \\dots, a_n)",
            "\\nabla f(a_1, \\dots, a_n)",
            "f",
            "f",
            "f",
            "x",
            "f",
            "x",
            "y",
            "f",
            "f",
            "\\mathbf{v}",
            "\\mathbf{x}",
            "\\frac{f(\\mathbf{x} + (k/\\lambda)(\\lambda\\mathbf{u})) - f(\\mathbf{x})}{k/\\lambda}\n= \\lambda\\cdot\\frac{f(\\mathbf{x} + k\\mathbf{u}) - f(\\mathbf{x})}{k}.",
            "f",
            "f",
            "\\mathbf{v}",
            "f",
            "\\R^n",
            "f",
            "f",
            "f",
            "\\mathbf{v}",
            "f'(\\mathbf{a})",
            "f",
            "\\mathbf{a}",
            "f'(\\mathbf{a}) \\colon \\R^n \\to \\R^m",
            "\\mathbf{h}",
            "\\R^n",
            "f'(\\mathbf{a}) \\mathbf{h}",
            "\\R^m",
            "v",
            "f'(\\mathbf{a}) \\mathbf{v}",
            "\\mathbf{v}",
            "f",
            "f",
            "f'(\\mathbf{a})\\mathbf{v}",
            "f",
            "f",
            "f",
            "\\mathbf{a}",
            "f(\\mathbf{a})",
            "f",
            "x \\mapsto f(a) + f'(a)(x-a)",
            "f",
            "k",
            "k",
            "\\mathbf{x}",
            "\\R^n",
            "k",
            "\\R^n",
            "\\R^m",
            "k",
            "f",
            "\\Delta",
            "\\mathbf{x} \\to (\\mathbf{x}, \\mathbf{x})",
            "f(\\mathbf{a})",
            "x_i - a_i",
            "\\mathbf{x}- \\mathbf{a}",
            "(Df)_i",
            "(D^2 f)_{jk}",
            "Df",
            "D^2 f",
            "\\C",
            "\\C",
            "\\R^2",
            "z",
            "\\C",
            "\\C",
            "\\R^2",
            "\\R^2",
            "M",
            "x",
            "f:M\\to N",
            "x",
            "M",
            "x",
            "N",
            "M"
        ]
    },
    "Mathematical_optimization": {
        "title": "Mathematical_optimization",
        "chunks": [
            {
                "text": "right|thumb|Graph of a surface given by z = f(x, y) = −(x² + y²) + 4. The global maximum at (x, y, z) = (0, 0, 4) is indicated by a blue dot. alt= Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criteria, from some set of available alternatives. \"The Nature of Mathematical Programming ,\" Mathematical Programming Glossary, INFORMS Computing Society. It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries. In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. Optimization problems Optimization problems can be divided into two categories, depending on whether the variables are continuous or discrete: An optimization problem with discrete variables is known as a discrete optimization, in which an object such as an integer, permutation or graph must be found from a countable set."
            },
            {
                "text": "A problem with continuous variables is known as a continuous optimization, in which optimal arguments from a continuous set must be found. They can include constrained problems and multimodal problems. An optimization problem can be represented in the following way: Given: a function $f : A → $\\mathbb R$$ from some set to the real numbers Sought: an element $x0 ∈ A$ such that $f(x0) ≤ f(x)$ for all $x ∈ A$ (\"minimization\") or such that $f(x0) ≥ f(x)$ for all $x ∈ A$ (\"maximization\"). Such a formulation is called an optimization problem or a mathematical programming problem (a term not directly related to computer programming, but still in use for example in linear programming – see History below). Many real-world and theoretical problems may be modeled in this general framework. Since the following is valid: $f(\\mathbf{x}_{0})\\geq f(\\mathbf{x}) \\Leftrightarrow -f(\\mathbf{x}_{0})\\leq -f(\\mathbf{x}),$ it suffices to solve only minimization problems."
            },
            {
                "text": "However, the opposite perspective of considering only maximization problems would be valid, too. Problems formulated using this technique in the fields of physics may refer to the technique as energy minimization, speaking of the value of the function as representing the energy of the system being modeled. In machine learning, it is always necessary to continuously evaluate the quality of a data model by using a cost function where a minimum implies a set of possibly optimal parameters with an optimal (lowest) error. Typically, is some subset of the Euclidean space $\\mathbb R^n$, often specified by a set of constraints, equalities or inequalities that the members of have to satisfy. The domain of is called the search space or the choice set, while the elements of are called candidate solutions or feasible solutions. The function is variously called an objective function, criterion function, loss function, cost function (minimization), utility function or fitness function (maximization), or, in certain fields, an energy function or energy functional. A feasible solution that minimizes (or maximizes) the objective function is called an optimal solution."
            },
            {
                "text": "In mathematics, conventional optimization problems are usually stated in terms of minimization. A local minimum $x*$ is defined as an element for which there exists some $δ > 0$ such that $\\forall\\mathbf{x}\\in A \\; \\text{where} \\;\\left\\Vert\\mathbf{x}-\\mathbf{x}^{\\ast}\\right\\Vert\\leq\\delta,\\,$ the expression $f(x*) ≤ f(x)$ holds; that is to say, on some region around $x*$ all of the function values are greater than or equal to the value at that element. Local maxima are defined similarly. While a local minimum is at least as good as any nearby elements, a global minimum is at least as good as every feasible element. Generally, unless the objective function is convex in a minimization problem, there may be several local minima. In a convex problem, if there is a local minimum that is interior (not on the edge of the set of feasible elements), it is also the global minimum, but a nonconvex problem may have more than one local minimum not all of which need be global minima."
            },
            {
                "text": "A large number of algorithms proposed for solving the nonconvex problems – including the majority of commercially available solvers – are not capable of making a distinction between locally optimal solutions and globally optimal solutions, and will treat the former as actual solutions to the original problem. Global optimization is the branch of applied mathematics and numerical analysis that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time to the actual optimal solution of a nonconvex problem. Notation Optimization problems are often expressed with special notation. Here are some examples: Minimum and maximum value of a function Consider the following notation: $\\min_{x\\in\\mathbb R}\\; \\left(x^2 + 1\\right)$ This denotes the minimum value of the objective function $x2 + 1$, when choosing from the set of real numbers $\\mathbb{R}$. The minimum value in this case is 1, occurring at $1=x = 0$. Similarly, the notation $\\max_{x\\in\\mathbb R}\\; 2x$ asks for the maximum value of the objective function $2x$, where may be any real number."
            },
            {
                "text": "In this case, there is no such maximum as the objective function is unbounded, so the answer is \"infinity\" or \"undefined\". Optimal input arguments Consider the following notation: $\\underset{x\\in(-\\infty,-1]}{\\operatorname{arg\\,min}} \\; x^2 + 1,$ or equivalently $\\underset{x}{\\operatorname{arg\\,min}} \\; x^2 + 1, \\; \\text{subject to:} \\; x\\in(-\\infty,-1].$ This represents the value (or values) of the argument in the interval $(−∞,−1]$ that minimizes (or minimize) the objective function $x2 + 1$ (the actual minimum value of that function is not what the problem asks for). In this case, the answer is $x , since $x is infeasible, that is, it does not belong to the feasible set."
            },
            {
                "text": "Similarly, $\\underset{x\\in[-5,5], \\; y\\in\\mathbb R}{\\operatorname{arg\\,max}} \\; x\\cos y,$ or equivalently $\\underset{x, \\; y}{\\operatorname{arg\\,max}} \\; x\\cos y, \\; \\text{subject to:} \\; x\\in[-5,5], \\; y\\in\\mathbb R,$ represents the ${x, y}$ pair (or pairs) that maximizes (or maximize) the value of the objective function $x cos y$, with the added constraint that lie in the interval $[−5,5]$ (again, the actual maximum value of the expression does not matter)."
            },
            {
                "text": "In this case, the solutions are the pairs of the form ${5, 2k} and ${−5, (2k + 1)}, where ranges over all integers. Operators $arg min$ and $arg max$ are sometimes also written as $argmin$ and $argmax$, and stand for argument of the minimum and argument of the maximum. History Fermat and Lagrange found calculus-based formulae for identifying optima, while Newton and Gauss proposed iterative methods for moving towards an optimum. The term \"linear programming\" for certain optimization cases was due to George B. Dantzig, although much of the theory had been introduced by Leonid Kantorovich in 1939. (Programming in this context does not refer to computer programming, but comes from the use of program by the United States military to refer to proposed training and logistics schedules, which were the problems Dantzig studied at that time.) Dantzig published the Simplex algorithm in 1947, and also John von Neumann and other researchers worked on the theoretical aspects of linear programming (like the theory of duality) around the same time."
            },
            {
                "text": "Other notable researchers in mathematical optimization include the following: Richard Bellman Dimitri Bertsekas Michel Bierlaire Stephen P. Boyd Roger Fletcher Martin Grötschel Ronald A. Howard Fritz John Narendra Karmarkar William Karush Leonid Khachiyan Bernard Koopman Harold Kuhn László Lovász David Luenberger Arkadi Nemirovski Yurii Nesterov Lev Pontryagin R. Tyrrell Rockafellar Naum Z. Shor Albert Tucker Major subfields Convex programming studies the case when the objective function is convex (minimization) or concave (maximization) and the constraint set is convex. This can be viewed as a particular case of nonlinear programming or as generalization of linear or convex quadratic programming. Linear programming (LP), a type of convex programming, studies the case in which the objective function f is linear and the constraints are specified using only linear equalities and inequalities. Such a constraint set is called a polyhedron or a polytope if it is bounded. Second-order cone programming (SOCP) is a convex program, and includes certain types of quadratic programs. Semidefinite programming (SDP) is a subfield of convex optimization where the underlying variables are semidefinite matrices. It is a generalization of linear and convex quadratic programming."
            },
            {
                "text": "Conic programming is a general form of convex programming. LP, SOCP and SDP can all be viewed as conic programs with the appropriate type of cone. Geometric programming is a technique whereby objective and inequality constraints expressed as posynomials and equality constraints as monomials can be transformed into a convex program. Integer programming studies linear programs in which some or all variables are constrained to take on integer values. This is not convex, and in general much more difficult than regular linear programming. Quadratic programming allows the objective function to have quadratic terms, while the feasible set must be specified with linear equalities and inequalities. For specific forms of the quadratic term, this is a type of convex programming. Fractional programming studies optimization of ratios of two nonlinear functions. The special class of concave fractional programs can be transformed to a convex optimization problem. Nonlinear programming studies the general case in which the objective function or the constraints or both contain nonlinear parts. This may or may not be a convex program. In general, whether the program is convex affects the difficulty of solving it."
            },
            {
                "text": "Stochastic programming studies the case in which some of the constraints or parameters depend on random variables. Robust optimization is, like stochastic programming, an attempt to capture uncertainty in the data underlying the optimization problem. Robust optimization aims to find solutions that are valid under all possible realizations of the uncertainties defined by an uncertainty set. Combinatorial optimization is concerned with problems where the set of feasible solutions is discrete or can be reduced to a discrete one. Stochastic optimization is used with random (noisy) function measurements or random inputs in the search process. Infinite-dimensional optimization studies the case when the set of feasible solutions is a subset of an infinite-dimensional space, such as a space of functions. Heuristics and metaheuristics make few or no assumptions about the problem being optimized. Usually, heuristics do not guarantee that any optimal solution need be found. On the other hand, heuristics are used to find approximate solutions for many complicated optimization problems. Constraint satisfaction studies the case in which the objective function f is constant (this is used in artificial intelligence, particularly in automated reasoning)."
            },
            {
                "text": "Constraint programming is a programming paradigm wherein relations between variables are stated in the form of constraints. Disjunctive programming is used where at least one constraint must be satisfied but not all. It is of particular use in scheduling. Space mapping is a concept for modeling and optimization of an engineering system to high-fidelity (fine) model accuracy exploiting a suitable physically meaningful coarse or surrogate model. In a number of subfields, the techniques are designed primarily for optimization in dynamic contexts (that is, decision making over time): Calculus of variations is concerned with finding the best way to achieve some goal, such as finding a surface whose boundary is a specific curve, but with the least possible area. Optimal control theory is a generalization of the calculus of variations which introduces control policies. Dynamic programming is the approach to solve the stochastic optimization problem with stochastic, randomness, and unknown model parameters. It studies the case in which the optimization strategy is based on splitting the problem into smaller subproblems. The equation that describes the relationship between these subproblems is called the Bellman equation."
            },
            {
                "text": "Mathematical programming with equilibrium constraints is where the constraints include variational inequalities or complementarities. Multi-objective optimization Adding more than one objective to an optimization problem adds complexity. For example, to optimize a structural design, one would desire a design that is both light and rigid. When two objectives conflict, a trade-off must be created. There may be one lightest design, one stiffest design, and an infinite number of designs that are some compromise of weight and rigidity. The set of trade-off designs that improve upon one criterion at the expense of another is known as the Pareto set. The curve created plotting weight against stiffness of the best designs is known as the Pareto frontier. A design is judged to be \"Pareto optimal\" (equivalently, \"Pareto efficient\" or in the Pareto set) if it is not dominated by any other design: If it is worse than another design in some respects and no better in any respect, then it is dominated and is not Pareto optimal. The choice among \"Pareto optimal\" solutions to determine the \"favorite solution\" is delegated to the decision maker."
            },
            {
                "text": "In other words, defining the problem as multi-objective optimization signals that some information is missing: desirable objectives are given but combinations of them are not rated relative to each other. In some cases, the missing information can be derived by interactive sessions with the decision maker. Multi-objective optimization problems have been generalized further into vector optimization problems where the (partial) ordering is no longer given by the Pareto ordering. Multi-modal or global optimization Optimization problems are often multi-modal; that is, they possess multiple good solutions. They could all be globally good (same cost function value) or there could be a mix of globally good and locally good solutions. Obtaining all (or at least some of) the multiple solutions is the goal of a multi-modal optimizer. Classical optimization techniques due to their iterative approach do not perform satisfactorily when they are used to obtain multiple solutions, since it is not guaranteed that different solutions will be obtained even with different starting points in multiple runs of the algorithm. Common approaches to global optimization problems, where multiple local extrema may be present include evolutionary algorithms, Bayesian optimization and simulated annealing."
            },
            {
                "text": "Classification of critical points and extrema Feasibility problem The satisfiability problem, also called the feasibility problem, is just the problem of finding any feasible solution at all without regard to objective value. This can be regarded as the special case of mathematical optimization where the objective value is the same for every solution, and thus any solution is optimal. Many optimization algorithms need to start from a feasible point. One way to obtain such a point is to relax the feasibility conditions using a slack variable; with enough slack, any starting point is feasible. Then, minimize that slack variable until the slack is null or negative. Existence The extreme value theorem of Karl Weierstrass states that a continuous real-valued function on a compact set attains its maximum and minimum value. More generally, a lower semi-continuous function on a compact set attains its minimum; an upper semi-continuous function on a compact set attains its maximum point or view. Necessary conditions for optimality One of Fermat's theorems states that optima of unconstrained problems are found at stationary points, where the first derivative or the gradient of the objective function is zero (see first derivative test)."
            },
            {
                "text": "More generally, they may be found at critical points, where the first derivative or gradient of the objective function is zero or is undefined, or on the boundary of the choice set. An equation (or set of equations) stating that the first derivative(s) equal(s) zero at an interior optimum is called a 'first-order condition' or a set of first-order conditions. Optima of equality-constrained problems can be found by the Lagrange multiplier method. The optima of problems with equality and/or inequality constraints can be found using the 'Karush–Kuhn–Tucker conditions'. Sufficient conditions for optimality While the first derivative test identifies points that might be extrema, this test does not distinguish a point that is a minimum from one that is a maximum or one that is neither. When the objective function is twice differentiable, these cases can be distinguished by checking the second derivative or the matrix of second derivatives (called the Hessian matrix) in unconstrained problems, or the matrix of second derivatives of the objective function and the constraints called the bordered Hessian in constrained problems. The conditions that distinguish maxima, or minima, from other stationary points are called 'second-order conditions' (see 'Second derivative test')."
            },
            {
                "text": "If a candidate solution satisfies the first-order conditions, then the satisfaction of the second-order conditions as well is sufficient to establish at least local optimality. Sensitivity and continuity of optima The envelope theorem describes how the value of an optimal solution changes when an underlying parameter changes. The process of computing this change is called comparative statics. The maximum theorem of Claude Berge (1963) describes the continuity of an optimal solution as a function of underlying parameters. Calculus of optimization For unconstrained problems with twice-differentiable functions, some critical points can be found by finding the points where the gradient of the objective function is zero (that is, the stationary points). More generally, a zero subgradient certifies that a local minimum has been found for minimization problems with convex functions and other locally Lipschitz functions, which meet in loss function minimization of the neural network. The positive-negative momentum estimation lets to avoid the local minimum and converges at the objective function global minimum. Further, critical points can be classified using the definiteness of the Hessian matrix: If the Hessian is positive definite at a critical point, then the point is a local minimum; if the Hessian matrix is negative definite, then the point is a local maximum; finally, if indefinite, then the point is some kind of saddle point."
            },
            {
                "text": "Constrained problems can often be transformed into unconstrained problems with the help of Lagrange multipliers. Lagrangian relaxation can also provide approximate solutions to difficult constrained problems. When the objective function is a convex function, then any local minimum will also be a global minimum. There exist efficient numerical techniques for minimizing convex functions, such as interior-point methods. Global convergence More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. The first and still popular method for ensuring convergence relies on line searches, which optimize a function along one dimension. A second and increasingly popular method for ensuring convergence uses trust regions. Both line searches and trust regions are used in modern methods of non-differentiable optimization. Usually, a global optimizer is much slower than advanced local optimizers (such as BFGS), so often an efficient global optimizer can be constructed by starting the local optimizer from different starting points. Computational optimization techniques To solve problems, researchers may use algorithms that terminate in a finite number of steps, or iterative methods that converge to a solution (on some specified class of problems), or heuristics that may provide approximate solutions to some problems (although their iterates need not converge)."
            },
            {
                "text": "Optimization algorithms Simplex algorithm of George Dantzig, designed for linear programming Extensions of the simplex algorithm, designed for quadratic programming and for linear-fractional programming Variants of the simplex algorithm that are especially suited for network optimization Combinatorial algorithms Quantum optimization algorithms Iterative methods The iterative methods used to solve problems of nonlinear programming differ according to whether they evaluate Hessians, gradients, or only function values. While evaluating Hessians (H) and gradients (G) improves the rate of convergence, for functions for which these quantities exist and vary sufficiently smoothly, such evaluations increase the computational complexity (or computational cost) of each iteration. In some cases, the computational complexity may be excessively high. One major criterion for optimizers is just the number of required function evaluations as this often is already a large computational effort, usually much more effort than within the optimizer itself, which mainly has to operate over the N variables. The derivatives provide detailed information for such optimizers, but are even harder to calculate, e.g. approximating the gradient takes at least N+1 function evaluations. For approximations of the 2nd derivatives (collected in the Hessian matrix), the number of function evaluations is in the order of N²."
            },
            {
                "text": "Newton's method requires the 2nd-order derivatives, so for each iteration, the number of function calls is in the order of N², but for a simpler pure gradient optimizer it is only N. However, gradient optimizers need usually more iterations than Newton's algorithm. Which one is best with respect to the number of function calls depends on the problem itself. Methods that evaluate Hessians (or approximate Hessians, using finite differences): Newton's method Sequential quadratic programming: A Newton-based method for small-medium scale constrained problems. Some versions can handle large-dimensional problems. Interior point methods: This is a large class of methods for constrained optimization, some of which use only (sub)gradient information and others of which require the evaluation of Hessians. Methods that evaluate gradients, or approximate gradients in some way (or even subgradients): Coordinate descent methods: Algorithms which update a single coordinate in each iteration Conjugate gradient methods: Iterative methods for large problems. (In theory, these methods terminate in a finite number of steps with quadratic objective functions, but this finite termination is not observed in practice on finite–precision computers.)"
            },
            {
                "text": "Gradient descent (alternatively, \"steepest descent\" or \"steepest ascent\"): A (slow) method of historical and theoretical interest, which has had renewed interest for finding approximate solutions of enormous problems. Subgradient methods: An iterative method for large locally Lipschitz functions using generalized gradients. Following Boris T. Polyak, subgradient–projection methods are similar to conjugate–gradient methods. Bundle method of descent: An iterative method for small–medium-sized problems with locally Lipschitz functions, particularly for convex minimization problems (similar to conjugate gradient methods). Ellipsoid method: An iterative method for small problems with quasiconvex objective functions and of great theoretical interest, particularly in establishing the polynomial time complexity of some combinatorial optimization problems. It has similarities with Quasi-Newton methods. Conditional gradient method (Frank–Wolfe) for approximate minimization of specially structured problems with linear constraints, especially with traffic networks. For general unconstrained problems, this method reduces to the gradient method, which is regarded as obsolete (for almost all problems). Quasi-Newton methods: Iterative methods for medium-large problems (e.g. N<1000). Simultaneous perturbation stochastic approximation (SPSA) method for stochastic optimization; uses random (efficient) gradient approximation."
            },
            {
                "text": "Methods that evaluate only function values: If a problem is continuously differentiable, then gradients can be approximated using finite differences, in which case a gradient-based method can be used. Interpolation methods Pattern search methods, which have better convergence properties than the Nelder–Mead heuristic (with simplices), which is listed below. Mirror descent Heuristics Besides (finitely terminating) algorithms and (convergent) iterative methods, there are heuristics. A heuristic is any algorithm which is not guaranteed (mathematically) to find the solution, but which is nevertheless useful in certain practical situations. List of some well-known heuristics: Differential evolution Dynamic relaxation Evolutionary algorithms Genetic algorithms Hill climbing with random restart Memetic algorithm Nelder–Mead simplicial heuristic: A popular heuristic for approximate minimization (without calling gradients) Particle swarm optimization Simulated annealing Stochastic tunneling Tabu search Applications Mechanics Problems in rigid body dynamics (in particular articulated rigid body dynamics) often require mathematical programming techniques, since you can view rigid body dynamics as attempting to solve an ordinary differential equation on a constraint manifold; the constraints are various nonlinear geometric constraints such as \"these two points must always coincide\", \"this surface must not penetrate any other\", or \"this point must always lie somewhere on this curve\"."
            },
            {
                "text": "Also, the problem of computing contact forces can be done by solving a linear complementarity problem, which can also be viewed as a QP (quadratic programming) problem. Many design problems can also be expressed as optimization programs. This application is called design optimization. One subset is the engineering optimization, and another recent and growing subset of this field is multidisciplinary design optimization, which, while useful in many problems, has in particular been applied to aerospace engineering problems. This approach may be applied in cosmology and astrophysics. Economics and finance Economics is closely enough linked to optimization of agents that an influential definition relatedly describes economics qua science as the \"study of human behavior as a relationship between ends and scarce means\" with alternative uses.Lionel Robbins (1935, 2nd ed.) An Essay on the Nature and Significance of Economic Science, Macmillan, p. 16. Modern optimization theory includes traditional optimization theory but also overlaps with game theory and the study of economic equilibria. The Journal of Economic Literature codes classify mathematical programming, optimization techniques, and related topics under JEL:C61-C63."
            },
            {
                "text": "In microeconomics, the utility maximization problem and its dual problem, the expenditure minimization problem, are economic optimization problems. Insofar as they behave consistently, consumers are assumed to maximize their utility, while firms are usually assumed to maximize their profit. Also, agents are often modeled as being risk-averse, thereby preferring to avoid risk. Asset prices are also modeled using optimization theory, though the underlying mathematics relies on optimizing stochastic processes rather than on static optimization. International trade theory also uses optimization to explain trade patterns between nations. The optimization of portfolios is an example of multi-objective optimization in economics. Since the 1970s, economists have modeled dynamic decisions over time using control theory. For example, dynamic search models are used to study labor-market behavior. A crucial distinction is between deterministic and stochastic models.A.G. Malliaris (2008). \"stochastic optimal control,\" The New Palgrave Dictionary of Economics, 2nd Edition. Abstract . Macroeconomists build dynamic stochastic general equilibrium (DSGE) models that describe the dynamics of the whole economy as the result of the interdependent optimizing decisions of workers, consumers, investors, and governments.From The New Palgrave Dictionary of Economics (2008), 2nd Edition with Abstract links:• \"numerical optimization methods in economics\" by Karl Schmedders• \"convex programming\" by Lawrence E. Blume• \"Arrow–Debreu model of general equilibrium\" by John Geanakoplos."
            },
            {
                "text": "Electrical engineering Some common applications of optimization techniques in electrical engineering include active filter design, stray field reduction in superconducting magnetic energy storage systems, space mapping design of microwave structures, handset antennas,N. Friedrich, “Space mapping outpaces EM optimization in handset-antenna design,” microwaves&rf, August 30, 2013. electromagnetics-based design. Electromagnetically validated design optimization of microwave components and antennas has made extensive use of an appropriate physics-based or empirical surrogate model and space mapping methodologies since the discovery of space mapping in 1993. Optimization techniques are also used in power-flow analysis. Civil engineering Optimization has been widely used in civil engineering. Construction management and transportation engineering are among the main branches of civil engineering that heavily rely on optimization. The most common civil engineering problems that are solved by optimization are cut and fill of roads, life-cycle analysis of structures and infrastructures, resource leveling, water resource allocation, traffic management and schedule optimization. Operations research Another field that uses optimization techniques extensively is operations research. Operations research also uses stochastic modeling and simulation to support improved decision-making. Increasingly, operations research uses stochastic programming to model dynamic decisions that adapt to events; such problems can be solved with large-scale optimization and stochastic optimization methods."
            },
            {
                "text": "Control engineering Mathematical optimization is used in much modern controller design. High-level controllers such as model predictive control (MPC) or real-time optimization (RTO) employ mathematical optimization. These algorithms run online and repeatedly determine values for decision variables, such as choke openings in a process plant, by iteratively solving a mathematical optimization problem including constraints and a model of the system to be controlled. Geophysics Optimization techniques are regularly used in geophysical parameter estimation problems. Given a set of geophysical measurements, e.g. seismic recordings, it is common to solve for the physical properties and geometrical shapes of the underlying rocks and fluids. The majority of problems in geophysics are nonlinear with both deterministic and stochastic methods being widely used. Molecular modeling Nonlinear optimization methods are widely used in conformational analysis. Computational systems biology Optimization techniques are used in many facets of computational systems biology such as model building, optimal experimental design, metabolic engineering, and synthetic biology. Linear programming has been applied to calculate the maximal possible yields of fermentation products, and to infer gene regulatory networks from multiple microarray datasets as well as transcriptional regulatory networks from high-throughput data."
            },
            {
                "text": "Nonlinear programming has been used to analyze energy metabolism and has been applied to metabolic engineering and parameter estimation in biochemical pathways. Machine learning Solvers See also Brachistochrone curve Curve fitting Deterministic global optimization Goal programming Important publications in optimization Least squares Mathematical Optimization Society (formerly Mathematical Programming Society) Mathematical optimization algorithms Mathematical optimization software Process optimization Simulation-based optimization Test functions for optimization Vehicle routing problem Notes Further reading G.L. Nemhauser, A.H.G. Rinnooy Kan and M.J. Todd (eds. ): Optimization, Elsevier, (1989). Stanislav Walukiewicz：Integer Programming, Springer，ISBN 978-9048140688, (1990). R. Fletcher: Practical Methods of Optimization, 2nd Ed.,Wiley, (2000). Panos M. Pardalos：Approximation and Complexity in Numerical Optimization: Continuous and Discrete Problems, Springer，ISBN 978-1-44194829-8, (2000). Xiaoqi Yang, K. L. Teo, Lou Caccetta (Eds. )：Optimization Methods and Applications，Springer, ISBN 978-0-79236866-3, (2001). Panos M. Pardalos, and Mauricio G. C. Resende(Eds. )：Handbook of Applied Optimization、Oxford Univ Pr on Demand, ISBN 978-0-19512594-8, (2002). Wil Michiels, Emile Aarts, and Jan Korst: Theoretical Aspects of Local Search, Springer, ISBN 978-3-64207148-5, (2006). Der-San Chen, Robert G. Batson,and Yu Dang： Applied Integer Programming: Modeling and Solution，Wiley，ISBN 978-0-47037306-4, (2010). Mykel J. Kochenderfer and Tim A. Wheeler: Algorithms for Optimization, The MIT Press, ISBN 978-0-26203942-0, (2019). Vladislav Bukshtynov: Optimization: Success in Practice, CRC Press (Taylor & Francis), ISBN 978-1-03222947-8, (2023) ."
            },
            {
                "text": "Rosario Toscano: Solving Optimization Problems with the Heuristic Kalman Algorithm: New Stochastic Methods, Springer, ISBN 978-3-031-52458-5 (2024). Immanuel M. Bomze, Tibor Csendes, Reiner Horst and Panos M. Pardalos: Developments in Global Optimization, Kluwer Academic, ISBN 978-1-4419-4768-0 (2010). External links Links to optimization source codes Category:Operations research Optimization"
            }
        ],
        "latex_formulas": [
            "''f'' : ''A'' → <math>\\mathbb R</math>",
            "'''x'''<sub>0</sub> ∈ ''A''",
            "''f''('''x'''<sub>0</sub>) ≤ ''f''('''x''')",
            "'''x''' ∈ ''A''",
            "''f''('''x'''<sub>0</sub>) ≥ ''f''('''x''')",
            "'''x''' ∈ ''A''",
            "'''x'''*",
            "''δ'' > 0",
            "''f''('''x'''*) ≤ ''f''('''x''')",
            "'''x'''*",
            "''x''<sup>2</sup> + 1",
            "''x'' = 0",
            "2''x''",
            "(−∞,−1]",
            "''x''<sup>2</sup> + 1",
            "''x'' {{=}} −1",
            "''x'' {{=}} 0",
            "{''x'', ''y''<nowiki>}</nowiki>",
            "''x'' cos ''y''",
            "[−5,5]",
            "{5, 2''k''{{pi}}<nowiki>}</nowiki>",
            "{−5, (2''k'' + 1){{pi}}<nowiki>}</nowiki>",
            "arg min",
            "arg max",
            "argmin",
            "argmax",
            "f(x)",
            "\\mathbb R",
            "f(\\mathbf{x}_{0})\\geq f(\\mathbf{x}) \\Leftrightarrow -f(\\mathbf{x}_{0})\\leq -f(\\mathbf{x}),",
            "\\mathbb R^n",
            "\\forall\\mathbf{x}\\in A \\; \\text{where} \\;\\left\\Vert\\mathbf{x}-\\mathbf{x}^{\\ast}\\right\\Vert\\leq\\delta,\\,",
            "\\min_{x\\in\\mathbb R}\\; \\left(x^2 + 1\\right)",
            "\\mathbb{R}",
            "\\max_{x\\in\\mathbb R}\\; 2x",
            "\\underset{x\\in(-\\infty,-1]}{\\operatorname{arg\\,min}} \\; x^2 + 1,",
            "\\underset{x}{\\operatorname{arg\\,min}} \\; x^2 + 1, \\; \\text{subject to:} \\; x\\in(-\\infty,-1].",
            "\\underset{x\\in[-5,5], \\; y\\in\\mathbb R}{\\operatorname{arg\\,max}} \\; x\\cos y,",
            "\\underset{x, \\; y}{\\operatorname{arg\\,max}} \\; x\\cos y, \\; \\text{subject to:} \\; x\\in[-5,5], \\; y\\in\\mathbb R,"
        ]
    },
    "Sparse_matrix": {
        "title": "Sparse_matrix",
        "chunks": [
            {
                "text": "right|thumb|A sparse matrix obtained when solving a finite element problem in two dimensions. The non-zero elements are shown in black. In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero. There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. By contrast, if most of the elements are non-zero, the matrix is considered dense. The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is sometimes referred to as the sparsity of the matrix. Conceptually, sparsity corresponds to systems with few pairwise interactions. For example, consider a line of balls connected by springs from one to the next: this is a sparse system, as only adjacent balls are coupled. By contrast, if the same line of balls were to have springs connecting each ball to all other balls, the system would correspond to a dense matrix."
            },
            {
                "text": "The concept of sparsity is useful in combinatorics and application areas such as network theory and numerical analysis, which typically have a low density of significant data or connections. Large sparse matrices often appear in scientific or engineering applications when solving partial differential equations. When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. Specialized computers have been made for sparse matrices, as they are common in the machine learning field. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and memory are wasted on the zeros. Sparse data is by nature more easily compressed and thus requires significantly less storage. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms. Special cases Banded An important special type of sparse matrices is band matrix, defined as follows. The lower bandwidth of a matrix $A$ is the smallest number $p$ such that the entry $ai,j$ vanishes whenever $i > j + p$."
            },
            {
                "text": "Similarly, the upper bandwidth is the smallest number $p$ such that $1=ai,j = 0$ whenever $i < j − p$ . For example, a tridiagonal matrix has lower bandwidth $1$ and upper bandwidth $1$. As another example, the following sparse matrix has lower and upper bandwidth both equal to 3. Notice that zeros are represented with dots for clarity. Matrices with reasonably small upper and lower bandwidth are known as band matrices and often lend themselves to simpler algorithms than general sparse matrices; or one can sometimes apply dense matrix algorithms and gain efficiency simply by looping over a reduced number of indices. By rearranging the rows and columns of a matrix $A$ it may be possible to obtain a matrix $A′$ with a lower bandwidth. A number of algorithms are designed for bandwidth minimization. Diagonal A very efficient structure for an extreme case of band matrices, the diagonal matrix, is to store just the entries in the main diagonal as a one-dimensional array, so a diagonal $n × n$ matrix requires only $n$ entries."
            },
            {
                "text": "Symmetric A symmetric sparse matrix arises as the adjacency matrix of an undirected graph; it can be stored efficiently as an adjacency list. Block diagonal A block-diagonal matrix consists of sub-matrices along its diagonal blocks. A block-diagonal matrix $A$ has the form where $Ak$ is a square matrix for all $1=k = 1, ..., n$. Use Reducing fill-in The fill-in of a matrix are those entries that change from an initial zero to a non-zero value during the execution of an algorithm. To reduce the memory requirements and the number of arithmetic operations used during an algorithm, it is useful to minimize the fill-in by switching rows and columns in the matrix. The symbolic Cholesky decomposition can be used to calculate the worst possible fill-in before doing the actual Cholesky decomposition. There are other methods than the Cholesky decomposition in use. Orthogonalization methods (such as QR factorization) are common, for example, when solving problems by least squares methods. While the theoretical fill-in is still the same, in practical terms the \"false non-zeros\" can be different for different methods."
            },
            {
                "text": "And symbolic versions of those algorithms can be used in the same manner as the symbolic Cholesky to compute worst case fill-in. Solving sparse matrix equations Both iterative and direct methods exist for sparse matrix solving. Iterative methods, such as conjugate gradient method and GMRES utilize fast computations of matrix-vector products $A x_i$, where matrix $A$ is sparse. The use of preconditioners can significantly accelerate convergence of such iterative methods. Storage A matrix is typically stored as a two-dimensional array. Each entry in the array represents an element $ai,j$ of the matrix and is accessed by the two indices $i$ and $j$. Conventionally, $i$ is the row index, numbered from top to bottom, and $j$ is the column index, numbered from left to right. For an $m × n$ matrix, the amount of memory required to store the matrix in this format is proportional to $m × n$ (disregarding the fact that the dimensions of the matrix also need to be stored). In the case of a sparse matrix, substantial memory requirement reductions can be realized by storing only the non-zero entries."
            },
            {
                "text": "Depending on the number and distribution of the non-zero entries, different data structures can be used and yield huge savings in memory when compared to the basic approach. The trade-off is that accessing the individual elements becomes more complex and additional structures are needed to be able to recover the original matrix unambiguously. Formats can be divided into two groups: Those that support efficient modification, such as DOK (Dictionary of keys), LIL (List of lists), or COO (Coordinate list). These are typically used to construct the matrices. Those that support efficient access and matrix operations, such as CSR (Compressed Sparse Row) or CSC (Compressed Sparse Column). Dictionary of keys (DOK) DOK consists of a dictionary that maps $(row, column)$-pairs to the value of the elements. Elements that are missing from the dictionary are taken to be zero. The format is good for incrementally constructing a sparse matrix in random order, but poor for iterating over non-zero values in lexicographical order. One typically constructs a matrix in this format and then converts to another more efficient format for processing.See scipy.sparse.dok_matrix List of lists (LIL) LIL stores one list per row, with each entry containing the column index and the value."
            },
            {
                "text": "Typically, these entries are kept sorted by column index for faster lookup. This is another format good for incremental matrix construction.See scipy.sparse.lil_matrix Coordinate list (COO) COO stores a list of $(row, column, value)$ tuples. Ideally, the entries are sorted first by row index and then by column index, to improve random access times. This is another format that is good for incremental matrix construction.See scipy.sparse.coo_matrix Compressed sparse row (CSR, CRS or Yale format) The compressed sparse row (CSR) or compressed row storage (CRS) or Yale format represents a matrix $M$ by three (one-dimensional) arrays, that respectively contain nonzero values, the extents of rows, and column indices. It is similar to COO, but compresses the row indices, hence the name. This format allows fast row access and matrix-vector multiplications ($Mx$). The CSR format has been in use since at least the mid-1960s, with the first complete description appearing in 1967. The CSR format stores a sparse $m × n$ matrix $M$ in row form using three (one-dimensional) arrays $(V, COL_INDEX, ROW_INDEX)$."
            },
            {
                "text": "Let $NNZ$ denote the number of nonzero entries in $M$. (Note that zero-based indices shall be used here.) The arrays $V$ and $COL_INDEX$ are of length $NNZ$, and contain the non-zero values and the column indices of those values respectively $COL_INDEX$ contains the column in which the corresponding entry $V$ is located. The array $ROW_INDEX$ is of length $m + 1$ and encodes the index in $V$ and $COL_INDEX$ where the given row starts. This is equivalent to $ROW_INDEX[j]$ encoding the total number of nonzeros above row $j$. The last element is $NNZ$ , i.e., the fictitious index in immediately after the last valid index $NNZ − 1$. For example, the matrix is a $4 × 4$ matrix with 4 nonzero elements, hence V = [ 5 8 3 6 ] COL_INDEX = [ 0 1 2 1 ] ROW_INDEX = [ 0 1 2 3 4 ] assuming a zero-indexed language."
            },
            {
                "text": "To extract a row, we first define: row_start = ROW_INDEX[row] row_end = ROW_INDEX[row + 1] Then we take slices from V and COL_INDEX starting at row_start and ending at row_end. To extract the row 1 (the second row) of this matrix we set row_start=1 and row_end=2. Then we make the slices V[1:2] = [8] and COL_INDEX[1:2] = [1]. We now know that in row 1 we have one element at column 1 with value 8. In this case the CSR representation contains 13 entries, compared to 16 in the original matrix. The CSR format saves on memory only when $NNZ < (m (n − 1) − 1) / 2$. Another example, the matrix is a $4 × 6$ matrix (24 entries) with 8 nonzero elements, so V = [ 10 20 30 40 50 60 70 80 ] COL_INDEX = [ 0 1 1 3 2 3 4 5 ] ROW_INDEX = [ 0 2 4 7 8 ] The whole is stored as 21 entries: 8 in $V$, 8 in $COL_INDEX$, and 5 in $ROW_INDEX$."
            },
            {
                "text": "$ROW_INDEX$ splits the array $V$ into rows: (10, 20) (30, 40) (50, 60, 70) (80), indicating the index of $V$ (and $COL_INDEX$) where each row starts and ends; $COL_INDEX$ aligns values in columns: (10, 20, ...) (0, 30, 0, 40, ...)(0, 0, 50, 60, 70, 0) (0, 0, 0, 0, 0, 80). Note that in this format, the first value of $ROW_INDEX$ is always zero and the last is always $NNZ$, so they are in some sense redundant (although in programming languages where the array length needs to be explicitly stored, $NNZ$ would not be redundant). Nonetheless, this does avoid the need to handle an exceptional case when computing the length of each row, as it guarantees the formula $ROW_INDEX[i + 1] − ROW_INDEX[i]$ works for any row $i$."
            },
            {
                "text": "Moreover, the memory cost of this redundant storage is likely insignificant for a sufficiently large matrix. The (old and new) Yale sparse matrix formats are instances of the CSR scheme. The old Yale format works exactly as described above, with three arrays; the new format combines $ROW_INDEX$ and $COL_INDEX$ into a single array and handles the diagonal of the matrix separately. For logical adjacency matrices, the data array can be omitted, as the existence of an entry in the row array is sufficient to model a binary adjacency relation. It is likely known as the Yale format because it was proposed in the 1977 Yale Sparse Matrix Package report from Department of Computer Science at Yale University. Compressed sparse column (CSC or CCS) CSC is similar to CSR except that values are read first by column, a row index is stored for each value, and column pointers are stored. For example, CSC is $(val, row_ind, col_ptr)$, where $val$ is an array of the (top-to-bottom, then left-to-right) non-zero values of the matrix; $row_ind$ is the row indices corresponding to the values; and, $col_ptr$ is the list of $val$ indexes where each column starts."
            },
            {
                "text": "The name is based on the fact that column index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, column slicing, and matrix-vector products. This is the traditional format for specifying a sparse matrix in MATLAB (via the sparse function). Software Many software libraries support sparse matrices, and provide solvers for sparse matrix equations. The following are open-source: PETSc, a large C library, containing many different matrix solvers for a variety of matrix storage formats. Trilinos, a large C++ library, with sub-libraries dedicated to the storage of dense and sparse matrices and solution of corresponding linear systems. Eigen3 is a C++ library that contains several sparse matrix solvers. However, none of them are parallelized. MUMPS (MUltifrontal Massively Parallel sparse direct Solver), written in Fortran90, is a frontal solver. deal.II, a finite element library that also has a sub-library for sparse linear systems and their solution. DUNE, another finite element library that also has a sub-library for sparse linear systems and their solution."
            },
            {
                "text": "Armadillo provides a user-friendly C++ wrapper for BLAS and LAPACK. SciPy provides support for several sparse matrix formats, linear algebra, and solvers. ALGLIB is a C++ and C# library with sparse linear algebra support ARPACK Fortran 77 library for sparse matrix diagonalization and manipulation, using the Arnoldi algorithm SLEPc Library for solution of large scale linear systems and sparse matrices scikit-learn, a Python library for machine learning, provides support for sparse matrices and solvers SparseArrays is a Julia standard library. PSBLAS, software toolkit to solve sparse linear systems supporting multiple formats also on GPU. History The term sparse matrix was possibly coined by Harry Markowitz who initiated some pioneering work but then left the field.Oral history interview with Harry M. Markowitz, pp. 9, 10. See also Notes References (This book, by a professor at the State University of New York at Stony Book, was the first book exclusively dedicated to Sparse Matrices. Graduate courses using this as a textbook were offered at that University in the early 1980s). Also NOAA Technical Memorandum NOS NGS-4, National Geodetic Survey, Rockville, MD. Referencing . (Open Access) Further reading Sparse Matrix Algorithms Research at the Texas A&M University. SuiteSparse Matrix Collection SMALL project A EU-funded project on sparse models, algorithms and dictionary learning for large-scale data. Category:Arrays"
            }
        ],
        "latex_formulas": [
            "'''A'''",
            "''p''",
            "''a''<sub>''i'',''j''</sub>",
            "''i'' > ''j'' + ''p''",
            "''p''",
            "''a''<sub>''i'',''j''</sub> = 0",
            "''i'' < ''j'' − ''p''",
            "1",
            "1",
            "'''A'''",
            "'''A'''′",
            "''n'' × ''n''",
            "''n''",
            "'''A'''",
            "'''A'''<sub>''k''</sub>",
            "''k'' = 1, ..., ''n''",
            "''a''<sub>''i'',''j''</sub>",
            "''i''",
            "''j''",
            "''i''",
            "''j''",
            "''m'' × ''n''",
            "''m'' × ''n''",
            "(row, column)",
            "(row, column, value)",
            "'''M'''",
            "'''M'''''x''",
            "''m'' × ''n''",
            "'''M'''",
            "(V, COL_INDEX, ROW_INDEX)",
            "NNZ",
            "'''M'''",
            "V",
            "COL_INDEX",
            "NNZ",
            "COL_INDEX",
            "V",
            "ROW_INDEX",
            "''m'' + 1",
            "V",
            "COL_INDEX",
            "ROW_INDEX[j]",
            "j",
            "NNZ",
            "V",
            "NNZ − 1",
            "4 × 4",
            "NNZ < (''m'' (''n'' − 1) − 1) / 2",
            "4 × 6",
            "V",
            "COL_INDEX",
            "ROW_INDEX",
            "ROW_INDEX",
            "V",
            "V",
            "COL_INDEX",
            "COL_INDEX",
            "ROW_INDEX",
            "NNZ",
            "NNZ",
            "ROW_INDEX[''i'' + 1] − ROW_INDEX[''i'']",
            "''i''",
            "ROW_INDEX",
            "COL_INDEX",
            "(val, row_ind, col_ptr)",
            "val",
            "row_ind",
            "col_ptr",
            "val",
            "\\left(\\begin{smallmatrix}\n11 & 22 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 33 & 44 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 55 & 66 & 77 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 88 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 99 \\\\\n\\end{smallmatrix}\\right)",
            "A x_i",
            "A"
        ]
    },
    "Computational_complexity_theory": {
        "title": "Computational_complexity_theory",
        "chunks": [
            {
                "text": "In theoretical computer science and mathematics, computational complexity theory focuses on classifying computational problems according to their resource usage, and explores the relationships between these classifications. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm. A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is part of the field of computational complexity."
            },
            {
                "text": "Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically. Computational problems A traveling salesman tour through 14 German cities Problem instances A computational problem can be viewed as an infinite collection of instances together with a set (possibly empty) of solutions for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved."
            },
            {
                "text": "In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g., 15) and the solution is \"yes\" if the number is prime and \"no\" otherwise (in this case, 15 is not prime and the answer is \"no\"). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input. To further highlight the difference between a problem and an instance, consider the following instance of the decision version of the travelling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany's 14 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances."
            },
            {
                "text": "Representing problem instances When considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary. Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently. Decision problems as formal languages thumb|A decision problem has only two possible outputs, yes or no (or alternately 1 or 0) on any input. Decision problems are one of the central objects of study in computational complexity theory. A decision problem is a type of computational problem where the answer is either yes or no (alternatively, 1 or 0)."
            },
            {
                "text": "A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer yes, the algorithm is said to accept the input string, otherwise it is said to reject the input. An example of a decision problem is the following. The input is an arbitrary graph. The problem consists in deciding whether the given graph is connected or not. The formal language associated with this decision problem is then the set of all connected graphs — to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings. Function problems A function problem is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem—that is, the output is not just yes or no."
            },
            {
                "text": "Notable examples include the traveling salesman problem and the integer factorization problem. It is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples $(a, b, c)$ such that the relation $a \\times b = c$ holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers. Measuring the size of an instance To measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance."
            },
            {
                "text": "The input size is typically measured in bits. Complexity theory studies how algorithms scale as input size increases. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with $2n$ vertices compared to the time taken for a graph with $n$ vertices? If the input size is $n$, the time taken can be expressed as a function of $n$. Since the time taken on different inputs of the same size can be different, the worst-case time complexity $T(n)$ is defined to be the maximum time taken over all inputs of size $n$. If $T(n)$ is a polynomial in $n$, then the algorithm is said to be a polynomial time algorithm. Cobham's thesis argues that a problem can be solved with a feasible amount of resources if it admits a polynomial-time algorithm. Machine models and complexity measures Turing machine An illustration of a Turing machine A Turing machine is a mathematical model of a general computing machine."
            },
            {
                "text": "It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a general model of a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway's Game of Life, cellular automata, lambda calculus or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory. Many types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines."
            },
            {
                "text": "They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others. A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes."
            },
            {
                "text": "For examples, see non-deterministic algorithm. Other machine models Many machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random-access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary.See What all these models have in common is that the machines operate deterministically. However, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems. Complexity measures For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine $M$ on input $x$ is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer (\"yes\" or \"no\")."
            },
            {
                "text": "A Turing machine $M$ is said to operate within time $f(n)$ if the time required by $M$ on each input of length $n$ is at most $f(n)$. A decision problem $A$ can be solved in time $f(n)$ if there exists a Turing machine operating in time $f(n)$ that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time $f(n)$ on a deterministic Turing machine is then denoted by DTIME($f(n)$). Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity."
            },
            {
                "text": "The complexity of an algorithm is often expressed using big O notation. Best, worst and average case complexity thumb|Visualization of the quicksort algorithm, which has average case performance $\\mathcal{O}(n\\log n)$ The best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size $n$ may be faster to solve than others, we define the following complexities: Best-case complexity: This is the complexity of solving the problem for the best input of size $n$. Average-case complexity: This is the complexity of solving the problem on an average. This complexity is only defined with respect to a probability distribution over the inputs. For instance, if all inputs of the same size are assumed to be equally likely to appear, the average case complexity can be defined with respect to the uniform distribution over all inputs of size $n$. Amortized analysis: Amortized analysis considers both the costly and less costly operations together over the whole series of operations of the algorithm."
            },
            {
                "text": "Worst-case complexity: This is the complexity of solving the problem for the worst input of size $n$. The order from cheap to costly is: Best, average (of discrete uniform distribution), amortized, worst. For example, the deterministic sorting algorithm quicksort addresses the problem of sorting a list of integers. The worst-case is when the pivot is always the largest or smallest value in the list (so the list is never divided). In this case, the algorithm takes time O($n^2$). If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is $O(n \\log n)$. The best case occurs when each pivoting divides the list in half, also needing $O(n \\log n)$ time. Upper and lower bounds on the complexity of problems To classify the computation time (or similar resources, such as space consumption), it is helpful to demonstrate upper and lower bounds on the maximum amount of time required by the most efficient algorithm to solve a given problem."
            },
            {
                "text": "The complexity of an algorithm is usually taken to be its worst-case complexity unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound $T(n)$ on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most $T(n)$. However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase \"all possible algorithms\" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of $T(n)$ for a problem requires showing that no algorithm can have time complexity lower than $T(n)$. Upper and lower bounds are usually stated using the big O notation, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if $T(n) = 7n^2 + 15n + 40$, in big O notation one would write $T(n) \\in O(n^2)$."
            },
            {
                "text": "Complexity classes Defining complexity classes A complexity class is a set of problems of related complexity. Simpler complexity classes are defined by the following factors: The type of computational problem: The most commonly used problems are decision problems. However, complexity classes can be defined based on function problems, counting problems, optimization problems, promise problems, etc. The model of computation: The most common model of computation is the deterministic Turing machine, but many complexity classes are based on non-deterministic Turing machines, Boolean circuits, quantum Turing machines, monotone circuits, etc. The resource (or resources) that is being bounded and the bound: These two properties are usually stated together, such as \"polynomial time\", \"logarithmic space\", \"constant depth\", etc. Some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following: The set of decision problems solvable by a deterministic Turing machine within time $f(n)$. (This complexity class is known as DTIME($f(n)$).)"
            },
            {
                "text": "But bounding the computation time above by some concrete function $f(n)$ often yields complexity classes that depend on the chosen machine model. For instance, the language $\\{xx \\mid x \\text{ is any binary string}\\}$ can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that \"the time complexities in any two reasonable and general models of computation are polynomially related\" . This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP. Important complexity classes A representation of the relation among complexity classes; L would be another step \"inside\" NL Many important complexity classes can be defined by bounding the time or space used by the algorithm."
            },
            {
                "text": "Some important complexity classes of decision problems defined in this manner are the following: Resource Determinism Complexity class Resource constraint Space Non-Deterministic NSPACE($f(n)$) $O(f(n))$ NL $O(\\log n)$ NPSPACE $O(\\text{poly}(n))$ NEXPSPACE $O(2^{\\text{poly}(n)})$ Deterministic DSPACE($f(n)$) $O(f(n))$ L $O(\\log n)$ PSPACE $O(\\text{poly}(n))$ EXPSPACE $O(2^{\\text{poly}(n)})$ Time Non-Deterministic NTIME($f(n)$) $O(f(n))$ NP $O(\\text{poly}(n))$ NEXPTIME $O(2^{\\text{poly}(n)})$ Deterministic DTIME($f(n)$) $O(f(n))$ P $O(\\text{poly}(n))$ EXPTIME $O(2^{\\text{poly}(n)})$ Logarithmic-space classes do not account for the space required to represent the problem."
            },
            {
                "text": "It turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch's theorem. Other important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems. Hierarchy theorems For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME($n$) is contained in DTIME($n^2$), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources."
            },
            {
                "text": "Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved. More precisely, the time hierarchy theorem states that $\\mathsf{DTIME}\\big(o(f(n)) \\big) \\subsetneq \\mathsf{DTIME} \\big(f(n) \\cdot \\log(f(n)) \\big)$. The space hierarchy theorem states that $\\mathsf{DSPACE}\\big(o(f(n))\\big) \\subsetneq \\mathsf{DSPACE} \\big(f(n) \\big)$. The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE. Reduction Many complexity classes are defined using the concept of a reduction."
            },
            {
                "text": "A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at most as difficult as another problem. For instance, if a problem $X$ can be solved using an algorithm for $Y$, $X$ is no more difficult than $Y$, and we say that $X$ reduces to $Y$. There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions. The most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication."
            },
            {
                "text": "This motivates the concept of a problem being hard for a complexity class. A problem $X$ is hard for a class of problems $C$ if every problem in $C$ can be reduced to $X$. Thus no problem in $C$ is harder than $X$, since an algorithm for $X$ allows us to solve any problem in $C$. The notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems. If a problem $X$ is in $C$ and hard for $C$, then $X$ is said to be complete for $C$. This means that $X$ is the hardest problem in $C$. (Since many problems could be equally hard, one might say that $X$ is one of the hardest problems in $C$.) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, $\\Pi_2$, to another problem, $\\Pi_1$, would indicate that there is no known polynomial-time solution for $\\Pi_1$."
            },
            {
                "text": "This is because a polynomial-time solution to $\\Pi_1$ would yield a polynomial-time solution to $\\Pi_2$. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP. Important open problems thumb|Diagram of complexity classes provided that P ≠ NP. The existence of problems in NP outside both P and NP-complete in this case was established by Ladner. P versus NP problem The complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP. The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution.See If the answer is yes, many important problems can be shown to have more efficient solutions."
            },
            {
                "text": "These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem. Problems in NP not known to be in P or NP-complete It was shown by Ladner that if $P \\neq NP$ then there exist problems in $NP$ that are neither in $P$ nor $NP$-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in $P$ or to be $NP$-complete. The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in $P$, $NP$-complete, or NP-intermediate."
            },
            {
                "text": "The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to László Babai and Eugene Luks has run time $O(2^{\\sqrt{n \\log n}})$ for graphs with $n$ vertices, although some recent work by Babai offers some potentially new perspectives on this. The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a prime factor less than $k$. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in $NP$ and in $co\\text{-}NP$ (and even in UP and co-UP)."
            },
            {
                "text": "If the problem is $NP$-complete, the polynomial time hierarchy will collapse to its first level (i.e., $NP$ will equal $co\\text{-}NP$). The best known algorithm for integer factorization is the general number field sieve, which takes time $O(e^{\\left(\\sqrt[3]{\\frac{64}{9}}\\right)\\sqrt[3]{(\\log n)}\\sqrt[3]{(\\log \\log n)^2}})$Wolfram MathWorld: Number Field Sieve to factor an odd integer $n$. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time. Unfortunately, this fact doesn't say much about where the problem lies with respect to non-quantum complexity classes. Separations between other complexity classes Many known complexity classes are suspected to be unequal, but this has not been proved. For instance $P \\subseteq NP \\subseteq PP \\subseteq PSPACE$, but it is possible that $P = PSPACE$."
            },
            {
                "text": "If $P$ is not equal to $NP$, then $P$ is not equal to $PSPACE$ either. Since there are many known complexity classes between $P$ and $PSPACE$, such as $RP$, $BPP$, $PP$, $BQP$, $MA$, $PH$, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory. Along the same lines, $co\\text{-}NP$ is the class containing the complement problems (i.e. problems with the yes/no answers reversed) of $NP$ problems. It is believedBoaz Barak's course on Computational Complexity Lecture 2 that $NP$ is not equal to $co\\text{-}NP$; however, it has not yet been proven. It is clear that if these two complexity classes are not equal then $P$ is not equal to $NP$, since $P = co\\text{-}P$."
            },
            {
                "text": "Thus if $P = NP$ we would have $co\\text{-}P = co\\text{-}NP$ whence $NP = P = co\\text{-}P = co\\text{-}NP$. Similarly, it is not known if $L$ (the set of all problems that can be solved in logarithmic space) is strictly contained in $P$ or equal to $P$. Again, there are many complexity classes between the two, such as $NL$ and $NC$, and it is not known if they are distinct or equal classes. It is suspected that $P$ and $BPP$ are equal. However, it is currently open if $BPP = NEXP$. Intractability A problem that can theoretically be solved, but requires impractical and finite resources (e.g., time) to do so, is known as an .Hopcroft, J.E., Motwani, R. and Ullman, J.D. (2007) Introduction to Automata Theory, Languages, and Computation, Addison Wesley, Boston/San Francisco/New York (page 368) Conversely, a problem that can be solved in practice is called a , literally \"a problem that can be handled\"."
            },
            {
                "text": "The term infeasible (literally \"cannot be done\") is sometimes used interchangeably with intractable, though this risks confusion with a feasible solution in mathematical optimization. Tractable problems are frequently identified with problems that have polynomial-time solutions ($P$, $PTIME$); this is known as the Cobham–Edmonds thesis. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If $NP$ is not the same as $P$, then NP-hard problems are also intractable in this sense. However, this identification is inexact: a polynomial-time solution with large degree or large leading coefficient grows quickly, and may be impractical for practical size problems; conversely, an exponential-time solution that grows slowly may be practical on realistic input, or a solution that takes a long time in the worst case may take a short time in most cases or the average case, and thus still be practical. Saying that a problem is not in $P$ does not imply that all large cases of the problem are hard or even that most of them are."
            },
            {
                "text": "For example, the decision problem in Presburger arithmetic has been shown not to be in $P$, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem. To see why exponential-time algorithms are generally unusable in practice, consider a program that makes $2^n$ operations before halting. For small $n$, say 100, and assuming for the sake of example that the computer does $10^{12}$ operations each second, the program would run for about $4 \\times 10^{10}$ years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. However, an exponential-time algorithm that takes $1.0001^n$ operations is practical until $n$ gets relatively large."
            },
            {
                "text": "Similarly, a polynomial time algorithm is not always practical. If its running time is, say, $n^{15}$, it is unreasonable to consider it efficient and it is still useless except on small instances. Indeed, in practice even $n^3$ or $n^2$ algorithms are often impractical on realistic sizes of problems. Continuous complexity theory Continuous complexity theory can refer to complexity theory of problems that involve continuous functions that are approximated by discretizations, as studied in numerical analysis. One approach to complexity theory of numerical analysis is information based complexity. Continuous complexity theory can also refer to complexity theory of the use of analog computation, which uses continuous dynamical systems and differential equations. Control theory can be considered a form of computation and differential equations are used in the modelling of continuous-time and hybrid discrete-continuous-time systems. History An early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lamé in 1844. Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers."
            },
            {
                "text": "Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer. The beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper \"On the Computational Complexity of Algorithms\" by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and space complexity, and proved the hierarchy theorems. In addition, in 1965 Edmonds suggested to consider a \"good\" algorithm to be one with running time bounded by a polynomial of the input size.Richard M. Karp, \"Combinatorics, Complexity, and Randomness\", 1985 Turing Award Lecture Earlier papers studying problems solvable by Turing machines with specific bounded resources include John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study of rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure.Trakhtenbrot, B.A."
            },
            {
                "text": ": Signalizing functions and tabular operators. Uchionnye Zapiski Penzenskogo Pedinstituta (Transactions of the Penza Pedagogoical Institute) 4, 75–87 (1956) (in Russian) As he remembers: In 1967, Manuel Blum formulated a set of axioms (now known as Blum axioms) specifying desirable properties of complexity measures on the set of computable functions and proved an important result, the so-called speed-up theorem. The field began to flourish in 1971 when Stephen Cook and Leonid Levin proved the existence of practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, \"Reducibility Among Combinatorial Problems\", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete. See also Computational complexity Descriptive complexity theory Game complexity Leaf language Limits of computation List of complexity classes List of computability and complexity topics List of unsolved problems in computer science Parameterized complexity Proof complexity Quantum complexity theory Structural complexity theory Transcomputational problem Computational complexity of mathematical operations Works on complexity References Citations Textbooks Surveys External links The Complexity Zoo Scott Aaronson: Why Philosophers Should Care About Computational Complexity Category:Computational fields of study"
            }
        ],
        "latex_formulas": [
            "(a, b, c)",
            "a \\times b = c",
            "2n",
            "n",
            "n",
            "n",
            "T(n)",
            "n",
            "T(n)",
            "n",
            "M",
            "x",
            "M",
            "f(n)",
            "M",
            "n",
            "f(n)",
            "A",
            "f(n)",
            "f(n)",
            "f(n)",
            "f(n)",
            "\\mathcal{O}(n\\log n)",
            "n",
            "n",
            "n",
            "n",
            "n^2",
            "O(n \\log n)",
            "O(n \\log n)",
            "T(n)",
            "T(n)",
            "T(n)",
            "T(n)",
            "T(n) = 7n^2 + 15n + 40",
            "T(n) \\in O(n^2)",
            "f(n)",
            "f(n)",
            "f(n)",
            "\\{xx \\mid x \\text{ is any binary string}\\}",
            "f(n)",
            "O(f(n))",
            "O(\\log n)",
            "O(\\text{poly}(n))",
            "O(2^{\\text{poly}(n)})",
            "f(n)",
            "O(f(n))",
            "O(\\log n)",
            "O(\\text{poly}(n))",
            "O(2^{\\text{poly}(n)})",
            "f(n)",
            "O(f(n))",
            "O(\\text{poly}(n))",
            "O(2^{\\text{poly}(n)})",
            "f(n)",
            "O(f(n))",
            "O(\\text{poly}(n))",
            "O(2^{\\text{poly}(n)})",
            "n",
            "n^2",
            "\\mathsf{DTIME}\\big(o(f(n)) \\big) \\subsetneq \\mathsf{DTIME} \\big(f(n) \\cdot \\log(f(n)) \\big)",
            "\\mathsf{DSPACE}\\big(o(f(n))\\big) \\subsetneq \\mathsf{DSPACE} \\big(f(n) \\big)",
            "X",
            "Y",
            "X",
            "Y",
            "X",
            "Y",
            "X",
            "C",
            "C",
            "X",
            "C",
            "X",
            "X",
            "C",
            "X",
            "C",
            "C",
            "X",
            "C",
            "X",
            "C",
            "X",
            "C",
            "\\Pi_2",
            "\\Pi_1",
            "\\Pi_1",
            "\\Pi_1",
            "\\Pi_2",
            "P \\neq NP",
            "NP",
            "P",
            "NP",
            "P",
            "NP",
            "P",
            "NP",
            "O(2^{\\sqrt{n \\log n}})",
            "n",
            "k",
            "NP",
            "co\\text{-}NP",
            "NP",
            "NP",
            "co\\text{-}NP",
            "O(e^{\\left(\\sqrt[3]{\\frac{64}{9}}\\right)\\sqrt[3]{(\\log n)}\\sqrt[3]{(\\log \\log n)^2}})",
            "n",
            "P \\subseteq NP \\subseteq PP \\subseteq PSPACE",
            "P = PSPACE",
            "P",
            "NP",
            "P",
            "PSPACE",
            "P",
            "PSPACE",
            "RP",
            "BPP",
            "PP",
            "BQP",
            "MA",
            "PH",
            "co\\text{-}NP",
            "NP",
            "NP",
            "co\\text{-}NP",
            "P",
            "NP",
            "P = co\\text{-}P",
            "P = NP",
            "co\\text{-}P = co\\text{-}NP",
            "NP = P = co\\text{-}P = co\\text{-}NP",
            "L",
            "P",
            "P",
            "NL",
            "NC",
            "P",
            "BPP",
            "BPP = NEXP",
            "P",
            "PTIME",
            "NP",
            "P",
            "P",
            "P",
            "2^n",
            "n",
            "10^{12}",
            "4 \\times 10^{10}",
            "1.0001^n",
            "n",
            "n^{15}",
            "n^3",
            "n^2"
        ]
    },
    "Receiver_operating_characteristic": {
        "title": "Receiver_operating_characteristic",
        "chunks": [
            {
                "text": "thumb|ROC curve of three predictors of peptide cleaving in the proteasome. A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier model (can be used for multi class classification as well) at varying threshold values. ROC analysis is commonly applied in the assessment of diagnostic test performance in clinical epidemiology. The ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting. The ROC can also be thought of as a plot of the statistical power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity as a function of false positive rate. Given that the probability distributions for both true positive and false positive are known, the ROC curve is obtained as the cumulative distribution function (CDF, area under the probability distribution from $-\\infty$ to the discrimination threshold) of the detection probability in the y-axis versus the CDF of the false positive probability on the x-axis."
            },
            {
                "text": "ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to the cost/benefit analysis of diagnostic decision making. Terminology The true-positive rate is also known as sensitivity or probability of detection. The false-positive rate is also known as the probability of false alarm and equals (1 − specificity). The ROC is also known as a relative operating characteristic curve, because it is a comparison of two operating characteristics (TPR and FPR) as the criterion changes.Swets, John A.; Signal detection theory and ROC analysis in psychology and diagnostics : collected papers, Lawrence Erlbaum Associates, Mahwah, NJ, 1996 History The ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields, starting in 1941, which led to its name (\"receiver operating characteristic\"). It was soon introduced to psychology to account for the perceptual detection of stimuli. ROC analysis has been used in medicine, radiology, biometrics, forecasting of natural hazards, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research."
            },
            {
                "text": "Basic concept A classification model (classifier or diagnosis) is a mapping of instances between certain classes/groups. Because the classifier or diagnosis result can be an arbitrary real value (continuous output), the classifier boundary between classes must be determined by a threshold value (for instance, to determine whether a person has hypertension based on a blood pressure measure). Or it can be a discrete class label, indicating one of the classes. Consider a two-class prediction problem (binary classification), in which the outcomes are labeled either as positive (p) or negative (n). There are four possible outcomes from a binary classifier. If the outcome from a prediction is p and the actual value is also p, then it is called a true positive (TP); however if the actual value is n then it is said to be a false positive (FP). Conversely, a true negative (TN) has occurred when both the prediction outcome and the actual value are n, and a false negative (FN) is when the prediction outcome is n while the actual value is p. To get an appropriate example in a real-world problem, consider a diagnostic test that seeks to determine whether a person has a certain disease."
            },
            {
                "text": "A false positive in this case occurs when the person tests positive, but does not actually have the disease. A false negative, on the other hand, occurs when the person tests negative, suggesting they are healthy, when they actually do have the disease. Consider an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows: ROC space The ROC space and plots of the four prediction examples. The ROC space for a \"better\" and \"worse\" classifier. The contingency table can derive several evaluation \"metrics\" (see infobox). To draw a ROC curve, only the true positive rate (TPR) and false positive rate (FPR) are needed (as functions of some classifier parameter). The TPR defines how many correct positive results occur among all positive samples available during the test. FPR, on the other hand, defines how many incorrect positive results occur among all negative samples available during the test. A ROC space is defined by FPR and TPR as x and y axes, respectively, which depicts relative trade-offs between true positive (benefits) and false positive (costs)."
            },
            {
                "text": "Since TPR is equivalent to sensitivity and FPR is equal to 1 − specificity, the ROC graph is sometimes called the sensitivity vs (1 − specificity) plot. Each prediction result or instance of a confusion matrix represents one point in the ROC space. The best possible prediction method would yield a point in the upper left corner or coordinate (0,1) of the ROC space, representing 100% sensitivity (no false negatives) and 100% specificity (no false positives). The (0,1) point is also called a perfect classification. A random guess would give a point along a diagonal line (the so-called line of no-discrimination) from the bottom left to the top right corners (regardless of the positive and negative base rates). An intuitive example of random guessing is a decision by flipping coins. As the size of the sample increases, a random classifier's ROC point tends towards the diagonal line. In the case of a balanced coin, it will tend to the point (0.5, 0.5). The diagonal divides the ROC space."
            },
            {
                "text": "Points above the diagonal represent good classification results (better than random); points below the line represent bad results (worse than random). Note that the output of a consistently bad predictor could simply be inverted to obtain a good predictor. Consider four prediction results from 100 positive and 100 negative instances: + A B C C′ TP = 63 FN = 37 100 FP = 28 TN = 72 100 91 109 200 TP = 77 FN = 23 100 FP = 77 TN = 23 100 154 46 200 TP = 24 FN = 76 100 FP = 88 TN = 12 100 112 88 200 TP = 76 FN = 24 100 FP = 12 TN = 88 100 88 112 200 TPR = 0.63 TPR = 0.77 TPR = 0.24 TPR = 0.76 FPR = 0.28 FPR = 0.77 FPR = 0.88 FPR = 0.12 PPV = 0.69 PPV = 0.50 PPV = 0.21 PPV = 0.86 F1 = 0.66 F1 = 0.61 F1 = 0.23 F1 = 0.81 ACC = 0.68 ACC = 0.50 ACC = 0.18 ACC = 0.82 Plots of the four results above in the ROC space are given in the figure."
            },
            {
                "text": "The result of method A clearly shows the best predictive power among A, B, and C. The result of B lies on the random guess line (the diagonal line), and it can be seen in the table that the accuracy of B is 50%. However, when C is mirrored across the center point (0.5,0.5), the resulting method C′ is even better than A. This mirrored method simply reverses the predictions of whatever method or test produced the C contingency table. Although the original C method has negative predictive power, simply reversing its decisions leads to a new predictive method C′ which has positive predictive power. When the C method predicts p or n, the C′ method would predict n or p, respectively. In this manner, the C′ test would perform the best. The closer a result from a contingency table is to the upper left corner, the better it predicts, but the distance from the random guess line in either direction is the best indicator of how much predictive power a method has. If the result is below the line (i.e."
            },
            {
                "text": "the method is worse than a random guess), all of the method's predictions must be reversed in order to utilize its power, thereby moving the result above the random guess line. Curves in ROC space 300px In binary classification, the class prediction for each instance is often made based on a continuous random variable $ X $, which is a \"score\" computed for the instance (e.g. the estimated probability in logistic regression). Given a threshold parameter $ T $, the instance is classified as \"positive\" if $ X>T $, and \"negative\" otherwise. $ X $ follows a probability density $ f_1 (x) $ if the instance actually belongs to class \"positive\", and $ f_0 (x) $ if otherwise. Therefore, the true positive rate is given by $ \\mbox{TPR}(T)= \\int_{T}^\\infty f_1(x) \\, dx $ and the false positive rate is given by $ \\mbox{FPR}(T)= \\int_{T}^\\infty f_0(x) \\, dx $."
            },
            {
                "text": "The ROC curve plots parametrically $\\mbox{TPR}(T)$ versus $\\mbox{FPR}(T)$ with $T$ as the varying parameter. For example, imagine that the blood protein levels in diseased people and healthy people are normally distributed with means of 2 g/dL and 1 g/dL respectively. A medical test might measure the level of a certain protein in a blood sample and classify any number above a certain threshold as indicating disease. The experimenter can adjust the threshold (green vertical line in the figure), which will in turn change the false positive rate. Increasing the threshold would result in fewer false positives (and more false negatives), corresponding to a leftward movement on the curve. The actual shape of the curve is determined by how much overlap the two distributions have. Criticisms thumb|Example of receiver operating characteristic (ROC) curve highlighting the area under the curve (AUC) sub-area with low sensitivity and low specificity in red and the sub-area with high or sufficient sensitivity and specificity in green. Several studies criticize certain applications of the ROC curve and its area under the curve as measurements for assessing binary classifications when they do not capture the information relevant to the application."
            },
            {
                "text": "The main criticism to the ROC curve described in these studies regards the incorporation of areas with low sensitivity and low specificity (both lower than 0.5) for the calculation of the total area under the curve (AUC)., as described in the plot on the right. According to the authors of these studies, that portion of area under the curve (with low sensitivity and low specificity) regards confusion matrices where binary predictions obtain bad results, and therefore should not be included for the assessment of the overall performance. Moreover, that portion of AUC indicates a space with high or low confusion matrix threshold which is rarely of interest for scientists performing a binary classification in any field. Another criticism to the ROC and its area under the curve is that they say nothing about precision and negative predictive value. A high ROC AUC, such as 0.9 for example, might correspond to low values of precision and negative predictive value, such as 0.2 and 0.1 in the [0, 1] range. If one performed a binary classification, obtained an ROC AUC of 0.9 and decided to focus only on this metric, they might overoptimistically believe their binary test was excellent."
            },
            {
                "text": "However, if this person took a look at the values of precision and negative predictive value, they might discover their values are low. The ROC AUC summarizes sensitivity and specificity, but does not inform regarding precision and negative predictive value. Further interpretations Sometimes, the ROC is used to generate a summary statistic. Common versions are: the intercept of the ROC curve with the line at 45 degrees orthogonal to the no-discrimination line - the balance point where Sensitivity = Specificity the intercept of the ROC curve with the tangent at 45 degrees parallel to the no-discrimination line that is closest to the error-free point (0,1) – also called Youden's J statistic and generalized as Informedness the area between the ROC curve and the no-discrimination line multiplied by two and subtraction of one is called the Gini coefficient, especially in the context of credit scoring. It should not be confused with the measure of statistical dispersion also called Gini coefficient. the area between the full ROC curve and the triangular ROC curve including only (0,0), (1,1) and one selected operating point $(tpr,fpr)$ – Consistency the area under the ROC curve, or \"AUC\" (\"area under curve\"), or A' (pronounced \"a-prime\"), or \"c-statistic\" (\"concordance statistic\")."
            },
            {
                "text": "the sensitivity index d′ (pronounced \"d-prime\"), the distance between the mean of the distribution of activity in the system under noise-alone conditions and its distribution under signal-alone conditions, divided by their standard deviation, under the assumption that both these distributions are normal with the same standard deviation. Under these assumptions, the shape of the ROC is entirely determined by d′. However, any attempt to summarize the ROC curve into a single number loses information about the pattern of tradeoffs of the particular discriminator algorithm. Probabilistic interpretation The area under the curve (often referred to as simply the AUC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative').Fawcett, Tom (2006); An introduction to ROC analysis, Pattern Recognition Letters, 27, 861–874. In other words, when given one randomly selected positive instance and one randomly selected negative instance, AUC is the probability that the classifier will be able to tell which one is which."
            },
            {
                "text": "This can be seen as follows: the area under the curve is given by (the integral boundaries are reversed as large threshold $ T $ has a lower value on the x-axis) $\\operatorname{TPR}(T): T \\to y(x)$ $\\operatorname{FPR}(T): T \\to x$ $ \\begin{align} A & = \\int_{x=0}^1 \\mbox{TPR}(\\mbox{FPR}^{-1}(x)) \\, dx \\\\[5pt] & = \\int_{\\infty}^{-\\infty} \\mbox{TPR}(T) \\mbox{FPR}'(T) \\, dT \\\\[5pt] & = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty I(T' \\ge T)f_1(T') f_0(T) \\, dT' \\, dT = P(X_1 \\ge X_0) \\end{align} $ where $ X_1 $ is the score for a positive instance and $ X_0 $ is the score for a negative instance, and $f_0$ and $f_1$ are probability densities as defined in previous section."
            },
            {
                "text": "If $ X_0 $ and $ X_1 $ follows two Gaussian distributions, then $ A = \\Phi\\left((\\mu_1-\\mu_0)/\\sqrt{\\sigma_1^2 + \\sigma_0^2}\\right) $. Area under the curve It can be shown that the AUC is closely related to the Mann–Whitney U, which tests whether positives are ranked higher than negatives. For a predictor , an unbiased estimator of its AUC can be expressed by the following Wilcoxon-Mann-Whitney statistic: $\\text{AUC}(f) = \\frac{\\sum _{t_0 \\in \\mathcal{D}^0} \\sum _{t_1 \\in \\mathcal{D}^1} \\textbf{1}[f(t_0) < f(t_1)]}{|\\mathcal{D}^0| \\cdot |\\mathcal{D}^1|}, $ where denotes an indicator function which returns 1 if $f(t_0) < f(t_1)$ otherwise return 0; $\\mathcal{D}^0$ is the set of negative examples, and $\\mathcal{D}^1$ is the set of positive examples. In the context of credit scoring, a rescaled version of AUC is often used: $G_1 = 2 \\operatorname{AUC} - 1$."
            },
            {
                "text": "$G_1$ is referred to as Gini index or Gini coefficient,Hand, David J.; and Till, Robert J. (2001); A simple generalization of the area under the ROC curve for multiple class classification problems, Machine Learning, 45, 171–186. but it should not be confused with the measure of statistical dispersion that is also called Gini coefficient. $G_1$ is a special case of Somers' D. It is also common to calculate the Area Under the ROC Convex Hull (ROC AUCH = ROCH AUC) as any point on the line segment between two prediction results can be achieved by randomly using one or the other system with probabilities proportional to the relative length of the opposite component of the segment. It is also possible to invert concavities – just as in the figure the worse solution can be reflected to become a better solution; concavities can be reflected in any line segment, but this more extreme form of fusion is much more likely to overfit the data. The machine learning community most often uses the ROC AUC statistic for model comparison."
            },
            {
                "text": "This practice has been questioned because AUC estimates are quite noisy and suffer from other problems. Nonetheless, the coherence of AUC as a measure of aggregated classification performance has been vindicated, in terms of a uniform rate distribution, and AUC has been linked to a number of other performance metrics such as the Brier score. Another problem with ROC AUC is that reducing the ROC Curve to a single number ignores the fact that it is about the tradeoffs between the different systems or performance points plotted and not the performance of an individual system, as well as ignoring the possibility of concavity repair, so that related alternative measures such as Informedness or DeltaP are recommended. These measures are essentially equivalent to the Gini for a single prediction point with DeltaP' = Informedness = 2AUC-1, whilst DeltaP = Markedness represents the dual (viz. predicting the prediction from the real class) and their geometric mean is the Matthews correlation coefficient. Whereas ROC AUC varies between 0 and 1 — with an uninformative classifier yielding 0.5 — the alternative measures known as Informedness, Certainty and Gini Coefficient (in the single parameterization or single system case) all have the advantage that 0 represents chance performance whilst 1 represents perfect performance, and −1 represents the \"perverse\" case of full informedness always giving the wrong response."
            },
            {
                "text": "Bringing chance performance to 0 allows these alternative scales to be interpreted as Kappa statistics. Informedness has been shown to have desirable characteristics for Machine Learning versus other common definitions of Kappa such as Cohen Kappa and Fleiss Kappa. Sometimes it can be more useful to look at a specific region of the ROC Curve rather than at the whole curve. It is possible to compute partial AUC. For example, one could focus on the region of the curve with low false positive rate, which is often of prime interest for population screening tests. Another common approach for classification problems in which P ≪ N (common in bioinformatics applications) is to use a logarithmic scale for the x-axis.Karplus, Kevin (2011); Better than Chance: the importance of null models, University of California, Santa Cruz, in Proceedings of the First International Workshop on Pattern Recognition in Proteomics, Structural Biology and Bioinformatics (PR PS BB 2011) The ROC area under the curve is also called c-statistic or c statistic. Other measures TOC Curve The Total Operating Characteristic (TOC) also characterizes diagnostic ability while revealing more information than the ROC."
            },
            {
                "text": "For each threshold, ROC reveals two ratios, TP/(TP + FN) and FP/(FP + TN). In other words, ROC reveals $\\frac{\\text{hits}}{\\text{hits}+\\text{misses}}$ and $\\frac{\\text{false alarms}}{\\text{false alarms} + \\text{correct rejections}}$. On the other hand, TOC shows the total information in the contingency table for each threshold. The TOC method reveals all of the information that the ROC method provides, plus additional important information that ROC does not reveal, i.e. the size of every entry in the contingency table for each threshold. TOC also provides the popular AUC of the ROC. ROC Curve These figures are the TOC and ROC curves using the same data and thresholds. Consider the point that corresponds to a threshold of 74. The TOC curve shows the number of hits, which is 3, and hence the number of misses, which is 7. Additionally, the TOC curve shows that the number of false alarms is 4 and the number of correct rejections is 16."
            },
            {
                "text": "At any given point in the ROC curve, it is possible to glean values for the ratios of $\\frac{\\text{false alarms}}{\\text{false alarms} + \\text{correct rejections}}$ and $\\frac{\\text{hits}}{\\text{hits}+\\text{misses}}$. For example, at threshold 74, it is evident that the x coordinate is 0.2 and the y coordinate is 0.3. However, these two values are insufficient to construct all entries of the underlying two-by-two contingency table. Detection error tradeoff graph thumb|Example DET graph An alternative to the ROC curve is the detection error tradeoff (DET) graph, which plots the false negative rate (missed detections) vs. the false positive rate (false alarms) on non-linearly transformed x- and y-axes. The transformation function is the quantile function of the normal distribution, i.e., the inverse of the cumulative normal distribution. It is, in fact, the same transformation as zROC, below, except that the complement of the hit rate, the miss rate or false negative rate, is used."
            },
            {
                "text": "This alternative spends more graph area on the region of interest. Most of the ROC area is of little interest; one primarily cares about the region tight against the y-axis and the top left corner – which, because of using miss rate instead of its complement, the hit rate, is the lower left corner in a DET plot. Furthermore, DET graphs have the useful property of linearity and a linear threshold behavior for normal distributions. The DET plot is used extensively in the automatic speaker recognition community, where the name DET was first used. The analysis of the ROC performance in graphs with this warping of the axes was used by psychologists in perception studies halfway through the 20th century, where this was dubbed \"double probability paper\". Z-score If a standard score is applied to the ROC curve, the curve will be transformed into a straight line. This z-score is based on a normal distribution with a mean of zero and a standard deviation of one. In memory strength theory, one must assume that the zROC is not only linear, but has a slope of 1.0."
            },
            {
                "text": "The normal distributions of targets (studied objects that the subjects need to recall) and lures (non studied objects that the subjects attempt to recall) is the factor causing the zROC to be linear. The linearity of the zROC curve depends on the standard deviations of the target and lure strength distributions. If the standard deviations are equal, the slope will be 1.0. If the standard deviation of the target strength distribution is larger than the standard deviation of the lure strength distribution, then the slope will be smaller than 1.0. In most studies, it has been found that the zROC curve slopes constantly fall below 1, usually between 0.5 and 0.9. Many experiments yielded a zROC slope of 0.8. A slope of 0.8 implies that the variability of the target strength distribution is 25% larger than the variability of the lure strength distribution. Another variable used is d' (d prime) (discussed above in \"Other measures\"), which can easily be expressed in terms of z-values. Although d' is a commonly used parameter, it must be recognized that it is only relevant when strictly adhering to the very strong assumptions of strength theory made above."
            },
            {
                "text": "The z-score of an ROC curve is always linear, as assumed, except in special situations. The Yonelinas familiarity-recollection model is a two-dimensional account of recognition memory. Instead of the subject simply answering yes or no to a specific input, the subject gives the input a feeling of familiarity, which operates like the original ROC curve. What changes, though, is a parameter for Recollection (R). Recollection is assumed to be all-or-none, and it trumps familiarity. If there were no recollection component, zROC would have a predicted slope of 1. However, when adding the recollection component, the zROC curve will be concave up, with a decreased slope. This difference in shape and slope result from an added element of variability due to some items being recollected. Patients with anterograde amnesia are unable to recollect, so their Yonelinas zROC curve would have a slope close to 1.0. History The ROC curve was first used during World War II for the analysis of radar signals before it was employed in signal detection theory. Following the attack on Pearl Harbor in 1941, the United States military began new research to increase the prediction of correctly detected Japanese aircraft from their radar signals."
            },
            {
                "text": "For these purposes they measured the ability of a radar receiver operator to make these important distinctions, which was called the Receiver Operating Characteristic. In the 1950s, ROC curves were employed in psychophysics to assess human (and occasionally non-human animal) detection of weak signals. In medicine, ROC analysis has been extensively used in the evaluation of diagnostic tests. ROC curves are also used extensively in epidemiology and medical research and are frequently mentioned in conjunction with evidence-based medicine. In radiology, ROC analysis is a common technique to evaluate new radiology techniques. In the social sciences, ROC analysis is often called the ROC Accuracy Ratio, a common technique for judging the accuracy of default probability models. ROC curves are widely used in laboratory medicine to assess the diagnostic accuracy of a test, to choose the optimal cut-off of a test and to compare diagnostic accuracy of several tests. ROC curves also proved useful for the evaluation of machine learning techniques. The first application of ROC in machine learning was by Spackman who demonstrated the value of ROC curves in comparing and evaluating different classification algorithms."
            },
            {
                "text": "ROC curves are also used in verification of forecasts in meteorology. Radar in detail As mentioned ROC curves are critical to radar operation and theory. The signals received at a receiver station, as reflected by a target, are often of very low energy, in comparison to the noise floor. The ratio of signal to noise is an important metric when determining if a target will be detected. This signal to noise ratio is directly correlated to the receiver operating characteristics of the whole radar system, which is used to quantify the ability of a radar system. Consider the development of a radar system. A specification for the abilities of the system may be provided in terms of probability of detect, $P_{D}$, with a certain tolerance for false alarms, $P_{FA}$. A simplified approximation of the required signal to noise ratio at the receiver station can be calculated by solving $P_{D}=\\frac{1}{2}\\operatorname{erfc}\\left(\\operatorname{erfc}^{-1}\\left(2P_{FA}\\right)-\\sqrt{\\mathcal{X}}\\right)$ for the signal to noise ratio $\\mathcal{X}$."
            },
            {
                "text": "Here, $\\mathcal{X}$ is not in decibels, as is common in many radar applications. Conversion to decibels is through $\\mathcal{X}_{dB}=10\\log_{10}\\mathcal{X}$. From this figure, the common entries in the radar range equation (with noise factors) may be solved, to estimate the required effective radiated power. ROC curves beyond binary classification The extension of ROC curves for classification problems with more than two classes is cumbersome. Two common approaches for when there are multiple classes are (1) average over all pairwise AUC values and (2) compute the volume under surface (VUS). To average over all pairwise classes, one computes the AUC for each pair of classes, using only the examples from those two classes as if there were no other classes, and then averages these AUC values over all possible pairs. When there are $c$ classes there will be $c(c − 1) / 2$ possible pairs of classes. The volume under surface approach has one plot a hypersurface rather than a curve and then measure the hypervolume under that hypersurface."
            },
            {
                "text": "Every possible decision rule that one might use for a classifier for $c$ classes can be described in terms of its true positive rates $(TPR. It is this set of rates that defines a point, and the set of all possible decision rules yields a cloud of points that define the hypersurface. With this definition, the VUS is the probability that the classifier will be able to correctly label all $c$ examples when it is given a set that has one randomly selected example from each class. The implementation of a classifier that knows that its input set consists of one example from each class might first compute a goodness-of-fit score for each of the $c possible pairings of an example to a class, and then employ the Hungarian algorithm to maximize the sum of the $c$ selected scores over all $c!$ possible ways to assign exactly one example to each class. Given the success of ROC curves for the assessment of classification models, the extension of ROC curves for other supervised tasks has also been investigated. Notable proposals for regression problems are the so-called regression error characteristic (REC) Curves and the Regression ROC (RROC) curves."
            },
            {
                "text": "In the latter, RROC curves become extremely similar to ROC curves for classification, with the notions of asymmetry, dominance and convex hull. Also, the area under RROC curves is proportional to the error variance of the regression model. See also Brier score Coefficient of determination Constant false alarm rate Detection error tradeoff Detection theory F1 score False alarm Hypothesis tests for accuracy Precision and recall ROCCET Sensitivity and specificity Total operating characteristic References External links easy ROC calculator Animated ROC demo ROC demo another ROC demo ROC video explanation An Introduction to the Total Operating Characteristic: Utility in Land Change Model Evaluation How to run the TOC Package in R TOC R package on Github Excel Workbook for generating TOC curves Further reading Balakrishnan, Narayanaswamy (1991); Handbook of the Logistic Distribution, Marcel Dekker, Inc., Gonen, Mithat (2007); Analyzing Receiver Operating Characteristic Curves Using SAS, SAS Press, Green, William H., (2003) Econometric Analysis, fifth edition, Prentice Hall, Hosmer, David W.; and Lemeshow, Stanley (2000); Applied Logistic Regression, 2nd ed., New York, NY: Wiley, Swets, John A.; Dawes, Robyn M.; and Monahan, John (2000); Better Decisions through Science, Scientific American, October, pp. 82–87 Category:Detection theory Category:Data mining Category:Biostatistics Category:Statistical classification Category:Summary statistics for contingency tables"
            }
        ],
        "latex_formulas": [
            "''c''",
            "''c''(''c'' − 1) / 2",
            "''c''",
            "(TPR{{sub|1}},&nbsp;.&nbsp;.&nbsp;.&nbsp;,&nbsp;TPR{{sub|''c''}})",
            "''c''",
            "''c''{{sup|2}}",
            "''c''",
            "''c''!",
            "-\\infty",
            "X",
            "T",
            "X>T",
            "X",
            "f_1 (x)",
            "f_0 (x)",
            "\\mbox{TPR}(T)= \\int_{T}^\\infty f_1(x) \\, dx",
            "\\mbox{FPR}(T)= \\int_{T}^\\infty f_0(x) \\, dx",
            "\\mbox{TPR}(T)",
            "\\mbox{FPR}(T)",
            "T",
            "(tpr,fpr)",
            "T",
            "\\operatorname{TPR}(T): T \\to y(x)",
            "\\operatorname{FPR}(T): T \\to x",
            "\\begin{align}\nA & = \\int_{x=0}^1 \\mbox{TPR}(\\mbox{FPR}^{-1}(x)) \\, dx \\\\[5pt]\n& = \\int_{\\infty}^{-\\infty} \\mbox{TPR}(T) \\mbox{FPR}'(T) \\, dT \\\\[5pt]\n& = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty I(T' \\ge T)f_1(T') f_0(T) \\, dT' \\, dT = P(X_1 \\ge X_0)\n\\end{align}",
            "X_1",
            "X_0",
            "f_0",
            "f_1",
            "X_0",
            "X_1",
            "A = \\Phi\\left((\\mu_1-\\mu_0)/\\sqrt{\\sigma_1^2 + \\sigma_0^2}\\right)",
            "\\text{AUC}(f) = \n  \\frac{\\sum _{t_0 \\in \\mathcal{D}^0} \\sum _{t_1 \\in \\mathcal{D}^1} \n  \\textbf{1}[f(t_0) < f(t_1)]}{|\\mathcal{D}^0| \\cdot |\\mathcal{D}^1|},",
            "f(t_0) < f(t_1)",
            "\\mathcal{D}^0",
            "\\mathcal{D}^1",
            "G_1 = 2 \\operatorname{AUC} - 1",
            "G_1",
            "G_1",
            "\\frac{\\text{hits}}{\\text{hits}+\\text{misses}}",
            "\\frac{\\text{false alarms}}{\\text{false alarms} + \\text{correct rejections}}",
            "\\frac{\\text{false alarms}}{\\text{false alarms} + \\text{correct rejections}}",
            "\\frac{\\text{hits}}{\\text{hits}+\\text{misses}}",
            "P_{D}",
            "P_{FA}",
            "P_{D}=\\frac{1}{2}\\operatorname{erfc}\\left(\\operatorname{erfc}^{-1}\\left(2P_{FA}\\right)-\\sqrt{\\mathcal{X}}\\right)",
            "\\mathcal{X}",
            "\\mathcal{X}",
            "\\mathcal{X}_{dB}=10\\log_{10}\\mathcal{X}"
        ]
    },
    "Data_visualization": {
        "title": "Data_visualization",
        "chunks": [
            {
                "text": "REDIRECT Data and information visualization Category:Data and information visualization Category:Visualization (graphics) Category:Statistical charts and diagrams"
            }
        ],
        "latex_formulas": []
    },
    "Scatter_plot": {
        "title": "Scatter_plot",
        "chunks": [
            {
                "text": "class=skin-invert-image|Waiting time between eruptions and the duration of the eruption for the Old Faithful Geyser in Yellowstone National Park, Wyoming, USA. This chart suggests there are generally two types of eruptions: short-wait-short-duration, and long-wait-long-duration. A 3D scatter plot allows the visualization of multivariate data. This scatter plot takes multiple scalar variables and uses them for different axes in phase space. The different variables are combined to form coordinates in the phase space and they are displayed using glyphs and coloured using another scalar variable.Visualizations that have been created with VisIt at wci.llnl.gov. Last updated: November 8, 2007. A scatter plot, also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram, is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.Utts, Jessica M. Seeing Through Statistics 3rd Edition, Thomson Brooks/Cole, 2005, pp 166-167."
            },
            {
                "text": "History According to Michael Friendly and Daniel Denis, the defining characteristic distinguishing scatter plots from line charts is the representation of specific observations of bivariate data where one variable is plotted on the horizontal axis and the other on the vertical axis. The two variables are often abstracted from a physical representation like the spread of bullets on a target or a geographic or celestial projection. While Edmund Halley created a bivariate plot of temperature and pressure in 1686, he omitted the specific data points used to demonstrate the relationship. Friendly and Denis claim his visualization was different from an actual scatter plot. Friendly and Denis attribute the first scatter plot to John Herschel. In 1833, Herschel plotted the angle between the central star in the constellation Virgo and Gamma Virginis over time to find how the angle changes over time, not through calculation but with freehand drawing and human judgment. Sir Francis Galton extended and popularized the scatter plot and many other statistical tools to pursue a scientific basis for eugenics. When, in 1886, Galton published a scatter plot and correlation ellipse of the height of parents and children, he extended Herschel's mere plotting of data points by binning and averaging adjacent cells to create a smoother visualization."
            },
            {
                "text": "Karl Pearson, R. A. Fischer, and other statisticians and eugenicists built on Galton's work and formalized correlations and significance testing. Overview A scatter plot can be used either when one continuous variable is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height would be on the -axis, and height would be on the -axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the dots' pattern slopes from lower left to upper right, it indicates a positive correlation between the variables being studied."
            },
            {
                "text": "If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a correct solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree to show nonlinear relationships between variables. The ability to do this can be enhanced by adding a smooth line such as LOESS. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. The scatter diagram is one of the seven basic tools of quality control. Scatter charts can be built in the form of bubble, marker, or/and line charts. Example For example, to display a link between a person's lung capacity, and how long that person could hold their breath, a researcher would choose a group of people to study, then measure each one's lung capacity (first variable) and how long that person could hold their breath (second variable)."
            },
            {
                "text": "The researcher would then plot the data in a scatter plot, assigning \"lung capacity\" to the horizontal axis, and \"time holding breath\" to the vertical axis. A person with a lung capacity of who held their breath for would be represented by a single dot on the scatter plot at the point (400, 21.7) in the Cartesian coordinates. The scatter plot of all the people in the study would enable the researcher to obtain a visual comparison of the two variables in the data set and will help to determine what kind of relationship there might be between the two variables. Scatter plot matrices For a set of data variables (dimensions) X1, X2, ... , Xk, the scatter plot matrix shows all the pairwise scatter plots of the variables on a single view with multiple scatterplots in a matrix format. For variables, the scatterplot matrix will contain rows and columns. A plot located on the intersection of row and th column is a plot of variables Xi versus Xj.Scatter Plot Matrix at itl.nist.gov. This means that each row and column is one dimension, and each cell plots a scatter plot of two dimensions. A generalized scatter plot matrix offers a range of displays of paired combinations of categorical and quantitative variables. A mosaic plot, fluctuation diagram, or faceted bar chart may be used to display two categorical variables. Other plots are used for one categorical and one quantitative variables."
            },
            {
                "text": "class=skin-invert-image|center|Visualization of 3D data along with the correspondent scatterplot matrix See also Data and information visualization Rug plot Bar graph Line chart Scagnostics Dot plot (statistics) Parity plot References Further reading Cattaneo, Matias D.; Crump, Richard K.; Farrell, Max H.; Feng, Yingjie (2024). \"On Binscatter\". American Economic Review. 114 (5): 1488–1514. External links What is a scatterplot? Correlation scatter-plot matrix for ordered-categorical data – Explanation and R code Density scatterplot for large datasets (hundreds of millions of points) Category:Statistical charts and diagrams Category:Quality control tools"
            }
        ],
        "latex_formulas": []
    },
    "Line_graph": {
        "title": "Line_graph",
        "chunks": [
            {
                "text": "In the mathematical discipline of graph theory, the line graph of an undirected graph is another graph $L(G)$ that represents the adjacencies between edges of . $L(G)$ is constructed in the following way: for each edge in , make a vertex in $L(G)$; for every two edges in that have a vertex in common, make an edge between their corresponding vertices in $L(G)$. The name line graph comes from a paper by although both and used the construction before this. Other terms used for the line graph include the covering graph, the derivative, the edge-to-vertex dual, the conjugate, the representative graph, and the θ-obrazom, as well as the edge graph, the interchange graph, the adjoint graph, and the derived graph., p. 71. proved that with one exceptional case the structure of a connected graph can be recovered completely from its line graph. Many other properties of line graphs follow by translating the properties of the underlying graph from vertices into edges, and by Whitney's theorem the same translation can also be done in the other direction."
            },
            {
                "text": "Line graphs are claw-free, and the line graphs of bipartite graphs are perfect. Line graphs are characterized by nine forbidden subgraphs and can be recognized in linear time. Various extensions of the concept of a line graph have been studied, including line graphs of line graphs, line graphs of multigraphs, line graphs of hypergraphs, and line graphs of weighted graphs. Formal definition Given a graph , its line graph $L(G)$ is a graph such that each vertex of $L(G)$ represents an edge of ; and two vertices of $L(G)$ are adjacent if and only if their corresponding edges share a common endpoint (\"are incident\") in . That is, it is the intersection graph of the edges of , representing each edge by the set of its two endpoints. Example The following figures show a graph (left, with blue vertices) and its line graph (right, with green vertices). Each vertex of the line graph is shown labeled with the pair of endpoints of the corresponding edge in the original graph."
            },
            {
                "text": "For instance, the green vertex on the right labeled 1,3 corresponds to the edge on the left between the blue vertices 1 and 3. Green vertex 1,3 is adjacent to three other green vertices: 1,4 and 1,2 (corresponding to edges sharing the endpoint 1 in the blue graph) and 4,3 (corresponding to an edge sharing the endpoint 3 in the blue graph). Properties Translated properties of the underlying graph Properties of a graph that depend only on adjacency between edges may be translated into equivalent properties in $L(G)$ that depend on adjacency between vertices. For instance, a matching in is a set of edges no two of which are adjacent, and corresponds to a set of vertices in $L(G)$ no two of which are adjacent, that is, an independent set. Thus, The line graph of a connected graph is connected. If is connected, it contains a path connecting any two of its edges, which translates into a path in $L(G)$ containing any two of the vertices of $L(G)$."
            },
            {
                "text": "However, a graph that has some isolated vertices, and is therefore disconnected, may nevertheless have a connected line graph.The need to consider isolated vertices when considering the connectivity of line graphs is pointed out by , p. 32. A line graph has an articulation point if and only if the underlying graph has a bridge for which neither endpoint has degree one. For a graph with vertices and edges, the number of vertices of the line graph $L(G)$ is , and the number of edges of $L(G)$ is half the sum of the squares of the degrees of the vertices in , minus ., Theorem 8.1, p. 72. An independent set in $L(G)$ corresponds to a matching in . In particular, a maximum independent set in $L(G)$ corresponds to maximum matching in . Since maximum matchings may be found in polynomial time, so may the maximum independent sets of line graphs, despite the hardness of the maximum independent set problem for more general families of graphs. Similarly, a rainbow-independent set in $L(G)$ corresponds to a rainbow matching in ."
            },
            {
                "text": "The edge chromatic number of a graph is equal to the vertex chromatic number of its line graph $L(G)$.. Also in free online edition, Chapter 5 (\"Colouring\"), p. 118. The line graph of an edge-transitive graph is vertex-transitive. This property can be used to generate families of graphs that (like the Petersen graph) are vertex-transitive but are not Cayley graphs: if is an edge-transitive graph that has at least five vertices, is not bipartite, and has odd vertex degrees, then $L(G)$ is a vertex-transitive non-Cayley graph.. Lauri and Scapellato credit this result to Mark Watkins. If a graph has an Euler cycle, that is, if is connected and has an even number of edges at each vertex, then the line graph of is Hamiltonian. However, not all Hamiltonian cycles in line graphs come from Euler cycles in this way; for instance, the line graph of a Hamiltonian graph is itself Hamiltonian, regardless of whether is also Eulerian., Theorem 8.8, p. 80. If two simple graphs are isomorphic then their line graphs are also isomorphic."
            },
            {
                "text": "The Whitney graph isomorphism theorem provides a converse to this for all but one pair of connected graphs. In the context of complex network theory, the line graph of a random network preserves many of the properties of the network such as the small-world property (the existence of short paths between all pairs of vertices) and the shape of its degree distribution. observe that any method for finding vertex clusters in a complex network can be applied to the line graph and used to cluster its edges instead. Whitney isomorphism theorem thumb|The diamond graph (left) and its more-symmetric line graph (right), an exception to the strong Whitney theorem If the line graphs of two connected graphs are isomorphic, then the underlying graphs are isomorphic, except in the case of the triangle graph $K and the claw $K, which have isomorphic line graphs but are not themselves isomorphic. ; ; , Theorem 8.3, p. 72. Harary gives a simplified proof of this theorem by . As well as $K and $K, there are some other exceptional small graphs with the property that their line graph has a higher degree of symmetry than the graph itself."
            },
            {
                "text": "For instance, the diamond graph $K (two triangles sharing an edge) has four graph automorphisms but its line graph $K has eight. In the illustration of the diamond graph shown, rotating the graph by 90 degrees is not a symmetry of the graph, but is a symmetry of its line graph. However, all such exceptional cases have at most four vertices. A strengthened version of the Whitney isomorphism theorem states that, for connected graphs with more than four vertices, there is a one-to-one correspondence between isomorphisms of the graphs and isomorphisms of their line graphs. ; . Analogues of the Whitney isomorphism theorem have been proven for the line graphs of multigraphs, but are more complicated in this case. Strongly regular and perfect line graphs A line perfect graph. The edges in each biconnected component are colored black if the component is bipartite, blue if the component is a tetrahedron, and red if the component is a book of triangles. The line graph of the complete graph is also known as the triangular graph, the Johnson graph $J(n, 2)$, or the complement of the Kneser graph $KG."
            },
            {
                "text": "Triangular graphs are characterized by their spectra, except for $1=n = 8$.. See in particular Proposition 8, p. 262. They may also be characterized (again with the exception of $K) as the strongly regular graphs with parameters $srg(n(n – 1)/2, 2(n – 2), n – 2, 4)$., Theorem 8.6, p. 79. Harary credits this result to independent papers by L. C. Chang (1959) and A. J. Hoffman (1960). The three strongly regular graphs with the same parameters and spectrum as $L(K are the Chang graphs, which may be obtained by graph switching from $L(K. The line graph of a bipartite graph is perfect (see Kőnig's theorem), but need not be bipartite as the example of the claw graph shows. The line graphs of bipartite graphs form one of the key building blocks of perfect graphs, used in the proof of the strong perfect graph theorem.. See also . A special case of these graphs are the rook's graphs, line graphs of complete bipartite graphs."
            },
            {
                "text": "Like the line graphs of complete graphs, they can be characterized with one exception by their numbers of vertices, numbers of edges, and number of shared neighbors for adjacent and non-adjacent points. The one exceptional case is $L(K, which shares its parameters with the Shrikhande graph. When both sides of the bipartition have the same number of vertices, these graphs are again strongly regular., Theorem 8.7, p. 79. Harary credits this characterization of line graphs of complete bipartite graphs to Moon and Hoffman. The case of equal numbers of vertices on both sides had previously been proven by Shrikhande. It has been shown that, except for $C , $C , and $C , all connected strongly regular graphs can be made non-strongly regular within two line graph transformations. The extension to disconnected graphs would require that the graph is not a disjoint union of $C. More generally, a graph is said to be a line perfect graph if $L(G)$ is a perfect graph. The line perfect graphs are exactly the graphs that do not contain a simple cycle of odd length greater than three."
            },
            {
                "text": "; . Equivalently, a graph is line perfect if and only if each of its biconnected components is either bipartite or of the form $K (the tetrahedron) or $K (a book of one or more triangles all sharing a common edge). Every line perfect graph is itself perfect. Other related graph families All line graphs are claw-free graphs, graphs without an induced subgraph in the form of a three-leaf tree. As with claw-free graphs more generally, every connected line graph $L(G)$ with an even number of edges has a perfect matching;. . equivalently, this means that if the underlying graph has an even number of edges, its edges can be partitioned into two-edge paths. The line graphs of trees are exactly the claw-free block graphs., Theorem 8.5, p. 78. Harary credits the result to Gary Chartrand. These graphs have been used to solve a problem in extremal graph theory, of constructing a graph with a given number of edges and vertices whose largest tree induced as a subgraph is as small as possible.. All eigenvalues of the adjacency matrix of a line graph are at least −2."
            },
            {
                "text": "The reason for this is that can be written as $A = J^\\mathsf{T}J - 2I$, where is the signless incidence matrix of the pre-line graph and is the identity. In particular, $A + 2I$ is the Gramian matrix of a system of vectors: all graphs with this property have been called generalized line graphs. Characterization and recognition Clique partition Partition of a line graph into cliques For an arbitrary graph , and an arbitrary vertex in , the set of edges incident to corresponds to a clique in the line graph $L(G)$. The cliques formed in this way partition the edges of $L(G)$. Each vertex of $L(G)$ belongs to exactly two of them (the two cliques corresponding to the two endpoints of the corresponding edge in ). The existence of such a partition into cliques can be used to characterize the line graphs: A graph is the line graph of some other graph or multigraph if and only if it is possible to find a collection of cliques in (allowing some of the cliques to be single vertices) that partition the edges of , such that each vertex of belongs to exactly two of the cliques., Theorem 8.4, p. 74, gives three equivalent characterizations of line graphs: the partition of the edges into cliques, the property of being claw-free and odd diamond-free, and the nine forbidden graphs of Beineke."
            },
            {
                "text": "It is the line graph of a graph (rather than a multigraph) if this set of cliques satisfies the additional condition that no two vertices of are both in the same two cliques. Given such a family of cliques, the underlying graph for which is the line graph can be recovered by making one vertex in for each clique, and an edge in for each vertex in with its endpoints being the two cliques containing the vertex in . By the strong version of Whitney's isomorphism theorem, if the underlying graph has more than four vertices, there can be only one partition of this type. For example, this characterization can be used to show that the following graph is not a line graph: 100px|class=skin-invert In this example, the edges going upward, to the left, and to the right from the central degree-four vertex do not have any cliques in common. Therefore, any partition of the graph's edges into cliques would have to have at least one clique for each of these three edges, and these three cliques would all intersect in that central vertex, violating the requirement that each vertex appear in exactly two cliques."
            },
            {
                "text": "Thus, the graph shown is not a line graph. Forbidden subgraphs The nine minimal non-line graphs, from Beineke's forbidden-subgraph characterization of line graphs. A graph is a line graph if and only if it does not contain one of these nine graphs as an induced subgraph. Another characterization of line graphs was proven in (and reported earlier without proof by ). He showed that there are nine minimal graphs that are not line graphs, such that any graph that is not a line graph has one of these nine graphs as an induced subgraph. That is, a graph is a line graph if and only if no subset of its vertices induces one of these nine graphs. In the example above, the four topmost vertices induce a claw (that is, a complete bipartite graph $K), shown on the top left of the illustration of forbidden subgraphs. Therefore, by Beineke's characterization, this example cannot be a line graph. For graphs with minimum degree at least 5, only the six subgraphs in the left and right columns of the figure are needed in the characterization."
            },
            {
                "text": "Algorithms and described linear time algorithms for recognizing line graphs and reconstructing their original graphs. generalized these methods to directed graphs. described an efficient data structure for maintaining a dynamic graph, subject to vertex insertions and deletions, and maintaining a representation of the input as a line graph (when it exists) in time proportional to the number of changed edges at each step. The algorithms of and are based on characterizations of line graphs involving odd triangles (triangles in the line graph with the property that there exists another vertex adjacent to an odd number of triangle vertices). However, the algorithm of uses only Whitney's isomorphism theorem. It is complicated by the need to recognize deletions that cause the remaining graph to become a line graph, but when specialized to the static recognition problem only insertions need to be performed, and the algorithm performs the following steps: Construct the input graph by adding vertices one at a time, at each step choosing a vertex to add that is adjacent to at least one previously-added vertex. While adding vertices to , maintain a graph for which $1=L = L(G)$; if the algorithm ever fails to find an appropriate graph , then the input is not a line graph and the algorithm terminates."
            },
            {
                "text": "When adding a vertex to a graph $L(G)$ for which has four or fewer vertices, it might be the case that the line graph representation is not unique. But in this case, the augmented graph is small enough that a representation of it as a line graph can be found by a brute force search in constant time. When adding a vertex to a larger graph that equals the line graph of another graph , let be the subgraph of formed by the edges that correspond to the neighbors of in . Check that has a vertex cover consisting of one vertex or two non-adjacent vertices. If there are two vertices in the cover, augment by adding an edge (corresponding to ) that connects these two vertices. If there is only one vertex in the cover, then add a new vertex to , adjacent to this vertex. Each step either takes constant time, or involves finding a vertex cover of constant size within a graph whose size is proportional to the number of neighbors of . Thus, the total time for the whole algorithm is proportional to the sum of the numbers of neighbors of all vertices, which (by the handshaking lemma) is proportional to the number of input edges."
            },
            {
                "text": "Iterating the line graph operator consider the sequence of graphs $G, L(G), L(L(G)), L(L(L(G))), \\dots.\\ $ They show that, when is a finite connected graph, only four behaviors are possible for this sequence: If is a cycle graph then $L(G)$ and each subsequent graph in this sequence are isomorphic to itself. These are the only connected graphs for which $L(G)$ is isomorphic to .This result is also Theorem 8.2 of . If is a claw $K, then $L(G)$ and all subsequent graphs in the sequence are triangles. If is a path graph then each subsequent graph in the sequence is a shorter path until eventually the sequence terminates with an empty graph. In all remaining cases, the sizes of the graphs in this sequence eventually increase without bound. If is not connected, this classification applies separately to each component of . For connected graphs that are not paths, all sufficiently high numbers of iteration of the line graph operation produce graphs that are Hamiltonian., Theorem 8.11, p. 81."
            },
            {
                "text": "Harary credits this result to Gary Chartrand. Generalizations Medial graphs and convex polyhedra When a planar graph has maximum vertex degree three, its line graph is planar, and every planar embedding of can be extended to an embedding of $L(G)$. However, there exist planar graphs with higher degree whose line graphs are nonplanar. These include, for example, the 5-star $K, the gem graph formed by adding two non-crossing diagonals within a regular pentagon, and all convex polyhedra with a vertex of degree four or more. ; . An alternative construction, the medial graph, coincides with the line graph for planar graphs with maximum degree three, but is always planar. It has the same vertices as the line graph, but potentially fewer edges: two vertices of the medial graph are adjacent if and only if the corresponding two edges are consecutive on some face of the planar embedding. The medial graph of the dual graph of a plane graph is the same as the medial graph of the original plane graph.. For regular polyhedra or simple polyhedra, the medial graph operation can be represented geometrically by the operation of cutting off each vertex of the polyhedron by a plane through the midpoints of all its incident edges.."
            },
            {
                "text": "This operation is known variously as the second truncation,. degenerate truncation,. or rectification. Total graphs The total graph $T(G)$ of a graph has as its vertices the elements (vertices or edges) of , and has an edge between two elements whenever they are either incident or adjacent. The total graph may also be obtained by subdividing each edge of and then taking the square of the subdivided graph., p. 82. Multigraphs The concept of the line graph of may naturally be extended to the case where is a multigraph. In this case, the characterizations of these graphs can be simplified: the characterization in terms of clique partitions no longer needs to prevent two vertices from belonging to the same to cliques, and the characterization by forbidden graphs has seven forbidden graphs instead of nine. However, for multigraphs, there are larger numbers of pairs of non-isomorphic graphs that have the same line graphs. For instance a complete bipartite graph $K has the same line graph as the dipole graph and Shannon multigraph with the same number of edges."
            },
            {
                "text": "Nevertheless, analogues to Whitney's isomorphism theorem can still be derived in this case. Line digraphs Construction of the de Bruijn graphs as iterated line digraphs It is also possible to generalize line graphs to directed graphs. If is a directed graph, its directed line graph or line digraph has one vertex for each edge of . Two vertices representing directed edges from to and from to in are connected by an edge from to in the line digraph when $1=v = w$. That is, each edge in the line digraph of represents a length-two directed path in . The de Bruijn graphs may be formed by repeating this process of forming directed line graphs, starting from a complete directed graph. Weighted line graphs In a line graph $L(G)$, each vertex of degree in the original graph creates $k(k − 1)/2$ edges in the line graph. For many types of analysis this means high-degree nodes in are over-represented in the line graph $L(G)$. For instance, consider a random walk on the vertices of the original graph ."
            },
            {
                "text": "This will pass along some edge with some frequency . On the other hand, this edge is mapped to a unique vertex, say , in the line graph $L(G)$. If we now perform the same type of random walk on the vertices of the line graph, the frequency with which is visited can be completely different from f. If our edge in was connected to nodes of degree $O(k)$, it will be traversed $O(k more frequently in the line graph $L(G)$. Put another way, the Whitney graph isomorphism theorem guarantees that the line graph almost always encodes the topology of the original graph faithfully but it does not guarantee that dynamics on these two graphs have a simple relationship. One solution is to construct a weighted line graph, that is, a line graph with weighted edges. There are several natural ways to do this. For instance if edges and in the graph are incident at a vertex with degree , then in the line graph $L(G)$ the edge connecting the two vertices and can be given weight $1/(k − 1)$."
            },
            {
                "text": "In this way every edge in (provided neither end is connected to a vertex of degree 1) will have strength 2 in the line graph $L(G)$ corresponding to the two ends that the edge has in . It is straightforward to extend this definition of a weighted line graph to cases where the original graph was directed or even weighted. The principle in all cases is to ensure the line graph $L(G)$ reflects the dynamics as well as the topology of the original graph . Line graphs of hypergraphs The edges of a hypergraph may form an arbitrary family of sets, so the line graph of a hypergraph is the same as the intersection graph of the sets from the family. Disjointness graph The disjointness graph of , denoted $D(G)$, is constructed in the following way: for each edge in , make a vertex in $D(G)$; for every two edges in that do not have a vertex in common, make an edge between their corresponding vertices in $D(G)$. In other words, $D(G)$ is the complement graph of $L(G)$. A clique in $D(G)$ corresponds to an independent set in $L(G)$, and vice versa. Notes References . . . . . . . . . . . . . . . . . . . . . . . . . . Translated into English as ."
            },
            {
                "text": "External links Line graphs, Information System on Graph Class Inclusions Category:Graph families Category:Intersection classes of graphs Category:Graph operations"
            }
        ],
        "latex_formulas": [
            "L(''G'')",
            "L(''G'')",
            "L(''G'')",
            "L(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''K''{{sub|3}}",
            "''K''{{sub|1,3}}",
            "''K''{{sub|3}}",
            "''K''{{sub|1,3}}",
            "''K''{{sub|1,1,2}}",
            "''K''{{sub|1,2,2}}",
            "''J''(''n'', 2)",
            "''KG''{{sub|''n'',2}}",
            "''n'' = 8",
            "''K''{{sub|8}}",
            "srg(''n''(''n'' – 1)/2, 2(''n'' – 2), ''n'' – 2, 4)",
            "''L''(''K''{{sub|8}})",
            "''L''(''K''{{sub|8}})",
            "''L''(''K''{{sub|4,4}})",
            "''C''{{sub|3}}",
            "''C''{{sub|4}}",
            "''C''{{sub|5}}",
            "''C''{{sub|3}}",
            "''L''(''G'')",
            "''K''{{sub|4}}",
            "''K''{{sub|1,1,''n''}}",
            "''L''(''G'')",
            "''A'' + 2''I''",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''K''{{sub|1,3}}",
            "''L'' = ''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''L''(''G'')",
            "''K''{{sub|1,3}}",
            "''L''(''G'')",
            "''L''(''G'')",
            "''K''{{sub|1,5}}",
            "''T''(''G'')",
            "''K''{{sub|1,''n''}}",
            "''v'' = ''w''",
            "''L''(''G'')",
            "''k''(''k'' − 1)/2",
            "''L''(''G'')",
            "''L''(''G'')",
            "''O''(''k'')",
            "''O''(''k''{{sup|2}})",
            "''L''(''G'')",
            "''L''(''G'')",
            "1/(''k'' − 1)",
            "''L''(''G'')",
            "''L''(''G'')",
            "''D''(''G'')",
            "''D''(''G'')",
            "''D''(''G'')",
            "''D''(''G'')",
            "''L''(''G'')",
            "''D''(''G'')",
            "''L''(''G'')",
            "A = J^\\mathsf{T}J - 2I",
            "G, L(G), L(L(G)), L(L(L(G))), \\dots.\\"
        ]
    },
    "Robust_statistics": {
        "title": "Robust_statistics",
        "chunks": [
            {
                "text": "Robust statistics are statistics that maintain their properties even if the underlying distributional assumptions are incorrect. Robust statistical methods have been developed for many common problems, such as estimating location, scale, and regression parameters. One motivation is to produce statistical methods that are not unduly affected by outliers. Another motivation is to provide methods with good performance when there are small departures from a parametric distribution. For example, robust methods work well for mixtures of two normal distributions with different standard deviations; under this model, non-robust methods like a t-test work poorly. Introduction Robust statistics seek to provide methods that emulate popular statistical methods, but are not unduly affected by outliers or other small departures from model assumptions. In statistics, classical estimation methods rely heavily on assumptions that are often not met in practice. In particular, it is often assumed that the data errors are normally distributed, at least approximately, or that the central limit theorem can be relied on to produce normally distributed estimates. Unfortunately, when there are outliers in the data, classical estimators often have very poor performance, when judged using the breakdown point and the influence function described below."
            },
            {
                "text": "The practical effect of problems seen in the influence function can be studied empirically by examining the sampling distribution of proposed estimators under a mixture model, where one mixes in a small amount (1–5% is often sufficient) of contamination. For instance, one may use a mixture of 95% a normal distribution, and 5% a normal distribution with the same mean but significantly higher standard deviation (representing outliers). Robust parametric statistics can proceed in two ways: by designing estimators so that a pre-selected behaviour of the influence function is achieved by replacing estimators that are optimal under the assumption of a normal distribution with estimators that are optimal for, or at least derived for, other distributions; for example, using the t-distribution with low degrees of freedom (high kurtosis) or with a mixture of two or more distributions. Robust estimates have been studied for the following problems: estimating location parameters estimating scale parameters estimating regression coefficients estimation of model-states in models expressed in state-space form, for which the standard method is equivalent to a Kalman filter. Definition There are various definitions of a \"robust statistic\"."
            },
            {
                "text": "Strictly speaking, a robust statistic is resistant to errors in the results, produced by deviations from assumptions, page 1. (e.g., of normality). This means that if the assumptions are only approximately met, the robust estimator will still have a reasonable efficiency, and reasonably small bias, as well as being asymptotically unbiased, meaning having a bias tending towards 0 as the sample size tends towards infinity. Usually, the most important case is distributional robustness - robustness to breaking of the assumptions about the underlying distribution of the data. Classical statistical procedures are typically sensitive to \"longtailedness\" (e.g., when the distribution of the data has longer tails than the assumed normal distribution). This implies that they will be strongly affected by the presence of outliers in the data, and the estimates they produce may be heavily distorted if there are extreme outliers in the data, compared to what they would be if the outliers were not included in the data. By contrast, more robust estimators that are not so sensitive to distributional distortions such as longtailedness are also resistant to the presence of outliers."
            },
            {
                "text": "Thus, in the context of robust statistics, distributionally robust and outlier-resistant are effectively synonymous. For one perspective on research in robust statistics up to 2000, see . Some experts prefer the term resistant statistics for distributional robustness, and reserve 'robustness' for non-distributional robustness, e.g., robustness to violation of assumptions about the probability model or estimator, but this is a minority usage. Plain 'robustness' to mean 'distributional robustness' is common. When considering how robust an estimator is to the presence of outliers, it is useful to test what happens when an extreme outlier is added to the dataset, and to test what happens when an extreme outlier replaces one of the existing data points, and then to consider the effect of multiple additions or replacements. Examples The mean is not a robust measure of central tendency. If the dataset is, e.g., the values {2,3,5,6,9}, then if we add another datapoint with value -1000 or +1000 to the data, the resulting mean will be very different from the mean of the original data. Similarly, if we replace one of the values with a datapoint of value -1000 or +1000 then the resulting mean will be very different from the mean of the original data."
            },
            {
                "text": "The median is a robust measure of central tendency. Taking the same dataset {2,3,5,6,9}, if we add another datapoint with value -1000 or +1000 then the median will change slightly, but it will still be similar to the median of the original data. If we replace one of the values with a data point of value -1000 or +1000 then the resulting median will still be similar to the median of the original data. Described in terms of breakdown points, the median has a breakdown point of 50%, meaning that half the points must be outliers before the median can be moved outside the range of the non-outliers, while the mean has a breakdown point of 0, as a single large observation can throw it off. The median absolute deviation and interquartile range are robust measures of statistical dispersion, while the standard deviation and range are not. Trimmed estimators and Winsorised estimators are general methods to make statistics more robust. L-estimators are a general class of simple statistics, often robust, while M-estimators are a general class of robust statistics, and are now the preferred solution, though they can be quite involved to calculate."
            },
            {
                "text": "Speed-of-light data Gelman et al. in Bayesian Data Analysis (2004) consider a data set relating to speed-of-light measurements made by Simon Newcomb. The data sets for that book can be found via the Classic data sets page, and the book's website contains more information on the data. Although the bulk of the data looks to be more or less normally distributed, there are two obvious outliers. These outliers have a large effect on the mean, dragging it towards them, and away from the center of the bulk of the data. Thus, if the mean is intended as a measure of the location of the center of the data, it is, in a sense, biased when outliers are present. Also, the distribution of the mean is known to be asymptotically normal due to the central limit theorem. However, outliers can make the distribution of the mean non-normal, even for fairly large data sets. Besides this non-normality, the mean is also inefficient in the presence of outliers and less variable measures of location are available. Estimation of location The plot below shows a density plot of the speed-of-light data, together with a rug plot (panel (a))."
            },
            {
                "text": "Also shown is a normal Q–Q plot (panel (b)). The outliers are visible in these plots. Panels (c) and (d) of the plot show the bootstrap distribution of the mean (c) and the 10% trimmed mean (d). The trimmed mean is a simple, robust estimator of location that deletes a certain percentage of observations (10% here) from each end of the data, then computes the mean in the usual way. The analysis was performed in R and 10,000 bootstrap samples were used for each of the raw and trimmed means. The distribution of the mean is clearly much wider than that of the 10% trimmed mean (the plots are on the same scale). Also whereas the distribution of the trimmed mean appears to be close to normal, the distribution of the raw mean is quite skewed to the left. So, in this sample of 66 observations, only 2 outliers cause the central limit theorem to be inapplicable. Image:speedOfLight.png Robust statistical methods, of which the trimmed mean is a simple example, seek to outperform classical statistical methods in the presence of outliers, or, more generally, when underlying parametric assumptions are not quite correct."
            },
            {
                "text": "Whilst the trimmed mean performs well relative to the mean in this example, better robust estimates are available. In fact, the mean, median and trimmed mean are all special cases of M-estimators. Details appear in the sections below. Estimation of scale The outliers in the speed-of-light data have more than just an adverse effect on the mean; the usual estimate of scale is the standard deviation, and this quantity is even more badly affected by outliers because the squares of the deviations from the mean go into the calculation, so the outliers' effects are exacerbated. The plots below show the bootstrap distributions of the standard deviation, the median absolute deviation (MAD) and the Rousseeuw–Croux (Qn) estimator of scale. The plots are based on 10,000 bootstrap samples for each estimator, with some Gaussian noise added to the resampled data (smoothed bootstrap). Panel (a) shows the distribution of the standard deviation, (b) of the MAD and (c) of Qn. Image:speedOfLightScale.png The distribution of standard deviation is erratic and wide, a result of the outliers."
            },
            {
                "text": "The MAD is better behaved, and Qn is a little bit more efficient than MAD. This simple example demonstrates that when outliers are present, the standard deviation cannot be recommended as an estimate of scale. Manual screening for outliers Traditionally, statisticians would manually screen data for outliers, and remove them, usually checking the source of the data to see whether the outliers were erroneously recorded. Indeed, in the speed-of-light example above, it is easy to see and remove the two outliers prior to proceeding with any further analysis. However, in modern times, data sets often consist of large numbers of variables being measured on large numbers of experimental units. Therefore, manual screening for outliers is often impractical. Outliers can often interact in such a way that they mask each other. As a simple example, consider a small univariate data set containing one modest and one large outlier. The estimated standard deviation will be grossly inflated by the large outlier. The result is that the modest outlier looks relatively normal. As soon as the large outlier is removed, the estimated standard deviation shrinks, and the modest outlier now looks unusual."
            },
            {
                "text": "This problem of masking gets worse as the complexity of the data increases. For example, in regression problems, diagnostic plots are used to identify outliers. However, it is common that once a few outliers have been removed, others become visible. The problem is even worse in higher dimensions. Robust methods provide automatic ways of detecting, downweighting (or removing), and flagging outliers, largely removing the need for manual screening. Care must be taken; initial data showing the ozone hole first appearing over Antarctica were rejected as outliers by non-human screening. Variety of applications Although this article deals with general principles for univariate statistical methods, robust methods also exist for regression problems, generalized linear models, and parameter estimation of various distributions. Measures of robustness The basic tools used to describe and measure robustness are the breakdown point, the influence function and the sensitivity curve. Breakdown point Intuitively, the breakdown point of an estimator is the proportion of incorrect observations (e.g. arbitrarily large observations) an estimator can handle before giving an incorrect (e.g., arbitrarily large) result."
            },
            {
                "text": "Usually, the asymptotic (infinite sample) limit is quoted as the breakdown point, although the finite-sample breakdown point may be more useful. For example, given $n$ independent random variables $(X_1,\\dots,X_n)$ and the corresponding realizations $x_1,\\dots,x_n$, we can use $\\overline{X_n}:=\\frac{X_1+\\cdots+X_n}{n}$ to estimate the mean. Such an estimator has a breakdown point of 0 (or finite-sample breakdown point of $1/n$) because we can make $\\overline{x}$ arbitrarily large just by changing any of $ x_1,\\dots,x_n$. The higher the breakdown point of an estimator, the more robust it is. Intuitively, we can understand that a breakdown point cannot exceed 50% because if more than half of the observations are contaminated, it is not possible to distinguish between the underlying distribution and the contaminating distribution . Therefore, the maximum breakdown point is 0.5 and there are estimators which achieve such a breakdown point. For example, the median has a breakdown point of 0.5."
            },
            {
                "text": "The X% trimmed mean has a breakdown point of X%, for the chosen level of X. and contain more details. The level and the power breakdown points of tests are investigated in . Statistics with high breakdown points are sometimes called resistant statistics.Resistant statistics, David B. Stephenson Example: speed-of-light data In the speed-of-light example, removing the two lowest observations causes the mean to change from 26.2 to 27.75, a change of 1.55. The estimate of scale produced by the Qn method is 6.3. We can divide this by the square root of the sample size to get a robust standard error, and we find this quantity to be 0.78. Thus, the change in the mean resulting from removing two outliers is approximately twice the robust standard error. The 10% trimmed mean for the speed-of-light data is 27.43. Removing the two lowest observations and recomputing gives 27.67. The trimmed mean is less affected by the outliers and has a higher breakdown point. If we replace the lowest observation, −44, by −1000, the mean becomes 11.73, whereas the 10% trimmed mean is still 27.43."
            },
            {
                "text": "In many areas of applied statistics, it is common for data to be log-transformed to make them near symmetrical. Very small values become large negative when log-transformed, and zeroes become negatively infinite. Therefore, this example is of practical interest. Empirical influence function The empirical influence function is a measure of the dependence of the estimator on the value of any one of the points in the sample. It is a model-free measure in the sense that it simply relies on calculating the estimator again with a different sample. On the right is Tukey's biweight function, which, as we will later see, is an example of what a \"good\" (in a sense defined later on) empirical influence function should look like."
            },
            {
                "text": "In mathematical terms, an influence function is defined as a vector in the space of the estimator, which is in turn defined for a sample which is a subset of the population: $(\\Omega,\\mathcal{A},P)$ is a probability space, $(\\mathcal{X},\\Sigma)$ is a measurable space (state space), $\\Theta$ is a parameter space of dimension $p\\in\\mathbb{N}^*$, $(\\Gamma,S)$ is a measurable space, For example, $(\\Omega,\\mathcal{A},P)$ is any probability space, $(\\mathcal{X},\\Sigma) = (\\mathbb{R},\\mathcal{B})$, $\\Theta = \\mathbb{R}\\times\\mathbb{R}^+$ $(\\Gamma,S) = (\\mathbb{R},\\mathcal{B})$, The empirical influence function is defined as follows."
            },
            {
                "text": "Let $n\\in\\mathbb{N}^*$ and $X_1,\\dots,X_n:(\\Omega, \\mathcal{A}) \\rightarrow (\\mathcal{X},\\Sigma)$ are i.i.d. and $(x_1,\\dots,x_n)$ is a sample from these variables. $T_n: (\\mathcal{X}^n,\\Sigma^n) \\rightarrow (\\Gamma,S)$ is an estimator. Let $i\\in\\{1,\\dots,n\\}$. The empirical influence function $EIF_i$ at observation $i$ is defined by: $EIF_i: x\\in\\mathcal{X} \\mapsto n\\cdot(T_n(x_1,\\dots,x_{i-1},x,x_{i+1},\\dots,x_n)-T_n(x_1,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n))$ What this means is that we are replacing the i-th value in the sample by an arbitrary value and looking at the output of the estimator. Alternatively, the EIF is defined as the effect, scaled by n+1 instead of n, on the estimator of adding the point $x$ to the sample."
            },
            {
                "text": "Influence function and sensitivity curve 300px|Influence function when Tukey's biweight function (see section M-estimators below) is used as a loss function. Points with large deviation have no influence (y=0). Instead of relying solely on the data, we could use the distribution of the random variables. The approach is quite different from that of the previous paragraph. What we are now trying to do is to see what happens to an estimator when we change the distribution of the data slightly: it assumes a distribution, and measures sensitivity to change in this distribution. By contrast, the empirical influence assumes a sample set, and measures sensitivity to change in the samples. Let $A$ be a convex subset of the set of all finite signed measures on $\\Sigma$. We want to estimate the parameter $\\theta\\in\\Theta$ of a distribution $F$ in $A$. Let the functional $T:A\\rightarrow\\Gamma$ be the asymptotic value of some estimator sequence $(T_n)_{n\\in\\mathbb{N}}$. We will suppose that this functional is Fisher consistent, i.e."
            },
            {
                "text": "$\\forall \\theta\\in\\Theta, T(F_\\theta)=\\theta$. This means that at the model $F$, the estimator sequence asymptotically measures the correct quantity. Let $G$ be some distribution in $A$. What happens when the data doesn't follow the model $F$ exactly but another, slightly different, \"going towards\" $G$? We're looking at: $dT_{G-F}(F) = \\lim_{t\\rightarrow 0^+}\\frac{T(tG + (1 - t)F) - T(F)}{t}$, which is the one-sided Gateaux derivative of $T$ at $F$, in the direction of $G-F$. Let $x\\in\\mathcal{X}$. $\\Delta_x$ is the probability measure which gives mass 1 to $\\{x\\}$. We choose $G=\\Delta_x$. The influence function is then defined by: $IF(x; T; F) := \\lim_{t\\rightarrow 0^+}\\frac{T(t\\Delta_x+(1-t)F) - T(F)}{t}.$ It describes the effect of an infinitesimal contamination at the point $x$ on the estimate we are seeking, standardized by the mass $t$ of the contamination (the asymptotic bias caused by contamination in the observations)."
            },
            {
                "text": "For a robust estimator, we want a bounded influence function, that is, one which does not go to infinity as x becomes arbitrarily large. The empirical influence function uses the empirical distribution function $\\hat{F}$ instead of the distribution function $F$, making use of the drop-in principle. Desirable properties Properties of an influence function that bestow it with desirable performance are: Finite rejection point $\\rho^*$, Small gross-error sensitivity $\\gamma^*$, Small local-shift sensitivity $\\lambda^*$. Rejection point $\\rho^* := \\inf_{r>0}\\{r:IF(x;T;F)=0, |x|>r\\}$ Gross-error sensitivity $\\gamma^*(T;F) := \\sup_{x\\in\\mathcal{X}}|IF(x; T ; F)|$ Local-shift sensitivity $\\lambda^*(T;F) := \\sup_{(x,y)\\in\\mathcal{X}^2\\atop x\\neq y}\\left\\|\\frac{IF(y ; T; F) - IF(x; T ; F)}{y-x}\\right\\|$ This value, which looks a lot like a Lipschitz constant, represents the effect of shifting an observation slightly from $x$ to a neighbouring point $y$, i.e., add an observation at $y$ and remove one at $x$."
            },
            {
                "text": "M-estimators (The mathematical context of this paragraph is given in the section on empirical influence functions.) Historically, several approaches to robust estimation were proposed, including R-estimators and L-estimators. However, M-estimators now appear to dominate the field as a result of their generality, their potential for high breakdown points and comparatively high efficiency. See . M-estimators are not inherently robust. However, they can be designed to achieve favourable properties, including robustness. M-estimator are a generalization of maximum likelihood estimators (MLEs) which is determined by maximizing or, equivalently, minimizing . In 1964, Huber proposed to generalize this to the minimization of , where $\\rho$ is some function. MLE are therefore a special case of M-estimators (hence the name: \"Maximum likelihood type\" estimators). Minimizing can often be done by differentiating $\\rho$ and solving , where (if $\\rho$ has a derivative). Several choices of $\\rho$ and $\\psi$ have been proposed. The two figures below show four $\\rho$ functions and their corresponding $\\psi$ functions."
            },
            {
                "text": "Image:RhoFunctions.png For squared errors, $\\rho(x)$ increases at an accelerating rate, whilst for absolute errors, it increases at a constant rate. When Winsorizing is used, a mixture of these two effects is introduced: for small values of x, $\\rho$ increases at the squared rate, but once the chosen threshold is reached (1.5 in this example), the rate of increase becomes constant. This Winsorised estimator is also known as the Huber loss function. Tukey's biweight (also known as bisquare) function behaves in a similar way to the squared error function at first, but for larger errors, the function tapers off. Image:PsiFunctions.png Properties of M-estimators M-estimators do not necessarily relate to a probability density function. Therefore, off-the-shelf approaches to inference that arise from likelihood theory can not, in general, be used. It can be shown that M-estimators are asymptotically normally distributed so that as long as their standard errors can be computed, an approximate approach to inference is available. Since M-estimators are normal only asymptotically, for small sample sizes it might be appropriate to use an alternative approach to inference, such as the bootstrap."
            },
            {
                "text": "However, M-estimates are not necessarily unique (i.e., there might be more than one solution that satisfies the equations). Also, it is possible that any particular bootstrap sample can contain more outliers than the estimator's breakdown point. Therefore, some care is needed when designing bootstrap schemes. Of course, as we saw with the speed-of-light example, the mean is only normally distributed asymptotically and when outliers are present the approximation can be very poor even for quite large samples. However, classical statistical tests, including those based on the mean, are typically bounded above by the nominal size of the test. The same is not true of M-estimators and the type I error rate can be substantially above the nominal level. These considerations do not \"invalidate\" M-estimation in any way. They merely make clear that some care is needed in their use, as is true of any other method of estimation. Influence function of an M-estimator It can be shown that the influence function of an M-estimator $T$ is proportional to $\\psi$,, page 45 which means we can derive the properties of such an estimator (such as its rejection point, gross-error sensitivity or local-shift sensitivity) when we know its $\\psi$ function."
            },
            {
                "text": "$IF(x;T,F) = M^{-1}\\psi(x,T(F))$ with the $p\\times p$ given by: $M = -\\int_{\\mathcal{X}} \\left(\\frac{\\partial \\psi(x,\\theta)}{\\partial \\theta} \\right)_{T(F)} \\, dF(x).$ Choice of ψ and ρ In many practical situations, the choice of the $\\psi$ function is not critical to gaining a good robust estimate, and many choices will give similar results that offer great improvements, in terms of efficiency and bias, over classical estimates in the presence of outliers. Theoretically, $\\psi$ functions are to be preferred, and Tukey's biweight (also known as bisquare) function is a popular choice. recommend the biweight function with efficiency at the normal set to 85%. Robust parametric approaches M-estimators do not necessarily relate to a density function and so are not fully parametric. Fully parametric approaches to robust modeling and inference, both Bayesian and likelihood approaches, usually deal with heavy-tailed distributions such as Student's t-distribution."
            },
            {
                "text": "For the t-distribution with $\\nu$ degrees of freedom, it can be shown that $\\psi(x) = \\frac{x}{x^2 + \\nu}.$ For $\\nu=1$, the t-distribution is equivalent to the Cauchy distribution. The degrees of freedom is sometimes known as the kurtosis parameter. It is the parameter that controls how heavy the tails are. In principle, $\\nu$ can be estimated from the data in the same way as any other parameter. In practice, it is common for there to be multiple local maxima when $\\nu$ is allowed to vary. As such, it is common to fix $\\nu$ at a value around 4 or 6. The figure below displays the $\\psi$-function for 4 different values of $\\nu$. Image:TDistPsi.png Example: speed-of-light data For the speed-of-light data, allowing the kurtosis parameter to vary and maximizing the likelihood, we get $ \\hat\\mu = 27.40,\\quad \\hat\\sigma = 3.81,\\quad \\hat\\nu = 2.13.$ Fixing $\\nu = 4$ and maximizing the likelihood gives $\\hat\\mu = 27.49,\\quad \\hat\\sigma = 4.51.$ Related concepts A pivotal quantity is a function of data, whose underlying population distribution is a member of a parametric family, that is not dependent on the values of the parameters."
            },
            {
                "text": "An ancillary statistic is such a function that is also a statistic, meaning that it is computed in terms of the data alone. Such functions are robust to parameters in the sense that they are independent of the values of the parameters, but not robust to the model in the sense that they assume an underlying model (parametric family), and in fact, such functions are often very sensitive to violations of the model assumptions. Thus test statistics, frequently constructed in terms of these to not be sensitive to assumptions about parameters, are still very sensitive to model assumptions. Replacing outliers and missing values Replacing missing data is called imputation. If there are relatively few missing points, there are some models which can be used to estimate values to complete the series, such as replacing missing values with the mean or median of the data. Simple linear regression can also be used to estimate missing values. ; . In addition, outliers can sometimes be accommodated in the data through the use of trimmed means, other scale estimators apart from standard deviation (e.g., MAD) and Winsorization."
            },
            {
                "text": "In calculations of a trimmed mean, a fixed percentage of data is dropped from each end of an ordered data, thus eliminating the outliers. The mean is then calculated using the remaining data. Winsorizing involves accommodating an outlier by replacing it with the next highest or next smallest value as appropriate. However, using these types of models to predict missing values or outliers in a long time series is difficult and often unreliable, particularly if the number of values to be in-filled is relatively high in comparison with total record length. The accuracy of the estimate depends on how good and representative the model is and how long the period of missing values extends. When dynamic evolution is assumed in a series, the missing data point problem becomes an exercise in multivariate analysis (rather than the univariate approach of most traditional methods of estimating missing values and outliers). In such cases, a multivariate model will be more representative than a univariate one for predicting missing values. The Kohonen self organising map (KSOM) offers a simple and robust multivariate model for data analysis, thus providing good possibilities to estimate missing values, taking into account their relationship or correlation with other pertinent variables in the data record."
            },
            {
                "text": "Standard Kalman filters are not robust to outliers. To this end have recently shown that a modification of Masreliez's theorem can deal with outliers. One common approach to handle outliers in data analysis is to perform outlier detection first, followed by an efficient estimation method (e.g., the least squares). While this approach is often useful, one must keep in mind two challenges. First, an outlier detection method that relies on a non-robust initial fit can suffer from the effect of masking, that is, a group of outliers can mask each other and escape detection. Second, if a high breakdown initial fit is used for outlier detection, the follow-up analysis might inherit some of the inefficiencies of the initial estimator. Use in machine learning Although influence functions have a long history in statistics, they were not widely used in machine learning due to several challenges. One of the primary obstacles is that traditional influence functions rely on expensive second-order derivative computations and assume model differentiability and convexity. These assumptions are limiting, especially in modern machine learning, where models are often non-differentiable, non-convex, and operate in high-dimensional spaces."
            },
            {
                "text": "addressed these challenges by introducing methods to efficiently approximate influence functions using second-order optimization techniques, such as those developed by , , and . Their approach remains effective even when the assumptions of differentiability and convexity degrade, enabling influence functions to be used in the context of non-convex deep learning models. They demonstrated that influence functions are a powerful and versatile tool that can be applied to a variety of tasks in machine learning, including: Understanding Model Behavior: Influence functions help identify which training points are most “responsible” for a given prediction, offering insights into how models generalize from training data. Debugging Models: Influence functions can assist in identifying domain mismatches—when the training data distribution does not match the test data distribution—which can cause models with high training accuracy to perform poorly on test data, as shown by . By revealing which training examples contribute most to errors, developers can address these mismatches. Dataset Error Detection: Noisy or corrupted labels are common in real-world data, especially when crowdsourced or adversarially attacked. Influence functions allow human experts to prioritize reviewing only the most impactful examples in the training set, facilitating efficient error detection and correction."
            },
            {
                "text": "Adversarial Attacks: Models that rely heavily on a small number of influential training points are vulnerable to adversarial perturbations. These perturbed inputs can significantly alter predictions and pose security risks in machine learning systems where attackers have access to the training data (See adversarial machine learning). Koh and Liang’s contributions have opened the door for influence functions to be used in various applications across machine learning, from interpretability to security, marking a significant advance in their applicability. See also Robust confidence intervals Robust regression Unit-weighted regression Notes References . . Republished in paperback, 2005. . . . 2nd ed., CRC Press, 2011. . Republished in paperback, 2004. 2nd ed., Wiley, 2009. . . . . . . . Republished in paperback, 2003. . Preprint . . . . . External links Peter Rousseeuw's introduction to univariate robust statistics. (archived here). Brian Ripley's robust statistics course notes. Nick Fieller's course notes on Statistical Modelling and Computation contain material on robust regression. David Olive's site contains course notes on robust statistics and some data sets. Online experiments using R and JSXGraph"
            }
        ],
        "latex_formulas": [
            "n",
            "(X_1,\\dots,X_n)",
            "x_1,\\dots,x_n",
            "\\overline{X_n}:=\\frac{X_1+\\cdots+X_n}{n}",
            "1/n",
            "\\overline{x}",
            "x_1,\\dots,x_n",
            "(\\Omega,\\mathcal{A},P)",
            "(\\mathcal{X},\\Sigma)",
            "\\Theta",
            "p\\in\\mathbb{N}^*",
            "(\\Gamma,S)",
            "(\\Omega,\\mathcal{A},P)",
            "(\\mathcal{X},\\Sigma) = (\\mathbb{R},\\mathcal{B})",
            "\\Theta = \\mathbb{R}\\times\\mathbb{R}^+",
            "(\\Gamma,S) = (\\mathbb{R},\\mathcal{B})",
            "n\\in\\mathbb{N}^*",
            "X_1,\\dots,X_n:(\\Omega, \\mathcal{A}) \\rightarrow (\\mathcal{X},\\Sigma)",
            "(x_1,\\dots,x_n)",
            "T_n: (\\mathcal{X}^n,\\Sigma^n) \\rightarrow (\\Gamma,S)",
            "i\\in\\{1,\\dots,n\\}",
            "EIF_i",
            "i",
            "EIF_i: x\\in\\mathcal{X} \\mapsto n\\cdot(T_n(x_1,\\dots,x_{i-1},x,x_{i+1},\\dots,x_n)-T_n(x_1,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n))",
            "x",
            "A",
            "\\Sigma",
            "\\theta\\in\\Theta",
            "F",
            "A",
            "T:A\\rightarrow\\Gamma",
            "(T_n)_{n\\in\\mathbb{N}}",
            "\\forall \\theta\\in\\Theta, T(F_\\theta)=\\theta",
            "F",
            "G",
            "A",
            "F",
            "G",
            "dT_{G-F}(F) = \\lim_{t\\rightarrow 0^+}\\frac{T(tG + (1 - t)F) - T(F)}{t}",
            "T",
            "F",
            "G-F",
            "x\\in\\mathcal{X}",
            "\\Delta_x",
            "\\{x\\}",
            "G=\\Delta_x",
            "IF(x; T; F) := \\lim_{t\\rightarrow 0^+}\\frac{T(t\\Delta_x+(1-t)F) - T(F)}{t}.",
            "x",
            "t",
            "\\hat{F}",
            "F",
            "\\rho^*",
            "\\gamma^*",
            "\\lambda^*",
            "\\rho^* := \\inf_{r>0}\\{r:IF(x;T;F)=0, |x|>r\\}",
            "\\gamma^*(T;F) := \\sup_{x\\in\\mathcal{X}}|IF(x; T ; F)|",
            "\\lambda^*(T;F) := \\sup_{(x,y)\\in\\mathcal{X}^2\\atop x\\neq y}\\left\\|\\frac{IF(y ; T; F) - IF(x; T ; F)}{y-x}\\right\\|",
            "x",
            "y",
            "y",
            "x",
            "\\rho",
            "\\rho",
            "\\rho",
            "\\rho",
            "\\psi",
            "\\rho",
            "\\psi",
            "\\rho(x)",
            "\\rho",
            "T",
            "\\psi",
            "\\psi",
            "IF(x;T,F) = M^{-1}\\psi(x,T(F))",
            "p\\times p",
            "M = -\\int_{\\mathcal{X}} \\left(\\frac{\\partial \\psi(x,\\theta)}{\\partial \\theta} \\right)_{T(F)} \\, dF(x).",
            "\\psi",
            "\\psi",
            "\\nu",
            "\\psi(x) = \\frac{x}{x^2 + \\nu}.",
            "\\nu=1",
            "\\nu",
            "\\nu",
            "\\nu",
            "\\psi",
            "\\nu",
            "\\hat\\mu = 27.40,\\quad \\hat\\sigma = 3.81,\\quad \\hat\\nu = 2.13.",
            "\\nu = 4",
            "\\hat\\mu = 27.49,\\quad \\hat\\sigma = 4.51."
        ]
    },
    "Margin_(machine_learning)": {
        "title": "Margin_(machine_learning)",
        "chunks": [
            {
                "text": "thumb|H1 does not separate the classes.H2 does, but only with a small margin.H3 separates them with the maximum margin. In machine learning, the margin of a single data point is defined to be the distance from the data point to a decision boundary. Note that there are many distances and decision boundaries that may be appropriate for certain datasets and goals. A margin classifier is a classification model that utilizes the margin of each example to learn such classification. There are theoretical justifications (based on the VC dimension) as to why maximizing the margin (under some suitable constraints) may be beneficial for machine learning and statistical inference algorithms. For a given dataset, there may be many hyperplanes that could classify it. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the classes. Hence, one should choose the hyperplane such that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane, and the linear classifier it defines is known as a maximum margin classifier (or, equivalently, the perceptron of optimal stability). Category:Support vector machines"
            }
        ],
        "latex_formulas": []
    },
    "Inverse_function": {
        "title": "Inverse_function",
        "chunks": [
            {
                "text": "A function and its inverse $f −1$. Because maps to 3, the inverse $f −1$ maps 3 back to . In mathematics, the inverse function of a function (also called the inverse of ) is a function that undoes the operation of . The inverse of exists if and only if is bijective, and if it exists, is denoted by $f^{-1} .$ For a function $f\\colon X\\to Y$, its inverse $f^{-1}\\colon Y\\to X$ admits an explicit description: it sends each element $y\\in Y$ to the unique element $x\\in X$ such that . As an example, consider the real-valued function of a real variable given by $1=f(x) = 5x − 7$. One can think of as the function which multiplies its input by 5 then subtracts 7 from the result. To undo this, one adds 7 to the input, then divides the result by 5. Therefore, the inverse of is the function $f^{-1}\\colon \\R\\to\\R$ defined by $f^{-1}(y) = \\frac{y+7}{5} .$ Definitions 240px|If maps to , then $f −1$ maps back to ."
            },
            {
                "text": "Let be a function whose domain is the set , and whose codomain is the set . Then is invertible if there exists a function from to such that $g(f(x))=x$ for all $x\\in X$ and $f(g(y))=y$ for all $y\\in Y$. If is invertible, then there is exactly one function satisfying this property. The function is called the inverse of , and is usually denoted as $f −1$, a notation introduced by John Frederick William Herschel in 1813. The function is invertible if and only if it is bijective. This is because the condition $g(f(x))=x$ for all $x\\in X$ implies that is injective, and the condition $f(g(y))=y$ for all $y\\in Y$ implies that is surjective. The inverse function $f −1$ to can be explicitly described as the function $f^{-1}(y)=(\\text{the unique element }x\\in X\\text{ such that }f(x)=y)$."
            },
            {
                "text": "Inverses and composition Recall that if is an invertible function with domain and codomain , then $ f^{-1}\\left(f(x)\\right) = x$, for every $x \\in X$ and $ f\\left(f^{-1}(y)\\right) = y$ for every $y \\in Y $. Using the composition of functions, this statement can be rewritten to the following equations between functions: $ f^{-1} \\circ f = \\operatorname{id}_X$ and $f \\circ f^{-1} = \\operatorname{id}_Y, $ where $idX$ is the identity function on the set ; that is, the function that leaves its argument unchanged. In category theory, this statement is used as the definition of an inverse morphism. Considering function composition helps to understand the notation $f −1$. Repeatedly composing a function $f: X→X$ with itself is called iteration. If is applied times, starting with the value , then this is written as $f n(x)$; so $f 2(x) , etc."
            },
            {
                "text": "Since $f −1(f (x)) , composing $f −1$ and $f n$ yields $f n−1$, \"undoing\" the effect of one application of . Notation While the notation $f −1(x)$ might be misunderstood, $(f(x))−1$ certainly denotes the multiplicative inverse of $f(x)$ and has nothing to do with the inverse function of . The notation $f^{\\langle -1\\rangle}$ might be used for the inverse function to avoid ambiguity with the multiplicative inverse.Helmut Sieber und Leopold Huber: Mathematische Begriffe und Formeln für Sekundarstufe I und II der Gymnasien. Ernst Klett Verlag. In keeping with the general notation, some English authors use expressions like $sin−1(x)$ to denote the inverse of the sine function applied to (actually a partial inverse; see below). Other authors feel that this may be confused with the notation for the multiplicative inverse of $sin (x)$, which can be denoted as $(sin (x))−1$."
            },
            {
                "text": "To avoid any confusion, an inverse trigonometric function is often indicated by the prefix \"arc\" (for Latin ). For instance, the inverse of the sine function is typically called the arcsine function, written as $arcsin(x)$. Similarly, the inverse of a hyperbolic function is indicated by the prefix \"ar\" (for Latin ). For instance, the inverse of the hyperbolic sine function is typically written as $arsinh(x)$. The expressions like $sin−1(x)$ can still be useful to distinguish the multivalued inverse from the partial inverse: $\\sin^{-1}(x) = \\{(-1)^n \\arcsin(x) + \\pi n : n \\in \\mathbb Z\\}$. Other inverse special functions are sometimes prefixed with the prefix \"inv\", if the ambiguity of the $f −1$ notation should be avoided. Examples Squaring and square root functions The function $f: R → [0,∞)$ given by $1=f(x) = x2$ is not injective because $(-x)^2=x^2$ for all $x\\in\\R$."
            },
            {
                "text": "Therefore, is not invertible. If the domain of the function is restricted to the nonnegative reals, that is, we take the function $f\\colon [0,\\infty)\\to [0,\\infty);\\ x\\mapsto x^2$ with the same rule as before, then the function is bijective and so, invertible. The inverse function here is called the (positive) square root function and is denoted by $x\\mapsto\\sqrt x$. Standard inverse functions The following table shows several standard functions and their inverses: +Inverse arithmetic functions Function $f(x)$ Inverse $f −1(y)$ Notes $x + a$ $y − a$ $a − x$ $a − y$ $mx$ $m ≠ 0$ (i.e. $x−1$) (i.e. $y−1$) $x, y ≠ 0$ $xp$ $\\sqrt[p]y$ (i.e. $y1/p$) integer $p > 0$; $x, y ≥ 0$ if $p$ is even $ax$ $loga y$ $y > 0$ and $a > 0$ and $a ≠ 1$ $xex$ $W (y)$ $x ≥ −1$ and $y ≥ −1/e$ trigonometric functions inverse trigonometric functions various restrictions (see table below) hyperbolic functions inverse hyperbolic functions various restrictions Formula for the inverse Many functions given by algebraic formulas possess a formula for their inverse."
            },
            {
                "text": "This is because the inverse $f^{-1} $ of an invertible function $f\\colon\\R\\to\\R$ has an explicit description as $f^{-1}(y)=(\\text{the unique element }x\\in \\R\\text{ such that }f(x)=y)$. This allows one to easily determine inverses of many functions that are given by algebraic formulas. For example, if is the function $f(x) = (2x + 8)^3 $ then to determine $f^{-1}(y) $ for a real number , one must find the unique real number such that $1= (2x + 8)3 = y$. This equation can be solved: $\\begin{align} y & = (2x+8)^3 \\\\ \\sqrt[3]{y} & = 2x + 8 \\\\ \\sqrt[3]{y} - 8 & = 2x \\\\ \\dfrac{\\sqrt[3]{y} - 8}{2} & = x . \\end{align}$ Thus the inverse function $f −1$ is given by the formula $f^{-1}(y) = \\frac{\\sqrt[3]{y} - 8} 2.$ Sometimes, the inverse of a function cannot be expressed by a closed-form formula."
            },
            {
                "text": "For example, if is the function $f(x) = x - \\sin x ,$ then is a bijection, and therefore possesses an inverse function $f −1$. The formula for this inverse has an expression as an infinite sum: $ f^{-1}(y) = \\sum_{n=1}^\\infty \\frac{y^{n/3}}{n!} \\lim_{ \\theta \\to 0} \\left( \\frac{\\mathrm{d}^{\\,n-1}}{\\mathrm{d} \\theta^{\\,n-1}} \\left( \\frac \\theta { \\sqrt[3]{ \\theta - \\sin( \\theta )} } \\right)^n \\right). $ Properties Since a function is a special type of binary relation, many of the properties of an inverse function correspond to properties of converse relations. Uniqueness If an inverse function exists for a given function , then it is unique. This follows since the inverse function must be the converse relation, which is completely determined by . Symmetry There is a symmetry between a function and its inverse."
            },
            {
                "text": "Specifically, if is an invertible function with domain and codomain , then its inverse $f −1$ has domain and image , and the inverse of $f −1$ is the original function . In symbols, for functions $f:X → Y$ and $f−1:Y → X$, $f^{-1}\\circ f = \\operatorname{id}_X $ and $ f \\circ f^{-1} = \\operatorname{id}_Y.$ This statement is a consequence of the implication that for to be invertible it must be bijective. The involutory nature of the inverse can be concisely expressed by $\\left(f^{-1}\\right)^{-1} = f.$ 240px|The inverse of $g ∘ f$ is $f −1 ∘ g −1$. The inverse of a composition of functions is given by $(g \\circ f)^{-1} = f^{-1} \\circ g^{-1}.$ Notice that the order of and have been reversed; to undo followed by , we must first undo , and then undo ."
            },
            {
                "text": "For example, let $1= f(x) = 3x$ and let $1= g(x) = x + 5$. Then the composition $g ∘ f$ is the function that first multiplies by three and then adds five, $(g \\circ f)(x) = 3x + 5.$ To reverse this process, we must first subtract five, and then divide by three, $(g \\circ f)^{-1}(x) = \\tfrac13(x - 5).$ This is the composition $(f −1 ∘ g −1)(x)$. Self-inverses If is a set, then the identity function on is its own inverse: ${\\operatorname{id}_X}^{-1} = \\operatorname{id}_X.$ More generally, a function $f : X → X$ is equal to its own inverse, if and only if the composition $f ∘ f$ is equal to $idX$. Such a function is called an involution. Graph of the inverse The graphs of $1= y = f(x)$ and $1= y = f −1(x)$."
            },
            {
                "text": "The dotted line is $1= y = x$. If is invertible, then the graph of the function $y = f^{-1}(x)$ is the same as the graph of the equation $x = f(y) .$ This is identical to the equation $1= y = f(x)$ that defines the graph of , except that the roles of and have been reversed. Thus the graph of $f −1$ can be obtained from the graph of by switching the positions of the and axes. This is equivalent to reflecting the graph across the line $1= y = x$. Inverses and derivatives By the inverse function theorem, a continuous function of a single variable $f\\colon A\\to\\mathbb{R}$ (where $A\\subseteq\\mathbb{R}$) is invertible on its range (image) if and only if it is either strictly increasing or decreasing (with no local maxima or minima). For example, the function $f(x) = x^3 + x$ is invertible, since the derivative $1= f′(x) = 3x2 + 1$ is always positive."
            },
            {
                "text": "If the function is differentiable on an interval and $f′(x) ≠ 0$ for each $x ∈ I$, then the inverse $f −1$ is differentiable on $f(I)$. If $1= y = f(x)$, the derivative of the inverse is given by the inverse function theorem, $\\left(f^{-1}\\right)^\\prime (y) = \\frac{1}{f'\\left(x \\right)}. $ Using Leibniz's notation the formula above can be written as $\\frac{dx}{dy} = \\frac{1}{dy / dx}. $ This result follows from the chain rule (see the article on inverse functions and differentiation). The inverse function theorem can be generalized to functions of several variables. Specifically, a continuously differentiable multivariable function $f : Rn → Rn$ is invertible in a neighborhood of a point as long as the Jacobian matrix of at is invertible. In this case, the Jacobian of $f −1$ at $f(p)$ is the matrix inverse of the Jacobian of at ."
            },
            {
                "text": "Real-world examples Let be the function that converts a temperature in degrees Celsius to a temperature in degrees Fahrenheit, then its inverse function converts degrees Fahrenheit to degrees Celsius, since Suppose assigns each child in a family its birth year. An inverse function would output which child was born in a given year. However, if the family has children born in the same year (for instance, twins or triplets, etc.) then the output cannot be known when the input is the common birth year. As well, if a year is given in which no child was born then a child cannot be named. But if each child was born in a separate year, and if we restrict attention to the three years in which a child was born, then we do have an inverse function. For example, Let be the function that leads to an percentage rise of some quantity, and be the function producing an percentage fall. Applied to $100 with = 10%, we find that applying the first function followed by the second does not restore the original value of $100, demonstrating the fact that, despite appearances, these two functions are not inverses of each other."
            },
            {
                "text": "The formula to calculate the pH of a solution is $1=pH = −log10[H+]$. In many cases we need to find the concentration of acid from a pH measurement. The inverse function $1=[H+] = 10−pH$ is used. Generalizations Partial inverses The square root of is a partial inverse to $1= f(x) = x2$. Even if a function is not one-to-one, it may be possible to define a partial inverse of by restricting the domain. For example, the function $f(x) = x^2$ is not one-to-one, since $1= x2 = (−x)2$. However, the function becomes one-to-one if we restrict to the domain $x ≥ 0$, in which case $f^{-1}(y) = \\sqrt{y} . $ (If we instead restrict to the domain $x ≤ 0$, then the inverse is the negative of the square root of .) Full inverses right|thumb|The inverse of this cubic function has three branches. Alternatively, there is no need to restrict the domain if we are content with the inverse being a multivalued function: $f^{-1}(y) = \\pm\\sqrt{y} ."
            },
            {
                "text": "$ Sometimes, this multivalued inverse is called the full inverse of , and the portions (such as and −) are called branches. The most important branch of a multivalued function (e.g. the positive square root) is called the principal branch, and its value at is called the principal value of $f −1(y)$. For a continuous function on the real line, one branch is required between each pair of local extrema. For example, the inverse of a cubic function with a local maximum and a local minimum has three branches (see the adjacent picture). Trigonometric inverses right|thumb|The arcsine is a partial inverse of the sine function. The above considerations are particularly important for defining the inverses of trigonometric functions. For example, the sine function is not one-to-one, since $\\sin(x + 2\\pi) = \\sin(x)$ for every real (and more generally $1= sin(x + 2 for every integer ). However, the sine is one-to-one on the interval , and the corresponding partial inverse is called the arcsine."
            },
            {
                "text": "This is considered the principal branch of the inverse sine, so the principal value of the inverse sine is always between − and . The following table describes the principal branch of each inverse trigonometric function: functionRange of usual principal value arcsin $− arccos $0 ≤ cos−1(x) ≤ arctan $− arccot $0 < cot−1(x) < arcsec $0 ≤ sec−1(x) ≤ arccsc $− Left and right inverses Function composition on the left and on the right need not coincide. In general, the conditions \"There exists such that $g(f(x))\" and \"There exists such that $f(g(x))\" imply different properties of . For example, let $f: R → denote the squaring map, such that $1=f(x) = x2$ for all in $R$, and let $ denote the square root map, such that $g(x) for all $x ≥ 0$. Then $1=f(g(x)) = x$ for all in ; that is, is a right inverse to ."
            },
            {
                "text": "However, is not a left inverse to , since, e.g., $1=g(f(−1)) = 1 ≠ −1$. Left inverses If $f: X → Y$, a left inverse for (or retraction of ) is a function $g: Y → X$ such that composing with from the left gives the identity function That is, the function satisfies the rule If $f(x), then $g(y). The function must equal the inverse of on the image of , but may take any values for elements of not in the image. A function with nonempty domain is injective if and only if it has a left inverse. An elementary proof runs as follows: If is the left inverse of , and $1=f(x) = f(y)$, then $1=g(f(x)) = g(f(y)) = x = y$. If nonempty $f: X → Y$ is injective, construct a left inverse $g: Y → X$ as follows: for all $y ∈ Y$, if is in the image of , then there exists $x ∈ X$ such that $1=f(x) = y$."
            },
            {
                "text": "Let $1=g(y) = x$; this definition is unique because is injective. Otherwise, let $g(y)$ be an arbitrary element of .For all $x ∈ X$, $f(x)$ is in the image of . By construction, $1=g(f(x)) = x$, the condition for a left inverse. In classical mathematics, every injective function with a nonempty domain necessarily has a left inverse; however, this may fail in constructive mathematics. For instance, a left inverse of the inclusion ${0,1} → R$ of the two-element set in the reals violates indecomposability by giving a retraction of the real line to the set ${0,1. Right inverses thumb|Example of right inverse with non-injective, surjective function A right inverse for (or section of ) is a function $h: Y → X$ such that $f \\circ h = \\operatorname{id}_Y . $ That is, the function satisfies the rule If $\\displaystyle h(y) = x$, then $\\displaystyle f(x) = y .$ Thus, $h(y)$ may be any of the elements of that map to under ."
            },
            {
                "text": "A function has a right inverse if and only if it is surjective (though constructing such an inverse in general requires the axiom of choice). If is the right inverse of , then is surjective. For all $y \\in Y$, there is $x = h(y)$ such that $f(x) = f(h(y)) = y$. If is surjective, has a right inverse , which can be constructed as follows: for all $y \\in Y$, there is at least one $x \\in X$ such that $f(x) = y$ (because is surjective), so we choose one to be the value of $h(y)$. Two-sided inverses An inverse that is both a left and right inverse (a two-sided inverse), if it exists, must be unique. In fact, if a function has a left inverse and a right inverse, they are both the same two-sided inverse, so it can be called the inverse. If $g$ is a left inverse and $h$ a right inverse of $f$, for all $y \\in Y$, $g(y) = g(f(h(y)) = h(y)$."
            },
            {
                "text": "A function has a two-sided inverse if and only if it is bijective. A bijective function is injective, so it has a left inverse (if is the empty function, $f \\colon \\varnothing \\to \\varnothing$ is its own left inverse). is surjective, so it has a right inverse. By the above, the left and right inverse are the same. If has a two-sided inverse , then is a left inverse and right inverse of , so is injective and surjective. Preimages If $f: X → Y$ is any function (not necessarily invertible), the preimage (or inverse image) of an element $y ∈ Y$ is defined to be the set of all elements of that map to : $f^{-1}(y) = \\left\\{ x\\in X : f(x) = y \\right\\} . $ The preimage of can be thought of as the image of under the (multivalued) full inverse of the function . The notion can be generalized to subsets of the range. Specifically, if is any subset of , the preimage of , denoted by $f^{-1}(S) $, is the set of all elements of that map to : $f^{-1}(S) = \\left\\{ x\\in X : f(x) \\in S \\right\\} ."
            },
            {
                "text": "$ For example, take the function $f: R → R; x ↦ x2$. This function is not invertible as it is not bijective, but preimages may be defined for subsets of the codomain, e.g. $f^{-1}(\\left\\{1,4,9,16\\right\\}) = \\left\\{-4,-3,-2,-1,1,2,3,4\\right\\}$. The original notion and its generalization are related by the identity $f^{-1}(y) = f^{-1}(\\{y\\}),$ The preimage of a single element $y ∈ Y$ – a singleton set ${y}$ – is sometimes called the fiber of . When is the set of real numbers, it is common to refer to $f −1({y})$ as a level set. See also Lagrange inversion theorem, gives the Taylor series expansion of the inverse function of an analytic function Integral of inverse functions Inverse Fourier transform Reversible computing Notes References Bibliography Further reading External links Category:Basic concepts in set theory Category:Unary operations"
            }
        ],
        "latex_formulas": [
            "''f''<sup> −1</sup>",
            "''f''<sup> −1</sup>",
            "''f''(''x'') = ''y''",
            "''f''(''x'') = 5''x'' − 7",
            "''f''<sup> −1</sup>",
            "''f''<sup> −1</sup>",
            "''f''<sup> −1</sup>",
            "id<sub>''X''</sub>",
            "''f''<sup> −1</sup>",
            "''f'': ''X''→''X''",
            "''f''<sup> ''n''</sup>(''x'')",
            "''f''<sup> 2</sup>(''x'') {{=}} ''f'' (''f'' (''x''))",
            "''f''<sup> −1</sup>(''f'' (''x'')) {{=}} ''x''",
            "''f''<sup> −1</sup>",
            "''f''<sup> ''n''</sup>",
            "''f''<sup> ''n''−1</sup>",
            "''f''<sup> −1</sup>(''x'')",
            "(''f''(''x''))<sup>−1</sup>",
            "''f''(''x'')",
            "sin<sup>−1</sup>(''x'')",
            "sin (''x'')",
            "(sin (''x''))<sup>−1</sup>",
            "[[arcsin]](''x'')",
            "[[arsinh]](''x'')",
            "sin<sup>−1</sup>(''x'')",
            "''f''<sup> −1</sup>",
            "''f'': '''R''' → [0,∞)",
            "''f''(''x'') = ''x''<sup>2</sup>",
            "''f''(''x'')",
            "''f''<sup> −1</sup>(''y'')",
            "''x'' [[addition|+]] ''a''",
            "''y'' [[subtraction|−]] ''a''",
            "''a'' − ''x''",
            "''a'' − ''y''",
            "[[multiplication|''mx'']]",
            "''m'' ≠ 0",
            "''x''<sup>−1</sup>",
            "''y''<sup>−1</sup>",
            "''x'', ''y'' ≠ 0",
            "''x''<sup>''p''</sup>",
            "''y''<sup>1/''p''</sup>",
            "''p'' > 0",
            "''x'', ''y'' ≥ 0",
            "p",
            "''a''<sup>''x''</sup>",
            "[[logarithm|log]]<sub>''a''</sub> ''y''",
            "''y'' > 0",
            "''a'' > 0",
            "''a'' ≠ 1",
            "''x''[[e (mathematical constant)|''e'']]<sup>''x''</sup>",
            "[[Lambert W function|W]] (''y'')",
            "''x'' ≥ −1",
            "''y'' ≥ −1/''e''",
            "(2''x'' + 8)<sup>3</sup> = ''y''",
            "''f''<sup> −1</sup>",
            "''f''<sup> −1</sup>",
            "''f''<sup> −1</sup>",
            "''f''<sup> −1</sup>",
            "''f'':''X'' → ''Y''",
            "''f''<sup>−1</sup>:''Y'' → ''X''",
            "''g'' ∘ ''f''",
            "''f''<sup> −1</sup> ∘ ''g''<sup> −1</sup>",
            "''f''(''x'') = 3''x''",
            "''g''(''x'') = ''x'' + 5",
            "''g'' ∘ ''f''",
            "(''f''<sup> −1</sup> ∘ ''g''<sup> −1</sup>)(''x'')",
            "''f'' : ''X'' → ''X''",
            "''f'' ∘ ''f''",
            "id<sub>''X''</sub>",
            "''y'' = ''f''(''x'')",
            "''y'' = ''f''<sup> −1</sup>(''x'')",
            "''y'' = ''x''",
            "''y'' = ''f''(''x'')",
            "''f''<sup> −1</sup>",
            "''y'' = ''x''",
            "''f&prime;''(''x'') = 3''x''<sup>2</sup> + 1",
            "''f&prime;''(''x'') ≠ 0",
            "''x'' ∈ ''I''",
            "''f''<sup> −1</sup>",
            "''f''(''I'')",
            "''y'' = ''f''(''x'')",
            "''f '': '''R'''<sup>''n''</sup> → '''R'''<sup>''n''</sup>",
            "''f''<sup> −1</sup>",
            "''f''(''p'')",
            "pH = −log<sub>10</sub>[H<sup>+</sup>]",
            "[H<sup>+</sup>] = 10<sup>−pH</sup>",
            "''f''(''x'') = ''x''<sup>2</sup>",
            "''x''<sup>2</sup> = (−''x'')<sup>2</sup>",
            "''x'' ≥ 0",
            "''x'' ≤ 0",
            "''f''<sup> −1</sup>(''y'')",
            "sin(''x'' + 2{{pi}}''n'') = sin(''x'')",
            "−{{sfrac|{{pi}}|2}} ≤ sin<sup>−1</sup>(''x'') ≤ {{sfrac|{{pi}}|2}}",
            "0 ≤ cos<sup>−1</sup>(''x'') ≤ {{pi}}",
            "−{{sfrac|π|2}} < tan<sup>−1</sup>(''x'') < {{sfrac|{{pi}}|2}}",
            "0 < cot<sup>−1</sup>(''x'') < {{pi}}",
            "0 ≤ sec<sup>−1</sup>(''x'') ≤ {{pi}}",
            "−{{sfrac|{{pi}}|2}} ≤ csc<sup>−1</sup>(''x'') ≤ {{sfrac|{{pi}}|2}}",
            "''g''(''f''(''x'')){{=}}''x''",
            "''f''(''g''(''x'')){{=}}''x''",
            "''f'': '''R''' → {{closed-open|0, ∞}}",
            "''f''(''x'') = ''x''<sup>2</sup>",
            "'''R'''",
            "{{mvar|g}}: {{closed-open|0, ∞}} → '''R'''",
            "''g''(''x'') {{=}}",
            "''x'' ≥ 0",
            "''f''(''g''(''x'')) = ''x''",
            "''g''(''f''(−1)) = 1 ≠ −1",
            "''f'': ''X'' → ''Y''",
            "''g'': ''Y'' → ''X''",
            "''f''(''x''){{=}}''y''",
            "''g''(''y''){{=}}''x''",
            "''f''(''x'') = ''f''(''y'')",
            "''g''(''f''(''x'')) = ''g''(''f''(''y'')) = ''x'' = ''y''",
            "''f'': ''X'' → ''Y''",
            "''g'': ''Y'' → ''X''",
            "''y'' ∈ ''Y''",
            "''x'' ∈ ''X''",
            "''f''(''x'') = ''y''",
            "''g''(''y'') = ''x''",
            "''g''(''y'')",
            "''x'' ∈ ''X''",
            "''f''(''x'')",
            "''g''(''f''(''x'')) = ''x''",
            "{0,1} → '''R'''",
            "{0,1{{)}}",
            "''h'': ''Y'' → ''X''",
            "''h''(''y'')",
            "''h''(''y'')",
            "''f'': ''X'' → ''Y''",
            "''y'' &isin; ''Y''",
            "''f'': '''R''' → '''R'''; ''x'' ↦ ''x''<sup>2</sup>",
            "''y'' &isin; ''Y''",
            "{''y''}",
            "''f''<sup> −1</sup>({''y''})",
            "f^{-1} .",
            "f\\colon X\\to Y",
            "f^{-1}\\colon Y\\to X",
            "y\\in Y",
            "x\\in X",
            "f^{-1}\\colon \\R\\to\\R",
            "f^{-1}(y) = \\frac{y+7}{5} .",
            "g(f(x))=x",
            "x\\in X",
            "f(g(y))=y",
            "y\\in Y",
            "g(f(x))=x",
            "x\\in X",
            "f(g(y))=y",
            "y\\in Y",
            "f^{-1}(y)=(\\text{the unique element }x\\in X\\text{ such that }f(x)=y)",
            "f^{-1}\\left(f(x)\\right) = x",
            "x \\in X",
            "f\\left(f^{-1}(y)\\right) = y",
            "y \\in Y",
            "f^{-1} \\circ f = \\operatorname{id}_X",
            "f \\circ f^{-1} = \\operatorname{id}_Y,",
            "f^{\\langle -1\\rangle}",
            "\\sin^{-1}(x) = \\{(-1)^n \\arcsin(x) + \\pi n : n \\in \\mathbb Z\\}",
            "(-x)^2=x^2",
            "x\\in\\R",
            "f\\colon [0,\\infty)\\to [0,\\infty);\\ x\\mapsto x^2",
            "x\\mapsto\\sqrt x",
            "f\\colon X \\to Y",
            "\\sqrt[p]y",
            "f^{-1}",
            "f\\colon\\R\\to\\R",
            "f^{-1}(y)=(\\text{the unique element }x\\in \\R\\text{ such that }f(x)=y)",
            "f(x) = (2x + 8)^3",
            "f^{-1}(y)",
            "\\begin{align}\n      y         & = (2x+8)^3 \\\\\n  \\sqrt[3]{y}   & = 2x + 8   \\\\\n\\sqrt[3]{y} - 8 & = 2x       \\\\\n\\dfrac{\\sqrt[3]{y} - 8}{2} & = x .\n\\end{align}",
            "f^{-1}(y) = \\frac{\\sqrt[3]{y} - 8} 2.",
            "f(x) = x - \\sin x ,",
            "f^{-1}(y) =\n\\sum_{n=1}^\\infty\n \\frac{y^{n/3}}{n!} \\lim_{ \\theta \\to 0} \\left(\n \\frac{\\mathrm{d}^{\\,n-1}}{\\mathrm{d} \\theta^{\\,n-1}} \\left(\n \\frac \\theta { \\sqrt[3]{ \\theta - \\sin( \\theta )} } \\right)^n\n\\right).",
            "f^{-1}\\circ f = \\operatorname{id}_X",
            "f \\circ f^{-1} = \\operatorname{id}_Y.",
            "\\left(f^{-1}\\right)^{-1} = f.",
            "(g \\circ f)^{-1} = f^{-1} \\circ g^{-1}.",
            "(g \\circ f)(x) = 3x + 5.",
            "(g \\circ f)^{-1}(x) = \\tfrac13(x - 5).",
            "{\\operatorname{id}_X}^{-1} = \\operatorname{id}_X.",
            "y = f^{-1}(x)",
            "x = f(y) .",
            "f\\colon A\\to\\mathbb{R}",
            "A\\subseteq\\mathbb{R}",
            "f(x) = x^3 + x",
            "\\left(f^{-1}\\right)^\\prime (y)  = \\frac{1}{f'\\left(x \\right)}.",
            "\\frac{dx}{dy} = \\frac{1}{dy / dx}.",
            "f(x) = x^2",
            "f^{-1}(y) = \\sqrt{y} .",
            "f^{-1}(y) = \\pm\\sqrt{y} .",
            "\\sin(x + 2\\pi) = \\sin(x)",
            "f \\circ h = \\operatorname{id}_Y .",
            "\\displaystyle h(y) = x",
            "\\displaystyle f(x) = y .",
            "y \\in Y",
            "x = h(y)",
            "f(x) = f(h(y)) = y",
            "y \\in Y",
            "x \\in X",
            "f(x) = y",
            "g",
            "h",
            "f",
            "y \\in Y",
            "g(y) = g(f(h(y)) = h(y)",
            "f \\colon \\varnothing \\to \\varnothing",
            "f^{-1}(y) = \\left\\{ x\\in X : f(x) = y \\right\\} .",
            "f^{-1}(S)",
            "f^{-1}(S) = \\left\\{ x\\in X : f(x) \\in S \\right\\} .",
            "f^{-1}(\\left\\{1,4,9,16\\right\\}) = \\left\\{-4,-3,-2,-1,1,2,3,4\\right\\}",
            "f^{-1}(y) = f^{-1}(\\{y\\}),"
        ]
    },
    "Multidimensional_scaling": {
        "title": "Multidimensional_scaling",
        "chunks": [
            {
                "text": "An example of classical multidimensional scaling applied to voting patterns in the United States House of Representatives. Each blue dot represents one Democratic member of the House, and each red dot one Republican. Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a data set. MDS is used to translate distances between each pair of objects in a set into a configuration of points mapped into an abstract Cartesian space. More technically, MDS refers to a set of related ordination techniques used in information visualization, in particular to display the information contained in a distance matrix. It is a form of non-linear dimensionality reduction. Given a distance matrix with the distances between each pair of objects in a set, and a chosen number of dimensions, N, an MDS algorithm places each object into N-dimensional space (a lower-dimensional representation) such that the between-object distances are preserved as well as possible. For N = 1, 2, and 3, the resulting points can be visualized on a scatter plot. Core theoretical contributions to MDS were made by James O. Ramsay of McGill University, who is also regarded as the founder of functional data analysis."
            },
            {
                "text": "Types MDS algorithms fall into a taxonomy, depending on the meaning of the input matrix: Classical multidimensional scaling It is also known as Principal Coordinates Analysis (PCoA), Torgerson Scaling or Torgerson–Gower scaling. It takes an input matrix giving dissimilarities between pairs of items and outputs a coordinate matrix whose configuration minimizes a loss function called strain, which is given by where $x_{i}$ denote vectors in N-dimensional space, $x_i^T x_j $ denotes the scalar product between $x_{i}$ and $x_{j}$, and $b_{ij}$ are the elements of the matrix $B$ defined on step 2 of the following algorithm, which are computed from the distances. Steps of a Classical MDS algorithm: Classical MDS uses the fact that the coordinate matrix $X$ can be derived by eigenvalue decomposition from . And the matrix can be computed from proximity matrix by using double centering.Wickelmaier, Florian. \"An introduction to MDS.\" Sound Quality Research Unit, Aalborg University, Denmark (2003): 46 Set up the squared proximity matrix Apply double centering: using the centering matrix , where is the number of objects, is the identity matrix, and is an matrix of all ones."
            },
            {
                "text": "Determine the largest eigenvalues and corresponding eigenvectors of (where is the number of dimensions desired for the output). Now, , where is the matrix of eigenvectors and is the diagonal matrix of eigenvalues of . Classical MDS assumes metric distances. So this is not applicable for direct dissimilarity ratings. Metric multidimensional scaling (mMDS) It is a superset of classical MDS that generalizes the optimization procedure to a variety of loss functions and input matrices of known distances with weights and so on. A useful loss function in this context is called stress, which is often minimized using a procedure called stress majorization. Metric MDS minimizes the cost function called “stress” which is a residual sum of squares:$\\text{Stress}_D(x_1,x_2,...,x_n)=\\sqrt{\\sum_{i\\ne j=1,...,n}\\bigl(d_{ij}-\\|x_i-x_j\\|\\bigr)^2}.$ Metric scaling uses a power transformation with a user-controlled exponent : and for distance. In classical scaling Non-metric scaling is defined by the use of isotonic regression to nonparametrically estimate a transformation of the dissimilarities."
            },
            {
                "text": "Non-metric multidimensional scaling (NMDS) In contrast to metric MDS, non-metric MDS finds both a non-parametric monotonic relationship between the dissimilarities in the item-item matrix and the Euclidean distances between items, and the location of each item in the low-dimensional space. Let $d_{ij}$ be the dissimilarity between points $i, j$. Let $\\hat d_{ij} = \\| x_i - x_j\\|$ be the Euclidean distance between embedded points $x_i, x_j$. Now, for each choice of the embedded points $x_i$ and is a monotonically increasing function $f$, define the \"stress\" function: $S(x_1, ..., x_n; f)=\\sqrt{\\frac{\\sum_{i<j}\\bigl(f(d_{ij})-\\hat d_{ij}\\bigr)^2}{\\sum_{i<j} \\hat d_{ij}^2}}.$ The factor of $\\sum_{i<j} \\hat d_{ij}^2$ in the denominator is necessary to prevent a \"collapse\"."
            },
            {
                "text": "Suppose we define instead $S=\\sqrt{\\sum_{i<j}\\bigl(f(d_{ij})-\\hat d_{ij})^2}$, then it can be trivially minimized by setting $f = 0$, then collapse every point to the same point. A few variants of this cost function exist. MDS programs automatically minimize stress in order to obtain the MDS solution. The core of a non-metric MDS algorithm is a twofold optimization process. First the optimal monotonic transformation of the proximities has to be found. Secondly, the points of a configuration have to be optimally arranged, so that their distances match the scaled proximities as closely as possible. NMDS needs to optimize two objectives simultaneously. This is usually done iteratively: Initialize $x_i$ randomly, e. g. by sampling from a normal distribution. Do until a stopping criterion (for example, $S < \\epsilon$) Solve for $f = \\arg\\min_f S(x_1, ..., x_n ; f)$ by isotonic regression. Solve for $x_1, ..., x_n = \\arg\\min_{x_1, ..., x_n} S(x_1, ..., x_n ; f)$ by gradient descent or other methods."
            },
            {
                "text": "Return $x_i$ and $f$ Louis Guttman's smallest space analysis (SSA) is an example of a non-metric MDS procedure. Generalized multidimensional scaling (GMD) An extension of metric multidimensional scaling, in which the target space is an arbitrary smooth non-Euclidean space. In cases where the dissimilarities are distances on a surface and the target space is another surface, GMDS allows finding the minimum-distortion embedding of one surface into another. Super multidimensional scaling (SMDS) An extension of MDS, known as Super MDS, incorporates both distance and angle information for improved source localization. Unlike traditional MDS, which uses only distance measurements, Super MDS processes both distance and angle-of-arrival (AOA) data algebraically (without iteration) to achieve better accuracy. The method proceeds in the following steps: Construct the Reduced Edge Gram Kernel: For a network of $N$ sources in an $\\eta$-dimensional space, define the edge vectors as $v_{i} = x_{m} - x_{n}$. The dissimilarity is given by $k_{i,j} = \\langle v_i, v_j \\rangle$."
            },
            {
                "text": "Assemble these into the full kernel $K = VV^T$, and then form the reduced kernel using the $N-1$ independent vectors: $\\bar{K} = [V]_{(N-1)\\times\\eta}\\ [V]_{(N-1)\\times\\eta}^T$, Eigen-Decomposition: Compute the eigen-decomposition of $\\bar{K}$, Estimate Edge Vectors: Recover the edge vectors as $ \\hat{V} = \\Bigl( U_{M \\times \\eta}\\, \\Lambda^{\\odot \\frac{1}{2}}_{\\eta \\times \\eta} \\Bigr)^T $, Procrustes Alignment: Retrieve $\\hat{V}$ from $V$ via Procrustes Transformation, Compute Coordinates: Solve the following linear equations to compute the coordinate estimates $\\begin{pmatrix} 1 \\vline \\mathbf{0}_{1 \\times N-1} \\\\ \\hline \\mathbf{[C]}_{N-1 \\times N} \\end{pmatrix} \\cdot \\begin{pmatrix}\\mathbf{x}_{1} \\\\ \\hline[\\mathbf{X}]_{N-1 \\times \\eta} \\end{pmatrix}=\\begin{pmatrix} \\mathbf{x}_{1} \\\\ \\hline[\\mathbf{V}]_{N-1 \\times \\eta} \\end{pmatrix}, $ This concise approach reduces the need for multiple anchors and enhances localization precision by leveraging angle constraints."
            },
            {
                "text": "Details The data to be analyzed is a collection of $M$ objects (colors, faces, stocks, . . .) on which a distance function is defined, $d_{i,j} :=$ distance between $i$-th and $j$-th objects. These distances are the entries of the dissimilarity matrix $ D := \\begin{pmatrix} d_{1,1} & d_{1,2} & \\cdots & d_{1,M} \\\\ d_{2,1} & d_{2,2} & \\cdots & d_{2,M} \\\\ \\vdots & \\vdots & & \\vdots \\\\ d_{M,1} & d_{M,2} & \\cdots & d_{M,M} \\end{pmatrix}. $ The goal of MDS is, given $D$, to find $M$ vectors $x_1,\\ldots,x_M \\in \\mathbb{R}^N$ such that $\\|x_i - x_j\\| \\approx d_{i,j}$ for all $i,j\\in {1,\\dots,M}$, where $\\|\\cdot\\|$ is a vector norm."
            },
            {
                "text": "In classical MDS, this norm is the Euclidean distance, but, in a broader sense, it may be a metric or arbitrary distance function.Kruskal, J. B., and Wish, M. (1978), Multidimensional Scaling, Sage University Paper series on Quantitative Application in the Social Sciences, 07-011. Beverly Hills and London: Sage Publications. For example, when dealing with mixed-type data that contain numerical as well as categorical descriptors, Gower's distance is a common alternative. In other words, MDS attempts to find a mapping from the $M$ objects into $\\mathbb{R}^N$ such that distances are preserved. If the dimension $N$ is chosen to be 2 or 3, we may plot the vectors $x_i$ to obtain a visualization of the similarities between the $M$ objects. Note that the vectors $x_i$ are not unique: With the Euclidean distance, they may be arbitrarily translated, rotated, and reflected, since these transformations do not change the pairwise distances $\\|x_i - x_j\\|$. (Note: The symbol $\\mathbb{R}$ indicates the set of real numbers, and the notation $\\mathbb{R}^N$ refers to the Cartesian product of $N$ copies of $\\mathbb{R}$, which is an $N$-dimensional vector space over the field of the real numbers.)"
            },
            {
                "text": "There are various approaches to determining the vectors $x_i$. Usually, MDS is formulated as an optimization problem, where $(x_1,\\ldots,x_M)$ is found as a minimizer of some cost function, for example, $ \\underset{x_1,\\ldots,x_M}{\\mathrm{argmin}} \\sum_{i<j} ( \\|x_i - x_j\\| - d_{i,j} )^2. \\, $ A solution may then be found by numerical optimization techniques. For some particularly chosen cost functions, minimizers can be stated analytically in terms of matrix eigendecompositions. Procedure There are several steps in conducting MDS research: Formulating the problem – What variables do you want to compare? How many variables do you want to compare? What purpose is the study to be used for? Obtaining input data – For example, :- Respondents are asked a series of questions. For each product pair, they are asked to rate similarity (usually on a 7-point Likert scale from very similar to very dissimilar). The first question could be for Coke/Pepsi for example, the next for Coke/Hires rootbeer, the next for Pepsi/Dr Pepper, the next for Dr Pepper/Hires rootbeer, etc."
            },
            {
                "text": "The number of questions is a function of the number of brands and can be calculated as $Q = N (N - 1) / 2$ where Q is the number of questions and N is the number of brands. This approach is referred to as the “Perception data : direct approach”. There are two other approaches. There is the “Perception data : derived approach” in which products are decomposed into attributes that are rated on a semantic differential scale. The other is the “Preference data approach” in which respondents are asked their preference rather than similarity. Running the MDS statistical program – Software for running the procedure is available in many statistical software packages. Often there is a choice between Metric MDS (which deals with interval or ratio level data), and Nonmetric MDS (which deals with ordinal data). Decide number of dimensions – The researcher must decide on the number of dimensions they want the computer to create. Interpretability of the MDS solution is often important, and lower dimensional solutions will typically be easier to interpret and visualize."
            },
            {
                "text": "However, dimension selection is also an issue of balancing underfitting and overfitting. Lower dimensional solutions may underfit by leaving out important dimensions of the dissimilarity data. Higher dimensional solutions may overfit to noise in the dissimilarity measurements. Model selection tools like AIC, BIC, Bayes factors, or cross-validation can thus be useful to select the dimensionality that balances underfitting and overfitting. Mapping the results and defining the dimensions – The statistical program (or a related module) will map the results. The map will plot each product (usually in two-dimensional space). The proximity of products to each other indicate either how similar they are or how preferred they are, depending on which approach was used. How the dimensions of the embedding actually correspond to dimensions of system behavior, however, are not necessarily obvious. Here, a subjective judgment about the correspondence can be made (see perceptual mapping). Test the results for reliability and validity – Compute R-squared to determine what proportion of variance of the scaled data can be accounted for by the MDS procedure. An R-square of 0.6 is considered the minimum acceptable level."
            },
            {
                "text": "An R-square of 0.8 is considered good for metric scaling and .9 is considered good for non-metric scaling. Other possible tests are Kruskal’s Stress, split data tests, data stability tests (i.e., eliminating one brand), and test-retest reliability. Report the results comprehensively – Along with the mapping, at least distance measure (e.g., Sorenson index, Jaccard index) and reliability (e.g., stress value) should be given. It is also very advisable to give the algorithm (e.g., Kruskal, Mather), which is often defined by the program used (sometimes replacing the algorithm report), if you have given a start configuration or had a random choice, the number of runs, the assessment of dimensionality, the Monte Carlo method results, the number of iterations, the assessment of stability, and the proportional variance of each axis (r-square). Implementations ELKI includes two MDS implementations. MATLAB includes two MDS implementations (for classical (cmdscale) and non-classical (mdscale) MDS respectively). The R programming language offers several MDS implementations, e.g. base cmdscale function, packages smacof (mMDS and nMDS), and vegan (weighted MDS). scikit-learn contains function sklearn.manifold.MDS. See also Data clustering t-distributed stochastic neighbor embedding Factor analysis Discriminant analysis Dimensionality reduction Distance geometry Cayley–Menger determinant Sammon mapping Iconography of correlations References Bibliography Category:Dimension reduction Category:Quantitative marketing research Category:Psychometrics"
            }
        ],
        "latex_formulas": [
            "x_{i}",
            "x_i^T x_j",
            "x_{i}",
            "x_{j}",
            "b_{ij}",
            "B",
            "X",
            "\\text{Stress}_D(x_1,x_2,...,x_n)=\\sqrt{\\sum_{i\\ne j=1,...,n}\\bigl(d_{ij}-\\|x_i-x_j\\|\\bigr)^2}.",
            "d_{ij}",
            "i, j",
            "\\hat d_{ij} = \\| x_i - x_j\\|",
            "x_i, x_j",
            "x_i",
            "f",
            "S(x_1, ..., x_n; f)=\\sqrt{\\frac{\\sum_{i<j}\\bigl(f(d_{ij})-\\hat d_{ij}\\bigr)^2}{\\sum_{i<j} \\hat d_{ij}^2}}.",
            "\\sum_{i<j} \\hat d_{ij}^2",
            "S=\\sqrt{\\sum_{i<j}\\bigl(f(d_{ij})-\\hat d_{ij})^2}",
            "f = 0",
            "x_i",
            "S < \\epsilon",
            "f = \\arg\\min_f S(x_1, ..., x_n ; f)",
            "x_1, ..., x_n = \\arg\\min_{x_1, ..., x_n}  S(x_1, ..., x_n ; f)",
            "x_i",
            "f",
            "N",
            "\\eta",
            "v_{i} = x_{m} - x_{n}",
            "k_{i,j} = \\langle v_i, v_j \\rangle",
            "K = VV^T",
            "N-1",
            "\\bar{K} = [V]_{(N-1)\\times\\eta}\\ [V]_{(N-1)\\times\\eta}^T",
            "\\bar{K}",
            "\\hat{V} = \\Bigl( U_{M \\times \\eta}\\, \\Lambda^{\\odot \\frac{1}{2}}_{\\eta \\times \\eta} \\Bigr)^T",
            "\\hat{V}",
            "V",
            "\\begin{pmatrix}\n1 \\vline \\mathbf{0}_{1 \\times N-1} \\\\\n\\hline\n\\mathbf{[C]}_{N-1 \\times N} \n\\end{pmatrix} \\cdot \\begin{pmatrix}\\mathbf{x}_{1} \\\\\n\\hline[\\mathbf{X}]_{N-1 \\times \\eta}\n\\end{pmatrix}=\\begin{pmatrix}\n\\mathbf{x}_{1} \\\\\n\\hline[\\mathbf{V}]_{N-1 \\times \\eta}\n\\end{pmatrix},",
            "M",
            "d_{i,j} :=",
            "i",
            "j",
            "D := \n\\begin{pmatrix}\nd_{1,1} & d_{1,2} & \\cdots & d_{1,M} \\\\\nd_{2,1} & d_{2,2} & \\cdots & d_{2,M} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\nd_{M,1} & d_{M,2} & \\cdots & d_{M,M}\n\\end{pmatrix}.",
            "D",
            "M",
            "x_1,\\ldots,x_M \\in \\mathbb{R}^N",
            "\\|x_i - x_j\\| \\approx d_{i,j}",
            "i,j\\in {1,\\dots,M}",
            "\\|\\cdot\\|",
            "M",
            "\\mathbb{R}^N",
            "N",
            "x_i",
            "M",
            "x_i",
            "\\|x_i - x_j\\|",
            "\\mathbb{R}",
            "\\mathbb{R}^N",
            "N",
            "\\mathbb{R}",
            "N",
            "x_i",
            "(x_1,\\ldots,x_M)",
            "\\underset{x_1,\\ldots,x_M}{\\mathrm{argmin}} \\sum_{i<j} ( \\|x_i - x_j\\| - d_{i,j} )^2. \\,",
            "Q = N (N - 1) / 2"
        ]
    },
    "Error_analysis_(mathematics)": {
        "title": "Error_analysis_(mathematics)",
        "chunks": [
            {
                "text": "In mathematics, error analysis is the study of kind and quantity of error, or uncertainty, that may be present in the solution to a problem. This issue is particularly prominent in applied areas such as numerical analysis and statistics. Error analysis in numerical modeling In numerical simulation or modeling of real systems, error analysis is concerned with the changes in the output of the model as the parameters to the model vary about a mean. For instance, in a system modeled as a function of two variables $z \\,=\\, f(x,y).$ Error analysis deals with the propagation of the numerical errors in $x$ and $y$ (around mean values $\\bar{x}$ and $\\bar{y}$) to error in $z$ (around a mean $\\bar{z}$). In numerical analysis, error analysis comprises both forward error analysis and backward error analysis. Forward error analysis Forward error analysis involves the analysis of a function $z' = f'(a_0,\\,a_1,\\,\\dots,\\,a_n)$ which is an approximation (usually a finite polynomial) to a function $z \\,=\\, f(a_0,a_1,\\dots,a_n)$ to determine the bounds on the error in the approximation; i.e., to find $\\epsilon$ such that $0 \\,\\le\\, |z - z'| \\,\\le\\, \\epsilon .$ The evaluation of forward errors is desired in validated numerics.Tucker, W. (2011)."
            },
            {
                "text": "Validated numerics: a short introduction to rigorous computations. Princeton University Press. Backward error analysis Backward error analysis involves the analysis of the approximation function $z' \\,=\\, f'(a_0,\\,a_1,\\,\\dots,\\,a_n) ,$ to determine the bounds on the parameters $a_i \\,=\\, \\bar{a_i} \\,\\pm\\, \\epsilon_i$ such that the result $z' \\,=\\, z .$ Backward error analysis, the theory of which was developed and popularized by James H. Wilkinson, can be used to establish that an algorithm implementing a numerical function is numerically stable. The basic approach is to show that although the calculated result, due to roundoff errors, will not be exactly correct, it is the exact solution to a nearby problem with slightly perturbed input data. If the perturbation required is small, on the order of the uncertainty in the input data, then the results are in some sense as accurate as the data \"deserves\". The algorithm is then defined as backward stable."
            },
            {
                "text": "Stability is a measure of the sensitivity to rounding errors of a given numerical procedure; by contrast, the condition number of a function for a given problem indicates the inherent sensitivity of the function to small perturbations in its input and is independent of the implementation used to solve the problem. Applications Global positioning system The analysis of errors computed using the global positioning system is important for understanding how GPS works, and for knowing what magnitude errors should be expected. The Global Positioning System makes corrections for receiver clock errors and other effects but there are still residual errors which are not corrected. The Global Positioning System (GPS) was created by the United States Department of Defense (DOD) in the 1970s. It has come to be widely used for navigation both by the U.S. military and the general public. Molecular dynamics simulation In molecular dynamics (MD) simulations, there are errors due to inadequate sampling of the phase space or infrequently occurring events, these lead to the statistical error due to random fluctuation in the measurements. For a series of measurements of a fluctuating property , the mean value is: When these measurements are independent, the variance of the mean $1= is: but in most MD simulations, there is correlation between quantity at different time, so the variance of the mean $1= will be underestimated as the effective number of independent measurements is actually less than ."
            },
            {
                "text": "In such situations we rewrite the variance as: where $\\phi_{\\mu}$ is the autocorrelation function defined by We can then use the auto correlation function to estimate the error bar. Luckily, we have a much simpler method based on block averaging.D. C. Rapaport, The Art of Molecular Dynamics Simulation, Cambridge University Press. Scientific data verification Measurements generally have a small amount of error, and repeated measurements of the same item will generally result in slight differences in readings. These differences can be analyzed, and follow certain known mathematical and statistical properties. Should a set of data appear to be too faithful to the hypothesis, i.e., the amount of error that would normally be in such measurements does not appear, a conclusion can be drawn that the data may have been forged. Error analysis alone is typically not sufficient to prove that data have been falsified or fabricated, but it may provide the supporting evidence necessary to confirm suspicions of misconduct. See also Error analysis (linguistics) Error bar Errors and residuals in statistics Propagation of uncertainty Validated numerics References External links All about error analysis. Category:Numerical analysis Category:Error"
            }
        ],
        "latex_formulas": [
            "{{angbr|''A''}}",
            "{{angbr|''A''}}",
            "z \\,=\\, f(x,y).",
            "x",
            "y",
            "\\bar{x}",
            "\\bar{y}",
            "z",
            "\\bar{z}",
            "z' = f'(a_0,\\,a_1,\\,\\dots,\\,a_n)",
            "z \\,=\\, f(a_0,a_1,\\dots,a_n)",
            "\\epsilon",
            "0 \\,\\le\\, |z - z'| \\,\\le\\, \\epsilon .",
            "z' \\,=\\, f'(a_0,\\,a_1,\\,\\dots,\\,a_n) ,",
            "a_i \\,=\\, \\bar{a_i} \\,\\pm\\, \\epsilon_i",
            "z' \\,=\\, z .",
            "\\phi_{\\mu}"
        ]
    },
    "Gradient": {
        "title": "Gradient",
        "chunks": [
            {
                "text": "The gradient, represented by the blue arrows, denotes the direction of greatest change of a scalar function. The values of the function are represented in greyscale and increase in value from white (low) to dark (high). In vector calculus, the gradient of a scalar-valued differentiable function $f$ of several variables is the vector field (or vector-valued function) $\\nabla f$ whose value at a point $p$ gives the direction and the rate of fastest increase. The gradient transforms like a vector under change of basis of the space of variables of $f$. If the gradient of a function is non-zero at a point $p$, the direction of the gradient is the direction in which the function increases most quickly from $p$, and the magnitude of the gradient is the rate of increase in that direction, the greatest absolute directional derivative. Further, a point where the gradient is the zero vector is known as a stationary point. The gradient thus plays a fundamental role in optimization theory, where it is used to minimize a function by gradient descent."
            },
            {
                "text": "In coordinate-free terms, the gradient of a function $f(\\mathbf{r})$ may be defined by: where $df$ is the total infinitesimal change in $f$ for an infinitesimal displacement $d\\mathbf{r}$, and is seen to be maximal when $d\\mathbf{r}$ is in the direction of the gradient $\\nabla f$. The nabla symbol $\\nabla$, written as an upside-down triangle and pronounced \"del\", denotes the vector differential operator. When a coordinate system is used in which the basis vectors are not functions of position, the gradient is given by the vector whose components are the partial derivatives of $f$ at $p$. That is, for $f \\colon \\R^n \\to \\R$, its gradient $\\nabla f \\colon \\R^n \\to \\R^n$ is defined at the point $p = (x_1,\\ldots,x_n)$ in n-dimensional space as the vector Note that the above definition for gradient is defined for the function $f$ only if $f$ is differentiable at $p$."
            },
            {
                "text": "There can be functions for which partial derivatives exist in every direction but fail to be differentiable. Furthermore, this definition as the vector of partial derivatives is only valid when the basis of the coordinate system is orthonormal. For any other basis, the metric tensor at that point needs to be taken into account. For example, the function $f(x,y)=\\frac {x^2 y}{x^2+y^2}$ unless at origin where $f(0,0)=0$, is not differentiable at the origin as it does not have a well defined tangent plane despite having well defined partial derivatives in every direction at the origin. In this particular example, under rotation of x-y coordinate system, the above formula for gradient fails to transform like a vector (gradient becomes dependent on choice of basis for coordinate system) and also fails to point towards the 'steepest ascent' in some orientations. For differentiable functions where the formula for gradient holds, it can be shown to always transform as a vector under transformation of the basis so as to always point towards the fastest increase."
            },
            {
                "text": "The gradient is dual to the total derivative $df$: the value of the gradient at a point is a tangent vector – a vector at each point; while the value of the derivative at a point is a cotangent vector – a linear functional on vectors. They are related in that the dot product of the gradient of $f$ at a point $p$ with another tangent vector $\\mathbf{v}$ equals the directional derivative of $f$ at $p$ of the function along $\\mathbf{v}$; that is, . The gradient admits multiple generalizations to more general functions on manifolds; see . Motivation Gradient of the 2D function $1=f(x, y) = xe−(x2 + y2)$ is plotted as arrows over the pseudocolor plot of the function. Consider a room where the temperature is given by a scalar field, $T$, so at each point $(x, y, z)$ the temperature is $T(x, y, z)$, independent of time."
            },
            {
                "text": "At each point in the room, the gradient of $T$ at that point will show the direction in which the temperature rises most quickly, moving away from $(x, y, z)$. The magnitude of the gradient will determine how fast the temperature rises in that direction. Consider a surface whose height above sea level at point $(x, y)$ is $H(x, y)$. The gradient of $H$ at a point is a plane vector pointing in the direction of the steepest slope or grade at that point. The steepness of the slope at that point is given by the magnitude of the gradient vector. The gradient can also be used to measure how a scalar field changes in other directions, rather than just the direction of greatest change, by taking a dot product. Suppose that the steepest slope on a hill is 40%. A road going directly uphill has slope 40%, but a road going around the hill at an angle will have a shallower slope. For example, if the road is at a 60° angle from the uphill direction (when both directions are projected onto the horizontal plane), then the slope along the road will be the dot product between the gradient vector and a unit vector along the road, as the dot product measures how much the unit vector along the road aligns with the steepest slope, which is 40% times the cosine of 60°, or 20%."
            },
            {
                "text": "More generally, if the hill height function $H$ is differentiable, then the gradient of $H$ dotted with a unit vector gives the slope of the hill in the direction of the vector, the directional derivative of $H$ along the unit vector. Notation The gradient of a function $f$ at point $a$ is usually written as $\\nabla f (a)$. It may also be denoted by any of the following: $\\vec{\\nabla} f (a)$ : to emphasize the vector nature of the result. $\\operatorname{grad} f$ $\\partial_i f$ and $f_{i}$ : Written with Einstein notation, where repeated indices ($i$) are summed over. Definition The gradient of the function $f(x,y) depicted as a projected vector field on the bottom plane. The gradient (or gradient vector field) of a scalar function $f(x1, x2, x3, …, xn)$ is denoted $∇f$ or $ where $∇$ (nabla) denotes the vector differential operator, del."
            },
            {
                "text": "The notation $grad f$ is also commonly used to represent the gradient. The gradient of $f$ is defined as the unique vector field whose dot product with any vector $v$ at each point $x$ is the directional derivative of $f$ along $v$. That is, where the right-hand side is the directional derivative and there are many ways to represent it. Formally, the derivative is dual to the gradient; see relationship with derivative. When a function also depends on a parameter such as time, the gradient often refers simply to the vector of its spatial derivatives only (see Spatial gradient). The magnitude and direction of the gradient vector are independent of the particular coordinate representation. Cartesian coordinates In the three-dimensional Cartesian coordinate system with a Euclidean metric, the gradient, if it exists, is given by where $i$, $j$, $k$ are the standard unit vectors in the directions of the $x$, $y$ and $z$ coordinates, respectively. For example, the gradient of the function is or In some applications it is customary to represent the gradient as a row vector or column vector of its components in a rectangular coordinate system; this article follows the convention of the gradient being a column vector, while the derivative is a row vector."
            },
            {
                "text": "Cylindrical and spherical coordinates In cylindrical coordinates, the gradient is given by: where $ρ$ is the axial distance, $φ$ is the azimuthal or azimuth angle, $z$ is the axial coordinate, and $eρ$, $eφ$ and $ez$ are unit vectors pointing along the coordinate directions. In spherical coordinates with a Euclidean metric, the gradient is given by:. where $r$ is the radial distance, $φ$ is the azimuthal angle and $θ$ is the polar angle, and $er$, $eθ$ and $eφ$ are again local unit vectors pointing in the coordinate directions (that is, the normalized covariant basis). For the gradient in other orthogonal coordinate systems, see Orthogonal coordinates (Differential operators in three dimensions). General coordinates We consider general coordinates, which we write as $x1, …, xi, …, xn$, where is the number of dimensions of the domain. Here, the upper index refers to the position in the list of the coordinate or component, so $x2$ refers to the second component—not the quantity $x$ squared."
            },
            {
                "text": "The index variable $i$ refers to an arbitrary element $xi$. Using Einstein notation, the gradient can then be written as: (Note that its dual is ), where $\\mathbf{e}^i = \\mathrm{d}x^i$ and $\\mathbf{e}_i = \\partial \\mathbf{x}/\\partial x^i$ refer to the unnormalized local covariant and contravariant bases respectively, $g^{ij}$ is the inverse metric tensor, and the Einstein summation convention implies summation over i and j. If the coordinates are orthogonal we can easily express the gradient (and the differential) in terms of the normalized bases, which we refer to as $\\hat{\\mathbf{e}}_i$ and $\\hat{\\mathbf{e}}^i$, using the scale factors (also known as Lamé coefficients) $h_i= \\lVert \\mathbf{e}_i \\rVert = \\sqrt{g_{i i}} = 1\\, / \\lVert \\mathbf{e}^i \\rVert$ : (and ), where we cannot use Einstein notation, since it is impossible to avoid the repetition of more than two indices."
            },
            {
                "text": "Despite the use of upper and lower indices, $\\mathbf{\\hat{e}}_i$, $\\mathbf{\\hat{e}}^i$, and $h_i$ are neither contravariant nor covariant. The latter expression evaluates to the expressions given above for cylindrical and spherical coordinates. Relationship with derivative Relationship with total derivative The gradient is closely related to the total derivative (total differential) $df$: they are transpose (dual) to each other. Using the convention that vectors in $\\R^n$ are represented by column vectors, and that covectors (linear maps $\\R^n \\to \\R$) are represented by row vectors, the gradient $\\nabla f$ and the derivative $df$ are expressed as a column and row vector, respectively, with the same components, but transpose of each other: While these both have the same components, they differ in what kind of mathematical object they represent: at each point, the derivative is a cotangent vector, a linear form (or covector) which expresses how much the (scalar) output changes for a given infinitesimal change in (vector) input, while at each point, the gradient is a tangent vector, which represents an infinitesimal change in (vector) input."
            },
            {
                "text": "In symbols, the gradient is an element of the tangent space at a point, $\\nabla f(p) \\in T_p \\R^n$, while the derivative is a map from the tangent space to the real numbers, $df_p \\colon T_p \\R^n \\to \\R$. The tangent spaces at each point of $\\R^n$ can be \"naturally\" identified with the vector space $\\R^n$ itself, and similarly the cotangent space at each point can be naturally identified with the dual vector space $(\\R^n)^*$ of covectors; thus the value of the gradient at a point can be thought of a vector in the original $\\R^n$, not just as a tangent vector. Computationally, given a tangent vector, the vector can be multiplied by the derivative (as matrices), which is equal to taking the dot product with the gradient: Differential or (exterior) derivative The best linear approximation to a differentiable function at a point $x$ in $\\R^n$ is a linear map from $\\R^n$ to $\\R$ which is often denoted by $df_x$ or $Df(x)$ and called the differential or total derivative of $f$ at $x$."
            },
            {
                "text": "The function $df$, which maps $x$ to $df_x$, is called the total differential or exterior derivative of $f$ and is an example of a differential 1-form. Much as the derivative of a function of a single variable represents the slope of the tangent to the graph of the function, the directional derivative of a function in several variables represents the slope of the tangent hyperplane in the direction of the vector. The gradient is related to the differential by the formula for any $v\\in\\R^n$, where $\\cdot$ is the dot product: taking the dot product of a vector with the gradient is the same as taking the directional derivative along the vector. If $\\R^n$ is viewed as the space of (dimension $n$) column vectors (of real numbers), then one can regard $df$ as the row vector with components so that $df_x(v)$ is given by matrix multiplication. Assuming the standard Euclidean metric on $\\R^n$, the gradient is then the corresponding column vector, that is, Linear approximation to a function The best linear approximation to a function can be expressed in terms of the gradient, rather than the derivative."
            },
            {
                "text": "The gradient of a function $f$ from the Euclidean space $\\R^n$ to $\\R$ at any particular point $x_0$ in $\\R^n$ characterizes the best linear approximation to $f$ at $x_0$. The approximation is as follows: for $x$ close to $x_0$, where $(\\nabla f)_{x_0}$ is the gradient of $f$ computed at $x_0$, and the dot denotes the dot product on $\\R^n$. This equation is equivalent to the first two terms in the multivariable Taylor series expansion of $f$ at $x_0$. Relationship with Let $U$ be an open set in $Rn$. If the function $f : U → R$ is differentiable, then the differential of $f$ is the Fréchet derivative of $f$. Thus $∇f$ is a function from $U$ to the space $Rn$ such that where · is the dot product."
            },
            {
                "text": "As a consequence, the usual properties of the derivative hold for the gradient, though the gradient is not a derivative itself, but rather dual to the derivative: Linearity The gradient is linear in the sense that if $f$ and $g$ are two real-valued functions differentiable at the point $a ∈ Rn$, and and are two constants, then $αf + βg$ is differentiable at $a$, and moreover Product rule If $f$ and $g$ are real-valued functions differentiable at a point $a ∈ Rn$, then the product rule asserts that the product $fg$ is differentiable at $a$, and Chain rule Suppose that $f : A → R$ is a real-valued function defined on a subset $A$ of $Rn$, and that $f$ is differentiable at a point $a$."
            },
            {
                "text": "There are two forms of the chain rule applying to the gradient. First, suppose that the function $g$ is a parametric curve; that is, a function $g : I → Rn$ maps a subset $I ⊂ R$ into $Rn$. If $g$ is differentiable at a point $c ∈ I$ such that $g(c) , then where ∘ is the composition operator: $1=(f ∘ g)(x) = f(g(x))$. More generally, if instead $I ⊂ Rk$, then the following holds: where $(Dg)$T denotes the transpose Jacobian matrix. For the second form of the chain rule, suppose that $h : I → R$ is a real valued function on a subset $I$ of $R$, and that $h$ is differentiable at the point $f(a) ∈ I$. Then Further properties and applications Level sets A level surface, or isosurface, is the set of all points where some function has a given value."
            },
            {
                "text": "If $f$ is differentiable, then the dot product $(∇f )x ⋅ v$ of the gradient at a point $x$ with a vector $v$ gives the directional derivative of $f$ at $x$ in the direction $v$. It follows that in this case the gradient of $f$ is orthogonal to the level sets of $f$. For example, a level surface in three-dimensional space is defined by an equation of the form $1=F(x, y, z) = c$. The gradient of $F$ is then normal to the surface. More generally, any embedded hypersurface in a Riemannian manifold can be cut out by an equation of the form $1=F(P) = 0$ such that $dF$ is nowhere zero. The gradient of $F$ is then normal to the hypersurface. Similarly, an affine algebraic hypersurface may be defined by an equation $1=F(x1, ..., xn) = 0$, where $F$ is a polynomial."
            },
            {
                "text": "The gradient of $F$ is zero at a singular point of the hypersurface (this is the definition of a singular point). At a non-singular point, it is a nonzero normal vector. Conservative vector fields and the gradient theorem The gradient of a function is called a gradient field. A (continuous) gradient field is always a conservative vector field: its line integral along any path depends only on the endpoints of the path, and can be evaluated by the gradient theorem (the fundamental theorem of calculus for line integrals). Conversely, a (continuous) conservative vector field is always the gradient of a function. Gradient is direction of steepest ascent The gradient of a function $f \\colon \\R^n \\to \\R$ at point $x$ is also the direction of its steepest ascent, i.e. it maximizes its directional derivative: Let $ v \\in \\R^n$ be an arbitrary unit vector. With the directional derivative defined as we get, by substituting the function $f(x + vh)$ with its Taylor series, where $R$ denotes higher order terms in $vh$."
            },
            {
                "text": "Dividing by $h$, and taking the limit yields a term which is bounded from above by the Cauchy-Schwarz inequality Choosing $v^* = \\nabla f/|\\nabla f|$ maximizes the directional derivative, and equals the upper bound Generalizations Jacobian The Jacobian matrix is the generalization of the gradient for vector-valued functions of several variables and differentiable maps between Euclidean spaces or, more generally, manifolds. A further generalization for a function between Banach spaces is the Fréchet derivative. Suppose $f : Rn → Rm$ is a function such that each of its first-order partial derivatives exist on $ℝn$. Then the Jacobian matrix of $f$ is defined to be an $m×n$ matrix, denoted by $\\mathbf{J}_\\mathbb{f}(\\mathbb{x})$ or simply $\\mathbf{J}$. The $(i,j)$th entry is . Explicitly Gradient of a vector field Since the total derivative of a vector field is a linear mapping from vectors to vectors, it is a tensor quantity. In rectangular coordinates, the gradient of a vector field $1=f = ( f is defined by: (where the Einstein summation notation is used and the tensor product of the vectors $ei$ and $ek$ is a dyadic tensor of type (2,0))."
            },
            {
                "text": "Overall, this expression equals the transpose of the Jacobian matrix: In curvilinear coordinates, or more generally on a curved manifold, the gradient involves Christoffel symbols: where $g are the components of the inverse metric tensor and the $ei$ are the coordinate basis vectors. Expressed more invariantly, the gradient of a vector field $f$ can be defined by the Levi-Civita connection and metric tensor:. where $∇c$ is the connection. Riemannian manifolds For any smooth function on a Riemannian manifold $(M, g)$, the gradient of $f$ is the vector field $∇f$ such that for any vector field $X$, that is, where $gx( , )$ denotes the inner product of tangent vectors at $x$ defined by the metric $g$ and $∂X f$ is the function that takes any point $x ∈ M$ to the directional derivative of $f$ in the direction $X$, evaluated at $x$. In other words, in a coordinate chart $φ$ from an open subset of $M$ to an open subset of $Rn$, $(∂X f )(x)$ is given by: where $X denotes the $j$th component of $X$ in this coordinate chart."
            },
            {
                "text": "So, the local form of the gradient takes the form: Generalizing the case $1=M = Rn$, the gradient of a function is related to its exterior derivative, since More precisely, the gradient $∇f$ is the vector field associated to the differential 1-form $df$ using the musical isomorphism (called \"sharp\") defined by the metric $g$. The relation between the exterior derivative and the gradient of a function on $Rn$ is a special case of this in which the metric is the flat metric given by the dot product. See also Notes References Further reading External links . Category:Differential operators Category:Differential calculus Category:Generalizations of the derivative Category:Linear operators in calculus Category:Vector calculus Category:Rates"
            }
        ],
        "latex_formulas": [
            "''f''(''x'', ''y'') = ''xe''<sup>−(''x''<sup>2</sup> + ''y''<sup>2</sup>)</sup>",
            "''T''",
            "(''x'', ''y'', ''z'')",
            "''T''(''x'', ''y'', ''z'')",
            "''T''",
            "(''x'', ''y'', ''z'')",
            "(''x'', ''y'')",
            "''H''(''x'', ''y'')",
            "''H''",
            "''H''",
            "''H''",
            "''H''",
            "i",
            "''f''(''x'',''y'') {{=}} −(cos<sup>2</sup>''x'' + cos<sup>2</sup>''y'')<sup>2</sup>",
            "''f''(''x''<sub>1</sub>, ''x''<sub>2</sub>, ''x''<sub>3</sub>, …, ''x<sub>n</sub>'')",
            "∇''f''",
            "{{vec|∇}}''f''",
            "∇",
            "grad ''f''",
            "''f''",
            "'''v'''",
            "''x''",
            "''f''",
            "'''v'''",
            "'''i'''",
            "'''j'''",
            "'''k'''",
            "''x''",
            "''y''",
            "''z''",
            "''ρ''",
            "''φ''",
            "''z''",
            "'''e'''<sub>''ρ''</sub>",
            "'''e'''<sub>''φ''</sub>",
            "'''e'''<sub>''z''</sub>",
            "''r''",
            "''φ''",
            "''θ''",
            "'''e'''<sub>''r''</sub>",
            "'''e'''<sub>''θ''</sub>",
            "'''e'''<sub>''φ''</sub>",
            "''x''<sup>1</sup>, …, ''x''<sup>''i''</sup>, …, ''x''<sup>''n''</sup>",
            "''x''<sup>2</sup>",
            "''x''",
            "''i''",
            "''x''<sup>''i''</sup>",
            "''U''",
            "'''R'''<sup>''n''</sup>",
            "''f'' : ''U'' → '''R'''",
            "''f''",
            "''f''",
            "∇''f''",
            "''U''",
            "'''R'''<sup>''n''</sup>",
            "''f''",
            "''g''",
            "''a'' ∈ '''R'''<sup>''n''</sup>",
            "''αf'' + ''βg''",
            "''a''",
            "''f''",
            "''g''",
            "''a'' ∈ '''R'''<sup>''n''</sup>",
            "''fg''",
            "''a''",
            "''f'' : ''A'' → '''R'''",
            "''A''",
            "'''R'''<sup>''n''</sup>",
            "''f''",
            "''a''",
            "''g''",
            "''g'' : ''I'' → '''R'''<sup>''n''</sup>",
            "''I'' ⊂ '''R'''",
            "'''R'''<sup>''n''</sup>",
            "''g''",
            "''c'' ∈ ''I''",
            "''g''(''c'') {{=}} ''a''",
            "(''f'' ∘ ''g'')(''x'') = ''f''(''g''(''x''))",
            "''I'' ⊂ '''R'''<sup>''k''</sup>",
            "(''Dg'')",
            "''h'' : ''I'' → '''R'''",
            "''I''",
            "'''R'''",
            "''h''",
            "''f''(''a'') ∈ ''I''",
            "''f''",
            "(∇''f''&thinsp;)<sub>''x''</sub> ⋅ ''v''",
            "''x''",
            "''v''",
            "''f''",
            "''x''",
            "''v''",
            "''f''",
            "''f''",
            "''F''(''x'', ''y'', ''z'') = ''c''",
            "''F''",
            "''F''(''P'') = 0",
            "''dF''",
            "''F''",
            "''F''(''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>) = 0",
            "''F''",
            "''F''",
            "''x''",
            "'''f''' : '''R'''<sup>''n''</sup> → '''R'''<sup>''m''</sup>",
            "ℝ<sup>''n''</sup>",
            "'''f'''",
            "''m''×''n''",
            "(''i'',''j'')",
            "'''f''' = (&thinsp;''f''{{i sup|1}}, ''f''{{i sup|2}}, ''f''{{i sup|3}})",
            "'''e'''<sub>''i''</sub>",
            "'''e'''<sub>''k''</sub>",
            "''g''{{i sup|''jk''}}",
            "'''e'''<sub>''i''</sub>",
            "'''f'''",
            "∇<sub>''c''</sub>",
            "(''M'', ''g'')",
            "''f''",
            "∇''f''",
            "''X''",
            "''g''<sub>''x''</sub>( , )",
            "''x''",
            "''g''",
            "∂<sub>''X''</sub>&thinsp;''f''",
            "''x'' ∈ ''M''",
            "''f''",
            "''X''",
            "''x''",
            "''φ''",
            "''M''",
            "'''R'''<sup>''n''</sup>",
            "(∂<sub>''X''</sub>&thinsp;''f''&thinsp;)(''x'')",
            "''X''{{isup|''j''}}",
            "''j''",
            "''X''",
            "''M'' = '''R'''<sup>''n''</sup>",
            "∇''f''",
            "''df''",
            "''g''",
            "'''R'''<sup>''n''</sup>",
            "f",
            "\\nabla f",
            "p",
            "f",
            "p",
            "p",
            "f(\\mathbf{r})",
            "df",
            "f",
            "d\\mathbf{r}",
            "d\\mathbf{r}",
            "\\nabla f",
            "\\nabla",
            "f",
            "p",
            "f \\colon \\R^n \\to \\R",
            "\\nabla f \\colon \\R^n \\to \\R^n",
            "p = (x_1,\\ldots,x_n)",
            "f \\colon \\R^n \\to T\\R^n",
            "T_p \\R^n",
            "\\R^n",
            "\\R^n",
            "f",
            "f",
            "p",
            "f(x,y)=\\frac {x^2 y}{x^2+y^2}",
            "f(0,0)=0",
            "df",
            "\\R^n",
            "\\R^n \\to \\R",
            "f",
            "p",
            "\\mathbf{v}",
            "f",
            "p",
            "\\mathbf{v}",
            "f",
            "a",
            "\\nabla f (a)",
            "\\vec{\\nabla} f (a)",
            "\\operatorname{grad} f",
            "\\partial_i f",
            "f_{i}",
            "\\mathbf{e}^i = \\mathrm{d}x^i",
            "\\mathbf{e}_i = \\partial \\mathbf{x}/\\partial x^i",
            "g^{ij}",
            "\\hat{\\mathbf{e}}_i",
            "\\hat{\\mathbf{e}}^i",
            "h_i= \\lVert \\mathbf{e}_i \\rVert = \\sqrt{g_{i i}} = 1\\, / \\lVert \\mathbf{e}^i \\rVert",
            "\\mathbf{\\hat{e}}_i",
            "\\mathbf{\\hat{e}}^i",
            "h_i",
            "df",
            "\\R^n",
            "\\R^n \\to \\R",
            "\\nabla f",
            "df",
            "\\nabla f(p) \\in T_p \\R^n",
            "df_p \\colon T_p \\R^n \\to \\R",
            "\\R^n",
            "\\R^n",
            "(\\R^n)^*",
            "\\R^n",
            "x",
            "\\R^n",
            "\\R^n",
            "\\R",
            "df_x",
            "Df(x)",
            "f",
            "x",
            "df",
            "x",
            "df_x",
            "f",
            "v\\in\\R^n",
            "\\cdot",
            "\\R^n",
            "n",
            "df",
            "df_x(v)",
            "\\R^n",
            "f",
            "\\R^n",
            "\\R",
            "x_0",
            "\\R^n",
            "f",
            "x_0",
            "x",
            "x_0",
            "(\\nabla f)_{x_0}",
            "f",
            "x_0",
            "\\R^n",
            "f",
            "x_0",
            "f \\colon \\R^n \\to \\R",
            "v \\in \\R^n",
            "f(x + vh)",
            "R",
            "vh",
            "h",
            "v^* = \\nabla f/|\\nabla f|",
            "\\mathbf{J}_\\mathbb{f}(\\mathbb{x})",
            "\\mathbf{J}"
        ]
    },
    "Penalty_method": {
        "title": "Penalty_method",
        "chunks": [
            {
                "text": "In mathematical optimization, penalty methods are a certain class of algorithms for solving constrained optimization problems. A penalty method replaces a constrained optimization problem by a series of unconstrained problems whose solutions ideally converge to the solution of the original constrained problem. The unconstrained problems are formed by adding a term, called a penalty function, to the objective function that consists of a penalty parameter multiplied by a measure of violation of the constraints. The measure of violation is nonzero when the constraints are violated and is zero in the region where constraints are not violated. Description Let us say we are solving the following constrained problem: $ \\min_x f(\\mathbf x) $ subject to $ c_i(\\mathbf x) \\le 0 ~\\forall i \\in I. $ This problem can be solved as a series of unconstrained minimization problems $ \\min f_p (\\mathbf x) := f (\\mathbf x) + p ~ \\sum_{i\\in I} ~ g(c_i(\\mathbf x)) $ where $ g(c_i(\\mathbf x))=\\max(0,c_i(\\mathbf x ))^2."
            },
            {
                "text": "$ In the above equations, $ g(c_i(\\mathbf x))$ is the exterior penalty function while $p$ is the penalty coefficient. When the penalty coefficient $p$ is 0, fp = f, meaning that we do not take the constraints into account. In each iteration of the method, we increase the penalty coefficient $p$ (e.g. by a factor of 10), solve the unconstrained problem and use the solution as the initial guess for the next iteration. Solutions of the successive unconstrained problems will asymptotically converge to the solution of the original constrained problem. Common penalty functions in constrained optimization are the quadratic penalty function and the deadzone-linear penalty function. Convergence We first consider the set of global optimizers of the original problem, X*.Assume that the objective f has bounded level sets, and that the original problem is feasible. Then: For every penalty coefficient p, the set of global optimizers of the penalized problem, Xp*, is non-empty. For every ε>0, there exists a penalty coefficient p such that the set Xp* is contained in an ε-neighborhood of the set X*."
            },
            {
                "text": "This theorem is helpful mostly when fp is convex, since in this case, we can find the global optimizers of fp. A second theorem considers local optimizers. Let x* be a non-degenerate local optimizer of the original problem (\"nondegenerate\" means that the gradients of the active constraints are linearly independent and the second-order sufficient optimality condition is satisfied). Then, there exists a neighborhood V* of x*, and some p0>0, such that for all p>p0, the penalized objective fp has exactly one critical point in V* (denoted by x*(p)), and x*(p) approaches x* as p→∞. Also, the objective value f(x*(p)) is weakly-increasing with p. Practical applications Image compression optimization algorithms can make use of penalty functions for selecting how best to compress zones of colour to single representative values. The penalty method is often used in computational mechanics, especially in the Finite element method, to enforce conditions such as e.g. contact. The advantage of the penalty method is that, once we have a penalized objective with no constraints, we can use any unconstrained optimization method to solve it."
            },
            {
                "text": "The disadvantage is that, as the penalty coefficient p grows, the unconstrained problem becomes ill-conditioned - the coefficients are very large, and this may cause numeric errors and slow convergence of the unconstrained minimization. See also Barrier methods constitute an alternative class of algorithms for constrained optimization. These methods also add a penalty-like term to the objective function, but in this case the iterates are forced to remain interior to the feasible domain and the barrier is in place to bias the iterates to remain away from the boundary of the feasible region. They are practically more efficient than penalty methods. Augmented Lagrangian methods are alternative penalty methods, which allow to get high-accuracy solutions without pushing the penalty coefficient to infinity. This makes the unconstrained penalized problems easier to solve. Other nonlinear programming algorithms: Sequential quadratic programming Successive linear programming Sequential linear-quadratic programming Interior point method References Smith, Alice E.; Coit David W. Penalty functions Handbook of Evolutionary Computation, Section C 5.2. Oxford University Press and Institute of Physics Publishing, 1996. Coello, A.C.: Theoretical and Numerical Constraint-Handling Techniques Used with Evolutionary Algorithms: A Survey of the State of the Art. Comput. Methods Appl. Mech. Engrg. 191(11-12), 1245-1287 Courant, R. Variational methods for the solution of problems of equilibrium and vibrations. Bull. Amer. Math. Soc., 49, 1–23, 1943. Wotao, Y. Optimization Algorithms for constrained optimization. Department of Mathematics, UCLA, 2015. Category:Optimization algorithms and methods"
            }
        ],
        "latex_formulas": [
            "\\min_x f(\\mathbf x)",
            "c_i(\\mathbf x) \\le 0 ~\\forall  i \\in I.",
            "\\min f_p (\\mathbf x) := f (\\mathbf x) + p ~ \\sum_{i\\in I} ~ g(c_i(\\mathbf x))",
            "g(c_i(\\mathbf x))=\\max(0,c_i(\\mathbf x ))^2.",
            "g(c_i(\\mathbf x))",
            "p",
            "p",
            "p"
        ]
    },
    "Bias-variance_tradeoff": {
        "title": "Bias-variance_tradeoff",
        "chunks": [
            {
                "text": "REDIRECT Bias–variance tradeoff"
            }
        ],
        "latex_formulas": []
    },
    "Feature_scaling": {
        "title": "Feature_scaling",
        "chunks": [
            {
                "text": "Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. Motivation Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance. Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it. It's also important to apply feature scaling if regularization is used as part of the loss function (so that coefficients are penalized appropriately). Empirically, feature scaling can improve the convergence speed of stochastic gradient descent. In support vector machines, it can reduce the time to find support vectors."
            },
            {
                "text": "Feature scaling is also often used in applications involving distances and similarities between data points, such as clustering and similarity search. As an example, the K-means clustering algorithm is sensitive to feature scales. Methods Rescaling (min-max normalization) Also known as min-max scaling or min-max normalization, rescaling is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data. The general formula for a min-max of [0, 1] is given as: $x' = \\frac{x - \\text{min}(x)}{\\text{max}(x)-\\text{min}(x)}$ where $x$ is an original value, $x'$ is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by 40 (the difference between the maximum and minimum weights)."
            },
            {
                "text": "To rescale a range between an arbitrary set of values [a, b], the formula becomes: $x' = a + \\frac{(x - \\text{min}(x))(b-a)}{\\text{max}(x)-\\text{min}(x)}$ where $a,b$ are the min-max values. Mean normalization $x' = \\frac{x - \\bar{x}}{\\text{max}(x)-\\text{min}(x)}$ where $x$ is an original value, $x'$ is the normalized value, $\\bar{x}=\\text{average}(x)$ is the mean of that feature vector. There is another form of the means normalization which divides by the standard deviation which is also called standardization. Standardization (Z-score Normalization) The effect of z-score normalization on k-means clustering. 4 gaussian clusters of points are generated, then squashed along the y-axis, and a $k = 4$ clustering was computed."
            },
            {
                "text": "Without normalization, the clusters were arranged along the x-axis, since it is the axis with most of variation. After normalization, the clusters are recovered as expected. In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple dimensions. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines, logistic regression, and artificial neural networks). The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation. $x' = \\frac{x - \\bar{x}}{\\sigma}$ Where $x$ is the original feature vector, $\\bar{x}=\\text{average}(x)$ is the mean of that feature vector, and $\\sigma$ is its standard deviation. Robust Scaling Robust scaling, also known as standardization using median and interquartile range (IQR), is designed to be robust to outliers. It scales features using the median and IQR as reference points instead of the mean and standard deviation:where $Q_1(x), Q_2(x), Q_3(x)$ are the three quartiles (25th, 50th, 75th percentile) of the feature."
            },
            {
                "text": "Unit vector normalization Unit vector normalization regards each individual data point as a vector, and divide each by its vector norm, to obtain $ x' = x/\\|x\\| $. Any vector norm can be used, but the most common ones are the L1 norm and the L2 norm. For example, if $ x = (v_1, v_2, v_3) $, then its Lp-normalized version is: See also Normalization (machine learning) Normalization (statistics) Standard score fMLLR, Feature space Maximum Likelihood Linear Regression References Further reading External links Lecture by Andrew Ng on feature scaling Category:Machine learning Category:Statistical data transformation"
            }
        ],
        "latex_formulas": [
            "x' = \\frac{x - \\text{min}(x)}{\\text{max}(x)-\\text{min}(x)}",
            "x",
            "x'",
            "x' = a + \\frac{(x - \\text{min}(x))(b-a)}{\\text{max}(x)-\\text{min}(x)}",
            "a,b",
            "x' = \\frac{x - \\bar{x}}{\\text{max}(x)-\\text{min}(x)}",
            "x",
            "x'",
            "\\bar{x}=\\text{average}(x)",
            "k = 4",
            "x' = \\frac{x - \\bar{x}}{\\sigma}",
            "x",
            "\\bar{x}=\\text{average}(x)",
            "\\sigma",
            "Q_1(x), Q_2(x), Q_3(x)",
            "x' = x/\\|x\\|",
            "x = (v_1, v_2, v_3)"
        ]
    },
    "Decision_rule": {
        "title": "Decision_rule",
        "chunks": [
            {
                "text": "In decision theory, a decision rule is a function which maps an observation to an appropriate action. Decision rules play an important role in the theory of statistics and economics, and are closely related to the concept of a strategy in game theory. In order to evaluate the usefulness of a decision rule, it is necessary to have a loss function detailing the outcome of each action under different states. Formal definition Given an observable random variable X over the probability space $ \\scriptstyle (\\mathcal{X},\\Sigma, P_\\theta)$, determined by a parameter θ ∈ Θ, and a set A of possible actions, a (deterministic) decision rule is a function δ : $\\scriptstyle\\mathcal{X}$→ A. Examples of decision rules An estimator is a decision rule used for estimating a parameter. In this case the set of actions is the parameter space, and a loss function details the cost of the discrepancy between the true value of the parameter and the estimated value. For example, in a linear model with a single scalar parameter $\\theta$, the domain of $\\theta$ may extend over $\\mathcal{R}$ (all real numbers). An associated decision rule for estimating $\\theta$ from some observed data might be, \"choose the value of the $\\theta$, say $\\hat{\\theta}$, that minimizes the sum of squared error between some observed responses, and responses predicted from the corresponding covariates given that you chose $\\hat{\\theta}$.\""
            },
            {
                "text": "Thus, the cost function is the sum of squared error, and one would aim to minimize this cost. Once the cost function is defined, $\\hat{\\theta}$ could be chosen, for instance, using some optimization algorithm. Out of sample prediction in regression and classification models. See also Admissible decision rule Bayes estimator Classification rule Scoring rule Category:Decision theory"
            }
        ],
        "latex_formulas": [
            "\\scriptstyle (\\mathcal{X},\\Sigma, P_\\theta)",
            "\\scriptstyle\\mathcal{X}",
            "\\theta",
            "\\theta",
            "\\mathcal{R}",
            "\\theta",
            "\\theta",
            "\\hat{\\theta}",
            "\\hat{\\theta}",
            "\\hat{\\theta}"
        ]
    },
    "Coefficient": {
        "title": "Coefficient",
        "chunks": [
            {
                "text": "In mathematics, a coefficient is a multiplicative factor involved in some term of a polynomial, a series, or any other type of expression. It may be a number without units, in which case it is known as a numerical factor. It may also be a constant with units of measurement, in which it is known as a constant multiplier. In general, coefficients may be any expression (including variables such as , and ). When the combination of variables and constants is not necessarily involved in a product, it may be called a parameter. For example, the polynomial $2x^2-x+3$ has coefficients 2, −1, and 3, and the powers of the variable $x$ in the polynomial $ax^2+bx+c$ have coefficient parameters $a$, $b$, and $c$. A , also known as constant term or simply constant, is a quantity either implicitly attached to the zeroth power of a variable or not attached to other variables in an expression; for example, the constant coefficients of the expressions above are the number 3 and the parameter c, involved in 3cx0."
            },
            {
                "text": "The coefficient attached to the highest degree of the variable in a polynomial of one variable is referred to as the leading coefficient; for example, in the example expressions above, the leading coefficients are 2 and a, respectively. In the context of differential equations, these equations can often be written in terms of polynomials in one or more unknown functions and their derivatives. In such cases, the coefficients of the differential equation are the coefficients of this polynomial, and these may be non-constant functions. A coefficient is a constant coefficient when it is a constant function. For avoiding confusion, in this context a coefficient that is not attached to unknown functions or their derivatives is generally called a constant term rather than a constant coefficient. In particular, in a linear differential equation with constant coefficient, the constant coefficient term is generally not assumed to be a constant function. Terminology and definition In mathematics, a coefficient is a multiplicative factor in some term of a polynomial, a series, or any expression. For example, in the polynomial with variables $x$ and $y$, the first two terms have the coefficients 7 and −3."
            },
            {
                "text": "The third term 1.5 is the constant coefficient. In the final term, the coefficient is 1 and is not explicitly written. In many scenarios, coefficients are numbers (as is the case for each term of the previous example), although they could be parameters of the problem—or any expression in these parameters. In such a case, one must clearly distinguish between symbols representing variables and symbols representing parameters. Following René Descartes, the variables are often denoted by , , ..., and the parameters by , , , ..., but this is not always the case. For example, if is considered a parameter in the above expression, then the coefficient of would be $−3y$, and the constant coefficient (with respect to ) would be $1.5 + y$. When one writes it is generally assumed that is the only variable, and that , and are parameters; thus the constant coefficient is in this case. Any polynomial in a single variable can be written as for some nonnegative integer $k$, where $a_k, \\dotsc, a_1, a_0$ are the coefficients."
            },
            {
                "text": "This includes the possibility that some terms have coefficient 0; for example, in $x^3 - 2x + 1$, the coefficient of $x^2$ is 0, and the term $0x^2$ does not appear explicitly. For the largest $i$ such that $a_i \\ne 0$ (if any), $a_i$ is called the leading coefficient of the polynomial. For example, the leading coefficient of the polynomial is 4. This can be generalised to multivariate polynomials with respect to a monomial order, see . Linear algebra In linear algebra, a system of linear equations is frequently represented by its coefficient matrix. For example, the system of equations the associated coefficient matrix is $\\begin{pmatrix} 2 & 3 \\\\ 5 & -4 \\end{pmatrix}. $ Coefficient matrices are used in algorithms such as Gaussian elimination and Cramer's rule to find solutions to the system. The leading entry (sometimes leading coefficient) of a row in a matrix is the first nonzero entry in that row. So, for example, in the matrix the leading coefficient of the first row is 1; that of the second row is 2; that of the third row is 4, while the last row does not have a leading coefficient. Though coefficients are frequently viewed as constants in elementary algebra, they can also be viewed as variables as the context broadens."
            },
            {
                "text": "For example, the coordinates $(x_1, x_2, \\dotsc, x_n)$ of a vector $v$ in a vector space with basis $\\lbrace e_1, e_2, \\dotsc, e_n \\rbrace $ are the coefficients of the basis vectors in the expression See also Correlation coefficient Degree of a polynomial Monic polynomial Binomial coefficient References Further reading Sabah Al-hadad and C.H. Scott (1979) College Algebra with Applications, page 42, Winthrop Publishers, Cambridge Massachusetts . Gordon Fuller, Walter L Wilson, Henry C Miller, (1982) College Algebra, 5th edition, page 24, Brooks/Cole Publishing, Monterey California . Category:Polynomials Category:Mathematical terminology Category:Algebra Category:Numbers Category:Variables (mathematics)"
            }
        ],
        "latex_formulas": [
            "−3''y''",
            "1.5 + ''y''",
            "2x^2-x+3",
            "x",
            "ax^2+bx+c",
            "a",
            "b",
            "c",
            "x",
            "y",
            "k",
            "a_k, \\dotsc, a_1, a_0",
            "x^3 - 2x + 1",
            "x^2",
            "0x^2",
            "i",
            "a_i \\ne 0",
            "a_i",
            "\\begin{pmatrix}\n2 & 3 \\\\\n5 & -4\n\\end{pmatrix}.",
            "(x_1, x_2, \\dotsc, x_n)",
            "v",
            "\\lbrace e_1, e_2, \\dotsc, e_n \\rbrace"
        ]
    },
    "Statistical_assumption": {
        "title": "Statistical_assumption",
        "chunks": [
            {
                "text": "Statistics, like all mathematical disciplines, does not infer valid conclusions from nothing. Inferring interesting conclusions about real statistical populations almost always requires some background assumptions. Those assumptions must be made carefully, because incorrect assumptions can generate wildly inaccurate conclusions. Here are some examples of statistical assumptions: Independence of observations from each other (this assumption is an especially common errorKruskall, 1988). Independence of observational error from potential confounding effects. Exact or approximate normality of observations (or errors). Linearity of graded responses to quantitative stimuli, e.g., in linear regression. Classes of assumptions There are two approaches to statistical inference: model-based inference and design-based inference.Koch G. G., Gillings D. B. (2006), \"Inference, design-based vs. model-based\", Encyclopedia of Statistical Sciences (editor—Kotz S.), Wiley-Interscience.Cox, 2006, ch.9de Gruijter et al., 2006, §2.2 Both approaches rely on some statistical model to represent the data-generating process. In the model-based approach, the model is taken to be initially unknown, and one of the goals is to select an appropriate model for inference."
            },
            {
                "text": "In the design-based approach, the model is taken to be known, and one of the goals is to ensure that the sample data are selected randomly enough for inference. Statistical assumptions can be put into two classes, depending upon which approach to inference is used. Model-based assumptions. These include the following three types: Distributional assumptions. Where a statistical model involves terms relating to random errors, assumptions may be made about the probability distribution of these errors.McPherson, 1990, §3.4.1 In some cases, the distributional assumption relates to the observations themselves. Structural assumptions. Statistical relationships between variables are often modelled by equating one variable to a function of another (or several others), plus a random error. Models often involve making a structural assumption about the form of the functional relationship, e.g. as in linear regression. This can be generalised to models involving relationships between underlying unobserved latent variables. Cross-variation assumptions. These assumptions involve the joint probability distributions of either the observations themselves or the random errors in a model. Simple models may include the assumption that observations or errors are statistically independent."
            },
            {
                "text": "Design-based assumptions. These relate to the way observations have been gathered, and often involve an assumption of randomization during sampling.McPherson, 1990, §3.3de Gruijter et al., 2006, §2.2.1 The model-based approach is the most commonly used in statistical inference; the design-based approach is used mainly with survey sampling. With the model-based approach, all the assumptions are effectively encoded in the model. Checking assumptions Given that the validity of any conclusion drawn from a statistical inference depends on the validity of the assumptions made, it is clearly important that those assumptions should be reviewed at some stage. Some instances—for example where data are lacking—may require that researchers judge whether an assumption is reasonable. Researchers can expand this somewhat to consider what effect a departure from the assumptions might produce. Where more extensive data are available, various types of procedures for statistical model validation are available—e.g. for regression model validation. Example: Independence of Observations Scenario: Imagine a study assessing the effectiveness of a new teaching method in multiple classrooms. If the classrooms are not treated as independent entities, but rather as a single unit, the assumption of independence is violated. Students within the same classroom may share common characteristics or experiences, leading to correlated observations. Consequence: Failure to account for this lack of independence may inflate the perceived impact of the teaching method, as the outcomes within a classroom may be more similar than assumed. This can result in an overestimation of the method's generalizability to diverse educational settings."
            },
            {
                "text": "See also Misuse of statistics Robust statistics Statistical hypothesis testing Statistical theory Notes References Cox D. R. (2006), Principles of Statistical Inference, Cambridge University Press. de Gruijter J., Brus D., Bierkens M., Knotters M. (2006), Sampling for Natural Resource Monitoring, Springer-Verlag. McPherson, G. (1990), Statistics in Scientific Investigation: Its Basis, Application and Interpretation, Springer-Verlag. Category:Statistical theory"
            }
        ],
        "latex_formulas": []
    },
    "Applications_of_machine_learning": {
        "title": "Applications_of_machine_learning",
        "chunks": [
            {
                "text": "REDIRECT Machine learning#Applications"
            }
        ],
        "latex_formulas": []
    },
    "Hyperplane": {
        "title": "Hyperplane",
        "chunks": [
            {
                "text": "thumb|Two intersecting planes: Two-dimensional planes are the hyperplanes in three-dimensional space. In geometry, a hyperplane is a generalization of a two-dimensional plane in three-dimensional space to mathematical spaces of arbitrary dimension. Like a plane in space, a hyperplane is a flat hypersurface, a subspace whose dimension is one less than that of the ambient space. Two lower-dimensional examples of hyperplanes are one-dimensional lines in a plane and zero-dimensional points on a line. Most commonly, the ambient space is -dimensional Euclidean space, in which case the hyperplanes are the $(n − 1)$-dimensional \"flats\", each of which separates the space into two half spaces. A reflection across a hyperplane is a kind of motion (geometric transformation preserving distance between points), and the group of all motions is generated by the reflections. A convex polytope is the intersection of half-spaces. In non-Euclidean geometry, the ambient space might be the -dimensional sphere or hyperbolic space, or more generally a pseudo-Riemannian space form, and the hyperplanes are the hypersurfaces consisting of all geodesics through a point which are perpendicular to a specific normal geodesic."
            },
            {
                "text": "In other kinds of ambient spaces, some properties from Euclidean space are no longer relevant. For example, in affine space, there is no concept of distance, so there are no reflections or motions. In a non-orientable space such as elliptic space or projective space, there is no concept of half-planes. In greatest generality, the notion of hyperplane is meaningful in any mathematical space in which the concept of the dimension of a subspace is defined. The difference in dimension between a subspace and its ambient space is known as its codimension. A hyperplane has codimension $1$. Technical description In geometry, a hyperplane of an n-dimensional space V is a subspace of dimension n − 1, or equivalently, of codimension 1 in V. The space V may be a Euclidean space or more generally an affine space, or a vector space or a projective space, and the notion of hyperplane varies correspondingly since the definition of subspace differs in these settings; in all cases however, any hyperplane can be given in coordinates as the solution of a single (due to the \"codimension 1\" constraint) algebraic equation of degree 1."
            },
            {
                "text": "If V is a vector space, one distinguishes \"vector hyperplanes\" (which are linear subspaces, and therefore must pass through the origin) and \"affine hyperplanes\" (which need not pass through the origin; they can be obtained by translation of a vector hyperplane). A hyperplane in a Euclidean space separates that space into two half spaces, and defines a reflection that fixes the hyperplane and interchanges those two half spaces. Special types of hyperplanes Several specific types of hyperplanes are defined with properties that are well suited for particular purposes. Some of these specializations are described here. Affine hyperplanes An affine hyperplane is an affine subspace of codimension 1 in an affine space. In Cartesian coordinates, such a hyperplane can be described with a single linear equation of the following form (where at least one of the $a_i$s is non-zero and $b$ is an arbitrary constant): $a_1x_1 + a_2x_2 + \\cdots + a_nx_n = b.\\ $ In the case of a real affine space, in other words when the coordinates are real numbers, this affine space separates the space into two half-spaces, which are the connected components of the complement of the hyperplane, and are given by the inequalities $a_1x_1 + a_2x_2 + \\cdots + a_nx_n < b\\ $ and $a_1x_1 + a_2x_2 + \\cdots + a_nx_n > b.\\ $ As an example, a point is a hyperplane in 1-dimensional space, a line is a hyperplane in 2-dimensional space, and a plane is a hyperplane in 3-dimensional space."
            },
            {
                "text": "A line in 3-dimensional space is not a hyperplane, and does not separate the space into two parts (the complement of such a line is connected). Any hyperplane of a Euclidean space has exactly two unit normal vectors: $\\pm\\hat{n}$. In particular, if we consider $\\mathbb{R}^{n+1}$ equipped with the conventional inner product (dot product), then one can define the affine subspace with normal vector $\\hat{n}$ and origin translation $\\tilde{b} \\in \\mathbb{R}^{n+1}$ as the set of all $x \\in \\mathbb{R}^{n+1}$ such that $\\hat{n} \\cdot (x-\\tilde{b})=0$. Affine hyperplanes are used to define decision boundaries in many machine learning algorithms such as linear-combination (oblique) decision trees, and perceptrons. Vector hyperplanes In a vector space, a vector hyperplane is a subspace of codimension 1, only possibly shifted from the origin by a vector, in which case it is referred to as a flat."
            },
            {
                "text": "Such a hyperplane is the solution of a single linear equation. Projective hyperplanes Projective hyperplanes, are used in projective geometry. A projective subspace is a set of points with the property that for any two points of the set, all the points on the line determined by the two points are contained in the set. Projective geometry can be viewed as affine geometry with vanishing points (points at infinity) added. An affine hyperplane together with the associated points at infinity forms a projective hyperplane. One special case of a projective hyperplane is the infinite or ideal hyperplane, which is defined with the set of all points at infinity. In projective space, a hyperplane does not divide the space into two parts; rather, it takes two hyperplanes to separate points and divide up the space. The reason for this is that the space essentially \"wraps around\" so that both sides of a lone hyperplane are connected to each other. Applications In convex geometry, two disjoint convex sets in n-dimensional Euclidean space are separated by a hyperplane, a result called the hyperplane separation theorem."
            },
            {
                "text": "In machine learning, hyperplanes are a key tool to create support vector machines for such tasks as computer vision and natural language processing. The datapoint and its predicted value via a linear model is a hyperplane. In astronomy, hyperplanes can be used to calculate shortest distance between star systems, galaxies and celestial bodies with regard of general relativity and curvature of space-time as optimized geodesic or paths influenced by gravitational fields. Dihedral angles The dihedral angle between two non-parallel hyperplanes of a Euclidean space is the angle between the corresponding normal vectors. The product of the transformations in the two hyperplanes is a rotation whose axis is the subspace of codimension 2 obtained by intersecting the hyperplanes, and whose angle is twice the angle between the hyperplanes. Support hyperplanes A hyperplane H is called a \"support\" hyperplane of the polyhedron P if P is contained in one of the two closed half-spaces bounded by H and $H\\cap P \\neq \\varnothing$.Polytopes, Rings and K-Theory by Bruns-Gubeladze The intersection of P and H is defined to be a \"face\" of the polyhedron. The theory of polyhedra and the dimension of the faces are analyzed by looking at these intersections involving hyperplanes. See also Hypersurface Decision boundary Ham sandwich theorem Arrangement of hyperplanes Supporting hyperplane theorem References Charles W. Curtis (1968) Linear Algebra, page 62, Allyn & Bacon, Boston. Heinrich Guggenheimer (1977) Applicable Geometry, page 7, Krieger, Huntington . Victor V. Prasolov & VM Tikhomirov (1997, 2001) Geometry, page 22, volume 200 in Translations of Mathematical Monographs, American Mathematical Society, Providence ."
            },
            {
                "text": "External links Category:Euclidean geometry Category:Affine geometry Category:Linear algebra Category:Projective geometry Category:Multi-dimensional geometry"
            }
        ],
        "latex_formulas": [
            "(''n''&thinsp;−&thinsp;1)",
            "1",
            "a_i",
            "b",
            "a_1x_1 + a_2x_2  + \\cdots + a_nx_n = b.\\",
            "a_1x_1 + a_2x_2  + \\cdots + a_nx_n < b\\",
            "a_1x_1 + a_2x_2  + \\cdots + a_nx_n > b.\\",
            "\\pm\\hat{n}",
            "\\mathbb{R}^{n+1}",
            "\\hat{n}",
            "\\tilde{b} \\in \\mathbb{R}^{n+1}",
            "x \\in \\mathbb{R}^{n+1}",
            "\\hat{n} \\cdot (x-\\tilde{b})=0",
            "H\\cap P \\neq \\varnothing"
        ]
    },
    "Linear_separability": {
        "title": "Linear_separability",
        "chunks": [
            {
                "text": "The existence of a line separating the two types of points means that the data is linearly separable In Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if the line is replaced by a hyperplane. The problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are, arises in several areas. In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept. Mathematical definition Let $X_{0}$ and $X_{1}$ be two sets of points in an n-dimensional Euclidean space."
            },
            {
                "text": "Then $X_{0}$ and $X_{1}$ are linearly separable if there exist n + 1 real numbers $w_{1}, w_{2},..,w_{n}, k$, such that every point $x \\in X_{0}$ satisfies $\\sum^{n}_{i=1} w_{i}x_{i} > k$ and every point $x \\in X_{1}$ satisfies $\\sum^{n}_{i=1} w_{i}x_{i} < k$, where $x_{i}$ is the $i$-th component of $x$. Equivalently, two sets are linearly separable precisely when their respective convex hulls are disjoint (colloquially, do not overlap). In simple 2D, it can also be imagined that the set of points under a linear transformation collapses into a line, on which there exists a value, k, greater than which one set of points will fall into, and lesser than which the other set of points fall."
            },
            {
                "text": "Examples Three non-collinear points in two classes ('+' and '-') are always linearly separable in two dimensions. This is illustrated by the three examples in the following figure (the all '+' case is not shown, but is similar to the all '-' case): File:VC1.svg File:VC2.svg File:VC3.svg However, not all sets of four points, no three collinear, are linearly separable in two dimensions. The following example would need two straight lines and thus is not linearly separable: File:VC4.svg Notice that three points which are collinear and of the form \"+ ⋅⋅⋅ — ⋅⋅⋅ +\" are also not linearly separable. Number of linear separations Let $T(N, K)$ be the number of ways to linearly separate N points (in general position) in K dimensions, thenWhen K is large, $T(N, K)/2^N$ is very close to one when $N \\leq 2K$, but very close to zero when $N> 2K$. In words, one perceptron unit can almost certainly memorize a random assignment of binary labels on N points when $N \\leq 2K$, but almost certainly not when $N> 2K$."
            },
            {
                "text": "Linear separability of Boolean functions in n variables A Boolean function in n variables can be thought of as an assignment of 0 or 1 to each vertex of a Boolean hypercube in n dimensions. This gives a natural division of the vertices into two sets. The Boolean function is said to be linearly separable provided these two sets of points are linearly separable. The number of distinct Boolean functions is $2^{2^{n}}$where n is the number of variables passed into the function. Such functions are also called linear threshold logic, or perceptrons. The classical theory is summarized in, as Knuth claims. The value is only known exactly up to $n=9$ case, but the order of magnitude is known quite exactly: it has upper bound $2^{n^2 - n \\log_2 n + O(n)}$ and lower bound $2^{n^2 - n \\log_2 n - O(n)}$. It is co-NP-complete to decide whether a Boolean function given in disjunctive or conjunctive normal form is linearly separable. +Number of linearly separable Boolean functions in each dimension Number of variablesBoolean functionsLinearly separable Boolean functions 2 16 14 3 256 104 4 65536 1882 5 4294967296 94572 6 18446744073709552000 15028134 7 3.402823669 ×10^38 8378070864 8 1.157920892 ×10^77 17561539552946 9 1.340780792 ×10^154 144130531453121108 Threshold logic A linear threshold logic gate is a Boolean function defined by $n$ weights $w_1, \\dots, w_n$ and a threshold $\\theta$."
            },
            {
                "text": "It takes $n$ binary inputs $x_1, \\dots, x_n$, and outputs 1 if $\\sum_i w_i x_i > \\theta$, and otherwise outputs 0. For any fixed $n$, because there are only finitely many Boolean functions that can be computed by a threshold logic unit, it is possible to set all $w_1, \\dots, w_n, \\theta$ to be integers. Let $W(n)$ be the smallest number $W$ such that every possible real threshold function of $n$ variables can be realized using integer weights of absolute value $\\leq W$. It is known thatSee for a literature review. Support vector machines H1 does not separate the sets. H2 does, but only with a small margin. H3 separates them with the maximum margin. Classifying data is a common task in machine learning. Suppose some data points, each belonging to one of two sets, are given and we wish to create a model that will decide which set a new data point will be in. In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of p numbers), and we want to know whether we can separate such points with a (p − 1)-dimensional hyperplane."
            },
            {
                "text": "This is called a linear classifier. There are many hyperplanes that might classify (separate) the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two sets. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier. More formally, given some training data $\\mathcal{D}$, a set of n points of the form $\\mathcal{D} = \\left\\{ (\\mathbf{x}_i, y_i)\\mid\\mathbf{x}_i \\in \\mathbb{R}^p,\\, y_i \\in \\{-1,1\\}\\right\\}_{i=1}^n$ where the yi is either 1 or −1, indicating the set to which the point $\\mathbf{x}_i $ belongs. Each $ \\mathbf{x}_i $ is a p-dimensional real vector. We want to find the maximum-margin hyperplane that divides the points having $y_i=1$ from those having $y_i=-1$. Any hyperplane can be written as the set of points $\\mathbf{x}$ satisfying $\\mathbf{w}\\cdot\\mathbf{x} - b=0,$ where $\\cdot$ denotes the dot product and ${\\mathbf{w}}$ the (not necessarily normalized) normal vector to the hyperplane."
            },
            {
                "text": "The parameter $\\tfrac{b}{\\|\\mathbf{w}\\|}$ determines the offset of the hyperplane from the origin along the normal vector ${\\mathbf{w}}$. If the training data are linearly separable, we can select two hyperplanes in such a way that they separate the data and there are no points between them, and then try to maximize their distance. See also Clustering (statistics) Hyperplane separation theorem Kirchberger's theorem Perceptron Vapnik–Chervonenkis dimension References Category:Geometry Category:Convex analysis Category:Machine learning"
            }
        ],
        "latex_formulas": [
            "X_{0}",
            "X_{1}",
            "X_{0}",
            "X_{1}",
            "w_{1}, w_{2},..,w_{n}, k",
            "x \\in X_{0}",
            "\\sum^{n}_{i=1} w_{i}x_{i} > k",
            "x \\in X_{1}",
            "\\sum^{n}_{i=1} w_{i}x_{i} < k",
            "x_{i}",
            "i",
            "x",
            "T(N, K)",
            "T(N, K)/2^N",
            "N \\leq 2K",
            "N> 2K",
            "N \\leq 2K",
            "N> 2K",
            "2^{2^{n}}",
            "n=9",
            "2^{n^2 - n \\log_2 n + O(n)}",
            "2^{n^2 - n \\log_2 n - O(n)}",
            "n",
            "w_1, \\dots, w_n",
            "\\theta",
            "n",
            "x_1, \\dots, x_n",
            "\\sum_i w_i x_i > \\theta",
            "n",
            "w_1, \\dots, w_n, \\theta",
            "W(n)",
            "W",
            "n",
            "\\leq W",
            "\\mathcal{D}",
            "\\mathcal{D} = \\left\\{ (\\mathbf{x}_i, y_i)\\mid\\mathbf{x}_i \\in \\mathbb{R}^p,\\, y_i \\in \\{-1,1\\}\\right\\}_{i=1}^n",
            "\\mathbf{x}_i",
            "\\mathbf{x}_i",
            "y_i=1",
            "y_i=-1",
            "\\mathbf{x}",
            "\\mathbf{w}\\cdot\\mathbf{x} - b=0,",
            "\\cdot",
            "{\\mathbf{w}}",
            "\\tfrac{b}{\\|\\mathbf{w}\\|}",
            "{\\mathbf{w}}"
        ]
    },
    "Radial_basis_function_kernel": {
        "title": "Radial_basis_function_kernel",
        "chunks": [
            {
                "text": "In machine learning, the radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine classification. The RBF kernel on two samples $\\mathbf{x}\\in \\mathbb{R}^{k}$ and $\\mathbf{x'}$, represented as feature vectors in some input space, is defined asJean-Philippe Vert, Koji Tsuda, and Bernhard Schölkopf (2004). \"A primer on kernel methods\". Kernel Methods in Computational Biology. $K(\\mathbf{x}, \\mathbf{x'}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x'}\\|^2}{2\\sigma^2}\\right)$ $\\textstyle\\|\\mathbf{x} - \\mathbf{x'}\\|^2$ may be recognized as the squared Euclidean distance between the two feature vectors. $\\sigma$ is a free parameter. An equivalent definition involves a parameter $\\textstyle\\gamma = \\tfrac{1}{2\\sigma^2}$: $K(\\mathbf{x}, \\mathbf{x'}) = \\exp(-\\gamma\\|\\mathbf{x} - \\mathbf{x'}\\|^2)$ Since the value of the RBF kernel decreases with distance and ranges between zero (in the infinite-distance limit) and one (when $x ), it has a ready interpretation as a similarity measure."
            },
            {
                "text": "The feature space of the kernel has an infinite number of dimensions; for $\\sigma = 1$, its expansion using the multinomial theorem is: $ \\begin{alignat}{2} \\exp\\left(-\\frac{1}{2}\\|\\mathbf{x} - \\mathbf{x'}\\|^2\\right) &= \\exp(\\frac{2}{2}\\mathbf{x}^\\top \\mathbf{x'} - \\frac{1}{2}\\|\\mathbf{x}\\|^2 - \\frac{1}{2}\\|\\mathbf{x'}\\|^2)\\\\[5pt] &= \\exp(\\mathbf{x}^\\top \\mathbf{x'}) \\exp( - \\frac{1}{2}\\|\\mathbf{x}\\|^2) \\exp( - \\frac{1}{2}\\|\\mathbf{x'}\\|^2) \\\\[5pt] &= \\sum_{j=0}^\\infty \\frac{(\\mathbf{x}^\\top \\mathbf{x'})^j}{j!} \\exp\\left(-\\frac{1}{2}\\|\\mathbf{x}\\|^2\\right) \\exp\\left(-\\frac{1}{2}\\|\\mathbf{x'}\\|^2\\right)\\\\[5pt] &= \\sum_{j=0}^\\infty \\quad \\sum_{n_1+n_2+\\dots +n_k=j} \\exp\\left(-\\frac{1}{2}\\|\\mathbf{x}\\|^2\\right) \\frac{x_1^{n_1}\\cdots x_k^{n_k} }{\\sqrt{n_1!"
            },
            {
                "text": "\\cdots n_k! }} \\exp\\left(-\\frac{1}{2}\\|\\mathbf{x'}\\|^2\\right) \\frac \\\\[5pt] &=\\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{x'}) \\rangle \\end{alignat} $ $ \\varphi(\\mathbf{x}) = \\exp\\left(-\\frac{1}{2}\\|\\mathbf{x}\\|^2\\right) \\left(a^{(0)}_{\\ell_0},a^{(1)}_1,\\dots,a^{(1)}_{\\ell_1},\\dots,a^{(j)}_1,\\dots,a^{(j)}_{\\ell_j},\\dots \\right ) $ where $\\ell_j=\\tbinom {k+j-1}{j}$, $ a^{(j)}_{\\ell}=\\frac{x_1^{n_1}\\cdots x_k^{n_k} }{\\sqrt{n_1! \\cdots n_k! }} \\quad|\\quad n_1+n_2+\\dots+n_k = j \\wedge 1\\leq \\ell\\leq \\ell_j $ Approximations Because support vector machines and other models employing the kernel trick do not scale well to large numbers of training samples or large numbers of features in the input space, several approximations to the RBF kernel (and similar kernels) have been introduced.Andreas Müller (2012)."
            },
            {
                "text": "Kernel Approximations for Efficient SVMs (and other feature extraction methods). Typically, these take the form of a function z that maps a single vector to a vector of higher dimensionality, approximating the kernel: $\\langle z(\\mathbf{x}), z(\\mathbf{x'}) \\rangle \\approx \\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{x'}) \\rangle = K(\\mathbf{x}, \\mathbf{x'})$ where $\\textstyle\\varphi$ is the implicit mapping embedded in the RBF kernel. Fourier random features One way to construct such a z is to randomly sample from the Fourier transformation of the kernelwhere $w_1, ..., w_D$ are independent samples from the normal distribution $N(0, \\sigma^{-2} I)$. Theorem: $ \\operatorname E[\\langle \\varphi(x), \\varphi(y)\\rangle] = e^{\\|x-y\\|^2/(2\\sigma^2)}. $ Proof: It suffices to prove the case of $D=1$. Use the trigonometric identity $\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b)$, the spherical symmetry of gaussian distribution, then evaluate the integral $\\int_{-\\infty}^\\infty \\frac{\\cos (k x) e^{-x^2 / 2}}{\\sqrt{2 \\pi}} d x=e^{-k^2 / 2}."
            },
            {
                "text": "$ Theorem: $\\operatorname{Var}[\\langle \\varphi(x), \\varphi(y)\\rangle] = O(D^{-1})$. (Appendix A.2). Nyström method Another approach uses the Nyström method to approximate the eigendecomposition of the Gram matrix K, using only a random sample of the training set. See also Gaussian function Kernel (statistics) Polynomial kernel Radial basis function Radial basis function network Obst kernel network References Category:Kernel methods for machine learning Category:Support vector machines"
            }
        ],
        "latex_formulas": [
            "'''x''' {{=}} '''x''''",
            "\\mathbf{x}\\in \\mathbb{R}^{k}",
            "\\mathbf{x'}",
            "K(\\mathbf{x}, \\mathbf{x'}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x'}\\|^2}{2\\sigma^2}\\right)",
            "\\textstyle\\|\\mathbf{x} - \\mathbf{x'}\\|^2",
            "\\sigma",
            "\\textstyle\\gamma = \\tfrac{1}{2\\sigma^2}",
            "K(\\mathbf{x}, \\mathbf{x'}) = \\exp(-\\gamma\\|\\mathbf{x} - \\mathbf{x'}\\|^2)",
            "\\sigma = 1",
            "\\begin{alignat}{2} \n\\exp\\left(-\\frac{1}{2}\\|\\mathbf{x} - \\mathbf{x'}\\|^2\\right)\n&= \\exp(\\frac{2}{2}\\mathbf{x}^\\top \\mathbf{x'} - \\frac{1}{2}\\|\\mathbf{x}\\|^2  - \\frac{1}{2}\\|\\mathbf{x'}\\|^2)\\\\[5pt]\n&= \\exp(\\mathbf{x}^\\top \\mathbf{x'}) \\exp( - \\frac{1}{2}\\|\\mathbf{x}\\|^2) \\exp( - \\frac{1}{2}\\|\\mathbf{x'}\\|^2) \\\\[5pt]\n&= \\sum_{j=0}^\\infty \\frac{(\\mathbf{x}^\\top \\mathbf{x'})^j}{j!} \\exp\\left(-\\frac{1}{2}\\|\\mathbf{x}\\|^2\\right) \\exp\\left(-\\frac{1}{2}\\|\\mathbf{x'}\\|^2\\right)\\\\[5pt]\n&= \\sum_{j=0}^\\infty \\quad \\sum_{n_1+n_2+\\dots +n_k=j} \n\\exp\\left(-\\frac{1}{2}\\|\\mathbf{x}\\|^2\\right) \n\\frac{x_1^{n_1}\\cdots x_k^{n_k} }{\\sqrt{n_1! \\cdots n_k! }} \n\\exp\\left(-\\frac{1}{2}\\|\\mathbf{x'}\\|^2\\right) \n\\frac{{x'}_1^{n_1}\\cdots {x'}_k^{n_k} }{\\sqrt{n_1! \\cdots n_k! }} \\\\[5pt]\n&=\\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{x'}) \\rangle\n\\end{alignat}",
            "\\varphi(\\mathbf{x})\n=\n\\exp\\left(-\\frac{1}{2}\\|\\mathbf{x}\\|^2\\right)\n\\left(a^{(0)}_{\\ell_0},a^{(1)}_1,\\dots,a^{(1)}_{\\ell_1},\\dots,a^{(j)}_1,\\dots,a^{(j)}_{\\ell_j},\\dots \\right )",
            "\\ell_j=\\tbinom {k+j-1}{j}",
            "a^{(j)}_{\\ell}=\\frac{x_1^{n_1}\\cdots x_k^{n_k} }{\\sqrt{n_1! \\cdots n_k! }} \\quad|\\quad n_1+n_2+\\dots+n_k = j \\wedge 1\\leq \\ell\\leq \\ell_j",
            "\\langle z(\\mathbf{x}), z(\\mathbf{x'}) \\rangle \\approx \\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{x'}) \\rangle = K(\\mathbf{x}, \\mathbf{x'})",
            "\\textstyle\\varphi",
            "w_1, ..., w_D",
            "N(0, \\sigma^{-2} I)",
            "\\operatorname E[\\langle \\varphi(x), \\varphi(y)\\rangle] = e^{\\|x-y\\|^2/(2\\sigma^2)}.",
            "D=1",
            "\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b)",
            "\\int_{-\\infty}^\\infty \\frac{\\cos (k x) e^{-x^2 / 2}}{\\sqrt{2 \\pi}} d x=e^{-k^2 / 2}.",
            "\\operatorname{Var}[\\langle \\varphi(x), \\varphi(y)\\rangle] = O(D^{-1})"
        ]
    },
    "Polynomial_kernel": {
        "title": "Polynomial_kernel",
        "chunks": [
            {
                "text": "Illustration of the mapping $\\varphi$. On the left a set of samples in the input space, on the right the same samples in the feature space where the polynomial kernel $K(x,y)$ (for some values of the parameters $c$ and $d$) is the inner product. The hyperplane learned in feature space by an SVM is an ellipse in the input space. In machine learning, the polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models. Intuitively, the polynomial kernel looks not only at the given features of input samples to determine their similarity, but also combinations of these. In the context of regression analysis, such combinations are known as interaction features. The (implicit) feature space of a polynomial kernel is equivalent to that of polynomial regression, but without the combinatorial blowup in the number of parameters to be learned."
            },
            {
                "text": "When the input features are binary-valued (booleans), then the features correspond to logical conjunctions of input features.Yoav Goldberg and Michael Elhadad (2008). splitSVM: Fast, Space-Efficient, non-Heuristic, Polynomial Kernel Computation for NLP Applications. Proc. ACL-08: HLT. Definition For degree- polynomials, the polynomial kernel is defined as $K(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\mathsf{T} \\mathbf{y} + c)^{d}$ where and are vectors of size in the input space, i.e. vectors of features computed from training or test samples and $c ≥ 0$ is a free parameter trading off the influence of higher-order versus lower-order terms in the polynomial. When $c , the kernel is called homogeneous. (A further generalized polykernel divides $xTy$ by a user-specified scalar parameter .) As a kernel, corresponds to an inner product in a feature space based on some mapping : $K(\\mathbf{x},\\mathbf{y}) = \\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{y}) \\rangle$ The nature of can be seen from an example."
            },
            {
                "text": "Let $d , so we get the special case of the quadratic kernel."
            },
            {
                "text": "After using the multinomial theorem (twice—the outermost application is the binomial theorem) and regrouping, $K(\\mathbf{x},\\mathbf{y}) = \\left(\\sum_{i=1}^n x_i y_i + c\\right)^2 = \\sum_{i=1}^n \\left(x_i^2\\right) \\left(y_i^2 \\right) + \\sum_{i=2}^n \\sum_{j=1}^{i-1} \\left( \\sqrt{2} x_i x_j \\right) \\left( \\sqrt{2} y_i y_j \\right) + \\sum_{i=1}^n \\left( \\sqrt{2c} x_i \\right) \\left( \\sqrt{2c} y_i \\right) + c^2 $ From this it follows that the feature map is given by: $ \\varphi(x) = \\left( x_n^2, \\ldots, x_1^2, \\sqrt{2} x_n x_{n-1}, \\ldots, \\sqrt{2} x_n x_1, \\sqrt{2} x_{n-1} x_{n-2}, \\ldots, \\sqrt{2} x_{n-1} x_{1}, \\ldots, \\sqrt{2} x_{2} x_{1}, \\sqrt{2c} x_n, \\ldots, \\sqrt{2c} x_1, c \\right) $ generalizing for $\\left(\\mathbf{x}^{T}\\mathbf{y} + c\\right)^d$, where $\\mathbf{x}\\in\\mathbb{R}^{n}$, $\\mathbf{y}\\in \\mathbb{R}^{n}$ and applying the multinomial theorem: $ \\begin{alignat}{2} \\left(\\mathbf{x}^{T}\\mathbf{y} + c\\right)^d & = \\sum_{j_1+j_2+\\dots +j_{n+1}=d} \\frac{\\sqrt{d!"
            },
            {
                "text": "}}{\\sqrt{j_1!"
            },
            {
                "text": "\\cdots j_n! j_{n+1}!}} x_1^{j_1}\\cdots x_n^{j_n} \\sqrt{c}^{j_{n+1}} \\frac{\\sqrt{d!}}{\\sqrt{j_1! \\cdots j_n! j_{n+1}!}} y_1^{j_1}\\cdots y_n^{j_n} \\sqrt{c}^{j_{n+1}}\\\\ &=\\varphi(\\mathbf{x})^{T} \\varphi(\\mathbf{y}) \\end{alignat} $ The last summation has $l_d=\\tbinom {n+d}{d}$ elements, so that: $ \\varphi(\\mathbf{x}) = \\left(a_{1},\\dots, a_{l},\\dots,a_{l_d} \\right ) $ where $l=(j_1,j_2,...,j_{n},j_{n+1})$ and $ a_{l}=\\frac{\\sqrt{d!} }{\\sqrt{j_1! \\cdots j_n!j_{n+1}! }} x_1^{j_1}\\cdots x_n^{j_n} \\sqrt{c}^{j_{n+1}}\\quad|\\quad j_1+j_2+\\dots+j_n +j_{n+1} = d $ Practical use Although the RBF kernel is more popular in SVM classification than the polynomial kernel, the latter is quite popular in natural language processing (NLP). The most common degree is $d (quadratic), since larger degrees tend to overfit on NLP problems."
            },
            {
                "text": "Various ways of computing the polynomial kernel (both exact and approximate) have been devised as alternatives to the usual non-linear SVM training algorithms, including: full expansion of the kernel prior to training/testing with a linear SVM, i.e. full computation of the mapping as in polynomial regression; basket mining (using a variant of the apriori algorithm) for the most commonly occurring feature conjunctions in a training set to produce an approximate expansion; inverted indexing of support vectors. One problem with the polynomial kernel is that it may suffer from numerical instability: when $xTy + c < 1$, $K(x, y) tends to zero with increasing , whereas when $xTy + c > 1$, $K(x, y)$ tends to infinity. References Category:Kernel methods for machine learning"
            }
        ],
        "latex_formulas": [
            "''c'' ≥ 0",
            "''c'' {{=}} 0",
            "''x''<sup>T</sup>''y''",
            "''d'' {{=}} 2",
            "''d'' {{=}} 2",
            "''x''<sup>T</sup>''y'' + ''c'' < 1",
            "''K''(''x'', ''y'') {{=}} (''x''<sup>T</sup>''y'' + ''c'')<sup>''d''</sup>",
            "''x''<sup>T</sup>''y'' + ''c'' > 1",
            "''K''(''x'', ''y'')",
            "\\varphi",
            "K(x,y)",
            "c",
            "d",
            "K(\\mathbf{x},\\mathbf{y}) = (\\mathbf{x}^\\mathsf{T} \\mathbf{y} + c)^{d}",
            "K(\\mathbf{x},\\mathbf{y}) = \\langle \\varphi(\\mathbf{x}), \\varphi(\\mathbf{y}) \\rangle",
            "K(\\mathbf{x},\\mathbf{y}) = \\left(\\sum_{i=1}^n x_i y_i + c\\right)^2 = \n\\sum_{i=1}^n \\left(x_i^2\\right) \\left(y_i^2 \\right) + \n\\sum_{i=2}^n \\sum_{j=1}^{i-1} \\left( \\sqrt{2} x_i x_j \\right) \\left( \\sqrt{2} y_i y_j \\right) \n+ \\sum_{i=1}^n \\left( \\sqrt{2c} x_i \\right) \\left( \\sqrt{2c} y_i \\right) + c^2",
            "\\varphi(x) = \\left( x_n^2, \\ldots, x_1^2, \\sqrt{2} x_n x_{n-1}, \\ldots, \\sqrt{2} x_n x_1, \\sqrt{2} x_{n-1} x_{n-2}, \\ldots, \\sqrt{2} x_{n-1} x_{1}, \\ldots, \\sqrt{2} x_{2} x_{1}, \\sqrt{2c} x_n, \\ldots, \\sqrt{2c} x_1, c \\right)",
            "\\left(\\mathbf{x}^{T}\\mathbf{y} + c\\right)^d",
            "\\mathbf{x}\\in\\mathbb{R}^{n}",
            "\\mathbf{y}\\in \\mathbb{R}^{n}",
            "\\begin{alignat}{2} \n\\left(\\mathbf{x}^{T}\\mathbf{y} + c\\right)^d & = \n\\sum_{j_1+j_2+\\dots +j_{n+1}=d} \n\\frac{\\sqrt{d!}}{\\sqrt{j_1! \\cdots j_n! j_{n+1}!}} x_1^{j_1}\\cdots x_n^{j_n} \\sqrt{c}^{j_{n+1}}\n\\frac{\\sqrt{d!}}{\\sqrt{j_1! \\cdots j_n! j_{n+1}!}} y_1^{j_1}\\cdots y_n^{j_n} \\sqrt{c}^{j_{n+1}}\\\\\n&=\\varphi(\\mathbf{x})^{T} \\varphi(\\mathbf{y})\n\\end{alignat}",
            "l_d=\\tbinom {n+d}{d}",
            "\\varphi(\\mathbf{x})\n=\n\\left(a_{1},\\dots, a_{l},\\dots,a_{l_d} \\right )",
            "l=(j_1,j_2,...,j_{n},j_{n+1})",
            "a_{l}=\\frac{\\sqrt{d!} }{\\sqrt{j_1! \\cdots j_n!j_{n+1}! }} x_1^{j_1}\\cdots x_n^{j_n} \\sqrt{c}^{j_{n+1}}\\quad|\\quad j_1+j_2+\\dots+j_n +j_{n+1} = d"
        ]
    },
    "Kernel_method": {
        "title": "Kernel_method",
        "chunks": [
            {
                "text": "In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM). These methods involve using linear classifiers to solve nonlinear problems. The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over all pairs of data points computed using inner products. The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user-input according to the representer theorem. Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing. Kernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space."
            },
            {
                "text": "This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the \"kernel trick\". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors. Algorithms capable of operating with kernels include the kernel perceptron, support-vector machines (SVM), Gaussian processes, principal components analysis (PCA), canonical correlation analysis, ridge regression, spectral clustering, linear adaptive filters and many others. Most kernel algorithms are based on convex optimization or eigenproblems and are statistically well-founded. Typically, their statistical properties are analyzed using statistical learning theory (for example, using Rademacher complexity). Motivation and informal explanation Kernel methods can be thought of as instance-based learners: rather than learning some fixed set of parameters corresponding to the features of their inputs, they instead \"remember\" the $i$-th training example $(\\mathbf{x}_i, y_i)$ and learn for it a corresponding weight $w_i$. Prediction for unlabeled inputs, i.e., those not in the training set, is treated by the application of a similarity function $k$, called a kernel, between the unlabeled input $\\mathbf{x'}$ and each of the training inputs $\\mathbf{x}_i$."
            },
            {
                "text": "For instance, a kernelized binary classifier typically computes a weighted sum of similarities where $\\hat{y} \\in \\{-1, +1\\}$ is the kernelized binary classifier's predicted label for the unlabeled input $\\mathbf{x'}$ whose hidden true label $y$ is of interest; $k \\colon \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ is the kernel function that measures similarity between any pair of inputs $\\mathbf{x}, \\mathbf{x'} \\in \\mathcal{X}$; the sum ranges over the labeled examples $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ in the classifier's training set, with $y_i \\in \\{-1, +1\\}$; the $w_i \\in \\mathbb{R}$ are the weights for the training examples, as determined by the learning algorithm; the sign function $\\sgn$ determines whether the predicted classification $\\hat{y}$ comes out positive or negative. Kernel classifiers were described as early as the 1960s, with the invention of the kernel perceptron."
            },
            {
                "text": "Cited in They rose to great prominence with the popularity of the support-vector machine (SVM) in the 1990s, when the SVM was found to be competitive with neural networks on tasks such as handwriting recognition. Mathematics: the kernel trick SVM with feature map given by $\\varphi((a, b)) = (a, b, a^2 + b^2)$ and thus with the kernel function $k(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y} + \\left\\| \\mathbf{x} \\right\\| ^2 \\left\\| \\mathbf{y} \\right\\| ^2 $. The training points are mapped to a 3-dimensional space where a separating hyperplane can be easily found. The kernel trick avoids the explicit mapping that is needed to get linear learning algorithms to learn a nonlinear function or decision boundary. For all $\\mathbf{x}$ and $\\mathbf{x'}$ in the input space $\\mathcal{X}$, certain functions $k(\\mathbf{x}, \\mathbf{x'})$ can be expressed as an inner product in another space $\\mathcal{V}$."
            },
            {
                "text": "The function $k \\colon \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ is often referred to as a kernel or a kernel function. The word \"kernel\" is used in mathematics to denote a weighting function for a weighted sum or integral. Certain problems in machine learning have more structure than an arbitrary weighting function $k$. The computation is made much simpler if the kernel can be written in the form of a \"feature map\" $\\varphi\\colon \\mathcal{X} \\to \\mathcal{V}$ which satisfies The key restriction is that $ \\langle \\cdot, \\cdot \\rangle_\\mathcal{V}$ must be a proper inner product. On the other hand, an explicit representation for $\\varphi$ is not necessary, as long as $\\mathcal{V}$ is an inner product space. The alternative follows from Mercer's theorem: an implicitly defined function $\\varphi$ exists whenever the space $\\mathcal{X}$ can be equipped with a suitable measure ensuring the function $k$ satisfies Mercer's condition."
            },
            {
                "text": "Mercer's theorem is similar to a generalization of the result from linear algebra that associates an inner product to any positive-definite matrix. In fact, Mercer's condition can be reduced to this simpler case. If we choose as our measure the counting measure $ \\mu(T) = |T| $ for all $ T \\subset X $, which counts the number of points inside the set $T$, then the integral in Mercer's theorem reduces to a summationIf this summation holds for all finite sequences of points $(\\mathbf{x}_1, \\dotsc, \\mathbf{x}_n)$ in $\\mathcal{X}$ and all choices of $n$ real-valued coefficients $(c_1, \\dots, c_n)$ (cf. positive definite kernel), then the function $k$ satisfies Mercer's condition. Some algorithms that depend on arbitrary relationships in the native space $\\mathcal{X}$ would, in fact, have a linear interpretation in a different setting: the range space of $\\varphi$. The linear interpretation gives us insight about the algorithm."
            },
            {
                "text": "Furthermore, there is often no need to compute $\\varphi$ directly during computation, as is the case with support-vector machines. Some cite this running time shortcut as the primary benefit. Researchers also use it to justify the meanings and properties of existing algorithms. Theoretically, a Gram matrix $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ with respect to $\\{\\mathbf{x}_1, \\dotsc, \\mathbf{x}_n\\}$ (sometimes also called a \"kernel matrix\"), where $K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$, must be positive semi-definite (PSD). Empirically, for machine learning heuristics, choices of a function $k$ that do not satisfy Mercer's condition may still perform reasonably if $k$ at least approximates the intuitive idea of similarity. Regardless of whether $k$ is a Mercer kernel, $k$ may still be referred to as a \"kernel\". If the kernel function $k$ is also a covariance function as used in Gaussian processes, then the Gram matrix $\\mathbf{K}$ can also be called a covariance matrix. Applications Application areas of kernel methods are diverse and include geostatistics, kriging, inverse distance weighting, 3D reconstruction, bioinformatics, cheminformatics, information extraction and handwriting recognition."
            },
            {
                "text": "Popular kernels Fisher kernel Graph kernels Kernel smoother Polynomial kernel Radial basis function kernel (RBF) String kernels Neural tangent kernel Neural network Gaussian process (NNGP) kernel See also Kernel methods for vector output Kernel density estimation Representer theorem Similarity learning Cover's theorem References Further reading External links Kernel-Machines Org—community website onlineprediction.net Kernel Methods Article Category:Kernel methods for machine learning Category:Geostatistics Category:Classification algorithms"
            }
        ],
        "latex_formulas": [
            "i",
            "(\\mathbf{x}_i, y_i)",
            "w_i",
            "k",
            "\\mathbf{x'}",
            "\\mathbf{x}_i",
            "\\hat{y} \\in \\{-1, +1\\}",
            "\\mathbf{x'}",
            "y",
            "k \\colon \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}",
            "\\mathbf{x}, \\mathbf{x'} \\in \\mathcal{X}",
            "\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n",
            "y_i \\in \\{-1, +1\\}",
            "w_i \\in \\mathbb{R}",
            "\\sgn",
            "\\hat{y}",
            "\\varphi((a, b)) = (a, b, a^2 + b^2)",
            "k(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y} + \\left\\| \\mathbf{x} \\right\\| ^2  \\left\\| \\mathbf{y} \\right\\| ^2",
            "\\mathbf{x}",
            "\\mathbf{x'}",
            "\\mathcal{X}",
            "k(\\mathbf{x}, \\mathbf{x'})",
            "\\mathcal{V}",
            "k \\colon \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}",
            "k",
            "\\varphi\\colon \\mathcal{X} \\to \\mathcal{V}",
            "\\langle \\cdot, \\cdot \\rangle_\\mathcal{V}",
            "\\varphi",
            "\\mathcal{V}",
            "\\varphi",
            "\\mathcal{X}",
            "k",
            "\\mu(T) = |T|",
            "T \\subset X",
            "T",
            "(\\mathbf{x}_1, \\dotsc, \\mathbf{x}_n)",
            "\\mathcal{X}",
            "n",
            "(c_1, \\dots, c_n)",
            "k",
            "\\mathcal{X}",
            "\\varphi",
            "\\varphi",
            "\\mathbf{K} \\in \\mathbb{R}^{n \\times n}",
            "\\{\\mathbf{x}_1, \\dotsc, \\mathbf{x}_n\\}",
            "K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)",
            "k",
            "k",
            "k",
            "k",
            "k",
            "\\mathbf{K}"
        ]
    },
    "Feature_space": {
        "title": "Feature_space",
        "chunks": [
            {
                "text": "REDIRECT Feature (machine learning)#Feature vectors"
            }
        ],
        "latex_formulas": []
    },
    "Lagrange_multipliers": {
        "title": "Lagrange_multipliers",
        "chunks": [
            {
                "text": "REDIRECT Lagrange multiplier"
            }
        ],
        "latex_formulas": []
    },
    "Duality_(optimization)": {
        "title": "Duality_(optimization)",
        "chunks": [
            {
                "text": "In mathematical optimization theory, duality or the duality principle is the principle that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem. If the primal is a minimization problem then the dual is a maximization problem (and vice versa). Any feasible solution to the primal (minimization) problem is at least as large as any feasible solution to the dual (maximization) problem. Therefore, the solution to the primal is an upper bound to the solution of the dual, and the solution of the dual is a lower bound to the solution of the primal. This fact is called weak duality. In general, the optimal values of the primal and dual problems need not be equal. Their difference is called the duality gap. For convex optimization problems, the duality gap is zero under a constraint qualification condition. This fact is called strong duality. Dual problem Usually the term \"dual problem\" refers to the Lagrangian dual problem but other dual problems are used – for example, the Wolfe dual problem and the Fenchel dual problem."
            },
            {
                "text": "The Lagrangian dual problem is obtained by forming the Lagrangian of a minimization problem by using nonnegative Lagrange multipliers to add the constraints to the objective function, and then solving for the primal variable values that minimize the original objective function. This solution gives the primal variables as functions of the Lagrange multipliers, which are called dual variables, so that the new problem is to maximize the objective function with respect to the dual variables under the derived constraints on the dual variables (including at least the nonnegativity constraints). In general given two dual pairs of separated locally convex spaces $\\left(X,X^*\\right)$ and $\\left(Y,Y^*\\right)$ and the function $f: X \\to \\mathbb{R} \\cup \\{+\\infty\\}$, we can define the primal problem as finding $\\hat{x}$ such that $f(\\hat{x}) = \\inf_{x \\in X} f(x). \\,$ In other words, if $\\hat{x}$ exists, $f(\\hat{x})$ is the minimum of the function $f$ and the infimum (greatest lower bound) of the function is attained."
            },
            {
                "text": "If there are constraint conditions, these can be built into the function $f$ by letting $\\tilde{f} = f + I_{\\mathrm{constraints}}$ where $I_{\\mathrm{constraints}}$ is a suitable function on $X$ that has a minimum 0 on the constraints, and for which one can prove that $\\inf_{x \\in X} \\tilde{f}(x) = \\inf_{x \\ \\mathrm{constrained}} f(x)$. The latter condition is trivially, but not always conveniently, satisfied for the characteristic function (i.e. $I_{\\mathrm{constraints}}(x) = 0 $ for $ x $ satisfying the constraints and $I_{\\mathrm{constraints}}(x) = \\infty $ otherwise). Then extend $\\tilde{f}$ to a perturbation function $F: X \\times Y \\to \\mathbb{R} \\cup \\{+\\infty\\}$ such that $F(x,0) = \\tilde{f}(x)$."
            },
            {
                "text": "The duality gap is the difference of the right and left hand sides of the inequality $\\sup_{y^* \\in Y^*} -F^*(0,y^*) \\le \\inf_{x \\in X} F(x,0), \\,$ where $F^*$ is the convex conjugate in both variables and $\\sup$ denotes the supremum (least upper bound). Duality gap The duality gap is the difference between the values of any primal solutions and any dual solutions. If $d^*$ is the optimal dual value and $p^*$ is the optimal primal value, then the duality gap is equal to $p^* - d^*$. This value is always greater than or equal to 0 (for minimization problems). The duality gap is zero if and only if strong duality holds. Otherwise the gap is strictly positive and weak duality holds. In computational optimization, another \"duality gap\" is often reported, which is the difference in value between any dual solution and the value of a feasible but suboptimal iterate for the primal problem."
            },
            {
                "text": "This alternative \"duality gap\" quantifies the discrepancy between the value of a current feasible but suboptimal iterate for the primal problem and the value of the dual problem; the value of the dual problem is, under regularity conditions, equal to the value of the convex relaxation of the primal problem: The convex relaxation is the problem arising replacing a non-convex feasible set with its closed convex hull and with replacing a non-convex function with its convex closure, that is the function that has the epigraph that is the closed convex hull of the original primal objective function. Linear case Linear programming problems are optimization problems in which the objective function and the constraints are all linear. In the primal problem, the objective function is a linear combination of n variables. There are m constraints, each of which places an upper bound on a linear combination of the n variables. The goal is to maximize the value of the objective function subject to the constraints. A solution is a vector (a list) of n values that achieves the maximum value for the objective function."
            },
            {
                "text": "In the dual problem, the objective function is a linear combination of the m values that are the limits in the m constraints from the primal problem. There are n dual constraints, each of which places a lower bound on a linear combination of m dual variables. Relationship between the primal problem and the dual problem In the linear case, in the primal problem, from each sub-optimal point that satisfies all the constraints, there is a direction or subspace of directions to move that increases the objective function. Moving in any such direction is said to remove slack between the candidate solution and one or more constraints. An infeasible value of the candidate solution is one that exceeds one or more of the constraints. In the dual problem, the dual vector multiplies the constraints that determine the positions of the constraints in the primal. Varying the dual vector in the dual problem is equivalent to revising the upper bounds in the primal problem. The lowest upper bound is sought. That is, the dual vector is minimized in order to remove slack between the candidate positions of the constraints and the actual optimum."
            },
            {
                "text": "An infeasible value of the dual vector is one that is too low. It sets the candidate positions of one or more of the constraints in a position that excludes the actual optimum. This intuition is made formal by the equations in Linear programming: Duality. Nonlinear case In nonlinear programming, the constraints are not necessarily linear. Nonetheless, many of the same principles apply. To ensure that the global maximum of a non-linear problem can be identified easily, the problem formulation often requires that the functions be convex and have compact lower level sets. This is the significance of the Karush–Kuhn–Tucker conditions. They provide necessary conditions for identifying local optima of non-linear programming problems. There are additional conditions (constraint qualifications) that are necessary so that it will be possible to define the direction to an optimal solution. An optimal solution is one that is a local optimum, but possibly not a global optimum. Lagrange duality Motivation Suppose we want to solve the following nonlinear programming problem:$\\begin{align} \\text{minimize } &f_0(x) \\\\ \\text{subject to } &f_i(x) \\leq 0,\\ i \\in \\left \\{1,\\ldots,m \\right \\} \\\\ \\end{align}$The problem has constraints; we would like to convert it to a program without constraints."
            },
            {
                "text": "Theoretically, it is possible to do it by minimizing the function $J(x)$, defined as$J(x) = f_0(x) + \\sum_i I[f_i(x)] $where $I$ is an infinite step function: $I[u]=0$ if $u \\leq 0$, and $I[u]=\\infty $ otherwise. But $ J(x) $ is hard to solve as it is not continuous. It is possible to \"approximate\" $I[u]$ by $\\lambda u$, where $\\lambda$ is a positive constant. This yields a function known as the Lagrangian:$L(x,\\lambda) = f_0(x) + \\sum_i \\lambda_i f_i(x) $Note that, for every $x$, $\\max_{\\lambda\\geq 0} L(x,\\lambda) = J(x) $.Proof: If x satisfies all constraints fi(x)≤0, then L(x,λ) is maximized when taking λ=0, and its value is then f(x); If x violates some constraint, fi(x)>0 for some i, then L(x,λ)→∞ when λi→∞."
            },
            {
                "text": "Therefore, the original problem is equivalent to:$\\min_x \\max_{\\lambda\\geq 0} L(x,\\lambda) $.By reversing the order of min and max, we get:$\\max_{\\lambda\\geq 0} \\min_x L(x,\\lambda) $.The dual function is the inner problem in the above formula:$g(\\lambda) := \\min_x L(x,\\lambda) $.The Lagrangian dual program is the program of maximizing g:$\\max_{\\lambda\\geq 0} g(\\lambda) $.The optimal solution to the dual program is a lower bound for the optimal solution of the original (primal) program; this is the weak duality principle. If the primal problem is convex and bounded from below, and there exists a point in which all nonlinear constraints are strictly satisfied (Slater's condition), then the optimal solution to the dual program equals the optimal solution of the primal program; this is the strong duality principle. In this case, we can solve the primal program by finding an optimal solution λ* to the dual program, and then solving:$\\min_x L(x,\\lambda^*) $.Note that, to use either the weak or the strong duality principle, we need a way to compute g(λ)."
            },
            {
                "text": "In general this may be hard, as we need to solve a different minimization problem for every λ. But for some classes of functions, it is possible to get an explicit formula for g(). Solving the primal and dual programs together is often easier than solving only one of them. Examples are linear programming and quadratic programming. A better and more general approach to duality is provided by Fenchel's duality theorem. Another condition in which the min-max and max-min are equal is when the Lagrangian has a saddle point: (x∗, λ∗) is a saddle point of the Lagrange function L if and only if x∗ is an optimal solution to the primal, λ∗ is an optimal solution to the dual, and the optimal values in the indicated problems are equal to each other."
            },
            {
                "text": "The strong Lagrange principle Given a nonlinear programming problem in standard form $\\begin{align} \\text{minimize } &f_0(x) \\\\ \\text{subject to } &f_i(x) \\leq 0,\\ i \\in \\left \\{1,\\ldots,m \\right \\} \\\\ &h_i(x) = 0,\\ i \\in \\left \\{1,\\ldots,p \\right \\} \\end{align}$ with the domain $\\mathcal{D} \\subset \\mathbb{R}^n$ having non-empty interior, the Lagrangian function $\\mathcal{L}: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R} $ is defined as $\\mathcal{L}(x,\\lambda,\\nu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\nu_i h_i(x).$ The vectors $\\lambda$ and $\\nu$ are called the dual variables or Lagrange multiplier vectors associated with the problem."
            },
            {
                "text": "The Lagrange dual function $g:\\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R} $ is defined as $g(\\lambda,\\nu) = \\inf_{x\\in\\mathcal{D}} \\mathcal{L}(x,\\lambda,\\nu) = \\inf_{x\\in\\mathcal{D}} \\left\\{ f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\nu_i h_i(x) \\right\\}.$ The dual function g is concave, even when the initial problem is not convex, because it is a point-wise infimum of affine functions. The dual function yields lower bounds on the optimal value $p^*$ of the initial problem; for any $\\lambda \\geq 0 $ and any $\\nu$ we have $g(\\lambda,\\nu) \\leq p^* $. If a constraint qualification such as Slater's condition holds and the original problem is convex, then we have strong duality, i.e. $d^* = \\max_{\\lambda \\ge 0, \\nu} g(\\lambda,\\nu) = \\inf f_0 = p^*$."
            },
            {
                "text": "Convex problems For a convex minimization problem with inequality constraints, $\\begin{align} &\\underset{x}{\\operatorname{minimize}}& & f(x) \\\\ &\\operatorname{subject\\;to} & &g_i(x) \\leq 0, \\quad i = 1,\\ldots,m \\end{align}$ the Lagrangian dual problem is $\\begin{align} &\\underset{u}{\\operatorname{maximize}}& & \\inf_x \\left(f(x) + \\sum_{j=1}^m u_j g_j(x)\\right) \\\\ &\\operatorname{subject\\;to} & &u_i \\geq 0, \\quad i = 1,\\ldots,m \\end{align}$ where the objective function is the Lagrange dual function. Provided that the functions $f$ and $g_1, \\ldots, g_m$ are continuously differentiable, the infimum occurs where the gradient is equal to zero. The problem $\\begin{align} &\\underset{x, u}{\\operatorname{maximize}}& & f(x) + \\sum_{j=1}^m u_j g_j(x) \\\\ &\\operatorname{subject\\;to} & & \\nabla f(x) + \\sum_{j=1}^m u_j \\, \\nabla g_j(x) = 0 \\\\ &&&u_i \\geq 0, \\quad i = 1,\\ldots,m \\end{align}$ is called the Wolfe dual problem."
            },
            {
                "text": "This problem may be difficult to deal with computationally, because the objective function is not concave in the joint variables $(u,x)$. Also, the equality constraint $\\nabla f(x) + \\sum_{j=1}^m u_j \\, \\nabla g_j(x)$ is nonlinear in general, so the Wolfe dual problem is typically a nonconvex optimization problem. In any case, weak duality holds. History According to George Dantzig, the duality theorem for linear optimization was conjectured by John von Neumann immediately after Dantzig presented the linear programming problem. Von Neumann noted that he was using information from his game theory, and conjectured that two person zero sum matrix game was equivalent to linear programming. Rigorous proofs were first published in 1948 by Albert W. Tucker and his group. (Dantzig's foreword to Nering and Tucker, 1993) Applications In support vector machines (SVMs), formulating the primal problem of SVMs as the dual problem can be used to implement the Kernel trick, but the latter has higher time complexity in the historical cases. See also Convex duality Duality Relaxation (approximation) Notes References Books Articles Duality in Linear Programming Gary D. Knott Category:Convex optimization Category:Linear programming"
            }
        ],
        "latex_formulas": [
            "\\left(X,X^*\\right)",
            "\\left(Y,Y^*\\right)",
            "f: X \\to \\mathbb{R} \\cup \\{+\\infty\\}",
            "\\hat{x}",
            "f(\\hat{x}) = \\inf_{x \\in X} f(x). \\,",
            "\\hat{x}",
            "f(\\hat{x})",
            "f",
            "f",
            "\\tilde{f} = f + I_{\\mathrm{constraints}}",
            "I_{\\mathrm{constraints}}",
            "X",
            "\\inf_{x \\in X} \\tilde{f}(x) = \\inf_{x \\ \\mathrm{constrained}} f(x)",
            "I_{\\mathrm{constraints}}(x) = 0",
            "x",
            "I_{\\mathrm{constraints}}(x) = \\infty",
            "\\tilde{f}",
            "F: X \\times Y \\to \\mathbb{R} \\cup \\{+\\infty\\}",
            "F(x,0) = \\tilde{f}(x)",
            "\\sup_{y^* \\in Y^*} -F^*(0,y^*) \\le \\inf_{x \\in X} F(x,0), \\,",
            "F^*",
            "\\sup",
            "d^*",
            "p^*",
            "p^* - d^*",
            "\\begin{align}\n\\text{minimize }    &f_0(x) \\\\\n\\text{subject to } &f_i(x) \\leq 0,\\ i \\in \\left \\{1,\\ldots,m \\right \\} \\\\\n\\end{align}",
            "J(x)",
            "J(x) = f_0(x) + \\sum_i I[f_i(x)]",
            "I",
            "I[u]=0",
            "u \\leq 0",
            "I[u]=\\infty",
            "J(x)",
            "I[u]",
            "\\lambda u",
            "\\lambda",
            "L(x,\\lambda) = f_0(x) + \\sum_i \\lambda_i f_i(x)",
            "x",
            "\\max_{\\lambda\\geq 0} L(x,\\lambda) = J(x)",
            "\\min_x \\max_{\\lambda\\geq 0} L(x,\\lambda)",
            "\\max_{\\lambda\\geq 0} \\min_x  L(x,\\lambda)",
            "g(\\lambda) := \\min_x L(x,\\lambda)",
            "\\max_{\\lambda\\geq 0} g(\\lambda)",
            "\\min_x  L(x,\\lambda^*)",
            "\\begin{align}\n\\text{minimize }    &f_0(x) \\\\\n\\text{subject to } &f_i(x) \\leq 0,\\ i \\in \\left \\{1,\\ldots,m \\right \\} \\\\\n                    &h_i(x) = 0,\\ i \\in \\left \\{1,\\ldots,p \\right \\}\n\\end{align}",
            "\\mathcal{D} \\subset \\mathbb{R}^n",
            "\\mathcal{L}: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R}",
            "\\mathcal{L}(x,\\lambda,\\nu) = f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\nu_i h_i(x).",
            "\\lambda",
            "\\nu",
            "g:\\mathbb{R}^m \\times \\mathbb{R}^p \\to \\mathbb{R}",
            "g(\\lambda,\\nu) = \\inf_{x\\in\\mathcal{D}} \\mathcal{L}(x,\\lambda,\\nu) = \\inf_{x\\in\\mathcal{D}} \\left\\{ f_0(x) + \\sum_{i=1}^m \\lambda_i f_i(x) + \\sum_{i=1}^p \\nu_i h_i(x) \\right\\}.",
            "p^*",
            "\\lambda \\geq 0",
            "\\nu",
            "g(\\lambda,\\nu) \\leq p^*",
            "d^* = \\max_{\\lambda \\ge 0, \\nu} g(\\lambda,\\nu) = \\inf f_0 = p^*",
            "\\begin{align}\n&\\underset{x}{\\operatorname{minimize}}& & f(x) \\\\\n&\\operatorname{subject\\;to}\n& &g_i(x) \\leq 0, \\quad i = 1,\\ldots,m\n\\end{align}",
            "\\begin{align}\n&\\underset{u}{\\operatorname{maximize}}& & \\inf_x \\left(f(x) + \\sum_{j=1}^m u_j g_j(x)\\right) \\\\\n&\\operatorname{subject\\;to}\n& &u_i \\geq 0, \\quad i = 1,\\ldots,m\n\\end{align}",
            "f",
            "g_1, \\ldots, g_m",
            "\\begin{align}\n&\\underset{x, u}{\\operatorname{maximize}}& & f(x) + \\sum_{j=1}^m u_j g_j(x) \\\\\n&\\operatorname{subject\\;to}\n& & \\nabla f(x) + \\sum_{j=1}^m u_j \\, \\nabla g_j(x) = 0 \\\\\n&&&u_i \\geq 0, \\quad i = 1,\\ldots,m\n\\end{align}",
            "(u,x)",
            "\\nabla f(x) + \\sum_{j=1}^m u_j \\, \\nabla g_j(x)"
        ]
    },
    "Karush–Kuhn–Tucker_conditions": {
        "title": "Karush–Kuhn–Tucker_conditions",
        "chunks": [
            {
                "text": "In mathematical optimization, the Karush–Kuhn–Tucker (KKT) conditions, also known as the Kuhn–Tucker conditions, are first derivative tests (sometimes called first-order necessary conditions) for a solution in nonlinear programming to be optimal, provided that some regularity conditions are satisfied. Allowing inequality constraints, the KKT approach to nonlinear programming generalizes the method of Lagrange multipliers, which allows only equality constraints. Similar to the Lagrange approach, the constrained maximization (minimization) problem is rewritten as a Lagrange function whose optimal point is a global maximum or minimum over the domain of the choice variables and a global minimum (maximum) over the multipliers. The Karush–Kuhn–Tucker theorem is sometimes referred to as the saddle-point theorem. The KKT conditions were originally named after Harold W. Kuhn and Albert W. Tucker, who first published the conditions in 1951. Later scholars discovered that the necessary conditions for this problem had been stated by William Karush in his master's thesis in 1939. Nonlinear optimization problem Consider the following nonlinear optimization problem in standard form: minimize $f(\\mathbf{x})$ subject to $ g_i(\\mathbf{x}) \\leq 0, $ $ h_j(\\mathbf{x}) =0."
            },
            {
                "text": "$ where $\\mathbf{x} \\in \\mathbf{X}$ is the optimization variable chosen from a convex subset of $\\mathbb{R}^{n}$, $f$ is the objective or utility function, $ g_i \\ (i = 1, \\ldots, m) $ are the inequality constraint functions and $ h_j \\ (j = 1, \\ldots, \\ell) $ are the equality constraint functions. The numbers of inequalities and equalities are denoted by $m$ and $\\ell$ respectively. Corresponding to the constrained optimization problem one can form the Lagrangian function where The Karush–Kuhn–Tucker theorem then states the following. Since the idea of this approach is to find a supporting hyperplane on the feasible set $\\mathbf{\\Gamma} = \\left\\{ \\mathbf{x} \\in \\mathbf{X} : g_i(\\mathbf{x}) \\leq 0, i = 1, \\ldots, m \\right\\}$, the proof of the Karush–Kuhn–Tucker theorem makes use of the hyperplane separation theorem. The system of equations and inequalities corresponding to the KKT conditions is usually not solved directly, except in the few special cases where a closed-form solution can be derived analytically."
            },
            {
                "text": "In general, many optimization algorithms can be interpreted as methods for numerically solving the KKT system of equations and inequalities. Necessary conditions Suppose that the objective function $f\\colon\\mathbb{R}^n \\rightarrow \\mathbb{R}$ and the constraint functions $g_i\\colon\\mathbb{R}^n \\rightarrow \\mathbb{R}$ and $h_j\\colon\\mathbb{R}^n \\rightarrow \\mathbb{R}$ have subderivatives at a point $x^* \\in \\mathbb{R}^n$."
            },
            {
                "text": "If $x^*$ is a local optimum and the optimization problem satisfies some regularity conditions (see below), then there exist constants $\\mu_i\\ (i = 1,\\ldots,m)$ and $\\lambda_j\\ (j = 1,\\ldots,\\ell)$, called KKT multipliers, such that the following four groups of conditions hold: Inequality constraint diagram for optimization problems Stationarity For minimizing $f(x)$: $\\partial f(x^*) + \\sum_{j=1}^\\ell \\lambda_j \\partial h_j(x^*) + \\sum_{i=1}^m \\mu_i \\partial g_i(x^*) \\ni \\mathbf 0$ For maximizing $f(x)$: $-\\partial f(x^*) + \\sum_{j=1}^\\ell \\lambda_j \\partial h_j(x^*) + \\sum_{i=1}^m \\mu_i \\partial g_i(x^*) \\ni \\mathbf 0$ Primal feasibility $h_j(x^*) = 0, \\text{ for } j = 1, \\ldots, \\ell \\,\\!$ $g_i(x^*) \\le 0, \\text{ for } i = 1, \\ldots, m$ Dual feasibility $\\mu_i \\ge 0, \\text{ for } i = 1, \\ldots, m$ Complementary slackness $\\sum_{i=1}^m \\mu_i g_i (x^*) = 0.$ The last condition is sometimes written in the equivalent form: $\\mu_i g_i (x^*) = 0, \\text{ for } i = 1, \\ldots, m.$ In the particular case $m=0$, i.e., when there are no inequality constraints, the KKT conditions turn into the Lagrange conditions, and the KKT multipliers are called Lagrange multipliers."
            },
            {
                "text": "Proof Interpretation: KKT conditions as balancing constraint-forces in state space The primal problem can be interpreted as moving a particle in the space of $x$, and subjecting it to three kinds of force fields: $f$ is a potential field that the particle is minimizing. The force generated by $f$ is $-\\partial f$. $g_i$ are one-sided constraint surfaces. The particle is allowed to move inside $g_i \\leq 0$, but whenever it touches $g_i=0$, it is pushed inwards. $h_j$ are two-sided constraint surfaces. The particle is allowed to move only on the surface $h_j$. Primal stationarity states that the \"force\" of $\\partial f(x^*)$ is exactly balanced by a linear sum of forces $\\partial h_j(x^*)$ and $\\partial g_i(x^*)$. Dual feasibility additionally states that all the $\\partial g_i(x^*)$ forces must be one-sided, pointing inwards into the feasible set for $x$. Complementary slackness states that if $g_i(x^*) < 0$, then the force coming from $\\partial g_i(x^*)$ must be zero i.e., $\\mu_i(x^*) = 0$, since the particle is not on the boundary, the one-sided constraint force cannot activate."
            },
            {
                "text": "Matrix representation The necessary conditions can be written with Jacobian matrices of the constraint functions. Let $\\mathbf g(x) : \\,\\!\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be defined as $\\mathbf{g}(x) = \\left( g_{1}(x), \\ldots, g_{m}(x) \\right)^\\top$ and let $\\mathbf h(x) : \\,\\!\\mathbb{R}^n \\rightarrow \\mathbb{R}^{\\ell}$ be defined as $\\mathbf{h}(x) = \\left( h_{1}(x), \\ldots, h_{\\ell}(x) \\right)^\\top$. Let $\\boldsymbol{\\mu} = \\left( \\mu_1, \\ldots, \\mu_m \\right)^\\top$ and $\\boldsymbol{\\lambda} = \\left( \\lambda_1, \\ldots, \\lambda_{\\ell} \\right)^\\top$."
            },
            {
                "text": "Then the necessary conditions can be written as: Stationarity For maximizing $f(x)$: $\\partial f(x^*) - D \\mathbf g(x^*)^{\\top} \\boldsymbol{\\mu} - D \\mathbf h(x^*)^{\\top} \\boldsymbol{\\lambda} = \\mathbf 0$ For minimizing $f(x)$: $\\partial f(x^*) + D \\mathbf g(x^*)^{\\top} \\boldsymbol{\\mu} + D \\mathbf h(x^*)^{\\top} \\boldsymbol{\\lambda} = \\mathbf 0$ Primal feasibility $\\mathbf g(x^*) \\le \\mathbf 0$ $\\mathbf h(x^*) = \\mathbf 0$ Dual feasibility $\\boldsymbol \\mu \\ge \\mathbf 0$ Complementary slackness $\\boldsymbol \\mu^{\\top} \\mathbf g (x^*) = 0.$ Regularity conditions (or constraint qualifications) One can ask whether a minimizer point $x^*$ of the original, constrained optimization problem (assuming one exists) has to satisfy the above KKT conditions."
            },
            {
                "text": "This is similar to asking under what conditions the minimizer $x^*$ of a function $f(x)$ in an unconstrained problem has to satisfy the condition $\\nabla f(x^*)=0$. For the constrained case, the situation is more complicated, and one can state a variety of (increasingly complicated) \"regularity\" conditions under which a constrained minimizer also satisfies the KKT conditions."
            },
            {
                "text": "Some common examples for conditions that guarantee this are tabulated in the following, with the LICQ the most frequently used one: ConstraintAcronymStatementLinearity constraint qualificationLCQIf $g_i$ and $h_j$ are affine functions, then no other condition is needed.Linear independence constraint qualificationLICQThe gradients of the active inequality constraints and the gradients of the equality constraints are linearly independent at $x^*$.Mangasarian-Fromovitz constraint qualificationMFCQThe gradients of the equality constraints are linearly independent at $x^*$ and there exists a vector $d \\in \\mathbb{R}^n$ such that $\\nabla g_i(x^*)^\\top d < 0$ for all active inequality constraints and $\\nabla h_j(x^*)^\\top d = 0$ for all equality constraints.Constant rank constraint qualificationCRCQFor each subset of the gradients of the active inequality constraints and the gradients of the equality constraints the rank at a vicinity of $x^*$ is constant.Constant positive linear dependence constraint qualificationCPLDFor each subset of gradients of active inequality constraints and gradients of equality constraints, if the subset of vectors is linearly dependent at $x^*$ with non-negative scalars associated with the inequality constraints, then it remains linearly dependent in a neighborhood of $x^*$.Quasi-normality constraint qualificationQNCQIf the gradients of the active inequality constraints and the gradients of the equality constraints are linearly dependent at $x^*$ with associated multipliers $\\lambda_j$ for equalities and $\\mu_i\\geq0$ for inequalities, then there is no sequence $x_k\\to x^*$ such that $\\lambda_j \\neq 0 \\Rightarrow \\lambda_j h_j(x_k)>0$ and $\\mu_i \\neq 0 \\Rightarrow \\mu_i g_i(x_k)>0.$Slater's conditionSCFor a convex problem (i.e., assuming minimization, $f,g_i$ are convex and $h_j$ is affine), there exists a point $x$ such that $h_j(x)=0$ and $ g_i(x) < 0."
            },
            {
                "text": "$ The strict implications can be shown LICQ ⇒ MFCQ ⇒ CPLD ⇒ QNCQ and LICQ ⇒ CRCQ ⇒ CPLD ⇒ QNCQ In practice weaker constraint qualifications are preferred since they apply to a broader selection of problems. Sufficient conditions In some cases, the necessary conditions are also sufficient for optimality. In general, the necessary conditions are not sufficient for optimality and additional information is required, such as the Second Order Sufficient Conditions (SOSC). For smooth functions, SOSC involve the second derivatives, which explains its name. The necessary conditions are sufficient for optimality if the objective function $f$ of a maximization problem is a differentiable concave function, the inequality constraints $g_j$ are differentiable convex functions, the equality constraints $h_i$ are affine functions, and Slater's condition holds. Similarly, if the objective function $f$ of a minimization problem is a differentiable convex function, the necessary conditions are also sufficient for optimality. It was shown by Martin in 1985 that the broader class of functions in which KKT conditions guarantees global optimality are the so-called Type 1 invex functions."
            },
            {
                "text": "Second-order sufficient conditions For smooth, non-linear optimization problems, a second order sufficient condition is given as follows. The solution $x^*, \\lambda^*, \\mu^*$ found in the above section is a constrained local minimum if for the Lagrangian, $ L(x,\\lambda,\\mu) = f(x) + \\sum_{i=1}^m \\mu_i g_i(x) + \\sum_{j=1}^\\ell \\lambda_j h_j(x) $ then, $s^T\\nabla ^2_{xx}L(x^*,\\lambda^*,\\mu^*)s \\ge 0$ where $s \\ne 0$ is a vector satisfying the following, $\\left[ \\nabla_x g_i(x^*), \\nabla_x h_j(x^*) \\right]^T s = 0_{\\mathbb{R}^{2}}$ where only those active inequality constraints $g_i(x)$ corresponding to strict complementarity (i.e. where $\\mu_i > 0$) are applied. The solution is a strict constrained local minimum in the case the inequality is also strict. If $s^T\\nabla ^2_{xx}L(x^*,\\lambda^*,\\mu^*)s = 0$, the third order Taylor expansion of the Lagrangian should be used to verify if $x^*$ is a local minimum."
            },
            {
                "text": "The minimization of $f(x_1,x_2)=(x_2-x_1^2)(x_2-3x_1^2)$ is a good counter-example, see also Peano surface. Economics Often in mathematical economics the KKT approach is used in theoretical models in order to obtain qualitative results. For example,Chiang, Alpha C. Fundamental Methods of Mathematical Economics, 3rd edition, 1984, pp. 750–752. consider a firm that maximizes its sales revenue subject to a minimum profit constraint. Letting $Q$ be the quantity of output produced (to be chosen), $R(Q)$ be sales revenue with a positive first derivative and with a zero value at zero output, $C(Q)$ be production costs with a positive first derivative and with a non-negative value at zero output, and $G_{\\min}$ be the positive minimal acceptable level of profit, then the problem is a meaningful one if the revenue function levels off so it eventually is less steep than the cost function."
            },
            {
                "text": "The problem expressed in the previously given minimization form is Minimize $ -R(Q)$ subject to $ G_{\\min} \\le R(Q) - C(Q) $ $ Q \\ge 0,$ and the KKT conditions are $ \\begin{align} & \\left(\\frac{\\text{d} R}{\\text{d} Q} \\right) (1+\\mu ) - \\mu \\left( \\frac{\\text{d} C}{\\text{d} Q} \\right) \\le 0, \\\\[5pt] & Q \\ge 0, \\\\[5pt] & Q \\left[ \\left( \\frac{\\text{d} R}{\\text{d} Q} \\right) (1+\\mu ) - \\mu \\left( \\frac{\\text{d} C}{\\text{d} Q}\\right) \\right] = 0, \\\\[5pt] & R(Q) - C(Q) - G_{\\min} \\ge 0, \\\\[5pt] & \\mu \\ge 0, \\\\[5pt] & \\mu [R(Q) - C(Q) - G_{\\min}] = 0."
            },
            {
                "text": "\\end{align} $ Since $Q = 0$ would violate the minimum profit constraint, we have $Q > 0$ and hence the third condition implies that the first condition holds with equality. Solving that equality gives $ \\frac{\\text{d} R}{\\text{d} Q} = \\frac{\\mu}{1+ \\mu} \\left( \\frac{\\text{d} C}{\\text{d} Q} \\right).$ Because it was given that $\\text{d} R / \\text{d} Q$ and $\\text{d} C / \\text{d} Q$ are strictly positive, this inequality along with the non-negativity condition on $\\mu$ guarantees that $\\mu$ is positive and so the revenue-maximizing firm operates at a level of output at which marginal revenue $ \\text{d} R / \\text{d} Q$ is less than marginal cost $ \\text{d} C / \\text{d} Q$ — a result that is of interest because it contrasts with the behavior of a profit maximizing firm, which operates at a level at which they are equal."
            },
            {
                "text": "Value function If we reconsider the optimization problem as a maximization problem with constant inequality constraints: $ \\text{Maximize }\\; f(x) $ $ \\text{subject to }\\ $ $ g_i(x) \\le a_i , h_j(x) = 0.$ The value function is defined as $V(a_1, \\ldots, a_n) = \\sup\\limits_x f(x) $ $ \\text{subject to }\\ $ $ g_i(x) \\le a_i , h_j(x) = 0 $ $ j \\in \\{1,\\ldots, \\ell\\}, i\\in\\{1,\\ldots,m\\},$ so the domain of $V$ is $\\{a \\in \\mathbb{R}^m \\mid \\text{for some }x\\in X, g_i(x) \\leq a_i, i \\in \\{1,\\ldots,m\\}\\}.$ Given this definition, each coefficient $\\mu_i$ is the rate at which the value function increases as $a_i$ increases. Thus if each $a_i$ is interpreted as a resource constraint, the coefficients tell you how much increasing a resource will increase the optimum value of our function $f$."
            },
            {
                "text": "This interpretation is especially important in economics and is used, for instance, in utility maximization problems. Generalizations With an extra multiplier $\\mu_0\\geq0$, which may be zero (as long as $(\\mu_0,\\mu,\\lambda)\\neq0$), in front of $\\nabla f(x^*)$ the KKT stationarity conditions turn into $ \\begin{align} & \\mu_0 \\,\\nabla f(x^*) + \\sum_{i=1}^m \\mu_i \\,\\nabla g_i(x^*) + \\sum_{j=1}^\\ell \\lambda_j \\,\\nabla h_j(x^*) = 0, \\\\[4pt] & \\mu_jg_i(x^*)=0, \\quad i=1,\\dots,m, \\end{align} $ which are called the Fritz John conditions. This optimality conditions holds without constraint qualifications and it is equivalent to the optimality condition KKT or (not-MFCQ). The KKT conditions belong to a wider class of the first-order necessary conditions (FONC), which allow for non-smooth functions using subderivatives. See also Farkas' lemma Lagrange multiplier The Big M method, for linear problems, which extends the simplex algorithm to problems that contain \"greater-than\" constraints. Interior-point method a method to solve the KKT conditions. Slack variable Slater's condition References Further reading External links Karush–Kuhn–Tucker conditions with derivation and examples Examples and Tutorials on the KKT Conditions Category:Mathematical optimization Category:Mathematical economics"
            }
        ],
        "latex_formulas": [
            "f(\\mathbf{x})",
            "g_i(\\mathbf{x}) \\leq 0,",
            "h_j(\\mathbf{x}) =0.",
            "\\mathbf{x} \\in \\mathbf{X}",
            "\\mathbb{R}^{n}",
            "f",
            "g_i \\ (i = 1, \\ldots, m)",
            "h_j \\ (j = 1, \\ldots, \\ell)",
            "m",
            "\\ell",
            "(\\mathbf{x}^{\\ast},\\mathbf{\\alpha}^\\ast)",
            "L(\\mathbf{x},\\mathbf{\\alpha})",
            "\\mathbf{x} \\in \\mathbf{X}",
            "\\mathbf{\\mu} \\geq \\mathbf{0}",
            "\\mathbf{x}^{\\ast}",
            "f(\\mathbf{x})",
            "g_i(\\mathbf{x})",
            "i = 1, \\ldots, m",
            "\\mathbf{X}",
            "\\mathbf{x}_{0} \\in \\operatorname{relint}(\\mathbf{X})",
            "\\mathbf{g}(\\mathbf{x}_{0}) < \\mathbf 0",
            "\\mathbf{x}^{\\ast}",
            "\\mathbf{\\alpha}^\\ast=\\begin{bmatrix}\\mu^* \\\\ \\lambda^*\\end{bmatrix}",
            "\\mathbf \\mu^*\\ge \\mathbf 0",
            "(\\mathbf{x}^{\\ast},\\mathbf{\\alpha}^\\ast)",
            "L(\\mathbf{x},\\mathbf{\\alpha})",
            "\\mathbf{\\Gamma} = \\left\\{ \\mathbf{x} \\in \\mathbf{X} : g_i(\\mathbf{x}) \\leq 0, i = 1, \\ldots, m \\right\\}",
            "f\\colon\\mathbb{R}^n \\rightarrow \\mathbb{R}",
            "g_i\\colon\\mathbb{R}^n \\rightarrow \\mathbb{R}",
            "h_j\\colon\\mathbb{R}^n \\rightarrow \\mathbb{R}",
            "x^* \\in \\mathbb{R}^n",
            "x^*",
            "\\mu_i\\ (i = 1,\\ldots,m)",
            "\\lambda_j\\ (j = 1,\\ldots,\\ell)",
            "f(x)",
            "\\partial f(x^*) + \\sum_{j=1}^\\ell \\lambda_j \\partial h_j(x^*) + \\sum_{i=1}^m \\mu_i \\partial g_i(x^*) \\ni \\mathbf 0",
            "f(x)",
            "-\\partial f(x^*) + \\sum_{j=1}^\\ell \\lambda_j \\partial h_j(x^*) + \\sum_{i=1}^m \\mu_i \\partial g_i(x^*) \\ni \\mathbf 0",
            "h_j(x^*) = 0, \\text{ for } j = 1, \\ldots, \\ell \\,\\!",
            "g_i(x^*) \\le 0, \\text{ for } i = 1, \\ldots, m",
            "\\mu_i \\ge 0, \\text{ for } i = 1, \\ldots, m",
            "\\sum_{i=1}^m \\mu_i g_i (x^*) = 0.",
            "\\mu_i g_i (x^*) = 0, \\text{ for } i = 1, \\ldots, m.",
            "m=0",
            "x^*",
            "(\\mu^*, \\lambda^*)",
            "x^*, (\\mu^*, \\lambda^*)",
            "x^*",
            "(\\mu^*, \\lambda^*)",
            "x^*, (\\mu^*, \\lambda^*)",
            "x^*, (\\mu^*, \\lambda^*)",
            "(\\mu^*, \\lambda^*)",
            "x",
            "x^*",
            "(\\mu, \\lambda)",
            "x^*, (\\mu^*, \\lambda^*)",
            "x^*, (\\mu^*, \\lambda^*)",
            "x",
            "f",
            "f",
            "-\\partial f",
            "g_i",
            "g_i \\leq 0",
            "g_i=0",
            "h_j",
            "h_j",
            "\\partial f(x^*)",
            "\\partial h_j(x^*)",
            "\\partial g_i(x^*)",
            "\\partial g_i(x^*)",
            "x",
            "g_i(x^*) < 0",
            "\\partial g_i(x^*)",
            "\\mu_i(x^*) = 0",
            "\\mathbf g(x) : \\,\\!\\mathbb{R}^n \\rightarrow \\mathbb{R}^m",
            "\\mathbf{g}(x) = \\left( g_{1}(x), \\ldots, g_{m}(x) \\right)^\\top",
            "\\mathbf h(x) : \\,\\!\\mathbb{R}^n \\rightarrow \\mathbb{R}^{\\ell}",
            "\\mathbf{h}(x) = \\left( h_{1}(x), \\ldots, h_{\\ell}(x) \\right)^\\top",
            "\\boldsymbol{\\mu} = \\left( \\mu_1, \\ldots, \\mu_m \\right)^\\top",
            "\\boldsymbol{\\lambda} = \\left( \\lambda_1, \\ldots, \\lambda_{\\ell} \\right)^\\top",
            "f(x)",
            "\\partial f(x^*) - D \\mathbf g(x^*)^{\\top} \\boldsymbol{\\mu} - D \\mathbf h(x^*)^{\\top} \\boldsymbol{\\lambda} = \\mathbf 0",
            "f(x)",
            "\\partial f(x^*) + D \\mathbf g(x^*)^{\\top} \\boldsymbol{\\mu} + D \\mathbf h(x^*)^{\\top} \\boldsymbol{\\lambda} = \\mathbf 0",
            "\\mathbf g(x^*) \\le \\mathbf 0",
            "\\mathbf h(x^*) = \\mathbf 0",
            "\\boldsymbol \\mu \\ge \\mathbf 0",
            "\\boldsymbol \\mu^{\\top} \\mathbf g (x^*) = 0.",
            "x^*",
            "x^*",
            "f(x)",
            "\\nabla f(x^*)=0",
            "g_i",
            "h_j",
            "x^*",
            "x^*",
            "d \\in \\mathbb{R}^n",
            "\\nabla g_i(x^*)^\\top d < 0",
            "\\nabla h_j(x^*)^\\top d = 0",
            "x^*",
            "x^*",
            "x^*",
            "x^*",
            "\\lambda_j",
            "\\mu_i\\geq0",
            "x_k\\to x^*",
            "\\lambda_j \\neq 0 \\Rightarrow \\lambda_j h_j(x_k)>0",
            "\\mu_i \\neq 0 \\Rightarrow \\mu_i g_i(x_k)>0.",
            "f,g_i",
            "h_j",
            "x",
            "h_j(x)=0",
            "g_i(x) < 0.",
            "f",
            "g_j",
            "h_i",
            "f",
            "x^*, \\lambda^*, \\mu^*",
            "L(x,\\lambda,\\mu) = f(x) + \\sum_{i=1}^m \\mu_i g_i(x) + \\sum_{j=1}^\\ell \\lambda_j h_j(x)",
            "s^T\\nabla ^2_{xx}L(x^*,\\lambda^*,\\mu^*)s \\ge 0",
            "s \\ne 0",
            "\\left[ \\nabla_x g_i(x^*),  \\nabla_x h_j(x^*) \\right]^T s = 0_{\\mathbb{R}^{2}}",
            "g_i(x)",
            "\\mu_i > 0",
            "s^T\\nabla ^2_{xx}L(x^*,\\lambda^*,\\mu^*)s = 0",
            "x^*",
            "f(x_1,x_2)=(x_2-x_1^2)(x_2-3x_1^2)",
            "Q",
            "R(Q)",
            "C(Q)",
            "G_{\\min}",
            "-R(Q)",
            "G_{\\min} \\le R(Q) - C(Q)",
            "Q \\ge 0,",
            "\\begin{align}\n& \\left(\\frac{\\text{d} R}{\\text{d} Q} \\right) (1+\\mu ) - \\mu \\left( \\frac{\\text{d} C}{\\text{d} Q} \\right) \\le 0, \\\\[5pt]\n& Q \\ge 0, \\\\[5pt]\n& Q \\left[ \\left( \\frac{\\text{d} R}{\\text{d} Q} \\right) (1+\\mu ) - \\mu \\left( \\frac{\\text{d} C}{\\text{d} Q}\\right) \\right] = 0, \\\\[5pt]\n& R(Q) - C(Q) - G_{\\min} \\ge 0, \\\\[5pt]\n& \\mu \\ge 0, \\\\[5pt]\n& \\mu [R(Q) - C(Q) - G_{\\min}] = 0.\n\\end{align}",
            "Q = 0",
            "Q > 0",
            "\\frac{\\text{d} R}{\\text{d} Q} = \\frac{\\mu}{1+ \\mu} \\left( \\frac{\\text{d} C}{\\text{d} Q} \\right).",
            "\\text{d} R / \\text{d} Q",
            "\\text{d} C / \\text{d} Q",
            "\\mu",
            "\\mu",
            "\\text{d} R / \\text{d} Q",
            "\\text{d} C / \\text{d} Q",
            "\\text{Maximize }\\; f(x)",
            "\\text{subject to }\\",
            "g_i(x) \\le a_i , h_j(x) = 0.",
            "V(a_1, \\ldots, a_n) = \\sup\\limits_x f(x)",
            "\\text{subject to }\\",
            "g_i(x) \\le a_i , h_j(x) = 0",
            "j \\in \\{1,\\ldots, \\ell\\}, i\\in\\{1,\\ldots,m\\},",
            "V",
            "\\{a \\in \\mathbb{R}^m \\mid \\text{for some }x\\in X, g_i(x) \\leq a_i, i \\in \\{1,\\ldots,m\\}\\}.",
            "\\mu_i",
            "a_i",
            "a_i",
            "f",
            "\\mu_0\\geq0",
            "(\\mu_0,\\mu,\\lambda)\\neq0",
            "\\nabla f(x^*)",
            "\\begin{align}\n& \\mu_0 \\,\\nabla f(x^*) + \\sum_{i=1}^m \\mu_i \\,\\nabla g_i(x^*) + \\sum_{j=1}^\\ell \\lambda_j \\,\\nabla h_j(x^*) = 0, \\\\[4pt]\n& \\mu_jg_i(x^*)=0, \\quad i=1,\\dots,m,\n\\end{align}"
        ]
    },
    "Hinge_loss": {
        "title": "Hinge_loss",
        "chunks": [
            {
                "text": "thumb|The vertical axis represents the value of the Hinge loss (in blue) and zero-one loss (in green) for fixed $t , while the horizontal axis represents the value of the prediction . The plot shows that the Hinge loss penalizes predictions $y < 1$, corresponding to the notion of a margin in a support vector machine. In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). For an intended output $t and a classifier score , the hinge loss of the prediction is defined as $\\ell(y) = \\max(0, 1-t \\cdot y)$ Note that $y$ should be the \"raw\" output of the classifier's decision function, not the predicted class label. For instance, in linear SVMs, $y = \\mathbf{w} \\cdot \\mathbf{x} + b$, where $(\\mathbf{w},b)$ are the parameters of the hyperplane and $\\mathbf{x}$ is the input variable(s)."
            },
            {
                "text": "When and have the same sign (meaning predicts the right class) and $|y| \\ge 1$, the hinge loss $\\ell(y) = 0$. When they have opposite signs, $\\ell(y)$ increases linearly with , and similarly if $|y| < 1$, even if it has the same sign (correct prediction, but not by enough margin). Extensions While binary SVMs are commonly extended to multiclass classification in a one-vs.-all or one-vs.-one fashion, it is also possible to extend the hinge loss itself for such an end. Several different variations of multiclass hinge loss have been proposed. For example, Crammer and Singer defined it for a linear classifier as $\\ell(y) = \\max(0, 1 + \\max_{y \\ne t} \\mathbf{w}_y \\mathbf{x} - \\mathbf{w}_t \\mathbf{x})$, where $t$ is the target label, $\\mathbf{w}_t$ and $\\mathbf{w}_y$ are the model parameters."
            },
            {
                "text": "Weston and Watkins provided a similar definition, but with a sum rather than a max: $\\ell(y) = \\sum_{y \\ne t} \\max(0, 1 + \\mathbf{w}_y \\mathbf{x} - \\mathbf{w}_t \\mathbf{x})$. In structured prediction, the hinge loss can be further extended to structured output spaces. Structured SVMs with margin rescaling use the following variant, where $w$ denotes the SVM's parameters, $y$ the SVM's predictions, the joint feature function, and $Δ$ the Hamming loss: $\\begin{align} \\ell(\\mathbf{y}) & = \\max(0, \\Delta(\\mathbf{y}, \\mathbf{t}) + \\langle \\mathbf{w}, \\phi(\\mathbf{x}, \\mathbf{y}) \\rangle - \\langle \\mathbf{w}, \\phi(\\mathbf{x}, \\mathbf{t}) \\rangle) \\\\ & = \\max(0, \\max_{y \\in \\mathcal{Y}} \\left( \\Delta(\\mathbf{y}, \\mathbf{t}) + \\langle \\mathbf{w}, \\phi(\\mathbf{x}, \\mathbf{y}) \\rangle \\right) - \\langle \\mathbf{w}, \\phi(\\mathbf{x}, \\mathbf{t}) \\rangle) \\end{align}$."
            },
            {
                "text": "Optimization The hinge loss is a convex function, so many of the usual convex optimizers used in machine learning can work with it. It is not differentiable, but has a subgradient with respect to model parameters $w$ of a linear SVM with score function $y = \\mathbf{w} \\cdot \\mathbf{x}$ that is given by $\\frac{\\partial\\ell}{\\partial w_i} = \\begin{cases} -t \\cdot x_i & \\text{if } t \\cdot y < 1, \\\\ 0 & \\text{otherwise}. \\end{cases}$ thumb|Plot of three variants of the hinge loss as a function of $z : the \"ordinary\" variant (blue), its square (green), and the piece-wise smooth version by Rennie and Srebro (red)."
            },
            {
                "text": "The y-axis is the $l(y)$ hinge loss, and the x-axis is the parameter However, since the derivative of the hinge loss at $ty = 1$ is undefined, smoothed versions may be preferred for optimization, such as Rennie and Srebro's $\\ell(y) = \\begin{cases} \\frac{1}{2} - ty & \\text{if} ~~ ty \\le 0, \\\\ \\frac{1}{2} (1 - ty)^2 & \\text{if} ~~ 0 < ty < 1, \\\\ 0 & \\text{if} ~~ 1 \\le ty \\end{cases}$ or the quadratically smoothed $\\ell_\\gamma(y) = \\begin{cases} \\frac{1}{2\\gamma} \\max(0, 1 - ty)^2 & \\text{if} ~~ ty \\ge 1 - \\gamma, \\\\ 1 - \\frac{\\gamma}{2} - ty & \\text{otherwise} \\end{cases}$ suggested by Zhang. The modified Huber loss $L$ is a special case of this loss function with $\\gamma = 2$, specifically $L(t,y) = 4 \\ell_2(y)$. See also References Category:Loss functions Category:Support vector machines"
            }
        ],
        "latex_formulas": [
            "''t'' {{=}} 1",
            "''y'' < 1",
            "''t'' {{=}} ±1",
            "'''w'''",
            "'''y'''",
            "Δ",
            "'''w'''",
            "''z'' {{=}} ''ty''",
            "''l(y)''",
            "\\ell(y) = \\max(0, 1-t \\cdot y)",
            "y",
            "y = \\mathbf{w} \\cdot \\mathbf{x} + b",
            "(\\mathbf{w},b)",
            "\\mathbf{x}",
            "|y| \\ge 1",
            "\\ell(y) = 0",
            "\\ell(y)",
            "|y| < 1",
            "\\ell(y) = \\max(0, 1 + \\max_{y \\ne t} \\mathbf{w}_y \\mathbf{x} - \\mathbf{w}_t \\mathbf{x})",
            "t",
            "\\mathbf{w}_t",
            "\\mathbf{w}_y",
            "\\ell(y) = \\sum_{y \\ne t} \\max(0, 1 + \\mathbf{w}_y \\mathbf{x} - \\mathbf{w}_t \\mathbf{x})",
            "\\begin{align}\n\\ell(\\mathbf{y}) & = \\max(0, \\Delta(\\mathbf{y}, \\mathbf{t}) + \\langle \\mathbf{w}, \\phi(\\mathbf{x}, \\mathbf{y}) \\rangle - \\langle \\mathbf{w}, \\phi(\\mathbf{x}, \\mathbf{t}) \\rangle) \\\\\n                 & = \\max(0, \\max_{y \\in \\mathcal{Y}} \\left( \\Delta(\\mathbf{y}, \\mathbf{t}) + \\langle \\mathbf{w}, \\phi(\\mathbf{x}, \\mathbf{y}) \\rangle \\right) - \\langle \\mathbf{w}, \\phi(\\mathbf{x}, \\mathbf{t}) \\rangle)\n\\end{align}",
            "y = \\mathbf{w} \\cdot \\mathbf{x}",
            "\\frac{\\partial\\ell}{\\partial w_i} = \\begin{cases}\n -t \\cdot x_i & \\text{if } t \\cdot y < 1, \\\\\n 0            & \\text{otherwise}.\n\\end{cases}",
            "ty = 1",
            "\\ell(y) = \\begin{cases}\n\\frac{1}{2} - ty       & \\text{if} ~~ ty \\le 0, \\\\\n\\frac{1}{2} (1 - ty)^2 & \\text{if} ~~ 0 < ty < 1, \\\\\n0                      & \\text{if} ~~ 1 \\le ty\n\\end{cases}",
            "\\ell_\\gamma(y) = \\begin{cases}\n\\frac{1}{2\\gamma} \\max(0, 1 - ty)^2 & \\text{if} ~~ ty \\ge 1 - \\gamma, \\\\\n1 - \\frac{\\gamma}{2} - ty           & \\text{otherwise}\n\\end{cases}",
            "L",
            "\\gamma = 2",
            "L(t,y) = 4 \\ell_2(y)"
        ]
    },
    "Nearest_neighbor_search": {
        "title": "Nearest_neighbor_search",
        "chunks": [
            {
                "text": "Nearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points. Most commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric."
            },
            {
                "text": "However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold. Applications The nearest neighbor search problem arises in numerous fields of application, including: Pattern recognition – in particular for optical character recognition Statistical classification – see k-nearest neighbor algorithm Computer vision – for point cloud registrationQiu, Deyuan, Stefan May, and Andreas Nüchter. \"GPU-accelerated nearest neighbor search for 3D registration.\" International conference on computer vision systems. Springer, Berlin, Heidelberg, 2009. Computational geometry – see Closest pair of points problem Cryptanalysis – for lattice problemBecker, Ducas, Gama, and Laarhoven. \"New directions in nearest neighbor searching with applications to lattice sieving.\" Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms (pp. 10-24). Society for Industrial and Applied Mathematics. Databases – e.g. content-based image retrieval Coding theory – see maximum likelihood decoding Semantic Search Data compression – see MPEG-2 standard Robotic sensing Recommendation systems, e.g. see Collaborative filtering Internet marketing – see contextual advertising and behavioral targeting DNA sequencing Spell checking – suggesting correct spelling Plagiarism detection Similarity scores for predicting career paths of professional athletes."
            },
            {
                "text": "Cluster analysis – assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense, usually based on Euclidean distance Chemical similarity Sampling-based motion planning Methods Various solutions to the NNS problem have been proposed. The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the curse of dimensionality states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time. Exact methods Linear search The simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the \"best so far\". This algorithm, sometimes referred to as the naive approach, has a running time of O(dN), where N is the cardinality of S and d is the dimensionality of S. There are no search data structures to maintain, so the linear search has no space complexity beyond the storage of the database."
            },
            {
                "text": "Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces. The absolute distance is not required for distance comparison, only the relative distance. In geometric coordinate systems the distance calculation can be sped up considerably by omitting the square root calculation from the distance calculation between two coordinates. The distance comparison will still yield identical results. Space partitioning Since the 1970s, the branch and bound methodology has been applied to the problem. In the case of Euclidean space, this approach encompasses spatial index or spatial access methods. Several space-partitioning methods have been developed for solving the NNS problem. Perhaps the simplest is the k-d tree, which iteratively bisects the search space into two regions containing half of the points of the parent region. Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is O(log N) in the case of randomly distributed points, worst case complexity is O(kN^(1-1/k)) Alternatively the R-tree data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the R* tree."
            },
            {
                "text": "R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances. In the case of general metric space, the branch-and-bound approach is known as the metric tree approach. Particular examples include vp-tree and BK-tree methods. Using a set of points taken from a 3-dimensional space and put into a BSP tree, and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm. (Strictly speaking, no such point may exist, because it may not be unique. But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.) The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point. This may not be the case, but it is a good heuristic. After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane."
            },
            {
                "text": "This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched. If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space. If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result. The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result. Approximation methods An approximate nearest neighbor search algorithm is allowed to return points whose distance from the query is at most $c$ times the distance from the query to its nearest points. The appeal of this approach is that, in many cases, an approximate nearest neighbor is almost as good as the exact one."
            },
            {
                "text": "In particular, if the distance measure accurately captures the notion of user quality, then small differences in the distance should not matter. Greedy search in proximity neighborhood graphs Proximity graph methods (such as navigable small world graphs and HNSW) are considered the current state-of-the-art for the approximate nearest neighbors search. The methods are based on greedy traversing in proximity neighborhood graphs $G(V,E)$ in which every point $x_i \\in S $ is uniquely associated with vertex $v_i \\in V $. The search for the nearest neighbors to a query q in the set S takes the form of searching for the vertex in the graph $G(V,E)$. The basic algorithm – greedy search – works as follows: search starts from an enter-point vertex $v_i \\in V $ by computing the distances from the query q to each vertex of its neighborhood $\\{v_j:(v_i,v_j) \\in E\\}$, and then finds a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new enter-point."
            },
            {
                "text": "The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself. The idea of proximity neighborhood graphs was exploited in multiple publications, including the seminal paper by Arya and Mount, in the VoroNet system for the plane, in the RayNet system for the $\\mathbb{E}^n$, and in the Navigable Small World, Metrized Small World and HNSW algorithms for the general case of spaces with a distance function. These works were preceded by a pioneering paper by Toussaint, in which he introduced the concept of a relative neighborhood graph. Locality sensitive hashing Locality sensitive hashing (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability. Nearest neighbor search in spaces with small intrinsic dimension The cover tree has a theoretical bound that is based on the dataset's doubling constant. The bound on search time is O(c12 log n) where c is the expansion constant of the dataset."
            },
            {
                "text": "Projected radial search In the special case where the data is a dense 3D map of geometric points, the projection geometry of the sensing technique can be used to dramatically simplify the search problem. This approach requires that the 3D data is organized by a projection to a two-dimensional grid and assumes that the data is spatially smooth across neighboring grid cells with the exception of object boundaries. These assumptions are valid when dealing with 3D sensor data in applications such as surveying, robotics and stereo vision but may not hold for unorganized data in general. In practice this technique has an average search time of O(1) or O(K) for the k-nearest neighbor problem when applied to real world stereo vision data. Vector approximation files In high-dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation."
            },
            {
                "text": "Compression/clustering based search The VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is Vector Quantization (VQ), implemented through clustering. The database is clustered and the most \"promising\" clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed. Also note the parallels between clustering and LSH. Variants There are numerous variants of the NNS problem and the two most well-known are the k-nearest neighbor search and the ε-approximate nearest neighbor search. k-nearest neighbors k-nearest neighbor search identifies the top k nearest neighbors to the query. This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. k-nearest neighbor graphs are graphs in which every point is connected to its k nearest neighbors. Approximate nearest neighbor In some applications it may be acceptable to retrieve a \"good guess\" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings."
            },
            {
                "text": "Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried. Algorithms that support the approximate nearest neighbor search include locality-sensitive hashing, best bin first and balanced box-decomposition tree based search. Nearest neighbor distance ratio Nearest neighbor distance ratio does not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in CBIR to retrieve pictures through a \"query by example\" using the similarity between local features. More generally it is involved in several matching problems. Fixed-radius near neighbors Fixed-radius near neighbors is the problem where one wants to efficiently find all points given in Euclidean space within a given fixed distance from a specified point. The distance is assumed to be fixed, but the query point is arbitrary. All nearest neighbors For some applications (e.g. entropy estimation), we may have N data-points and wish to know which is the nearest neighbor for every one of those N points."
            },
            {
                "text": "This could, of course, be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these N queries to produce a more efficient search. As a simple example: when we find the distance from point X to point Y, that also tells us the distance from point Y to point X, so the same calculation can be reused in two different queries. Given a fixed dimension, a semi-definite positive norm (thereby including every Lp norm), and n points in this space, the nearest neighbour of every point can be found in O(n log n) time and the m nearest neighbours of every point can be found in O(mn log n) time.. See also Ball tree Closest pair of points problem Cluster analysis Content-based image retrieval Curse of dimensionality Digital signal processing Dimension reduction Fixed-radius near neighbors Fourier analysis Instance-based learning k-nearest neighbor algorithm Linear least squares Locality sensitive hashing Maximum inner-product search MinHash Multidimensional analysis Nearest-neighbor interpolation Neighbor joining Principal component analysis Range search Similarity learning Singular value decomposition Sparse distributed memory Statistical distance Time series Voronoi diagram Wavelet References Citations Sources Further reading External links Nearest Neighbors and Similarity Search – a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching."
            },
            {
                "text": "Maintained by Yury Lifshits Similarity Search Wiki – a collection of links, people, ideas, keywords, papers, slides, code and data sets on nearest neighbours Category:Approximation algorithms Category:Classification algorithms Category:Data mining Category:Discrete geometry Category:Geometric algorithms Category:Mathematical optimization Category:Search algorithms"
            }
        ],
        "latex_formulas": [
            "c",
            "G(V,E)",
            "x_i \\in S",
            "v_i \\in V",
            "G(V,E)",
            "v_i \\in V",
            "\\{v_j:(v_i,v_j) \\in E\\}",
            "\\mathbb{E}^n"
        ]
    },
    "Euclidean_distance": {
        "title": "Euclidean_distance",
        "chunks": [
            {
                "text": "Using the Pythagorean theorem to compute two-dimensional Euclidean distance In mathematics, the Euclidean distance between two points in Euclidean space is the length of the line segment between them. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, and therefore is occasionally called the Pythagorean distance. These names come from the ancient Greek mathematicians Euclid and Pythagoras. In the Greek deductive geometry exemplified by Euclid's Elements, distances were not represented as numbers but line segments of the same length, which were considered \"equal\". The notion of distance is inherent in the compass tool used to draw a circle, whose points all have the same distance from a common center point. The connection from the Pythagorean theorem to distance calculation was not made until the 18th century. The distance between two objects that are not points is usually defined to be the smallest distance among pairs of points from the two objects. Formulas are known for computing distances between different types of objects, such as the distance from a point to a line. In advanced mathematics, the concept of distance has been generalized to abstract metric spaces, and other distances than Euclidean have been studied."
            },
            {
                "text": "In some applications in statistics and optimization, the square of the Euclidean distance is used instead of the distance itself. Distance formulas One dimension The distance between any two points on the real line is the absolute value of the numerical difference of their coordinates, their absolute difference. Thus if $p$ and $q$ are two points on the real line, then the distance between them is given by: A more complicated formula, giving the same value, but generalizing more readily to higher dimensions, is: In this formula, squaring and then taking the square root leaves any positive number unchanged, but replaces any negative number by its absolute value. Two dimensions In the Euclidean plane, let point $p$ have Cartesian coordinates $(p_1,p_2)$ and let point $q$ have coordinates $(q_1,q_2)$. Then the distance between $p$ and $q$ is given by: This can be seen by applying the Pythagorean theorem to a right triangle with horizontal and vertical sides, having the line segment from $p$ to $q$ as its hypotenuse."
            },
            {
                "text": "The two squared formulas inside the square root give the areas of squares on the horizontal and vertical sides, and the outer square root converts the area of the square on the hypotenuse into the length of the hypotenuse. In terms of the Pythagorean addition operation $\\oplus$, available in many software libraries as hypot, the same formula can be expressed as: It is also possible to compute the distance for points given by polar coordinates. If the polar coordinates of $p$ are $(r,\\theta)$ and the polar coordinates of $q$ are $(s,\\psi)$, then their distance is given by the law of cosines: When $p$ and $q$ are expressed as complex numbers in the complex plane, the same formula for one-dimensional points expressed as real numbers can be used, although here the absolute value sign indicates the complex norm: Higher dimensions Deriving the $n$-dimensional Euclidean distance formula by repeatedly applying the Pythagorean theorem In three dimensions, for points given by their Cartesian coordinates, the distance is In general, for points given by Cartesian coordinates in $n$-dimensional Euclidean space, the distance is The Euclidean distance may also be expressed more compactly in terms of the Euclidean norm of the Euclidean vector difference: Objects other than points For pairs of objects that are not both points, the distance can most simply be defined as the smallest distance between any two points from the two objects, although more complicated generalizations from points to sets such as Hausdorff distance are also commonly used."
            },
            {
                "text": "Formulas for computing distances between different types of objects include: The distance from a point to a line, in the Euclidean plane The distance from a point to a plane in three-dimensional Euclidean space The distance between two lines in three-dimensional Euclidean space The distance from a point to a curve can be used to define its parallel curve, another curve all of whose points have the same distance to the given curve. Properties The Euclidean distance is the prototypical example of the distance in a metric space, and obeys all the defining properties of a metric space: It is symmetric, meaning that for all points $p$ and $q$, $d(p,q)=d(q,p)$. That is (unlike road distance with one-way streets) the distance between two points does not depend on which of the two points is the start and which is the destination. It is positive, meaning that the distance between every two distinct points is a positive number, while the distance from any point to itself is zero. It obeys the triangle inequality: for every three points $p$, $q$, and $r$, $d(p,q)+d(q,r)\\ge d(p,r)$."
            },
            {
                "text": "Intuitively, traveling from $p$ to $r$ via $q$ cannot be any shorter than traveling directly from $p$ to $r$. Another property, Ptolemy's inequality, concerns the Euclidean distances among four points $p$, $q$, $r$, and $s$. It states that For points in the plane, this can be rephrased as stating that for every quadrilateral, the products of opposite sides of the quadrilateral sum to at least as large a number as the product of its diagonals. However, Ptolemy's inequality applies more generally to points in Euclidean spaces of any dimension, no matter how they are arranged. For points in metric spaces that are not Euclidean spaces, this inequality may not be true. Euclidean distance geometry studies properties of Euclidean distance such as Ptolemy's inequality, and their application in testing whether given sets of distances come from points in a Euclidean space. According to the Beckman–Quarles theorem, any transformation of the Euclidean plane or of a higher-dimensional Euclidean space that preserves unit distances must be an isometry, preserving all distances."
            },
            {
                "text": "Squared Euclidean distance In many applications, and in particular when comparing distances, it may be more convenient to omit the final square root in the calculation of Euclidean distances, as the square root does not change the order ($d_1^2 > d_2^2$ if and only if $d_1 > d_2$). The value resulting from this omission is the square of the Euclidean distance, and is called the squared Euclidean distance. For instance, the Euclidean minimum spanning tree can be determined using only the ordering between distances, and not their numeric values. Comparing squared distances produces the same result but avoids an unnecessary square-root calculation and sidesteps issues of numerical precision. As an equation, the squared distance can be expressed as a sum of squares: Beyond its application to distance comparison, squared Euclidean distance is of central importance in statistics, where it is used in the method of least squares, a standard method of fitting statistical estimates to data by minimizing the average of the squared distances between observed and estimated values, and as the simplest form of divergence to compare probability distributions."
            },
            {
                "text": "The addition of squared distances to each other, as is done in least squares fitting, corresponds to an operation on (unsquared) distances called Pythagorean addition. In cluster analysis, squared distances can be used to strengthen the effect of longer distances. Squared Euclidean distance does not form a metric space, as it does not satisfy the triangle inequality. However it is a smooth, strictly convex function of the two points, unlike the distance, which is non-smooth (near pairs of equal points) and convex but not strictly convex. The squared distance is thus preferred in optimization theory, since it allows convex analysis to be used. Since squaring is a monotonic function of non-negative values, minimizing squared distance is equivalent to minimizing the Euclidean distance, so the optimization problem is equivalent in terms of either, but easier to solve using squared distance. The collection of all squared distances between pairs of points from a finite set may be stored in a Euclidean distance matrix, and is used in this form in distance geometry. Generalizations In more advanced areas of mathematics, when viewing Euclidean space as a vector space, its distance is associated with a norm called the Euclidean norm, defined as the distance of each vector from the origin."
            },
            {
                "text": "One of the important properties of this norm, relative to other norms, is that it remains unchanged under arbitrary rotations of space around the origin. By Dvoretzky's theorem, every finite-dimensional normed vector space has a high-dimensional subspace on which the norm is approximately Euclidean; the Euclidean norm is the only norm with this property. It can be extended to infinite-dimensional vector spaces as the $L2$ norm or $L2$ distance. The Euclidean distance gives Euclidean space the structure of a topological space, the Euclidean topology, with the open balls (subsets of points at less than a given distance from a given point) as its neighborhoods. thumb|Comparison of Chebyshev, Euclidean and taxicab distances for the hypotenuse of a 3-4-5 triangle on a chessboard Other common distances in real coordinate spaces and function spaces: Chebyshev distance ($L∞$ distance), which measures distance as the maximum of the distances in each coordinate. Taxicab distance ($L1$ distance), also called Manhattan distance, which measures distance as the sum of the distances in each coordinate. Minkowski distance ($Lp$ distance), a generalization that unifies Euclidean distance, taxicab distance, and Chebyshev distance."
            },
            {
                "text": "For points on surfaces in three dimensions, the Euclidean distance should be distinguished from the geodesic distance, the length of a shortest curve that belongs to the surface. In particular, for measuring great-circle distances on the Earth or other spherical or near-spherical surfaces, distances that have been used include the haversine distance giving great-circle distances between two points on a sphere from their longitudes and latitudes, and Vincenty's formulae also known as \"Vincent distance\" for distance on a spheroid. History Euclidean distance is the distance in Euclidean space. Both concepts are named after ancient Greek mathematician Euclid, whose Elements became a standard textbook in geometry for many centuries. Concepts of length and distance are widespread across cultures, can be dated to the earliest surviving \"protoliterate\" bureaucratic documents from Sumer in the fourth millennium BC (far before Euclid), and have been hypothesized to develop in children earlier than the related concepts of speed and time. But the notion of a distance, as a number defined from two points, does not actually appear in Euclid's Elements. Instead, Euclid approaches this concept implicitly, through the congruence of line segments, through the comparison of lengths of line segments, and through the concept of proportionality. The Pythagorean theorem is also ancient, but it could only take its central role in the measurement of distances after the invention of Cartesian coordinates by René Descartes in 1637. The distance formula itself was first published in 1731 by Alexis Clairaut. Because of this formula, Euclidean distance is also sometimes called Pythagorean distance."
            },
            {
                "text": "Although accurate measurements of long distances on the Earth's surface, which are not Euclidean, had again been studied in many cultures since ancient times (see history of geodesy), the idea that Euclidean distance might not be the only way of measuring distances between points in mathematical spaces came even later, with the 19th-century formulation of non-Euclidean geometry. The definition of the Euclidean norm and Euclidean distance for geometries of more than three dimensions also first appeared in the 19th century, in the work of Augustin-Louis Cauchy. References Category:Distance Category:Length Category:Metric geometry Category:Pythagorean theorem distance"
            }
        ],
        "latex_formulas": [
            "''L''<sup>2</sup>",
            "''L''<sup>2</sup>",
            "''L''<sup>∞</sup>",
            "''L''<sup>1</sup>",
            "''L''<sup>''p''</sup>",
            "p",
            "q",
            "p",
            "(p_1,p_2)",
            "q",
            "(q_1,q_2)",
            "p",
            "q",
            "p",
            "q",
            "\\oplus",
            "p",
            "(r,\\theta)",
            "q",
            "(s,\\psi)",
            "p",
            "q",
            "n",
            "n",
            "p",
            "q",
            "d(p,q)=d(q,p)",
            "p",
            "q",
            "r",
            "d(p,q)+d(q,r)\\ge d(p,r)",
            "p",
            "r",
            "q",
            "p",
            "r",
            "p",
            "q",
            "r",
            "s",
            "d_1^2 > d_2^2",
            "d_1 > d_2"
        ]
    },
    "Manhattan_distance": {
        "title": "Manhattan_distance",
        "chunks": [
            {
                "text": "REDIRECT Taxicab geometry"
            }
        ],
        "latex_formulas": []
    },
    "Minkowski_distance": {
        "title": "Minkowski_distance",
        "chunks": [
            {
                "text": "The Minkowski distance or Minkowski metric is a metric in a normed vector space which can be considered as a generalization of both the Euclidean distance and the Manhattan distance. It is named after the Polish mathematician Hermann Minkowski. thumb|Comparison of Chebyshev, Euclidean and taxicab/Manhattan distances for the hypotenuse of a 3-4-5 triangle on a chessboard Definition The Minkowski distance of order $p$ (where $p$ is an integer) between two points is defined as: For $p \\geq 1,$ the Minkowski distance is a metric as a result of the Minkowski inequality. When $p < 1,$ the distance between $(0, 0)$ and $(1, 1)$ is $2^{1/p} > 2,$ but the point $(0, 1)$ is at a distance $1$ from both of these points. Since this violates the triangle inequality, for $p < 1$ it is not a metric. However, a metric can be obtained for these values by simply removing the exponent of $1/p.$ The resulting metric is also an F-norm."
            },
            {
                "text": "Minkowski distance is typically used with $p$ being 1 or 2, which correspond to the Manhattan distance and the Euclidean distance, respectively. In the limiting case of $p$ reaching infinity, we obtain the Chebyshev distance: Similarly, for $p$ reaching negative infinity, we have: The Minkowski distance can also be viewed as a multiple of the power mean of the component-wise differences between $P$ and $Q.$ The following figure shows unit circles (the level set of the distance function where all points are at the unit distance from the center) with various values of $p$: Applications The Minkowski metric is very useful in the field of machine learning and AI. Many popular machine learning algorithms use specific distance metrics such as the aforementioned to compare the similarity of two data points. Depending on the nature of the data being analyzed, various metrics can be used. The Minkowski metric is most useful for numerical datasets where one wants to determine the similarity of size between multiple datapoint vectors. See also References External links Unit Balls for Different p-Norms in 2D and 3D at wolfram.com Unit-Norm Vectors under Different p-Norms at wolfram.com Simple IEEE 754 implementation in C++ NPM JavaScript Package/Module Category:Distance Category:Hermann Minkowski Category:Metric geometry Category:Normed spaces"
            }
        ],
        "latex_formulas": [
            "p",
            "p",
            "p \\geq 1,",
            "p < 1,",
            "(0, 0)",
            "(1, 1)",
            "2^{1/p} > 2,",
            "(0, 1)",
            "1",
            "p < 1",
            "1/p.",
            "p",
            "p",
            "p",
            "P",
            "Q.",
            "p",
            "L^p",
            "p"
        ]
    },
    "Distance_measure": {
        "title": "Distance_measure",
        "chunks": [
            {
                "text": "Distance measures are used in physical cosmology to give a natural notion of the distance between two objects or events in the universe. They are often used to tie some observable quantity (such as the luminosity of a distant quasar, the redshift of a distant galaxy, or the angular size of the acoustic peaks in the cosmic microwave background (CMB) power spectrum) to another quantity that is not directly observable, but is more convenient for calculations (such as the comoving coordinates of the quasar, galaxy, etc.). The distance measures discussed here all reduce to the common notion of Euclidean distance at low redshift. In accord with our present understanding of cosmology, these measures are calculated within the context of general relativity, where the Friedmann–Lemaître–Robertson–Walker solution is used to describe the universe. Overview There are a few different definitions of \"distance\" in cosmology which are all asymptotic one to another for small redshifts. The expressions for these distances are most practical when written as functions of redshift $z$, since redshift is always the observable. They can also be written as functions of scale factor $a=1/(1+z).$ In the remainder of this article, the peculiar velocity is assumed to be negligible unless specified otherwise."
            },
            {
                "text": "We first give formulas for several distance measures, and then describe them in more detail further down. Defining the \"Hubble distance\" as where $c$ is the speed of light, $H_0$ is the Hubble parameter today, and is the dimensionless Hubble constant, all the distances are asymptotic to $z\\cdot d_H$ for small . According to the Friedmann equations, we also define a dimensionless Hubble parameter: Here, $\\Omega_r, \\Omega_m,$ and $\\Omega_\\Lambda$ are normalized values of the present radiation energy density, matter density, and \"dark energy density\", respectively (the latter representing the cosmological constant), and $\\Omega_k = 1-\\Omega_r-\\Omega_m-\\Omega_\\Lambda$ determines the curvature. The Hubble parameter at a given redshift is then $H(z) = H_0E(z)$. The formula for comoving distance, which serves as the basis for most of the other formulas, involves an integral. Although for some limited choices of parameters (see below) the comoving distance integral has a closed analytic form, in general—and specifically for the parameters of our universe—we can only find a solution numerically."
            },
            {
                "text": "Cosmologists commonly use the following measures for distances from the observer to an object at redshift $z$ along the line of sight (LOS): Comoving distance: Transverse comoving distance: Angular diameter distance: Luminosity distance: Light-travel distance: 400px|A comparison of cosmological distance measures, from redshift zero to redshift of 0.5. The background cosmology is Hubble parameter 72 km/s/Mpc, $\\Omega_\\Lambda=0.732$, $\\Omega_{\\rm matter}=0.266$, $\\Omega_{\\rm radiation}=0.266/3454$, and $\\Omega_k$ chosen so that the sum of Omega parameters is 1. Edwin Hubble made use of galaxies up to a redshift of a bit over 0.003 (Messier 60). 400px|A comparison of cosmological distance measures, from redshift zero to redshift of 10,000, corresponding to the epoch of matter/radiation equality. The background cosmology is Hubble parameter 72 km/s/Mpc, $\\Omega_\\Lambda=0.732$, $\\Omega_{\\rm matter}=0.266$, $\\Omega_{\\rm radiation}=0.266/3454$, and $\\Omega_k$ chosen so that the sum of Omega parameters is one. Alternative terminology Peebles calls the transverse comoving distance the \"angular size distance\", which is not to be mistaken for the angular diameter distance."
            },
            {
                "text": "Occasionally, the symbols $\\chi$ or $r$ are used to denote both the comoving and the angular diameter distance. Sometimes, the light-travel distance is also called the \"lookback distance\" and/or \"lookback time\". Details Peculiar velocity In real observations, the movement of the Earth with respect to the Hubble flow has an effect on the observed redshift. There are actually two notions of redshift. One is the redshift that would be observed if both the Earth and the object were not moving with respect to the \"comoving\" surroundings (the Hubble flow), defined by the cosmic microwave background. The other is the actual redshift measured, which depends both on the peculiar velocity of the object observed and on their peculiar velocity. Since the Solar System is moving at around 370 km/s in a direction between Leo and Crater, this decreases $1+z$ for distant objects in that direction by a factor of about 1.0012 and increases it by the same factor for distant objects in the opposite direction. (The speed of the motion of the Earth around the Sun is only 30 km/s.)"
            },
            {
                "text": "Comoving distance The comoving distance $d_C$ between fundamental observers, i.e. observers that are both moving with the Hubble flow, does not change with time, as comoving distance accounts for the expansion of the universe. Comoving distance is obtained by integrating the proper distances of nearby fundamental observers along the line of sight (LOS), whereas the proper distance is what a measurement at constant cosmic time would yield. In standard cosmology, comoving distance and proper distance are two closely related distance measures used by cosmologists to measure distances between objects; the comoving distance is the proper distance at the present time. The comoving distance (with a small correction for our own motion) is the distance that would be obtained from parallax, because the parallax in degrees equals the ratio of an astronomical unit to the circumference of a circle at the present time going through the sun and centred on the distant object, multiplied by 360°. However, objects beyond a megaparsec have parallax too small to be measured (the Gaia space telescope measures the parallax of the brightest stars with a precision of 7 microarcseconds), so the parallax of galaxies outside our Local Group is too small to be measured."
            },
            {
                "text": "There is a closed-form expression for the integral in the definition of the comoving distance if $\\Omega_r=\\Omega_m=0$ or, by substituting the scale factor $a$ for $1/(1+z)$, if $\\Omega_\\Lambda=0$. Our universe now seems to be closely represented by $\\Omega_r=\\Omega_k=0.$ In this case, we have: where The comoving distance should be calculated using the value of that would pertain if neither the object nor we had a peculiar velocity. Together with the scale factor it gives the proper distance of the object when the light we see now was emitted by the it, and set off on its journey to us: Proper distance Proper distance roughly corresponds to where a distant object would be at a specific moment of cosmological time, which can change over time due to the expansion of the universe. Comoving distance factors out the expansion of the universe, which gives a distance that does not change in time due to the expansion of space (though this may change due to other, local factors, such as the motion of a galaxy within a cluster); the comoving distance is the proper distance at the present time."
            },
            {
                "text": "Transverse comoving distance Two comoving objects at constant redshift $z$ that are separated by an angle $\\delta\\theta$ on the sky are said to have the distance $\\delta\\theta d_M(z)$, where the transverse comoving distance $d_M$ is defined appropriately. Angular diameter distance An object of size $x$ at redshift $z$ that appears to have angular size $\\delta\\theta$ has the angular diameter distance of $d_A(z)=x/\\delta\\theta$. This is commonly used to observe so called standard rulers, for example in the context of baryon acoustic oscillations. When accounting for the earth's peculiar velocity, the redshift that would pertain in that case should be used but $d_A$ should be corrected for the motion of the solar system by a factor between 0.99867 and 1.00133, depending on the direction. (If one starts to move with velocity towards an object, at any distance, the angular diameter of that object decreases by a factor of ) Luminosity distance If the intrinsic luminosity $L$ of a distant object is known, we can calculate its luminosity distance by measuring the flux $S$ and determine , which turns out to be equivalent to the expression above for $d_L(z)$."
            },
            {
                "text": "This quantity is important for measurements of standard candles like type Ia supernovae, which were first used to discover the acceleration of the expansion of the universe. When accounting for the earth's peculiar velocity, the redshift that would pertain in that case should be used for $d_M,$ but the factor $(1+z)$ should use the measured redshift, and another correction should be made for the peculiar velocity of the object by multiplying by where now is the component of the object's peculiar velocity away from us. In this way, the luminosity distance will be equal to the angular diameter distance multiplied by $(1+z)^2,$ where is the measured redshift, in accordance with Etherington's reciprocity theorem (see below). Light-travel distance (also known as \"lookback time\" or \"lookback distance\") This distance $d_T$ is the time that it took light to reach the observer from the object multiplied by the speed of light. For instance, the radius of the observable universe in this distance measure becomes the age of the universe multiplied by the speed of light (1 light year/year), which turns out to be approximately 13.8 billion light years."
            },
            {
                "text": "There is a closed-form solution of the light-travel distance if $\\Omega_r = \\Omega_m = 0$ involving the inverse hyperbolic functions $\\text{arcosh}$ or $\\text{arsinh}$ (or involving inverse trigonometric functions if the cosmological constant has the other sign). If $\\Omega_r = \\Omega_\\Lambda = 0$ then there is a closed-form solution for $d_T(z)$ but not for $z(d_T).$ Note that the comoving distance is recovered from the transverse comoving distance by taking the limit $\\Omega_k \\to 0$, such that the two distance measures are equivalent in a flat universe. There are websites for calculating light-travel distance from redshift. Light travel distance was calculated from redshift value using the UCLA Cosmological Calculator, with parameters values as of 2015: H0=67.74 and OmegaM=0.3089 (see Table/Planck2015 at \"Lambda-CDM model#Parameters\" ) Light travel distance was calculated from redshift value using the UCLA Cosmological Calculator, with parameters values as of 2018: H0=67.4 and OmegaM=0.315 (see Table/Planck2018 at \"Lambda-CDM model#Parameters\" ) ICRAR Cosmology Calculator - Set H0=67.4 and OmegaM=0.315 (see Table/Planck2018 at \"Lambda-CDM model#Parameters\") KEMP Cosmology Calculator - Set H0=67.4, OmegaM=0.315, and OmegaΛ=0.6847 (see Table/Planck2018 at \"Lambda-CDM model#Parameters\") The age of the universe then becomes $\\lim_{z\\to\\infty} d_T(z)/c$, and the time elapsed since redshift $z$ until now is: $ t(z) = d_T(z)/c.$ Etherington's distance duality The Etherington's distance-duality equation I.M.H. Etherington, “LX."
            },
            {
                "text": "On the Definition of Distance in General Relativity”, Philosophical Magazine, Vol. 15, S. 7 (1933), pp. 761-773. is the relationship between the luminosity distance of standard candles and the angular-diameter distance. It is expressed as follows: $d_L = (1+z)^2 d_A $ See also Big Bang Comoving and proper distances Friedmann equations Parsec Physical cosmology Cosmic distance ladder Friedmann–Lemaître–Robertson–Walker metric Subatomic scale References Scott Dodelson, Modern Cosmology. Academic Press (2003). External links 'The Distance Scale of the Universe' compares different cosmological distance measures. 'Distance measures in cosmology' explains in detail how to calculate the different distance measures as a function of world model and redshift. iCosmos: Cosmology Calculator (With Graph Generation ) calculates the different distance measures as a function of cosmological model and redshift, and generates plots for the model from redshift 0 to 20. Category:Physical cosmology Category:Physical quantities Category:Length, distance, or range measuring devices"
            }
        ],
        "latex_formulas": [
            "z",
            "a=1/(1+z).",
            "c",
            "H_0",
            "z\\cdot d_H",
            "\\Omega_r, \\Omega_m,",
            "\\Omega_\\Lambda",
            "\\Omega_k = 1-\\Omega_r-\\Omega_m-\\Omega_\\Lambda",
            "H(z) = H_0E(z)",
            "z",
            "\\Omega_\\Lambda=0.732",
            "\\Omega_{\\rm matter}=0.266",
            "\\Omega_{\\rm radiation}=0.266/3454",
            "\\Omega_k",
            "\\Omega_\\Lambda=0.732",
            "\\Omega_{\\rm matter}=0.266",
            "\\Omega_{\\rm radiation}=0.266/3454",
            "\\Omega_k",
            "\\chi",
            "r",
            "1+z",
            "d_C",
            "\\Omega_r=\\Omega_m=0",
            "a",
            "1/(1+z)",
            "\\Omega_\\Lambda=0",
            "\\Omega_r=\\Omega_k=0.",
            "z",
            "\\delta\\theta",
            "\\delta\\theta d_M(z)",
            "d_M",
            "x",
            "z",
            "\\delta\\theta",
            "d_A(z)=x/\\delta\\theta",
            "d_A",
            "L",
            "S",
            "d_L(z)",
            "d_M,",
            "(1+z)",
            "(1+z)^2,",
            "d_T",
            "\\Omega_r = \\Omega_m = 0",
            "\\text{arcosh}",
            "\\text{arsinh}",
            "\\Omega_r = \\Omega_\\Lambda = 0",
            "d_T(z)",
            "z(d_T).",
            "\\Omega_k \\to 0",
            "\\lim_{z\\to\\infty} d_T(z)/c",
            "z",
            "t(z) = d_T(z)/c.",
            "d_L = (1+z)^2 d_A"
        ]
    },
    "Majority_voting": {
        "title": "Majority_voting",
        "chunks": [
            {
                "text": "REDIRECT majority rule"
            }
        ],
        "latex_formulas": []
    },
    "Standard_score": {
        "title": "Standard_score",
        "chunks": [
            {
                "text": "Comparison of the various grading methods in a normal distribution, including: standard deviations, cumulative percentages, percentile equivalents, z-scores, T-scores In statistics, the standard score or z-score is the number of standard deviations by which the value of a raw score (i.e., an observed value or data point) is above or below the mean value of what is being observed or measured. Raw scores above the mean have positive standard scores, while those below the mean have negative standard scores. It is calculated by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This process of converting a raw score into a standard score is called standardizing or normalizing (however, \"normalizing\" can refer to many types of ratios; see Normalization for more). Standard scores are most commonly called z-scores; the two terms may be used interchangeably, as they are in this article. Other equivalent terms in use include z-value, z-statistic, normal score, standardized variable and pull in high energy physics. Computing a z-score requires knowledge of the mean and standard deviation of the complete population to which a data point belongs; if one only has a sample of observations from the population, then the analogous computation using the sample mean and sample standard deviation yields the t-statistic."
            },
            {
                "text": "Calculation If the population mean and population standard deviation are known, a raw score x is converted into a standard score by $z = {x- \\mu \\over \\sigma}$ where: μ is the mean of the population, σ is the standard deviation of the population. The absolute value of z represents the distance between that raw score x and the population mean in units of the standard deviation. z is negative when the raw score is below the mean, positive when above. Calculating z using this formula requires use of the population mean and the population standard deviation, not the sample mean or sample deviation. However, knowing the true mean and standard deviation of a population is often an unrealistic expectation, except in cases such as standardized testing, where the entire population is measured. When the population mean and the population standard deviation are unknown, the standard score may be estimated by using the sample mean and sample standard deviation as estimates of the population values. In these cases, the z-score is given by $z = {x- \\bar{x} \\over S}$ where: $ \\bar{x} $ is the mean of the sample, S is the standard deviation of the sample."
            },
            {
                "text": "Though it should always be stated, the distinction between use of the population and sample statistics often is not made. In either case, the numerator and denominator of the equations have the same units of measure so that the units cancel out through division and z is left as a dimensionless quantity. Applications Z-test The z-score is often used in the z-test in standardized testing – the analog of the Student's t-test for a population whose parameters are known, rather than estimated. As it is very unusual to know the entire population, the t-test is much more widely used. Prediction intervals The standard score can be used in the calculation of prediction intervals. A prediction interval [L,U], consisting of a lower endpoint designated L and an upper endpoint designated U, is an interval such that a future observation X will lie in the interval with high probability $\\gamma$, i.e. $P(L<X<U) =\\gamma,$ For the standard score Z of X it gives: $P\\left( \\frac{L-\\mu}{\\sigma} < Z < \\frac{U-\\mu}{\\sigma} \\right) = \\gamma.$ By determining the quantile z such that $P\\left( -z < Z < z \\right) = \\gamma$ it follows: $L=\\mu-z\\sigma,\\ U=\\mu+z\\sigma$ Process control In process control applications, the Z value provides an assessment of the degree to which a process is operating off-target."
            },
            {
                "text": "Comparison of scores measured on different scales: ACT and SAT The z score for Student A was 1, meaning Student A was 1 standard deviation above the mean. Thus, Student A performed in the 84.13 percentile on the SAT. When scores are measured on different scales, they may be converted to z-scores to aid comparison. Dietz et al. give the following example, comparing student scores on the (old) SAT and ACT high school tests. The table shows the mean and standard deviation for total scores on the SAT and ACT. Suppose that student A scored 1800 on the SAT, and student B scored 24 on the ACT. Which student performed better relative to other test-takers? SAT ACT Mean 1500 21 Standard deviation 300 5 thumb|The z score for Student B was 0.6, meaning Student B was 0.6 standard deviation above the mean. Thus, Student B performed in the 72.57 percentile on the SAT. The z-score for student A is $z = {x- \\mu \\over \\sigma} = {1800- 1500 \\over 300} = 1 $ The z-score for student B is $z = {x- \\mu \\over \\sigma} = {24- 21 \\over 5} = 0.6 $ Because student A has a higher z-score than student B, student A performed better compared to other test-takers than did student B."
            },
            {
                "text": "Percentage of observations below a z-score Continuing the example of ACT and SAT scores, if it can be further assumed that both ACT and SAT scores are normally distributed (which is approximately correct), then the z-scores may be used to calculate the percentage of test-takers who received lower scores than students A and B. Cluster analysis and multidimensional scaling \"For some multivariate techniques such as multidimensional scaling and cluster analysis, the concept of distance between the units in the data is often of considerable interest and importance… When the variables in a multivariate data set are on different scales, it makes more sense to calculate the distances after some form of standardization.\" Principal components analysis In principal components analysis, \"Variables measured on different scales or on a common scale with widely differing ranges are often standardized.\" Relative importance of variables in multiple regression: standardized regression coefficients Standardization of variables prior to multiple regression analysis is sometimes used as an aid to interpretation. (page 95) state the following. \"The standardized regression slope is the slope in the regression equation if X and Y are standardized … Standardization of X and Y is done by subtracting the respective means from each set of observations and dividing by the respective standard deviations … In multiple regression, where several X variables are used, the standardized regression coefficients quantify the relative contribution of each X variable.\""
            },
            {
                "text": "However, Kutner et al. (p 278) give the following caveat: \"… one must be cautious about interpreting any regression coefficients, whether standardized or not. The reason is that when the predictor variables are correlated among themselves, … the regression coefficients are affected by the other predictor variables in the model … The magnitudes of the standardized regression coefficients are affected not only by the presence of correlations among the predictor variables but also by the spacings of the observations on each of these variables. Sometimes these spacings may be quite arbitrary. Hence, it is ordinarily not wise to interpret the magnitudes of standardized regression coefficients as reflecting the comparative importance of the predictor variables.\""
            },
            {
                "text": "Standardizing in mathematical statistics In mathematical statistics, a random variable X is standardized by subtracting its expected value $\\operatorname{E}[X]$ and dividing the difference by its standard deviation $\\sigma(X) = \\sqrt{\\operatorname{Var}(X)}:$ $Z = {X - \\operatorname{E}[X] \\over \\sigma(X)}$ If the random variable under consideration is the sample mean of a random sample $ \\ X_1,\\dots, X_n$ of X: $\\bar{X}={1 \\over n} \\sum_{i=1}^n X_i$ then the standardized version is $Z = \\frac{\\bar{X}-\\operatorname{E}[\\bar{X}]}{\\sigma(X)/\\sqrt{n}}$ Where the standardised sample mean's variance was calculated as follows: $ \\begin{array}{l} \\operatorname{Var}\\left(\\sum x_{i}\\right) =\\sum \\operatorname{Var}(x_{i}) =n\\operatorname{Var}(x_{i}) =n\\sigma ^{2}\\\\ \\operatorname{Var}(\\overline{X}) =\\operatorname{Var}\\left(\\frac{\\sum x_{i}}{n}\\right) =\\frac{1}{n^{2}} \\operatorname{Var}\\left(\\sum x_{i}\\right) =\\frac{n\\sigma ^{2}}{n^{2}} =\\frac{\\sigma ^{2}}{n} \\end{array}$ T-score In educational assessment, T-score is a standard score Z shifted and scaled to have a mean of 50 and a standard deviation of 10."
            },
            {
                "text": "It is also known as hensachi in Japanese, where the concept is much more widely known and used in the context of high school and university admissions. In bone density measurements, the T-score is the standard score of the measurement compared to the population of healthy 30-year-old adults, and has the usual mean of 0 and standard deviation of 1. See also Coefficient of variation Error function Mahalanobis distance Normalization (statistics) Omega ratio Standard normal deviate Studentized residual References Further reading External links z-score calculator Category:Statistical ratios"
            }
        ],
        "latex_formulas": [
            "z = {x- \\mu \\over \\sigma}",
            "z = {x- \\bar{x} \\over S}",
            "\\bar{x}",
            "\\gamma",
            "P(L<X<U) =\\gamma,",
            "P\\left( \\frac{L-\\mu}{\\sigma} < Z < \\frac{U-\\mu}{\\sigma} \\right) = \\gamma.",
            "P\\left( -z < Z < z \\right) = \\gamma",
            "L=\\mu-z\\sigma,\\ U=\\mu+z\\sigma",
            "z = {x- \\mu \\over \\sigma} = {1800- 1500 \\over 300} = 1",
            "z = {x- \\mu \\over \\sigma} = {24- 21 \\over 5} = 0.6",
            "\\operatorname{E}[X]",
            "\\sigma(X) = \\sqrt{\\operatorname{Var}(X)}:",
            "Z = {X - \\operatorname{E}[X] \\over \\sigma(X)}",
            "\\ X_1,\\dots, X_n",
            "\\bar{X}={1 \\over n} \\sum_{i=1}^n X_i",
            "Z = \\frac{\\bar{X}-\\operatorname{E}[\\bar{X}]}{\\sigma(X)/\\sqrt{n}}",
            "\\begin{array}{l}\n\\operatorname{Var}\\left(\\sum x_{i}\\right) =\\sum \\operatorname{Var}(x_{i}) =n\\operatorname{Var}(x_{i}) =n\\sigma ^{2}\\\\\n\\operatorname{Var}(\\overline{X}) =\\operatorname{Var}\\left(\\frac{\\sum x_{i}}{n}\\right) =\\frac{1}{n^{2}} \\operatorname{Var}\\left(\\sum x_{i}\\right) =\\frac{n\\sigma ^{2}}{n^{2}} =\\frac{\\sigma ^{2}}{n}\n\\end{array}"
        ]
    },
    "Gini_impurity": {
        "title": "Gini_impurity",
        "chunks": [
            {
                "text": "REDIRECT Decision tree learning#Gini impurity"
            }
        ],
        "latex_formulas": []
    },
    "Entropy_(information_theory)": {
        "title": "Entropy_(information_theory)",
        "chunks": [
            {
                "text": "In information theory, the entropy of a random variable quantifies the average level of uncertainty or information associated with the variable's potential states or possible outcomes. This measures the expected amount of information needed to describe the state of the variable, considering the distribution of probabilities across all potential states. Given a discrete random variable $X$, which may be any member $x$ within the set $\\mathcal{X}$ and is distributed according to $p\\colon \\mathcal{X}\\to[0, 1]$, the entropy is where $\\Sigma$ denotes the sum over the variable's possible values. The choice of base for $\\log$, the logarithm, varies for different applications. Base 2 gives the unit of bits (or \"shannons\"), while base e gives \"natural units\" nat, and base 10 gives units of \"dits\", \"bans\", or \"hartleys\". An equivalent definition of entropy is the expected value of the self-information of a variable. Two bits of entropy: In the case of two fair coin tosses, the information entropy in bits is the base-2 logarithm of the number of possible outcomeswith two coins there are four possible outcomes, and two bits of entropy."
            },
            {
                "text": "Generally, information entropy is the average amount of information conveyed by an event, when considering all possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper \"A Mathematical Theory of Communication\", (PDF, archived from here ) (PDF, archived from here ) and is also referred to as Shannon entropy. Shannon's theory defines a data communication system composed of three elements: a source of data, a communication channel, and a receiver. The \"fundamental problem of communication\" – as expressed by Shannon – is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel. Shannon considered various ways to encode, compress, and transmit messages from a data source, and proved in his source coding theorem that the entropy represents an absolute mathematical limit on how well data from the source can be losslessly compressed onto a perfectly noiseless channel. Shannon strengthened this result considerably for noisy channels in his noisy-channel coding theorem. Entropy in information theory is directly analogous to the entropy in statistical thermodynamics."
            },
            {
                "text": "The analogy results when the values of the random variable designate energies of microstates, so Gibbs's formula for the entropy is formally identical to Shannon's formula. Entropy has relevance to other areas of mathematics such as combinatorics and machine learning. The definition can be derived from a set of axioms establishing that entropy should be a measure of how informative the average outcome of a variable is. For a continuous random variable, differential entropy is analogous to entropy. The definition $\\mathbb{E}[-\\log p(X)] $ generalizes the above. Introduction The core idea of information theory is that the \"informational value\" of a communicated message depends on the degree to which the content of the message is surprising. If a highly likely event occurs, the message carries very little information. On the other hand, if a highly unlikely event occurs, the message is much more informative. For instance, the knowledge that some particular number will not be the winning number of a lottery provides very little information, because any particular chosen number will almost certainly not win."
            },
            {
                "text": "However, knowledge that a particular number will win a lottery has high informational value because it communicates the occurrence of a very low probability event. The information content, also called the surprisal or self-information, of an event $E$ is a function that increases as the probability $p(E)$ of an event decreases. When $p(E)$ is close to 1, the surprisal of the event is low, but if $p(E)$ is close to 0, the surprisal of the event is high. This relationship is described by the function where $\\log$ is the logarithm, which gives 0 surprise when the probability of the event is 1. In fact, $log$ is the only function that satisfies а specific set of conditions defined in section . Hence, we can define the information, or surprisal, of an event $E$ by or equivalently, Entropy measures the expected (i.e., average) amount of information conveyed by identifying the outcome of a random trial. This implies that rolling a die has higher entropy than tossing a coin because each outcome of a die toss has smaller probability ($p=1/6$) than each outcome of a coin toss ($p=1/2$)."
            },
            {
                "text": "Consider a coin with probability $p$ of landing on heads and probability $1 − p$ of landing on tails. The maximum surprise is when $p , for which one outcome is not expected over the other. In this case a coin flip has an entropy of one bit (similarly, one trit with equiprobable values contains $\\log_2 3$ (about 1.58496) bits of information because it can have one of three values). The minimum surprise is when $p (impossibility) or $p (certainty) and the entropy is zero bits. When the entropy is zero, sometimes referred to as unity, there is no uncertainty at all – no freedom of choice – no information. Other values of p give entropies between zero and one bits. Example Information theory is useful to calculate the smallest amount of information required to convey a message, as in data compression. For example, consider the transmission of sequences comprising the 4 characters 'A', 'B', 'C', and 'D' over a binary channel."
            },
            {
                "text": "If all 4 letters are equally likely (25%), one cannot do better than using two bits to encode each letter. 'A' might code as '00', 'B' as '01', 'C' as '10', and 'D' as '11'. However, if the probabilities of each letter are unequal, say 'A' occurs with 70% probability, 'B' with 26%, and 'C' and 'D' with 2% each, one could assign variable length codes. In this case, 'A' would be coded as '0', 'B' as '10', 'C' as '110', and 'D' as '111'. With this representation, 70% of the time only one bit needs to be sent, 26% of the time two bits, and only 4% of the time 3 bits. On average, fewer than 2 bits are required since the entropy is lower (owing to the high prevalence of 'A' followed by 'B' – together 96% of characters)."
            },
            {
                "text": "The calculation of the sum of probability-weighted log probabilities measures and captures this effect. English text, treated as a string of characters, has fairly low entropy; i.e. it is fairly predictable. We can be fairly certain that, for example, 'e' will be far more common than 'z', that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'. After the first few letters one can often guess the rest of the word. English text has between 0.6 and 1.3 bits of entropy per character of the message.Schneier, B: Applied Cryptography, Second edition, John Wiley and Sons. Definition Named after Boltzmann's Η-theorem, Shannon defined the entropy $Η$ (Greek capital letter eta) of a discrete random variable , which takes values in the set $\\mathcal{X}$ and is distributed according to $p: \\mathcal{X} \\to [0, 1]$ such that $p(x) := \\mathbb{P}[X = x]$: Here $\\mathbb{E}$ is the expected value operator, and $I$ is the information content of $X$."
            },
            {
                "text": "$\\operatorname{I}(X)$ is itself a random variable. The entropy can explicitly be written as: where $b$ is the base of the logarithm used. Common values of $b$ are 2, Euler's number $e$, and 10, and the corresponding units of entropy are the bits for $b , nats for $b , and bans for $b .Schneider, T.D, Information theory primer with an appendix on logarithms, National Cancer Institute, 14 April 2007. In the case of $p(x) = 0$ for some $x \\in \\mathcal{X}$, the value of the corresponding summand $0 logb(0)$ is taken to be $0$, which is consistent with the limit: One may also define the conditional entropy of two variables $X$ and $Y$ taking values from sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively, as: where $p_{X,Y}(x,y) := \\mathbb{P}[X=x,Y=y]$ and $p_Y(y) = \\mathbb{P}[Y = y]$."
            },
            {
                "text": "This quantity should be understood as the remaining randomness in the random variable $X$ given the random variable $Y$. Measure theory Entropy can be formally defined in the language of measure theory as follows: Let $(X, \\Sigma, \\mu)$ be a probability space. Let $A \\in \\Sigma$ be an event. The surprisal of $A$ is The expected surprisal of $A$ is A $\\mu$-almost partition is a set family $P \\subseteq \\mathcal{P}(X)$ such that $\\mu(\\mathop{\\cup} P) = 1$ and $\\mu(A \\cap B) = 0$ for all distinct $A, B \\in P$. (This is a relaxation of the usual conditions for a partition.) The entropy of $P$ is Let $M$ be a sigma-algebra on $X$. The entropy of $M$ is Finally, the entropy of the probability space is $\\Eta_\\mu(\\Sigma)$, that is, the entropy with respect to $\\mu$ of the sigma-algebra of all measurable subsets of $X$."
            },
            {
                "text": "Example thumbnail|right|200px|Entropy $Η(X)$ (i.e. the expected surprisal) of a coin flip, measured in bits, graphed versus the bias of the coin $Pr(X , where $X represents a result of heads.Here, the entropy is at most 1 bit, and to communicate the outcome of a coin flip (2 possible values) will require an average of at most 1 bit (exactly 1 bit for a fair coin). The result of a fair die (6 possible values) would have entropy log26 bits. Consider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this can be modeled as a Bernoulli process. The entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2). This is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full bit of information."
            },
            {
                "text": "This is because However, if we know the coin is not fair, but comes up heads or tails with probabilities $p$ and $q$, where $p ≠ q$, then there is less uncertainty. Every time it is tossed, one side is more likely to come up than the other. The reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full bit of information. For example, if $p$ = 0.7, then Uniform probability yields maximum uncertainty and therefore maximum entropy. Entropy, then, can only decrease from the value associated with uniform probability. The extreme case is that of a double-headed coin that never comes up tails, or a double-tailed coin that never results in a head. Then there is no uncertainty. The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain. Characterization To understand the meaning of $−Σ pi log(pi)$, first define an information function $I$ in terms of an event $i$ with probability $pi$."
            },
            {
                "text": "The amount of information acquired due to the observation of event $i$ follows from Shannon's solution of the fundamental properties of information: $I(p)$ is monotonically decreasing in $p$: an increase in the probability of an event decreases the information from an observed event, and vice versa. $I(1) : events that always occur do not communicate information. $I(p1·p2) : the information learned from independent events is the sum of the information learned from each event. Given two independent events, if the first event can yield one of $n$ equiprobable outcomes and another has one of $m$ equiprobable outcomes then there are $mn$ equiprobable outcomes of the joint event. This means that if $log2(n)$ bits are needed to encode the first value and $log2(m)$ to encode the second, one needs $log2(mn) to encode both. Shannon discovered that a suitable choice of $\\operatorname{I}$ is given by:Chakrabarti, C. G., and Indranil Chakrabarty."
            },
            {
                "text": "\"Shannon entropy: axiomatic characterization and application.\" International Journal of Mathematics and Mathematical Sciences 2005. 17 (2005): 2847-2854 url In fact, the only possible values of $\\operatorname{I}$ are $\\operatorname{I}(u) = k \\log u$ for $k<0$. Additionally, choosing a value for $k$ is equivalent to choosing a value $x>1$ for $k = - 1/\\log x$, so that $x$ corresponds to the base for the logarithm. Thus, entropy is characterized by the above four properties."
            },
            {
                "text": "{| class=\"toccolours collapsible collapsed\" width=\"80%\" style=\"text-align:left\" !Proof |- |Let be the information function which one assumes to be twice continuously differentiable, one has: $\\begin{align} & \\operatorname{I}(p_1 p_2) &=\\ & \\operatorname{I}(p_1) + \\operatorname{I}(p_2) && \\quad \\text{Starting from property 3} \\\\ & p_2 \\operatorname{I}'(p_1 p_2) &=\\ & \\operatorname{I}'(p_1) && \\quad \\text{taking the derivative w.r.t}\\ p_1 \\\\ & \\operatorname{I}'(p_1 p_2) + p_1 p_2 \\operatorname{I}(p_1 p_2) &=\\ & 0 && \\quad \\text{taking the derivative w.r.t}\\ p_2 \\\\ & \\operatorname{I}'(u) + u \\operatorname{I}(u) &=\\ & 0 && \\quad \\text{introducing}\\, u = p_1 p_2 \\\\ & (u\\operatorname{I}'(u))' &=\\ & 0 && \\quad \\text{combining terms into one}\\ \\\\ & u\\operatorname{I}'(u) - k &=\\ & 0 && \\quad \\text{integrating w.r.t}\\ u, \\text{producing constant}\\, k \\\\ \\end{align}$ This differential equation leads to the solution $\\operatorname{I}(u) = k \\log u + c$ for some $k, c \\in \\mathbb{R}$."
            },
            {
                "text": "Property 2 gives $c = 0$. Property 1 and 2 give that $\\operatorname{I}(p)\\ge 0$ for all $p\\in [0,1]$, so that $k < 0$. |} The different units of information (bits for the binary logarithm $log2$, nats for the natural logarithm $ln$, bans for the decimal logarithm $log10$ and so on) are constant multiples of each other. For instance, in case of a fair coin toss, heads provides $log2(2) bit of information, which is approximately 0.693 nats or 0.301 decimal digits. Because of additivity, $n$ tosses provide $n$ bits of information, which is approximately $0.693n$ nats or $0.301n$ decimal digits. The meaning of the events observed (the meaning of messages) does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves."
            },
            {
                "text": "Alternative characterization Another characterization of entropy uses the following properties. We denote $pi and $Ηn(p1, ..., pn) . Continuity: $H$ should be continuous, so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount. Symmetry: $H$ should be unchanged if the outcomes $xi$ are re-ordered. That is, $\\Eta_n\\left(p_1, p_2, \\ldots, p_n \\right) = \\Eta_n\\left(p_{i_1}, p_{i_2}, \\ldots, p_{i_n} \\right)$ for any permutation $\\{i_1, ..., i_n\\}$ of $\\{1, ..., n\\}$. Maximum: $\\Eta_n$ should be maximal if all the outcomes are equally likely i.e. $\\Eta_n(p_1,\\ldots,p_n) \\le \\Eta_n\\left(\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right)$. Increasing number of outcomes: for equiprobable events, the entropy should increase with the number of outcomes i.e."
            },
            {
                "text": "$\\Eta_n\\bigg(\\underbrace{\\frac{1}{n}, \\ldots, \\frac{1}{n}}_{n}\\bigg) < \\Eta_{n+1}\\bigg(\\underbrace{\\frac{1}{n+1}, \\ldots, \\frac{1}{n+1}}_{n+1}\\bigg).$ Additivity: given an ensemble of $n$ uniformly distributed elements that are partitioned into $k$ boxes (sub-systems) with $b1, ..., bk$ elements each, the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes, each weighted with the probability of being in that particular box. Discussion The rule of additivity has the following consequences: for positive integers $bi$ where $b1 + ... + bk , $\\Eta_n\\left(\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right) = \\Eta_k\\left(\\frac{b_1}{n}, \\ldots, \\frac{b_k}{n}\\right) + \\sum_{i=1}^k \\frac{b_i}{n} \\, \\Eta_{b_i}\\left(\\frac{1}{b_i}, \\ldots, \\frac{1}{b_i}\\right).$ Choosing $k , $b1 this implies that the entropy of a certain outcome is zero: $Η1(1) ."
            },
            {
                "text": "This implies that the efficiency of a source set with $n$ symbols can be defined simply as being equal to its $n$-ary entropy. See also Redundancy (information theory). The characterization here imposes an additive property with respect to a partition of a set. Meanwhile, the conditional probability is defined in terms of a multiplicative property, $P(A\\mid B)\\cdot P(B)=P(A\\cap B)$. Observe that a logarithm mediates between these two operations. The conditional entropy and related quantities inherit simple relation, in turn. The measure theoretic definition in the previous section defined the entropy as a sum over expected surprisals $\\mu(A)\\cdot \\ln\\mu(A)$ for an extremal partition. Here the logarithm is ad hoc and the entropy is not a measure in itself. At least in the information theory of a binary string, $\\log_2$ lends itself to practical interpretations. Motivated by such relations, a plethora of related and competing quantities have been defined. For example, David Ellerman's analysis of a \"logic of partitions\" defines a competing measure in structures dual to that of subsets of a universal set."
            },
            {
                "text": "Information is quantified as \"dits\" (distinctions), a measure on partitions. \"Dits\" can be converted into Shannon's bits, to get the formulas for conditional entropy, and so on. Alternative characterization via additivity and subadditivity Another succinct axiomatic characterization of Shannon entropy was given by Aczél, Forte and Ng, via the following properties: Subadditivity: $\\Eta(X,Y) \\le \\Eta(X)+\\Eta(Y)$ for jointly distributed random variables $X,Y$. Additivity: $\\Eta(X,Y) = \\Eta(X)+\\Eta(Y)$ when the random variables $X,Y$ are independent. Expansibility: $\\Eta_{n+1}(p_1, \\ldots, p_n, 0) = \\Eta_n(p_1, \\ldots, p_n)$, i.e., adding an outcome with probability zero does not change the entropy. Symmetry: $\\Eta_n(p_1, \\ldots, p_n)$ is invariant under permutation of $p_1, \\ldots, p_n$. Small for small probabilities: $\\lim_{q \\to 0^+} \\Eta_2(1-q, q) = 0$."
            },
            {
                "text": "Discussion It was shown that any function $\\Eta$ satisfying the above properties must be a constant multiple of Shannon entropy, with a non-negative constant. Compared to the previously mentioned characterizations of entropy, this characterization focuses on the properties of entropy as a function of random variables (subadditivity and additivity), rather than the properties of entropy as a function of the probability vector $p_1,\\ldots ,p_n$. It is worth noting that if we drop the \"small for small probabilities\" property, then $\\Eta$ must be a non-negative linear combination of the Shannon entropy and the Hartley entropy. Further properties The Shannon entropy satisfies the following properties, for some of which it is useful to interpret entropy as the expected amount of information learned (or uncertainty eliminated) by revealing the value of a random variable $X$: Adding or removing an event with probability zero does not contribute to the entropy: $\\Eta_{n+1}(p_1,\\ldots,p_n,0) = \\Eta_n(p_1,\\ldots,p_n)$. The maximal entropy of an event with n different outcomes is $logb(n)$: it is attained by the uniform probability distribution."
            },
            {
                "text": "That is, uncertainty is maximal when all possible events are equiprobable: $\\Eta(p_1,\\dots,p_n) \\leq \\log_b n$. The entropy or the amount of information revealed by evaluating $(X,Y)$ (that is, evaluating $X$ and $Y$ simultaneously) is equal to the information revealed by conducting two consecutive experiments: first evaluating the value of $Y$, then revealing the value of $X$ given that you know the value of $Y$. This may be written as: $ \\Eta(X,Y)=\\Eta(X|Y)+\\Eta(Y)=\\Eta(Y|X)+\\Eta(X).$ If $Y=f(X)$ where $f$ is a function, then $\\Eta(f(X)|X) = 0$. Applying the previous formula to $\\Eta(X,f(X))$ yields $ \\Eta(X)+\\Eta(f(X)|X)=\\Eta(f(X))+\\Eta(X|f(X)),$ so $\\Eta(f(X)) \\le \\Eta(X)$, the entropy of a variable can only decrease when the latter is passed through a function."
            },
            {
                "text": "If $X$ and $Y$ are two independent random variables, then knowing the value of $Y$ doesn't influence our knowledge of the value of $X$ (since the two don't influence each other by independence): $ \\Eta(X|Y)=\\Eta(X).$ More generally, for any random variables $X$ and $Y$, we have $ \\Eta(X|Y)\\leq \\Eta(X)$. The entropy of two simultaneous events is no more than the sum of the entropies of each individual event i.e., $ \\Eta(X,Y)\\leq \\Eta(X)+\\Eta(Y)$, with equality if and only if the two events are independent. The entropy $\\Eta(p)$ is concave in the probability mass function $p$, i.e. $\\Eta(\\lambda p_1 + (1-\\lambda) p_2) \\ge \\lambda \\Eta(p_1) + (1-\\lambda) \\Eta(p_2)$ for all probability mass functions $p_1,p_2$ and $ 0 \\le \\lambda \\le 1$."
            },
            {
                "text": "Accordingly, the negative entropy (negentropy) function is convex, and its convex conjugate is LogSumExp. Aspects Relationship to thermodynamic entropy The inspiration for adopting the word entropy in information theory came from the close resemblance between Shannon's formula and very similar known formulae from statistical mechanics. In statistical thermodynamics the most general formula for the thermodynamic entropy $S$ of a thermodynamic system is the Gibbs entropy $S = - k_\\text{B} \\sum p_i \\ln p_i \\,,$ where $kB$ is the Boltzmann constant, and $pi$ is the probability of a microstate. The Gibbs entropy was defined by J. Willard Gibbs in 1878 after earlier work by Ludwig Boltzmann (1872).Compare: Boltzmann, Ludwig (1896, 1898). Vorlesungen über Gastheorie : 2 Volumes – Leipzig 1895/98 UB: O 5262-6. English version: Lectures on gas theory. Translated by Stephen G. Brush (1964) Berkeley: University of California Press; (1995) New York: Dover The Gibbs entropy translates over almost unchanged into the world of quantum physics to give the von Neumann entropy introduced by John von Neumann in 1927: $S = - k_\\text{B} \\,{\\rm Tr}(\\rho \\ln \\rho) \\,,$ where ρ is the density matrix of the quantum mechanical system and Tr is the trace."
            },
            {
                "text": "At an everyday practical level, the links between information entropy and thermodynamic entropy are not evident. Physicists and chemists are apt to be more interested in changes in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the second law of thermodynamics, rather than an unchanging probability distribution. As the minuteness of the Boltzmann constant $kB$ indicates, the changes in $S / kB$ for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing. In classical thermodynamics, entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy. The connection between thermodynamics and what is now known as information theory was first made by Boltzmann and expressed by his equation: $S=k_\\text{B} \\ln W,$ where $S$ is the thermodynamic entropy of a particular macrostate (defined by thermodynamic parameters such as temperature, volume, energy, etc. ), $W$ is the number of microstates (various combinations of particles in various energy states) that can yield the given macrostate, and $kB$ is the Boltzmann constant."
            },
            {
                "text": "It is assumed that each microstate is equally likely, so that the probability of a given microstate is $1=pi = 1/W$. When these probabilities are substituted into the above expression for the Gibbs entropy (or equivalently $kB$ times the Shannon entropy), Boltzmann's equation results. In information theoretic terms, the information entropy of a system is the amount of \"missing\" information needed to determine a microstate, given the macrostate. In the view of Jaynes (1957), thermodynamic entropy, as explained by statistical mechanics, should be seen as an application of Shannon's information theory: the thermodynamic entropy is interpreted as being proportional to the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just the Boltzmann constant. Adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables, making any complete state description longer."
            },
            {
                "text": "(See article: maximum entropy thermodynamics). Maxwell's demon can (hypothetically) reduce the thermodynamic entropy of a system by using information about the states of individual molecules; but, as Landauer (from 1961) and co-workers have shown, to function the demon himself must increase thermodynamic entropy in the process, by at least the amount of Shannon information he proposes to first acquire and store; and so the total thermodynamic entropy does not decrease (which resolves the paradox). Landauer's principle imposes a lower bound on the amount of heat a computer must generate to process a given amount of information, though modern computers are far less efficient. Data compression Shannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits. Shannon's entropy measures the information contained in a message as opposed to the portion of the message that is determined (or predictable). Examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs, triplets etc."
            },
            {
                "text": "The minimum channel capacity can be realized in theory by using the typical set or in practice using Huffman, Lempel–Ziv or arithmetic coding. (See also Kolmogorov complexity.) In practice, compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors. The entropy rate of a data source is the average number of bits per symbol needed to encode it. Shannon's experiments with human predictors show an information rate between 0.6 and 1.3 bits per character in English; the PPM compression algorithm can achieve a compression ratio of 1.5 bits per character in English text. If a compression scheme is lossless – one in which you can always recover the entire original message by decompression – then a compressed message has the same quantity of information as the original but is communicated in fewer characters. It has more information (higher entropy) per character. A compressed message has less redundancy. Shannon's source coding theorem states a lossless compression scheme cannot compress messages, on average, to have more than one bit of information per bit of message, but that any value less than one bit of information per bit of message can be attained by employing a suitable coding scheme."
            },
            {
                "text": "The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains. Shannon's theorem also implies that no lossless compression scheme can shorten all messages. If some messages come out shorter, at least one must come out longer due to the pigeonhole principle. In practical use, this is generally not a problem, because one is usually only interested in compressing certain types of messages, such as a document in English, as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger. A 2011 study in Science estimates the world's technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007, therefore estimating the entropy of the technologically available sources. \"The World's Technological Capacity to Store, Communicate, and Compute Information\" , Martin Hilbert and Priscila López (2011), Science, 332(6025); free access to the article through here: martinhilbert.net/WorldInfoCapacity.html + All figures in entropically compressed exabytes Type of Information 1986 2007 Storage 2.6 295 Broadcast 432 1900 Telecommunications 0.281 65 The authors estimate humankind technological capacity to store information (fully entropically compressed) in 1986 and again in 2007."
            },
            {
                "text": "They break the information into three categories—to store information on a medium, to receive information through one-way broadcast networks, or to exchange information through two-way telecommunications networks. Entropy as a measure of diversity Entropy is one of several ways to measure biodiversity and is applied in the form of the Shannon index. A diversity index is a quantitative statistical measure of how many different types exist in a dataset, such as species in a community, accounting for ecological richness, evenness, and dominance. Specifically, Shannon entropy is the logarithm of $1D$, the true diversity index with parameter equal to 1. The Shannon index is related to the proportional abundances of types. Entropy of a sequence There are a number of entropy-related concepts that mathematically quantify information content of a sequence or message: the self-information of an individual message or symbol taken from a given probability distribution (message or sequence seen as an individual event), the joint entropy of the symbols forming the message or sequence (seen as a set of events), the entropy rate of a stochastic process (message or sequence is seen as a succession of events)."
            },
            {
                "text": "(The \"rate of self-information\" can also be defined for a particular sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate in the case of a stationary process.) Other quantities of information are also used to compare or relate different sources of information. It is important not to confuse the above concepts. Often it is only clear from context which one is meant. For example, when someone says that the \"entropy\" of the English language is about 1 bit per character, they are actually modeling the English language as a stochastic process and talking about its entropy rate. Shannon himself used the term in this way. If very large blocks are used, the estimate of per-character entropy rate may become artificially low because the probability distribution of the sequence is not known exactly; it is only an estimate. If one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book, and if there are $N$ published books, and each book is only published once, the estimate of the probability of each book is $1/N$, and the entropy (in bits) is $−log2(1/N) ."
            },
            {
                "text": "As a practical code, this corresponds to assigning each book a unique identifier and using it in place of the text of the book whenever one wants to refer to the book. This is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: it is not possible to reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books. The key idea is that the complexity of the probabilistic model must be considered. Kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortest program for a universal computer that outputs the sequence. A code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e. the probabilistic model), is one such program, but it may not be the shortest. The Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, .... treating the sequence as a message and each number as a symbol, there are almost as many symbols as there are characters in the message, giving an entropy of approximately $log2(n)$."
            },
            {
                "text": "The first 128 symbols of the Fibonacci sequence has an entropy of approximately 7 bits/symbol, but the sequence can be expressed using a formula [$F(n) for $n , $F(1) , $F(2) ] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence. Limitations of entropy in cryptography In cryptanalysis, entropy is often roughly used as a measure of the unpredictability of a cryptographic key, though its real uncertainty is unmeasurable. For example, a 128-bit key that is uniformly and randomly generated has 128 bits of entropy. It also takes (on average) $2^{127}$ guesses to break by brute force. Entropy fails to capture the number of guesses required if the possible keys are not chosen uniformly. Instead, a measure called guesswork can be used to measure the effort required for a brute force attack. Other problems may arise from non-uniform distributions used in cryptography. For example, a 1,000,000-digit binary one-time pad using exclusive or. If the pad has 1,000,000 bits of entropy, it is perfect."
            },
            {
                "text": "If the pad has 999,999 bits of entropy, evenly distributed (each individual bit of the pad having 0.999999 bits of entropy) it may provide good security. But if the pad has 999,999 bits of entropy, where the first bit is fixed and the remaining 999,999 bits are perfectly random, the first bit of the ciphertext will not be encrypted at all. Data as a Markov process A common way to define entropy for text is based on the Markov model of text. For an order-0 source (each character is selected independent of the last characters), the binary entropy is: $\\Eta(\\mathcal{S}) = - \\sum p_i \\log p_i ,$ where $pi$ is the probability of $i$. For a first-order Markov source (one in which the probability of selecting a character is dependent only on the immediately preceding character), the entropy rate is: $\\Eta(\\mathcal{S}) = - \\sum_i p_i \\sum_j \\ p_i (j) \\log p_i (j) ,$ where $i$ is a state (certain preceding characters) and $p_i(j)$ is the probability of $j$ given $i$ as the previous character."
            },
            {
                "text": "For a second order Markov source, the entropy rate is $\\Eta(\\mathcal{S}) = -\\sum_i p_i \\sum_j p_i(j) \\sum_k p_{i,j}(k)\\ \\log \\ p_{i,j}(k) .$ Efficiency (normalized entropy) A source set $\\mathcal{X}$ with a non-uniform distribution will have less entropy than the same set with a uniform distribution (i.e. the \"optimized alphabet\"). This deficiency in entropy can be expressed as a ratio called efficiency:Indices of Qualitative Variation. AR Wilcox - 1967 https://www.osti.gov/servlets/purl/4167340 $\\eta(X) = \\frac{H}{H_\\text{max}} = -\\sum_{i=1}^n \\frac{p(x_i) \\log_b (p(x_i))}{\\log_b (n)}. $ Applying the basic properties of the logarithm, this quantity can also be expressed as: $\\eta(X) = -\\sum_{i=1}^n \\frac{p(x_i) \\log_b (p(x_i))}{\\log_b (n)} \\sum_{i1}^n \\frac{\\log_b(p(x_i)^{-p(x_i)})}{\\log_b(n)} \\sum_{i1}^n \\log_n(p(x_i)^{-p(x_i)}) \\log_n \\left(\\prod_{i1}^n p(x_i)^{-p(x_i)}\\right)."
            },
            {
                "text": "$ Efficiency has utility in quantifying the effective use of a communication channel. This formulation is also referred to as the normalized entropy, as the entropy is divided by the maximum entropy ${\\log_b (n)}$. Furthermore, the efficiency is indifferent to the choice of (positive) base $b$, as indicated by the insensitivity within the final logarithm above thereto. Entropy for continuous random variables Differential entropy The Shannon entropy is restricted to random variables taking discrete values. The corresponding formula for a continuous random variable with probability density function $f(x)$ with finite or infinite support $\\mathbb X$ on the real line is defined by analogy, using the above form of the entropy as an expectation: $\\Eta(X) = \\mathbb{E}[-\\log f(X)] = -\\int_\\mathbb X f(x) \\log f(x)\\, \\mathrm{d}x.$ This is the differential entropy (or continuous entropy). A precursor of the continuous entropy $h[f]$ is the expression for the functional $Η$ in the H-theorem of Boltzmann."
            },
            {
                "text": "Although the analogy between both functions is suggestive, the following question must be set: is the differential entropy a valid extension of the Shannon discrete entropy? Differential entropy lacks a number of properties that the Shannon discrete entropy has – it can even be negative – and corrections have been suggested, notably limiting density of discrete points. To answer this question, a connection must be established between the two functions: In order to obtain a generally finite measure as the bin size goes to zero. In the discrete case, the bin size is the (implicit) width of each of the $n$ (finite or infinite) bins whose probabilities are denoted by $pn$. As the continuous domain is generalized, the width must be made explicit. To do this, start with a continuous function $f$ discretized into bins of size $\\Delta$. By the mean-value theorem there exists a value $xi$ in each bin such that the integral of the function $f$ can be approximated (in the Riemannian sense) by where this limit and \"bin size goes to zero\" are equivalent."
            },
            {
                "text": "We will denote and expanding the logarithm, we have As $Δ → 0$, we have $\\begin{align} \\sum_{i=-\\infty}^{\\infty} f(x_i) \\Delta &\\to \\int_{-\\infty}^{\\infty} f(x)\\, dx = 1 \\\\ \\sum_{i=-\\infty}^{\\infty} f(x_i) \\Delta \\log (f(x_i)) &\\to \\int_{-\\infty}^{\\infty} f(x) \\log f(x)\\, dx. \\end{align}$ Note; $log(Δ) → −∞$ as $Δ → 0$, requires a special definition of the differential or continuous entropy: $h[f] = \\lim_{\\Delta \\to 0} \\left(\\Eta^{\\Delta} + \\log \\Delta\\right) = -\\int_{-\\infty}^{\\infty} f(x) \\log f(x)\\,dx,$ which is, as said before, referred to as the differential entropy. This means that the differential entropy is not a limit of the Shannon entropy for $n → ∞$."
            },
            {
                "text": "Rather, it differs from the limit of the Shannon entropy by an infinite offset (see also the article on information dimension). Limiting density of discrete points It turns out as a result that, unlike the Shannon entropy, the differential entropy is not in general a good measure of uncertainty or information. For example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations. This problem may be illustrated by a change of units when $x$ is a dimensioned variable. $f(x)$ will then have the units of $1/x$. The argument of the logarithm must be dimensionless, otherwise it is improper, so that the differential entropy as given above will be improper. If $Δ$ is some \"standard\" value of $x$ (i.e. \"bin size\") and therefore has the same units, then a modified differential entropy may be written in proper form as: and the result will be the same for any choice of units for $x$. In fact, the limit of discrete entropy as $ N \\rightarrow \\infty $ would also include a term of $ \\log(N)$, which would in general be infinite."
            },
            {
                "text": "This is expected: continuous variables would typically have infinite entropy when discretized. The limiting density of discrete points is really a measure of how much easier a distribution is to describe than a distribution that is uniform over its quantization scheme. Relative entropy Another useful measure of entropy that works equally well in the discrete and the continuous case is the relative entropy of a distribution. It is defined as the Kullback–Leibler divergence from the distribution to a reference measure $m$ as follows. Assume that a probability distribution $p$ is absolutely continuous with respect to a measure $m$, i.e. is of the form $p(dx) for some non-negative $m$-integrable function $f$ with $m$-integral 1, then the relative entropy can be defined as $D_{\\mathrm{KL}}(p \\| m ) = \\int \\log (f(x)) p(dx) = \\int f(x)\\log (f(x)) m(dx) .$ In this form the relative entropy generalizes (up to change in sign) both the discrete entropy, where the measure $m$ is the counting measure, and the differential entropy, where the measure $m$ is the Lebesgue measure."
            },
            {
                "text": "If the measure $m$ is itself a probability distribution, the relative entropy is non-negative, and zero if $p as measures. It is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly takes into account the transformation of the measure $m$. The relative entropy, and (implicitly) entropy and differential entropy, do depend on the \"reference\" measure $m$. Use in number theory Terence Tao used entropy to make a useful connection trying to solve the Erdős discrepancy problem. Intuitively the idea behind the proof was if there is low information in terms of the Shannon entropy between consecutive random variables (here the random variable is defined using the Liouville function (which is a useful mathematical function for studying distribution of primes) $XH$ $\\lambda(n+H)$. And in an interval [n, n+H] the sum over that interval could become arbitrary large. For example, a sequence of +1's (which are values of $XH$ could take) have trivially low entropy and their sum would become big."
            },
            {
                "text": "But the key insight was showing a reduction in entropy by non negligible amounts as one expands H leading inturn to unbounded growth of a mathematical object over this random variable is equivalent to showing the unbounded growth per the Erdős discrepancy problem. The proof is quite involved and it brought together breakthroughs not just in novel use of Shannon entropy, but also it used the Liouville function along with averages of modulated multiplicative functionshttps://arxiv.org/pdf/1502.02374.pdf in short intervals. Proving it also broke the \"parity barrier\"https://terrytao.wordpress.com/2007/06/05/open-question-the-parity-problem-in-sieve-theory/ for this specific problem. While the use of Shannon entropy in the proof is novel it is likely to open new research in this direction. Use in combinatorics Entropy has become a useful quantity in combinatorics."
            },
            {
                "text": "Loomis–Whitney inequality A simple example of this is an alternative proof of the Loomis–Whitney inequality: for every subset $A ⊆ Zd$, we have $ |A|^{d-1}\\leq \\prod_{i=1}^{d} |P_{i}(A)|$ where $Pi$ is the orthogonal projection in the $i$th coordinate: $ P_{i}(A)=\\{(x_{1}, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_{d}) : (x_{1}, \\ldots, x_{d})\\in A\\}.$ The proof follows as a simple corollary of Shearer's inequality: if $X1, ..., Xd$ are random variables and $S1, ..., Sn$ are subsets of ${1, ..., d$} such that every integer between 1 and $d$ lies in exactly $r$ of these subsets, then $ \\Eta[(X_{1}, \\ldots ,X_{d})]\\leq \\frac{1}{r}\\sum_{i=1}^{n}\\Eta[(X_{j})_{j\\in S_{i}}]$ where $ (X_{j})_{j\\in S_{i}}$ is the Cartesian product of random variables $Xj$ with indexes $j$ in $Si$ (so the dimension of this vector is equal to the size of $Si$)."
            },
            {
                "text": "We sketch how Loomis–Whitney follows from this: Indeed, let $X$ be a uniformly distributed random variable with values in $A$ and so that each point in $A$ occurs with equal probability. Then (by the further properties of entropy mentioned above) $Η(X) , where $ denotes the cardinality of $A$. Let $Si }. The range of $(X_{j})_{j\\in S_{i}}$ is contained in $Pi(A)$ and hence $ \\Eta[(X_{j})_{j\\in S_{i}}]\\leq \\log |P_{i}(A)|$. Now use this to bound the right side of Shearer's inequality and exponentiate the opposite sides of the resulting inequality you obtain. Approximation to binomial coefficient For integers $0 < k < n$ let $q . Then $\\frac{2^{n\\Eta(q)}}{n+1} \\leq \\tbinom nk \\leq 2^{n\\Eta(q)},$ where $\\Eta(q) = -q \\log_2(q) - (1-q) \\log_2(1-q).$Aoki, New Approaches to Macroeconomic Modeling."
            },
            {
                "text": "{| class=\"toccolours collapsible collapsed\" width=\"80%\" style=\"text-align:left\" !Proof (sketch) |- |Note that $\\tbinom nk q^{qn}(1-q)^{n-nq}$ is one term of the expression $\\sum_{i=0}^n \\tbinom ni q^i(1-q)^{n-i} = (q + (1-q))^n = 1.$ Rearranging gives the upper bound. For the lower bound one first shows, using some algebra, that it is the largest term in the summation. But then, $\\binom nk q^{qn}(1-q)^{n-nq} \\geq \\frac{1}{n+1}$ since there are $n + 1$ terms in the summation. Rearranging gives the lower bound. |} A nice interpretation of this is that the number of binary strings of length $n$ with exactly $k$ many 1's is approximately $2^{n\\Eta(k/n)}$.Probability and Computing, M. Mitzenmacher and E. Upfal, Cambridge University Press Use in machine learning Machine learning techniques arise largely from statistics and also information theory."
            },
            {
                "text": "In general, entropy is a measure of uncertainty and the objective of machine learning is to minimize uncertainty. Decision tree learning algorithms use relative entropy to determine the decision rules that govern the data at each node. The information gain in decision trees $IG(Y,X)$, which is equal to the difference between the entropy of $Y$ and the conditional entropy of $Y$ given $X$, quantifies the expected information, or the reduction in entropy, from additionally knowing the value of an attribute $X$. The information gain is used to identify which attributes of the dataset provide the most information and should be used to split the nodes of the tree optimally. Bayesian inference models often apply the principle of maximum entropy to obtain prior probability distributions. The idea is that the distribution that best represents the current state of knowledge of a system is the one with the largest entropy, and is therefore suitable to be the prior. Classification in machine learning performed by logistic regression or artificial neural networks often employs a standard loss function, called cross-entropy loss, that minimizes the average cross entropy between ground truth and predicted distributions."
            },
            {
                "text": "In general, cross entropy is a measure of the differences between two datasets similar to the KL divergence (also known as relative entropy). See also Approximate entropy (ApEn) Entropy (thermodynamics) Cross entropy – is a measure of the average number of bits needed to identify an event from a set of possibilities between two probability distributions Entropy (arrow of time) Entropy encoding – a coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols. Entropy estimation Entropy power inequality Fisher information Graph entropy Hamming distance History of entropy History of information theory Information fluctuation complexity Information geometry Kolmogorov–Sinai entropy in dynamical systems Levenshtein distance Mutual information Perplexity Qualitative variation – other measures of statistical dispersion for nominal distributions Quantum relative entropy – a measure of distinguishability between two quantum states. Rényi entropy – a generalization of Shannon entropy; it is one of a family of functionals for quantifying the diversity, uncertainty or randomness of a system. Randomness Sample entropy (SampEn) Shannon index Theil index Typoglycemia Notes References Further reading Textbooks on information theory Cover, T.M., Thomas, J.A. (2006), Elements of Information Theory – 2nd Ed., Wiley-Interscience, MacKay, D.J.C. (2003), Information Theory, Inference and Learning Algorithms, Cambridge University Press, Arndt, C. (2004), Information Measures: Information and its Description in Science and Engineering, Springer, Gray, R. M. (2011), Entropy and Information Theory, Springer. Shannon, C.E., Weaver, W. (1949) The Mathematical Theory of Communication, Univ of Illinois Press."
            },
            {
                "text": "Stone, J. V. (2014), Chapter 1 of Information Theory: A Tutorial Introduction , University of Sheffield, England. . External links \"Entropy\" at Rosetta Code—repository of implementations of Shannon entropy in different programming languages. Entropy an interdisciplinary journal on all aspects of the entropy concept. Open access. Category:Information theory Category:Statistical randomness Category:Complex systems theory Category:Data compression"
            }
        ],
        "latex_formulas": [
            "log",
            "''p''",
            "1 − ''p''",
            "''p'' {{=}} 1/2",
            "''p'' {{=}} 0",
            "''p'' {{=}} 1",
            "&Eta;",
            "I",
            "''X''",
            "''b''",
            "''b''",
            "''e''",
            "''b'' {{=}} 2",
            "''b'' {{=}} ''e''",
            "''b'' {{=}} 10",
            "0 log<sub>''b''</sub>(0)",
            "0",
            "Η(''X'')",
            "Pr(''X'' {{=}} 1)",
            "''X'' {{=}} 1",
            "''p''",
            "''q''",
            "''p'' ≠ ''q''",
            "''p''",
            "−Σ ''p''<sub>''i''</sub> log(''p''<sub>''i''</sub>)",
            "I",
            "''i''",
            "''p''<sub>''i''</sub>",
            "''i''",
            "I(''p'')",
            "''p''",
            "I(1) {{=}} 0",
            "I(''p''<sub>1</sub>·''p''<sub>2</sub>) {{=}} I(''p''<sub>1</sub>) + I(''p''<sub>2</sub>)",
            "''n''",
            "''m''",
            "''mn''",
            "log<sub>2</sub>(''n'')",
            "log<sub>2</sub>(''m'')",
            "log<sub>2</sub>(''mn'') {{=}} log<sub>2</sub>(''m'') + log<sub>2</sub>(''n'')",
            "''k''",
            "''x''",
            "log<sub>2</sub>",
            "ln",
            "log<sub>10</sub>",
            "log<sub>2</sub>(2) {{=}} 1",
            "''n''",
            "''n''",
            "0.693''n''",
            "0.301''n''",
            "''p''<sub>''i''</sub> {{=}} Pr(''X'' {{=}} ''x''<sub>''i''</sub>)",
            "Η<sub>''n''</sub>(''p''<sub>1</sub>, ..., ''p''<sub>''n''</sub>) {{=}} Η(''X'')",
            "H",
            "H",
            "''x''<sub>''i''</sub>",
            "''n''",
            "''k''",
            "''b''<sub>1</sub>, ..., ''b''<sub>''k''</sub>",
            "''b''<sub>''i''</sub>",
            "''b''<sub>1</sub> + ... + ''b''<sub>''k''</sub> {{=}} ''n''",
            "''k'' {{=}} ''n''",
            "''b''<sub>1</sub> {{=}} ... {{=}} ''b''<sub>''n''</sub> {{=}} 1",
            "Η<sub>1</sub>(1) {{=}} 0",
            "''n''",
            "''n''",
            "''X''",
            "log<sub>''b''</sub>(''n'')",
            "(''X'',''Y'')",
            "''X''",
            "''Y''",
            "''Y''",
            "''X''",
            "''Y''",
            "''X''",
            "''Y''",
            "''Y''",
            "''X''",
            "''X''",
            "''Y''",
            "''S''",
            "''k''<sub>B</sub>",
            "''p''<sub>''i''</sub>",
            "''k''<sub>B</sub>",
            "''S'' / ''k''<sub>B</sub>",
            "''W''",
            "''k''<sub>B</sub>",
            "''p''<sub>''i''</sub> = 1/''W''",
            "''k''<sub>B</sub>",
            "<sup>1</sup>D",
            "''N''",
            "1/''N''",
            "−log<sub>2</sub>(1/''N'') {{=}} log<sub>2</sub>(''N'')",
            "log<sub>2</sub>(''n'')",
            "F(''n'') {{=}} F(''n''−1) + F(''n''−2)",
            "''n'' {{=}} 3, 4, 5, ...",
            "F(1) {{=}}1",
            "F(2) {{=}} 1",
            "''p''<sub>''i''</sub>",
            "''i''",
            "''i''",
            "''j''",
            "''i''",
            "''b''",
            "''f''(''x'')",
            "''h''[''f'']",
            "''Η''",
            "''n''",
            "''p''<sub>''n''</sub>",
            "''f''",
            "''x''<sub>''i''</sub>",
            "''f''",
            "Δ → 0",
            "log(Δ) → −∞",
            "Δ → 0",
            "''n'' → ∞",
            "''x''",
            "''f''(''x'')",
            "1/''x''",
            "''&Delta;''",
            "''x''",
            "''x''",
            "''m''",
            "''p''",
            "''m''",
            "''p''(''dx'') {{=}} ''f''(''x'')''m''(''dx'')",
            "''m''",
            "''f''",
            "''m''",
            "''m''",
            "''m''",
            "''m''",
            "''p'' {{=}} ''m''",
            "''m''",
            "''m''",
            "''X''<sub>''H''</sub>",
            "''X''<sub>''H''</sub>",
            "''A'' ⊆ '''Z'''<sup>''d''</sup>",
            "''P''<sub>''i''</sub>",
            "''i''",
            "''X''<sub>1</sub>, ..., ''X''<sub>''d''</sub>",
            "''S''<sub>1</sub>, ..., ''S''<sub>''n''</sub>",
            "{1, ..., ''d''",
            "''d''",
            "''r''",
            "''X''<sub>''j''</sub>",
            "''j''",
            "''S''<sub>''i''</sub>",
            "''S''<sub>''i''</sub>",
            "''X''",
            "''A''",
            "''A''",
            "Η(''X'') {{=}} log{{abs|''A''}}",
            "{{abs|''A''}}",
            "''A''",
            "''S''<sub>''i''</sub> {{=}} {1, 2, ..., ''i''−1, ''i''+1, ..., ''d''",
            "''P''<sub>''i''</sub>(''A'')",
            "0 < ''k'' < ''n''",
            "''q'' {{=}} ''k''/''n''",
            "''n'' + 1",
            "''n''",
            "''k''",
            "X",
            "x",
            "\\mathcal{X}",
            "p\\colon \\mathcal{X}\\to[0, 1]",
            "\\Sigma",
            "\\log",
            "\\mathbb{E}[-\\log p(X)]",
            "E",
            "p(E)",
            "p(E)",
            "p(E)",
            "\\log",
            "E",
            "p=1/6",
            "p=1/2",
            "\\log_2 3",
            "\\mathcal{X}",
            "p: \\mathcal{X} \\to [0, 1]",
            "p(x) := \\mathbb{P}[X = x]",
            "\\mathbb{E}",
            "\\operatorname{I}(X)",
            "p(x) = 0",
            "x \\in \\mathcal{X}",
            "X",
            "Y",
            "\\mathcal{X}",
            "\\mathcal{Y}",
            "p_{X,Y}(x,y) := \\mathbb{P}[X=x,Y=y]",
            "p_Y(y) = \\mathbb{P}[Y = y]",
            "X",
            "Y",
            "(X, \\Sigma, \\mu)",
            "A \\in \\Sigma",
            "A",
            "A",
            "\\mu",
            "P \\subseteq \\mathcal{P}(X)",
            "\\mu(\\mathop{\\cup} P) = 1",
            "\\mu(A \\cap B) = 0",
            "A, B \\in P",
            "P",
            "M",
            "X",
            "M",
            "\\Eta_\\mu(\\Sigma)",
            "\\mu",
            "X",
            "\\operatorname{I}",
            "\\operatorname{I}",
            "\\operatorname{I}(u) = k \\log u",
            "k<0",
            "x>1",
            "k = - 1/\\log x",
            "\\begin{align}\n& \\operatorname{I}(p_1 p_2) &=\\ & \\operatorname{I}(p_1) +  \\operatorname{I}(p_2) && \\quad \\text{Starting from property 3} \\\\\n& p_2 \\operatorname{I}'(p_1 p_2) &=\\ & \\operatorname{I}'(p_1) && \\quad \\text{taking the derivative w.r.t}\\ p_1 \\\\\n& \\operatorname{I}'(p_1 p_2) + p_1 p_2 \\operatorname{I}''(p_1 p_2) &=\\ & 0 && \\quad \\text{taking the derivative w.r.t}\\ p_2 \\\\\n& \\operatorname{I}'(u) + u \\operatorname{I}''(u) &=\\ & 0 && \\quad \\text{introducing}\\, u = p_1 p_2 \\\\\n& (u\\operatorname{I}'(u))' &=\\ & 0 && \\quad \\text{combining terms into one}\\ \\\\\n& u\\operatorname{I}'(u) - k  &=\\ & 0 && \\quad \\text{integrating w.r.t}\\ u, \\text{producing constant}\\, k \\\\\n\\end{align}",
            "\\operatorname{I}(u) = k \\log u + c",
            "k, c \\in \\mathbb{R}",
            "c = 0",
            "\\operatorname{I}(p)\\ge 0",
            "p\\in [0,1]",
            "k < 0",
            "\\Eta_n\\left(p_1, p_2, \\ldots, p_n \\right) = \\Eta_n\\left(p_{i_1}, p_{i_2}, \\ldots, p_{i_n} \\right)",
            "\\{i_1, ..., i_n\\}",
            "\\{1, ..., n\\}",
            "\\Eta_n",
            "\\Eta_n(p_1,\\ldots,p_n) \\le \\Eta_n\\left(\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right)",
            "\\Eta_n\\bigg(\\underbrace{\\frac{1}{n}, \\ldots, \\frac{1}{n}}_{n}\\bigg) < \\Eta_{n+1}\\bigg(\\underbrace{\\frac{1}{n+1}, \\ldots, \\frac{1}{n+1}}_{n+1}\\bigg).",
            "\\Eta_n\\left(\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right) = \\Eta_k\\left(\\frac{b_1}{n}, \\ldots, \\frac{b_k}{n}\\right) + \\sum_{i=1}^k \\frac{b_i}{n} \\, \\Eta_{b_i}\\left(\\frac{1}{b_i}, \\ldots, \\frac{1}{b_i}\\right).",
            "P(A\\mid B)\\cdot P(B)=P(A\\cap B)",
            "\\mu(A)\\cdot \\ln\\mu(A)",
            "\\log_2",
            "\\Eta(X,Y) \\le \\Eta(X)+\\Eta(Y)",
            "X,Y",
            "\\Eta(X,Y) = \\Eta(X)+\\Eta(Y)",
            "X,Y",
            "\\Eta_{n+1}(p_1, \\ldots, p_n, 0) = \\Eta_n(p_1, \\ldots, p_n)",
            "\\Eta_n(p_1, \\ldots, p_n)",
            "p_1, \\ldots, p_n",
            "\\lim_{q \\to 0^+} \\Eta_2(1-q, q) = 0",
            "\\Eta",
            "p_1,\\ldots ,p_n",
            "\\Eta",
            "\\Eta_{n+1}(p_1,\\ldots,p_n,0) = \\Eta_n(p_1,\\ldots,p_n)",
            "\\Eta(p_1,\\dots,p_n) \\leq \\log_b n",
            "\\Eta(X,Y)=\\Eta(X|Y)+\\Eta(Y)=\\Eta(Y|X)+\\Eta(X).",
            "Y=f(X)",
            "f",
            "\\Eta(f(X)|X) = 0",
            "\\Eta(X,f(X))",
            "\\Eta(X)+\\Eta(f(X)|X)=\\Eta(f(X))+\\Eta(X|f(X)),",
            "\\Eta(f(X)) \\le \\Eta(X)",
            "\\Eta(X|Y)=\\Eta(X).",
            "\\Eta(X|Y)\\leq \\Eta(X)",
            "\\Eta(X,Y)\\leq \\Eta(X)+\\Eta(Y)",
            "\\Eta(p)",
            "p",
            "\\Eta(\\lambda p_1 + (1-\\lambda) p_2) \\ge \\lambda \\Eta(p_1) + (1-\\lambda) \\Eta(p_2)",
            "p_1,p_2",
            "0 \\le \\lambda \\le 1",
            "S = - k_\\text{B} \\sum p_i \\ln p_i \\,,",
            "S = - k_\\text{B} \\,{\\rm Tr}(\\rho \\ln \\rho) \\,,",
            "S=k_\\text{B} \\ln W,",
            "S",
            "2^{127}",
            "\\Eta(\\mathcal{S}) = - \\sum p_i \\log p_i ,",
            "\\Eta(\\mathcal{S}) = - \\sum_i p_i \\sum_j  \\  p_i (j) \\log p_i (j) ,",
            "p_i(j)",
            "\\Eta(\\mathcal{S}) = -\\sum_i p_i \\sum_j p_i(j) \\sum_k p_{i,j}(k)\\ \\log \\  p_{i,j}(k) .",
            "\\mathcal{X}",
            "\\eta(X) = \\frac{H}{H_\\text{max}} = -\\sum_{i=1}^n \\frac{p(x_i) \\log_b (p(x_i))}{\\log_b (n)}.",
            "\\eta(X) = -\\sum_{i=1}^n \\frac{p(x_i) \\log_b (p(x_i))}{\\log_b (n)}\n= \\sum_{i=1}^n \\frac{\\log_b(p(x_i)^{-p(x_i)})}{\\log_b(n)}\n= \\sum_{i=1}^n \\log_n(p(x_i)^{-p(x_i)})\n= \\log_n \\left(\\prod_{i=1}^n p(x_i)^{-p(x_i)}\\right).",
            "{\\log_b (n)}",
            "\\mathbb X",
            "\\Eta(X) = \\mathbb{E}[-\\log f(X)] = -\\int_\\mathbb X f(x) \\log f(x)\\, \\mathrm{d}x.",
            "\\Delta",
            "\\begin{align}\n\\sum_{i=-\\infty}^{\\infty} f(x_i) \\Delta &\\to \\int_{-\\infty}^{\\infty} f(x)\\, dx = 1 \\\\\n\\sum_{i=-\\infty}^{\\infty} f(x_i) \\Delta \\log (f(x_i)) &\\to \\int_{-\\infty}^{\\infty} f(x) \\log f(x)\\, dx.\n\\end{align}",
            "h[f] = \\lim_{\\Delta \\to 0} \\left(\\Eta^{\\Delta} + \\log \\Delta\\right) = -\\int_{-\\infty}^{\\infty} f(x) \\log f(x)\\,dx,",
            "N \\rightarrow \\infty",
            "\\log(N)",
            "D_{\\mathrm{KL}}(p \\| m ) = \\int \\log (f(x)) p(dx) = \\int f(x)\\log (f(x)) m(dx) .",
            "\\lambda(n+H)",
            "|A|^{d-1}\\leq \\prod_{i=1}^{d} |P_{i}(A)|",
            "P_{i}(A)=\\{(x_{1}, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_{d}) : (x_{1}, \\ldots, x_{d})\\in A\\}.",
            "\\Eta[(X_{1}, \\ldots ,X_{d})]\\leq \\frac{1}{r}\\sum_{i=1}^{n}\\Eta[(X_{j})_{j\\in S_{i}}]",
            "(X_{j})_{j\\in S_{i}}",
            "(X_{j})_{j\\in S_{i}}",
            "\\Eta[(X_{j})_{j\\in S_{i}}]\\leq \\log |P_{i}(A)|",
            "\\frac{2^{n\\Eta(q)}}{n+1} \\leq \\tbinom nk \\leq 2^{n\\Eta(q)},",
            "\\Eta(q) = -q \\log_2(q) - (1-q) \\log_2(1-q).",
            "\\tbinom nk q^{qn}(1-q)^{n-nq}",
            "\\sum_{i=0}^n \\tbinom ni q^i(1-q)^{n-i} = (q + (1-q))^n = 1.",
            "\\binom nk q^{qn}(1-q)^{n-nq} \\geq \\frac{1}{n+1}",
            "2^{n\\Eta(k/n)}",
            "IG(Y,X)",
            "Y",
            "Y",
            "X",
            "X",
            "\\log(0)",
            "\\lim\\limits_{x\\rightarrow0}x\\log(x)=0",
            "0\\log(0)",
            "p\\colon \\mathcal{X}\\to(0, 1]"
        ]
    },
    "Information_gain": {
        "title": "Information_gain",
        "chunks": [
            {
                "text": "REDIRECT Kullback–Leibler divergence"
            }
        ],
        "latex_formulas": []
    },
    "Decision_tree_learning": {
        "title": "Decision_tree_learning",
        "chunks": [
            {
                "text": "Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. More generally, the concept of regression tree can be extended to any kind of object equipped with pairwise dissimilarities such as categorical sequences. Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making). General thumb|A tree showing survival of passengers on the Titanic (\"sibsp\" is the number of spouses or siblings aboard)."
            },
            {
                "text": "The figures under the leaves show the probability of survival and the percentage of observations in the leaf. Summarizing: Your chances of survival were good if you were (i) a female or (ii) a male at most 9.5 years old with strictly fewer than 3 siblings. Decision tree learning is a method commonly used in data mining. The goal is to create a model that predicts the value of a target variable based on several input variables. A decision tree is a simple representation for classifying examples. For this section, assume that all of the input features have finite discrete domains, and there is a single target feature called the \"classification\". Each element of the domain of the classification is called a class. A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with an input feature are labeled with each of the possible values of the target feature or the arc leads to a subordinate decision node on a different input feature."
            },
            {
                "text": "Each leaf of the tree is labeled with a class or a probability distribution over the classes, signifying that the data set has been classified by the tree into either a specific class, or into a particular probability distribution (which, if the decision tree is well-constructed, is skewed towards certain subsets of classes). A tree is built by splitting the source set, constituting the root node of the tree, into subsets—which constitute the successor children. The splitting is based on a set of splitting rules based on classification features. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same values of the target variable, or when splitting no longer adds value to the predictions. This process of top-down induction of decision trees (TDIDT) is an example of a greedy algorithm, and it is by far the most common strategy for learning decision trees from data. In data mining, decision trees can be described also as the combination of mathematical and computational techniques to aid the description, categorization and generalization of a given set of data."
            },
            {
                "text": "Data comes in records of the form: $(\\textbf{x},Y) = (x_1, x_2, x_3, ..., x_k, Y)$ The dependent variable, $Y$, is the target variable that we are trying to understand, classify or generalize. The vector $\\textbf{x}$ is composed of the features, $x_1, x_2, x_3$ etc., that are used for that task. alt=Three different representations of a regression tree of kyphosis data| An example tree which estimates the probability of kyphosis after spinal surgery, given the age of the patient and the vertebra at which surgery was started. The same tree is shown in three different ways. Left The colored leaves show the probability of kyphosis after spinal surgery, and percentage of patients in the leaf. Middle The tree as a perspective plot. Right Aerial view of the middle plot. The probability of kyphosis after surgery is higher in the darker areas. (Note: The treatment of kyphosis has advanced considerably since this rather small set of data was collected.)"
            },
            {
                "text": "Decision tree types Decision trees used in data mining are of two main types: Classification tree analysis is when the predicted outcome is the class (discrete) to which the data belongs. Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient's length of stay in a hospital). The term classification and regression tree (CART) analysis is an umbrella term used to refer to either of the above procedures, first introduced by Breiman et al. in 1984. Trees used for regression and trees used for classification have some similarities – but also some differences, such as the procedure used to determine where to split. Some techniques, often called ensemble methods, construct more than one decision tree: Boosted trees Incrementally building an ensemble by training each new instance to emphasize the training instances previously mis-modeled. A typical example is AdaBoost. These can be used for regression-type and classification-type problems.Friedman, J. H. (1999). Stochastic gradient boosting . Stanford University.Hastie, T., Tibshirani, R., Friedman, J. H. (2001)."
            },
            {
                "text": "The elements of statistical learning : Data mining, inference, and prediction. New York: Springer Verlag. Committees of decision trees (also called k-DTHeath, D., Kasif, S. and Salzberg, S. (1993). k-DT: A multi-tree learning method. In Proceedings of the Second Intl. Workshop on Multistrategy Learning, pp. 138-149. ), an early method that used randomized decision tree algorithms to generate multiple different trees from the training data, and then combine them using majority voting to generate output.Heath, D., Kasif, S., and Salzberg, S. L. (1996). Committees of decision trees. In B. Gorayska and J. Mey (Eds. ), Cognitive Technology: In Search of a Humane Interface (pp. 305–317). Amsterdam: Elsevier Science B.V. Bootstrap aggregated (or bagged) decision trees, an early ensemble method, builds multiple decision trees by repeatedly resampling training data with replacement, and voting the trees for a consensus prediction. A random forest classifier is a specific type of bootstrap aggregating Rotation forest – in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features."
            },
            {
                "text": "A special case of a decision tree is a decision list, which is a one-sided decision tree, so that every internal node has exactly 1 leaf node and exactly 1 internal node as a child (except for the bottommost node, whose only child is a single leaf node). While less expressive, decision lists are arguably easier to understand than general decision trees due to their added sparsity, permit non-greedy learning methods and monotonic constraints to be imposed. Notable decision tree algorithms include: ID3 (Iterative Dichotomiser 3) C4.5 (successor of ID3) CART (Classification And Regression Tree) OC1 (Oblique classifier 1). First method that created multivariate splits at each node. Chi-square automatic interaction detection (CHAID). Performs multi-level splits when computing classification trees.Ritschard, G. (2013), \"CHAID and Earlier Supervised Tree Methods\", in J.J. McArdle and G. Ritschard (eds), Contemporary Issues in Exploratory Data Mining in the Behavioral Sciences, Quantitative Methodology Series, New York: Routledge, pages 48-74. Preprint MARS: extends decision trees to handle numerical data better."
            },
            {
                "text": "Conditional Inference Trees. Statistics-based approach that uses non-parametric tests as splitting criteria, corrected for multiple testing to avoid overfitting. This approach results in unbiased predictor selection and does not require pruning. ID3 and CART were invented independently at around the same time (between 1970 and 1980), yet follow a similar approach for learning a decision tree from training tuples. It has also been proposed to leverage concepts of fuzzy set theory for the definition of a special version of decision tree, known as Fuzzy Decision Tree (FDT). In this type of fuzzy classification, generally, an input vector $\\textbf{x}$ is associated with multiple classes, each with a different confidence value. Boosted ensembles of FDTs have been recently investigated as well, and they have shown performances comparable to those of other very efficient fuzzy classifiers. Metrics Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items. Different algorithms use different metrics for measuring \"best\". These generally measure the homogeneity of the target variable within the subsets."
            },
            {
                "text": "Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split. Depending on the underlying metric, the performance of various heuristic algorithms for decision tree learning may vary significantly. Estimate of Positive Correctness A simple and effective metric can be used to identify the degree to which true positives outweigh false positives (see Confusion matrix). This metric, \"Estimate of Positive Correctness\" is defined below: $ E_P = TP - FP $ In this equation, the total false positives (FP) are subtracted from the total true positives (TP). The resulting number gives an estimate on how many positive examples the feature could correctly identify within the data, with higher numbers meaning that the feature could correctly classify more positive samples. Below is an example of how to use the metric when the full confusion matrix of a certain feature is given: Feature A Confusion Matrix CancerNon-cancerCancer83Non-cancer25 Here we can see that the TP value would be 8 and the FP value would be 2 (the underlined numbers in the table)."
            },
            {
                "text": "When we plug these numbers in the equation we are able to calculate the estimate: $E_p = TP - FP = 8 - 2 = 6$. This means that using the estimate on this feature would have it receive a score of 6. However, it should be worth noting that this number is only an estimate. For example, if two features both had a FP value of 2 while one of the features had a higher TP value, that feature would be ranked higher than the other because the resulting estimate when using the equation would give a higher value. This could lead to some inaccuracies when using the metric if some features have more positive samples than others. To combat this, one could use a more powerful metric known as Sensitivity that takes into account the proportions of the values from the confusion matrix to give the actual true positive rate (TPR). The difference between these metrics is shown in the example below: +Feature A Confusion Matrix CancerNon-cancerCancer83Non-cancer25 Feature B Confusion Matrix CancerNon-cancerCancer62Non-cancer28$E_p = TP - FP = 8 - 2 = 6$ $TPR = TP / (TP + FN) = 8 / (8 + 3) \\approx 0.73 $ $E_p = TP - FP = 6 - 2 = 4$ $TPR = TP / (TP + FN) = 6 / (6 + 2) = 0.75 $ In this example, Feature A had an estimate of 6 and a TPR of approximately 0.73 while Feature B had an estimate of 4 and a TPR of 0.75."
            },
            {
                "text": "This shows that although the positive estimate for some feature may be higher, the more accurate TPR value for that feature may be lower when compared to other features that have a lower positive estimate. Depending on the situation and knowledge of the data and decision trees, one may opt to use the positive estimate for a quick and easy solution to their problem. On the other hand, a more experienced user would most likely prefer to use the TPR value to rank the features because it takes into account the proportions of the data and all the samples that should have been classified as positive. Gini impurity Gini impurity, Gini's diversity index, or Gini-Simpson Index in biodiversity research, is named after Italian mathematician Corrado Gini and used by the CART (classification and regression tree) algorithm for classification trees. Gini impurity measures how often a randomly chosen element of a set would be incorrectly labeled if it were labeled randomly and independently according to the distribution of labels in the set. It reaches its minimum (zero) when all cases in the node fall into a single target category."
            },
            {
                "text": "For a set of items with $J$ classes and relative frequencies $p_i$, $i \\in \\{1, 2, ...,J\\}$, the probability of choosing an item with label $i$ is $p_i$, and the probability of miscategorizing that item is $\\sum_{k \\ne i} p_k = 1-p_i$. The Gini impurity is computed by summing pairwise products of these probabilities for each class label: $\\operatorname{I}_G(p) = \\sum_{i=1}^J \\left( p_i \\sum_{k\\neq i} p_k \\right) = \\sum_{i=1}^J p_i (1-p_i) = \\sum_{i=1}^J (p_i - p_i^2) = \\sum_{i=1}^J p_i - \\sum_{i=1}^J p_i^2 = 1 - \\sum^J_{i=1} p_i^2. $ The Gini impurity is also an information theoretic measure and corresponds to Tsallis Entropy with deformation coefficient $q=2$, which in physics is associated with the lack of information in out-of-equilibrium, non-extensive, dissipative and quantum systems. For the limit $q\\to 1$ one recovers the usual Boltzmann-Gibbs or Shannon entropy."
            },
            {
                "text": "In this sense, the Gini impurity is nothing but a variation of the usual entropy measure for decision trees. Information gain Used by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy and information content from information theory. Entropy is defined as below $\\Eta(T) = \\operatorname{I}_{E}\\left(p_1, p_2, \\ldots, p_J\\right) = - \\sum^J_{i=1} p_i \\log_2 p_i$ where $p_1, p_2, \\ldots$ are fractions that add up to 1 and represent the percentage of each class present in the child node that results from a split in the tree. $=-\\sum_{i=1}^J p_i\\log_2 p_i - \\sum_{i=1}^J - \\Pr(i\\mid a)\\log_2 \\Pr(i\\mid a)$ Averaging over the possible values of $A$, $=-\\sum_{i=1}^J p_i\\log_2 p_i - \\sum_a p(a)\\sum_{i=1}^J-\\Pr(i\\mid a) \\log_2 \\Pr(i\\mid a) $ Where weighted sum of entropies is given by, ${\\Eta(T\\mid A)}= \\sum_a p(a)\\sum_{i=1}^J-\\Pr(i\\mid a) \\log_2 \\Pr(i\\mid a)$ That is, the expected information gain is the mutual information, meaning that on average, the reduction in the entropy of T is the mutual information."
            },
            {
                "text": "Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the most consistent child nodes. A commonly used measure of consistency is called information which is measured in bits. For each node of the tree, the information value \"represents the expected amount of information that would be needed to specify whether a new instance should be classified yes or no, given that the example reached that node\". Consider an example data set with four attributes: outlook (sunny, overcast, rainy), temperature (hot, mild, cool), humidity (high, normal), and windy (true, false), with a binary (yes or no) target variable, play, and 14 data points. To construct a decision tree on this data, we need to compare the information gain of each of four trees, each split on one of the four features."
            },
            {
                "text": "The split with the highest information gain will be taken as the first split and the process will continue until all children nodes each have consistent data, or until the information gain is 0. To find the information gain of the split using windy, we must first calculate the information in the data before the split. The original data contained nine yes's and five no's. $ I_E([9,5]) = -\\frac 9 {14}\\log_2 \\frac 9 {14} - \\frac 5 {14}\\log_2 \\frac 5 {14} = 0.94 $ The split using the feature windy results in two children nodes, one for a windy value of true and one for a windy value of false. In this data set, there are six data points with a true windy value, three of which have a play (where play is the target variable) value of yes and three with a play value of no. The eight remaining data points with a windy value of false contain two no's and six yes's. The information of the windy=true node is calculated using the entropy equation above."
            },
            {
                "text": "Since there is an equal number of yes's and no's in this node, we have $ I_E([3,3]) = -\\frac 3 6\\log_2 \\frac 3 6 - \\frac 3 6\\log_2 \\frac 3 6 = -\\frac 1 2\\log_2 \\frac 1 2 - \\frac 1 2\\log_2 \\frac 1 2 = 1 $ For the node where windy=false there were eight data points, six yes's and two no's. Thus we have $ I_E([6,2]) = -\\frac 6 8\\log_2 \\frac 6 8 - \\frac 2 8\\log_2 \\frac 2 8 = -\\frac 3 4\\log_2 \\frac 3 4 - \\frac 1 4\\log_2 \\frac 1 4 = 0.81 $ To find the information of the split, we take the weighted average of these two numbers based on how many observations fell into which node. $ I_E([3,3],[6,2]) = I_E(\\text{windy or not}) = \\frac 6 {14} \\cdot 1 + \\frac 8 {14} \\cdot 0.81 = 0.89 $ Now we can calculate the information gain achieved by splitting on the windy feature. $ \\operatorname{IG}(\\text{windy}) = I_E([9,5]) - I_E([3,3],[6,2]) = 0.94 - 0.89 = 0.05 $ To build the tree, the information gain of each possible first split would need to be calculated."
            },
            {
                "text": "The best first split is the one that provides the most information gain. This process is repeated for each impure node until the tree is complete. This example is adapted from the example appearing in Witten et al. Information gain is also known as Shannon index in bio diversity research. Variance reduction Introduced in CART, variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node is defined as the total reduction of the variance of the target variable due to the split at this node: $ I_V(N) = \\frac{1}{|S|^2}\\sum_{i\\in S} \\sum_{j\\in S} \\frac{1}{2}(y_i - y_j)^2 - \\left(\\frac{|S_t|^2}{|S|^2}\\frac{1}{|S_t|^2}\\sum_{i\\in S_t} \\sum_{j\\in S_t} \\frac{1}{2}(y_i - y_j)^2 + \\frac{|S_f|^2}{|S|^2}\\frac{1}{|S_f|^2}\\sum_{i\\in S_f} \\sum_{j\\in S_f} \\frac{1}{2}(y_i - y_j)^2\\right) $ where $S$, $S_t$, and $S_f$ are the set of presplit sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively."
            },
            {
                "text": "Each of the above summands are indeed variance estimates, though, written in a form without directly referring to the mean. By replacing $(y_i - y_j)^2$ in the formula above with the dissimilarity $d_{ij}$ between two objects $i$ and $j$, the variance reduction criterion applies to any kind of object for which pairwise dissimilarities can be computed. Measure of \"goodness\" Used by CART in 1984, the measure of \"goodness\" is a function that seeks to optimize the balance of a candidate split's capacity to create pure children with its capacity to create equally-sized children. This process is repeated for each impure node until the tree is complete. The function $\\varphi(s\\mid t)$, where $s$ is a candidate split at node $t$, is defined as below $ \\varphi(s\\mid t) = 2P_L P_R \\sum_{j=1}^\\text{class count}|P(j\\mid t_L) - P(j\\mid t_R)| $ where $t_L$ and $t_R$ are the left and right children of node $t$ using split $s$, respectively; $P_L$ and $P_R$ are the proportions of records in $t$ in $t_L$ and $t_R$, respectively; and $P(j\\mid t_L)$ and $P(j\\mid t_R)$ are the proportions of class $j$ records in $t_L$ and $t_R$, respectively."
            },
            {
                "text": "Consider an example data set with three attributes: savings(low, medium, high), assets(low, medium, high), income(numerical value), and a binary target variable credit risk(good, bad) and 8 data points. The full data is presented in the table below. To start a decision tree, we will calculate the maximum value of $\\varphi(s\\mid t)$ using each feature to find which one will split the root node. This process will continue until all children are pure or all $\\varphi(s\\mid t)$ values are below a set threshold. Customer Savings Assets Income ($1000s) Credit risk 1 Medium High 75 Good 2 Low Low 50 Bad 3 High Medium 25 Bad 4 Medium Medium 50 Good 5 Low Medium 100 Good 6 High High 25 Good 7 Low Low 25 Bad 8 Medium Medium 75 Good To find $\\varphi(s\\mid t)$ of the feature savings, we need to note the quantity of each value. The original data contained three low's, three medium's, and two high's."
            },
            {
                "text": "Out of the low's, one had a good credit risk while out of the medium's and high's, 4 had a good credit risk. Assume a candidate split $s$ such that records with a low savings will be put in the left child and all other records will be put into the right child. $ \\varphi(s\\mid\\text{root}) = 2\\cdot\\frac 3 8\\cdot\\frac 5 8\\cdot \\left(\\left|\\left(\\frac 1 3 - \\frac 4 5\\right)\\right| + \\left|\\left(\\frac 2 3 - \\frac 1 5\\right)\\right|\\right) = 0.44 $ To build the tree, the \"goodness\" of all candidate splits for the root node need to be calculated. The candidate with the maximum value will split the root node, and the process will continue for each impure node until the tree is complete. Compared to other metrics such as information gain, the measure of \"goodness\" will attempt to create a more balanced tree, leading to more-consistent decision time. However, it sacrifices some priority for creating pure children which can lead to additional splits that are not present with other metrics."
            },
            {
                "text": "Uses Advantages Amongst other data mining methods, decision trees have various advantages: Simple to understand and interpret. People are able to understand decision tree models after a brief explanation. Trees can also be displayed graphically in a way that is easy for non-experts to interpret. Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. (For example, relation rules can be used only with nominal variables while neural networks can be used only with numerical variables or categoricals converted to 0-1 values.) Early decision trees were only capable of handling categorical variables, but more recent versions, such as C4.5, do not have this limitation. Requires little data preparation. Other techniques often require data normalization. Since trees can handle qualitative predictors, there is no need to create dummy variables. Uses a white box or open-box model. If a given situation is observable in a model the explanation for the condition is easily explained by Boolean logic. By contrast, in a black box model, the explanation for the results is typically difficult to understand, for example with an artificial neural network."
            },
            {
                "text": "Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model. Non-parametric approach that makes no assumptions of the training data or prediction residuals; e.g., no distributional, independence, or constant variance assumptions Performs well with large datasets. Large amounts of data can be analyzed using standard computing resources in reasonable time. Accuracy with flexible modeling. These methods may be applied to healthcare research with increased accuracy. Mirrors human decision making more closely than other approaches. This could be useful when modeling human decisions/behavior. Robust against co-linearity, particularly boosting. In built feature selection. Additional irrelevant feature will be less used so that they can be removed on subsequent runs. The hierarchy of attributes in a decision tree reflects the importance of attributes. It means that the features on top are the most informative. Decision trees can approximate any Boolean function e.g. XOR. Limitations Trees can be very non-robust. A small change in the training data can result in a large change in the tree and consequently the final predictions. The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts.Murthy S. (1998)."
            },
            {
                "text": "\"Automatic construction of decision trees from data: A multidisciplinary survey\". Data Mining and Knowledge Discovery Consequently, practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. To reduce the greedy effect of local optimality, some methods such as the dual information distance (DID) tree were proposed. Decision-tree learners can create over-complex trees that do not generalize well from the training data. (This is known as overfitting.) Mechanisms such as pruning are necessary to avoid this problem (with the exception of some algorithms such as the Conditional Inference approach, that does not require pruning). The average depth of the tree that is defined by the number of nodes or tests till classification is not guaranteed to be minimal or small under various splitting criteria. For data including categorical variables with different numbers of levels, information gain in decision trees is biased in favor of attributes with more levels. To counter this problem, instead of choosing the attribute with highest information gain, one can choose the attribute with the highest information gain ratio among the attributes whose information gain is greater than the mean information gain."
            },
            {
                "text": "This biases the decision tree against considering attributes with a large number of distinct values, while not giving an unfair advantage to attributes with very low information gain. Alternatively, the issue of biased predictor selection can be avoided by the Conditional Inference approach, a two-stage approach, or adaptive leave-one-out feature selection. Implementations Many data mining software packages provide implementations of one or more decision tree algorithms (e.g. random forest). Open source examples include: ALGLIB, a C++, C# and Java numerical analysis library with data analysis features (random forest) KNIME, a free and open-source data analytics, reporting and integration platform (decision trees, random forest) Orange, an open-source data visualization, machine learning and data mining toolkit (random forest) R (an open-source software environment for statistical computing, which includes several CART implementations such as rpart, party and randomForest packages), scikit-learn (a free and open-source machine learning library for the Python programming language). Weka (a free and open-source data-mining suite, contains many decision tree algorithms), Notable commercial software: MATLAB, Microsoft SQL Server, and RapidMiner, SAS Enterprise Miner, IBM SPSS Modeler, Extensions Decision graphs In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or AND."
            },
            {
                "text": "In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using minimum message length (MML). Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph.Tan & Dowe (2003) The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring. In general, decision graphs infer models with fewer leaves than decision trees. Alternative search methods Evolutionary algorithms have been used to avoid local optimal decisions and search the decision tree space with little a priori bias. It is also possible for a tree to be sampled using MCMC. The tree can be searched for in a bottom-up fashion. Or several trees can be constructed parallelly to reduce the expected number of tests till classification. See also Decision tree pruning Binary decision diagram CHAID CART ID3 algorithm C4.5 algorithm Decision stumps, used in e.g. AdaBoosting Decision list Incremental decision tree Alternating decision tree Structured data analysis (statistics) Logistic model tree Hierarchical clustering References Further reading External links Evolutionary Learning of Decision Trees in C++ A very detailed explanation of information gain as splitting criterion Category:Decision trees Category:Classification algorithms"
            }
        ],
        "latex_formulas": [
            "(\\textbf{x},Y) = (x_1, x_2, x_3, ..., x_k, Y)",
            "Y",
            "\\textbf{x}",
            "x_1, x_2, x_3",
            "\\textbf{x}",
            "E_P = TP - FP",
            "E_p = TP - FP = 8 - 2 = 6",
            "E_p = TP - FP = 8 - 2 = 6",
            "TPR = TP / (TP + FN) = 8 / (8 + 3) \\approx 0.73",
            "E_p = TP - FP = 6 - 2 = 4",
            "TPR = TP / (TP + FN) = 6 / (6 + 2) = 0.75",
            "J",
            "p_i",
            "i \\in \\{1, 2, ...,J\\}",
            "i",
            "p_i",
            "\\sum_{k \\ne i} p_k = 1-p_i",
            "\\operatorname{I}_G(p) = \\sum_{i=1}^J \\left( p_i \\sum_{k\\neq i} p_k \\right)\n = \\sum_{i=1}^J p_i (1-p_i)\n = \\sum_{i=1}^J (p_i - p_i^2)\n = \\sum_{i=1}^J p_i - \\sum_{i=1}^J p_i^2\n = 1 - \\sum^J_{i=1} p_i^2.",
            "q=2",
            "q\\to 1",
            "\\Eta(T) = \\operatorname{I}_{E}\\left(p_1, p_2, \\ldots, p_J\\right)\n = - \\sum^J_{i=1} p_i \\log_2 p_i",
            "p_1, p_2, \\ldots",
            "=-\\sum_{i=1}^J p_i\\log_2 p_i - \\sum_{i=1}^J - \\Pr(i\\mid a)\\log_2 \\Pr(i\\mid a)",
            "A",
            "=-\\sum_{i=1}^J p_i\\log_2 p_i - \\sum_a p(a)\\sum_{i=1}^J-\\Pr(i\\mid a) \\log_2 \\Pr(i\\mid a)",
            "{\\Eta(T\\mid A)}= \\sum_a p(a)\\sum_{i=1}^J-\\Pr(i\\mid a) \\log_2 \\Pr(i\\mid a)",
            "I_E([9,5]) = -\\frac 9 {14}\\log_2 \\frac 9 {14} - \\frac 5 {14}\\log_2 \\frac 5 {14} = 0.94",
            "I_E([3,3]) = -\\frac 3 6\\log_2 \\frac 3 6 - \\frac 3 6\\log_2 \\frac 3 6 = -\\frac 1 2\\log_2 \\frac 1 2 - \\frac 1 2\\log_2 \\frac 1 2 = 1",
            "I_E([6,2]) = -\\frac 6 8\\log_2 \\frac 6 8 - \\frac 2 8\\log_2 \\frac 2 8 = -\\frac 3 4\\log_2 \\frac 3 4 - \\frac 1 4\\log_2 \\frac 1 4 = 0.81",
            "I_E([3,3],[6,2]) = I_E(\\text{windy or not}) = \\frac 6 {14} \\cdot 1 + \\frac 8 {14} \\cdot 0.81 = 0.89",
            "\\operatorname{IG}(\\text{windy}) = I_E([9,5]) - I_E([3,3],[6,2]) = 0.94 - 0.89 = 0.05",
            "I_V(N) = \\frac{1}{|S|^2}\\sum_{i\\in S} \\sum_{j\\in S} \\frac{1}{2}(y_i - y_j)^2 - \\left(\\frac{|S_t|^2}{|S|^2}\\frac{1}{|S_t|^2}\\sum_{i\\in S_t} \\sum_{j\\in S_t} \\frac{1}{2}(y_i - y_j)^2 + \\frac{|S_f|^2}{|S|^2}\\frac{1}{|S_f|^2}\\sum_{i\\in S_f} \\sum_{j\\in S_f} \\frac{1}{2}(y_i - y_j)^2\\right)",
            "S",
            "S_t",
            "S_f",
            "(y_i - y_j)^2",
            "d_{ij}",
            "i",
            "j",
            "\\varphi(s\\mid t)",
            "s",
            "t",
            "\\varphi(s\\mid t) = 2P_L P_R \\sum_{j=1}^\\text{class count}|P(j\\mid t_L) - P(j\\mid t_R)|",
            "t_L",
            "t_R",
            "t",
            "s",
            "P_L",
            "P_R",
            "t",
            "t_L",
            "t_R",
            "P(j\\mid t_L)",
            "P(j\\mid t_R)",
            "j",
            "t_L",
            "t_R",
            "\\varphi(s\\mid t)",
            "\\varphi(s\\mid t)",
            "\\varphi(s\\mid t)",
            "s",
            "\\varphi(s\\mid\\text{root}) = 2\\cdot\\frac 3 8\\cdot\\frac 5 8\\cdot \\left(\\left|\\left(\\frac 1 3 - \\frac 4 5\\right)\\right| + \\left|\\left(\\frac 2 3 - \\frac 1 5\\right)\\right|\\right) = 0.44"
        ]
    },
    "Pruning_(decision_trees)": {
        "title": "Pruning_(decision_trees)",
        "chunks": [
            {
                "text": "REDIRECT Decision tree pruning"
            }
        ],
        "latex_formulas": []
    },
    "Random_forest": {
        "title": "Random_forest",
        "chunks": [
            {
                "text": "Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the output is the average of the predictions of the trees. Random forests correct for decision trees' habit of overfitting to their training set. The first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg. An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registeredU.S. trademark registration number 3185828, registered 2006/12/19. \"Random Forests\" as a trademark in 2006 (, owned by Minitab, Inc.). The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance."
            },
            {
                "text": "History The general method of random decision forests was first proposed by Salzberg and Heath in 1993,Heath, D., Kasif, S. and Salzberg, S. (1993). k-DT: A multi-tree learning method. In Proceedings of the Second Intl. Workshop on Multistrategy Learning, pp. 138-149. with a method that used a randomized decision tree algorithm to create multiple trees and then combine them using majority voting. This idea was developed further by Ho in 1995. Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions. A subsequent work along the same lines concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions. This observation that a more complex classifier (a larger forest) gets more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting."
            },
            {
                "text": "The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination. The early development of Breiman's notion of random forests was influenced by the work of Amit and Geman who introduced the idea of searching over a random subset of the available decisions when splitting a node, in the context of growing a single tree. The idea of random subspace selection from Ho was also influential in the design of random forests. This method grows a forest of trees, and introduces variation among the trees by projecting the training data into a randomly chosen subspace before fitting each tree or each node. Finally, the idea of randomized node optimization, where the decision at each node is selected by a randomized procedure, rather than a deterministic optimization was first introduced by Thomas G. Dietterich. The proper introduction of random forests was made in a paper by Leo Breiman. This paper describes a method of building a forest of uncorrelated trees using a CART like procedure, combined with randomized node optimization and bagging. In addition, this paper combines several ingredients, some previously known and some novel, which form the basis of the modern practice of random forests, in particular: Using out-of-bag error as an estimate of the generalization error."
            },
            {
                "text": "Measuring variable importance through permutation. The report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation. Algorithm Preliminaries: decision tree learning Decision trees are a popular method for various machine learning tasks. Tree learning is almost \"an off-the-shelf procedure for data mining\", say Hastie et al., \"because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate\". In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model."
            },
            {
                "text": "Bagging thumb|Illustration of training a Random Forest model. The training dataset (in this case, of 250 rows and 100 columns) is randomly sampled with replacement n times. Then, a decision tree is trained on each sample. Finally, for prediction, the results of all n trees are aggregated to produce a final decision. The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set = , ..., with responses = , ..., , bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to these samples: After training, predictions for unseen samples can be made by averaging the predictions from all the individual regression trees on : or by taking the plurality vote in the case of classification trees. This bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated."
            },
            {
                "text": "Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets. Additionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on : The number of samples (equivalently, of trees) is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. can be optimized using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample , using only the trees that did not have in their bootstrap sample. The training and test error tend to level off after some number of trees have been fit. From bagging to random forests The above procedure describes the original bagging algorithm for trees. Random forests also include another type of bagging scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features."
            },
            {
                "text": "This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho. Typically, for a classification problem with features, (rounded down) features are used in each split. For regression problems the inventors recommend $p/3$ (rounded down) with a minimum node size of 5 as the default. In practice, the best values for these parameters should be tuned on a case-to-case basis for every problem. ExtraTrees Adding one further step of randomization yields extremely randomized trees, or ExtraTrees. As with ordinary random forests, they are an ensemble of individual trees, but there are two main differences: (1) each tree is trained using the whole learning sample (rather than a bootstrap sample), and (2) the top-down splitting is randomized: for each feature under consideration, a number of random cut-points are selected, instead of computing the locally optimal cut-point (based on, e.g., information gain or the Gini impurity)."
            },
            {
                "text": "The values are chosen from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly chosen splits, the split that yields the highest score is chosen to split the node. Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are $\\sqrt{p}$ for classification and $p$ for regression, where $p$ is the number of features in the model. Random forests for high-dimensional data The basic random forest procedure may not work well in situations where there are a large number of features but only a small proportion of these features are informative with respect to sample classification. This can be addressed by encouraging the procedure to focus mainly on features and trees that are informative. Some methods for accomplishing this are: Prefiltering: Eliminate features that are mostly just noise.Dessi, N. & Milia, G. & Pes, B. (2013). Enhancing random forests performance in microarray data classification."
            },
            {
                "text": "Conference paper, 99-103. 10.1007/978-3-642-38326-7_15.Ye, Y., Li, H., Deng, X., and Huang, J. (2008) Feature weighting random forest for detection of hidden web search interfaces. Journal of Computational Linguistics and Chinese Language Processing, 13, 387–404. Enriched random forest (ERF): Use weighted random sampling instead of simple random sampling at each node of each tree, giving greater weight to features that appear to be more informative.Amaratunga, D., Cabrera, J., Lee, Y.S. (2008) Enriched Random Forest. Bioinformatics, 24, 2010-2014.Ghosh D, Cabrera J. (2022) Enriched random forest for high dimensional genomic data. IEEE/ACM Trans Comput Biol Bioinform. 19(5):2817-2828. doi:10.1109/TCBB.2021.3089417. Tree-weighted random forest (TWRF): Give more weight to more accurate trees.Winham, Stacey & Freimuth, Robert & Biernacka, Joanna. (2013). A weighted random forests approach to improve predictive performance. Statistical Analysis and Data Mining. 6. 10.1002/sam.11196. Li, H. B., Wang, W., Ding, H. W., & Dong, J."
            },
            {
                "text": "(2010, 10-12 Nov. 2010). Trees weighting random forest method for classifying high-dimensional noisy data. Paper presented at the 2010 IEEE 7th International Conference on E-Business Engineering. Properties Variable importance Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way. The following technique was described in Breiman's original paper and is implemented in the R package randomForest. Permutation importance To measure a feature's importance in a data set $\\mathcal{D}_n =\\{(X_i, Y_i)\\}_{i=1}^n$, first a random forest is trained on the data. During training, the out-of-bag error for each data point is recorded and averaged over the forest. (If bagging is not used during training, we can instead compute errors on an independent test set.) After training, the values of the feature are permuted in the out-of-bag samples and the out-of-bag error is again computed on this perturbed data set. The importance for the feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees."
            },
            {
                "text": "The score is normalized by the standard deviation of these differences. Features which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu et al. This method of determining variable importance has some drawbacks: When features have different numbers of values, random forests favor features with more values. Solutions to this problem include partial permutations and growing unbiased trees. If the data contain groups of correlated features of similar relevance, then smaller groups are favored over large groups. If there are collinear features, the procedure may fail to identify important features. A solution is to permute groups of correlated features together. Mean decrease in impurity feature importance This approach to feature importance for random forests considers as important the variables which decrease a lot the impurity during splitting. It is described in the book Classification and Regression Trees by Leo Breiman and is the default implementation in sci-kit learn and R. The definition is:where $x$ is a feature $n_T$ is the number of trees in the forest $T_i$ is tree $i$ $p_{T_i}(j)=\\frac{n_j}{n}$ is the fraction of samples reaching node $j$ $\\Delta i_{T_i}(j)$ is the change in impurity in tree $t$ at node $j$."
            },
            {
                "text": "As impurity measure for samples falling in a node e.g. the following statistics can be used: Entropy Gini coefficient Mean squared error The normalized importance is then obtained by normalizing over all features, so that the sum of normalized feature importances is 1. The sci-kit learn default implementation can report misleading feature importance: it favors high cardinality features it uses training statistics and so does not reflect a feature's usefulness for predictions on a test sethttps://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html 31. Aug. 2023 Relationship to nearest neighbors A relationship between random forests and the -nearest neighbor algorithm (-NN) was pointed out by Lin and Jeon in 2002. Both can be viewed as so-called weighted neighborhoods schemes. These are models built from a training set $\\{(x_i, y_i)\\}_{i=1}^n$ that make predictions $\\hat{y}$ for new points by looking at the \"neighborhood\" of the point, formalized by a weight function :Here, $W(x_i, x')$ is the non-negative weight of the 'th training point relative to the new point in the same tree."
            },
            {
                "text": "For any , the weights for points $x_i$ must sum to 1. Weight functions are as follows: In -NN, $W(x_i, x') = \\frac{1}{k}$ if is one of the points closest to , and zero otherwise. In a tree, $W(x_i, x') = \\frac{1}{k'}$ if is one of the points in the same leaf as , and zero otherwise. Since a forest averages the predictions of a set of trees with individual weight functions $W_j$, its predictions are This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of in this interpretation are the points $x_i$ sharing the same leaf in any tree $j$. In this way, the neighborhood of depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature."
            },
            {
                "text": "Unsupervised learning As part of their construction, random forest predictors naturally lead to a dissimilarity measure among observations. One can analogously define dissimilarity between unlabeled data, by training a forest to distinguish original \"observed\" data from suitably generated synthetic data drawn from a reference distribution. A random forest dissimilarity is attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. Random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the \"Addcl 1\" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. Random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data. Variants Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers. In cases that the relationship between the predictors and the target variable is linear, the base learners may have an equally high accuracy as the ensemble learner."
            },
            {
                "text": "Kernel random forest In machine learning, kernel random forests (KeRF) establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze. History Leo Breiman was the first person to notice the link between random forest and kernel methods. He pointed out that random forests trained using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani proposed Kernel Random Forest (KeRF) and showed that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest and uniform random forest, two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency."
            },
            {
                "text": "Notations and definitions Preliminaries: Centered forests Centered forest is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level $k$ is built, where $k \\in\\mathbb{N} $ is a parameter of the algorithm. Uniform forest Uniform forest is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature. From random forest to KeRF Given a training sample $\\mathcal{D}_n =\\{(\\mathbf{X}_i, Y_i)\\}_{i=1}^n$ of $[0,1]^p\\times\\mathbb{R}$-valued independent random variables distributed as the independent prototype pair $(\\mathbf{X}, Y)$, where $\\operatorname{E}[Y^2]<\\infty$. We aim at predicting the response $Y$, associated with the random variable $\\mathbf{X}$, by estimating the regression function $m(\\mathbf{x})=\\operatorname{E}[Y \\mid \\mathbf{X} = \\mathbf{x}]$."
            },
            {
                "text": "A random regression forest is an ensemble of $M$ randomized regression trees. Denote $m_n(\\mathbf{x},\\mathbf{\\Theta}_j)$ the predicted value at point $\\mathbf{x}$ by the $j$-th tree, where $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_M $ are independent random variables, distributed as a generic random variable $\\mathbf{\\Theta}$, independent of the sample $\\mathcal{D}_n$. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate $m_{M, n}(\\mathbf{x},\\Theta_1,\\ldots,\\Theta_M) = \\frac{1}{M}\\sum_{j=1}^M m_n(\\mathbf{x},\\Theta_j)$. For regression trees, we have $m_n = \\sum_{i=1}^n\\frac{Y_i\\mathbf{1}_{\\mathbf{X}_i\\in A_n(\\mathbf{x},\\Theta_j)}}{N_n(\\mathbf{x}, \\Theta_j)}$, where $A_n(\\mathbf{x},\\Theta_j)$ is the cell containing $\\mathbf{x}$, designed with randomness $\\Theta_j$ and dataset $\\mathcal{D}_n$, and $ N_n(\\mathbf{x}, \\Theta_j) = \\sum_{i=1}^n \\mathbf{1}_{\\mathbf{X}_i\\in A_n(\\mathbf{x}, \\Theta_j)}$."
            },
            {
                "text": "Thus random forest estimates satisfy, for all $\\mathbf{x}\\in[0,1]^d$, $ m_{M,n}(\\mathbf{x}, \\Theta_1,\\ldots,\\Theta_M) =\\frac{1}{M}\\sum_{j=1}^M \\left(\\sum_{i=1}^n\\frac{Y_i\\mathbf{1}_{\\mathbf{X}_i\\in A_n(\\mathbf{x},\\Theta_j)}}{N_n(\\mathbf{x}, \\Theta_j)}\\right)$. Random regression forest has two levels of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet defined KeRF by which is equal to the mean of the $Y_i$'s falling in the cells containing $\\mathbf{x}$ in the forest. If we define the connection function of the $M$ finite forest as $K_{M,n}(\\mathbf{x}, \\mathbf{z}) = \\frac{1}{M} \\sum_{j=1}^M \\mathbf{1}_{\\mathbf{z} \\in A_n (\\mathbf{x}, \\Theta_j)}$, i.e."
            },
            {
                "text": "the proportion of cells shared between $\\mathbf{x}$ and $\\mathbf{z}$, then almost surely we have $\\tilde{m}_{M,n}(\\mathbf{x}, \\Theta_1,\\ldots,\\Theta_M) = \\frac{\\sum_{i=1}^n Y_i K_{M,n}(\\mathbf{x}, \\mathbf{x}_i)}{\\sum_{\\ell=1}^n K_{M,n}(\\mathbf{x}, \\mathbf{x}_{\\ell})}$, which defines the KeRF."
            },
            {
                "text": "Centered KeRF The construction of Centered KeRF of level $k$ is the same as for centered forest, except that predictions are made by $\\tilde{m}_{M,n}(\\mathbf{x}, \\Theta_1,\\ldots,\\Theta_M) $, the corresponding kernel function, or connection function is Uniform KeRF Uniform KeRF is built in the same way as uniform forest, except that predictions are made by $\\tilde{m}_{M,n}(\\mathbf{x}, \\Theta_1,\\ldots,\\Theta_M) $, the corresponding kernel function, or connection function is Properties Relation between KeRF and random forest Predictions given by KeRF and random forests are close if the number of points in each cell is controlled: Assume that there exist sequences $ (a_n),(b_n) $ such that, almost surely, Then almost surely, Relation between infinite KeRF and infinite random forest When the number of trees $M$ goes to infinity, then we have infinite random forest and infinite KeRF."
            },
            {
                "text": "Their estimates are close if the number of observations in each cell is bounded: Assume that there exist sequences $(\\varepsilon_n), (a_n),(b_n)$ such that, almost surely $\\operatorname{E}[N_n(\\mathbf{x},\\Theta)] \\ge 1,$ $\\operatorname{P}[a_n\\le N_n(\\mathbf{x},\\Theta) \\le b_n\\mid \\mathcal{D}_n] \\ge 1-\\varepsilon_n/2,$ $\\operatorname{P}[a_n\\le \\operatorname{E}_\\Theta [N_n(\\mathbf{x},\\Theta)] \\le b_n\\mid \\mathcal{D}_n] \\ge 1-\\varepsilon_n/2,$ Then almost surely, Consistency results Assume that $Y = m(\\mathbf{X}) + \\varepsilon$, where $\\varepsilon$ is a centered Gaussian noise, independent of $\\mathbf{X}$, with finite variance $\\sigma^2<\\infty$. Moreover, $\\mathbf{X}$ is uniformly distributed on $[0,1]^d$ and $m$ is Lipschitz. Scornet proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF."
            },
            {
                "text": "Consistency of centered KeRF Providing $k\\rightarrow\\infty$ and $n/2^k\\rightarrow\\infty$, there exists a constant $C_1>0$ such that, for all $n$, $ \\mathbb{E}[\\tilde{m}_n^{cc}(\\mathbf{X}) - m(\\mathbf{X})]^2 \\le C_1 n^{-1/(3+d\\log 2)}(\\log n)^2$. Consistency of uniform KeRF Providing $k\\rightarrow\\infty$ and $n/2^k\\rightarrow\\infty$, there exists a constant $C>0$ such that, $\\mathbb{E}[\\tilde{m}_n^{uf}(\\mathbf{X})-m(\\mathbf{X})]^2\\le Cn^{-2/(6+3d\\log2)}(\\log n)^2$. Disadvantages While random forests often achieve higher accuracy than a single decision tree, they sacrifice the intrinsic interpretability of decision trees. Decision trees are among a fairly small family of machine learning models that are easily interpretable along with linear models, rule-based models, and attention-based models. This interpretability is one of the main advantages of decision trees."
            },
            {
                "text": "It allows developers to confirm that the model has learned realistic information from the data and allows end-users to have trust and confidence in the decisions made by the model. For example, following the path that a decision tree takes to make its decision is quite trivial, but following the paths of tens or hundreds of trees is much harder. To achieve both performance and interpretability, some model compression techniques allow transforming a random forest into a minimal \"born-again\" decision tree that faithfully reproduces the same decision function. Another limitation of random forests is that if features are linearly correlated with the target, random forest may not enhance the accuracy of the base learner. Likewise in problems with multiple categorical variables. See also References Further reading External links Random Forests classifier description (Leo Breiman's site) Liaw, Andy & Wiener, Matthew \"Classification and Regression by randomForest\" R News (2002) Vol. 2/3 p. 18 (Discussion of the use of the random forest package for R) Category:Classification algorithms Category:Ensemble learning Category:Decision trees Category:Decision theory Category:Computational statistics"
            }
        ],
        "latex_formulas": [
            "''p''/3",
            "\\sqrt{p}",
            "p",
            "p",
            "\\mathcal{D}_n =\\{(X_i, Y_i)\\}_{i=1}^n",
            "x",
            "n_T",
            "T_i",
            "i",
            "p_{T_i}(j)=\\frac{n_j}{n}",
            "j",
            "\\Delta i_{T_i}(j)",
            "t",
            "j",
            "\\{(x_i, y_i)\\}_{i=1}^n",
            "\\hat{y}",
            "W(x_i, x')",
            "x_i",
            "W(x_i, x') = \\frac{1}{k}",
            "W(x_i, x') = \\frac{1}{k'}",
            "W_j",
            "x_i",
            "j",
            "k",
            "k \\in\\mathbb{N}",
            "\\mathcal{D}_n =\\{(\\mathbf{X}_i, Y_i)\\}_{i=1}^n",
            "[0,1]^p\\times\\mathbb{R}",
            "(\\mathbf{X}, Y)",
            "\\operatorname{E}[Y^2]<\\infty",
            "Y",
            "\\mathbf{X}",
            "m(\\mathbf{x})=\\operatorname{E}[Y \\mid \\mathbf{X} = \\mathbf{x}]",
            "M",
            "m_n(\\mathbf{x},\\mathbf{\\Theta}_j)",
            "\\mathbf{x}",
            "j",
            "\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_M",
            "\\mathbf{\\Theta}",
            "\\mathcal{D}_n",
            "m_{M, n}(\\mathbf{x},\\Theta_1,\\ldots,\\Theta_M) = \\frac{1}{M}\\sum_{j=1}^M m_n(\\mathbf{x},\\Theta_j)",
            "m_n = \\sum_{i=1}^n\\frac{Y_i\\mathbf{1}_{\\mathbf{X}_i\\in A_n(\\mathbf{x},\\Theta_j)}}{N_n(\\mathbf{x}, \\Theta_j)}",
            "A_n(\\mathbf{x},\\Theta_j)",
            "\\mathbf{x}",
            "\\Theta_j",
            "\\mathcal{D}_n",
            "N_n(\\mathbf{x}, \\Theta_j) = \\sum_{i=1}^n \\mathbf{1}_{\\mathbf{X}_i\\in A_n(\\mathbf{x}, \\Theta_j)}",
            "\\mathbf{x}\\in[0,1]^d",
            "m_{M,n}(\\mathbf{x}, \\Theta_1,\\ldots,\\Theta_M) =\\frac{1}{M}\\sum_{j=1}^M \\left(\\sum_{i=1}^n\\frac{Y_i\\mathbf{1}_{\\mathbf{X}_i\\in A_n(\\mathbf{x},\\Theta_j)}}{N_n(\\mathbf{x}, \\Theta_j)}\\right)",
            "Y_i",
            "\\mathbf{x}",
            "M",
            "K_{M,n}(\\mathbf{x}, \\mathbf{z}) = \\frac{1}{M} \\sum_{j=1}^M \\mathbf{1}_{\\mathbf{z} \\in A_n (\\mathbf{x}, \\Theta_j)}",
            "\\mathbf{x}",
            "\\mathbf{z}",
            "\\tilde{m}_{M,n}(\\mathbf{x}, \\Theta_1,\\ldots,\\Theta_M) =\n\\frac{\\sum_{i=1}^n Y_i K_{M,n}(\\mathbf{x}, \\mathbf{x}_i)}{\\sum_{\\ell=1}^n K_{M,n}(\\mathbf{x}, \\mathbf{x}_{\\ell})}",
            "k",
            "\\tilde{m}_{M,n}(\\mathbf{x}, \\Theta_1,\\ldots,\\Theta_M)",
            "\\tilde{m}_{M,n}(\\mathbf{x}, \\Theta_1,\\ldots,\\Theta_M)",
            "(a_n),(b_n)",
            "M",
            "(\\varepsilon_n), (a_n),(b_n)",
            "\\operatorname{E}[N_n(\\mathbf{x},\\Theta)] \\ge 1,",
            "\\operatorname{P}[a_n\\le N_n(\\mathbf{x},\\Theta) \\le b_n\\mid \\mathcal{D}_n] \\ge 1-\\varepsilon_n/2,",
            "\\operatorname{P}[a_n\\le \\operatorname{E}_\\Theta [N_n(\\mathbf{x},\\Theta)] \\le b_n\\mid \\mathcal{D}_n] \\ge 1-\\varepsilon_n/2,",
            "Y = m(\\mathbf{X}) + \\varepsilon",
            "\\varepsilon",
            "\\mathbf{X}",
            "\\sigma^2<\\infty",
            "\\mathbf{X}",
            "[0,1]^d",
            "m",
            "k\\rightarrow\\infty",
            "n/2^k\\rightarrow\\infty",
            "C_1>0",
            "n",
            "\\mathbb{E}[\\tilde{m}_n^{cc}(\\mathbf{X}) - m(\\mathbf{X})]^2 \\le C_1 n^{-1/(3+d\\log 2)}(\\log n)^2",
            "k\\rightarrow\\infty",
            "n/2^k\\rightarrow\\infty",
            "C>0",
            "\\mathbb{E}[\\tilde{m}_n^{uf}(\\mathbf{X})-m(\\mathbf{X})]^2\\le Cn^{-2/(6+3d\\log2)}(\\log n)^2"
        ]
    },
    "Ensemble_learning": {
        "title": "Ensemble_learning",
        "chunks": [
            {
                "text": "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives. Overview Supervised learning algorithms search through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if this space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form one which should be theoretically better. Ensemble learning trains two or more machine learning algorithms on a specific classification or regression task. The algorithms within the ensemble model are generally referred as \"base models\", \"base learners\", or \"weak learners\" in literature. These base models can be constructed using a single modelling algorithm, or several different algorithms. The idea is to train a diverse set of weak models on the same modelling task, such that the outputs of each weak learner have poor predictive ability (i.e., high bias), and among all weak learners, the outcome and error values exhibit high variance."
            },
            {
                "text": "Fundamentally, an ensemble learning model trains at least two high-bias (weak) and high-variance (diverse) models to be combined into a better-performing model. The set of weak models — which would not produce satisfactory predictive results individually — are combined or averaged to produce a single, high performing, accurate, and low-variance model to fit the task as required. Ensemble learning typically refers to bagging (bootstrap aggregating), boosting or stacking/blending techniques to induce high variance among the base models. Bagging creates diversity by generating random samples from the training observations and fitting the same model to each different sample — also known as homogeneous parallel ensembles. Boosting follows an iterative process by sequentially training each base model on the up-weighted errors of the previous base model, producing an additive model to reduce the final model errors — also known as sequential ensemble learning. Stacking or blending consists of different base models, each trained independently (i.e. diverse/high variance) to be combined into the ensemble model — producing a heterogeneous parallel ensemble. Common applications of ensemble learning include random forests (an extension of bagging), Boosted Tree models, and Gradient Boosted Tree Models."
            },
            {
                "text": "Models in applications of stacking are generally more task-specific — such as combining clustering techniques with other parametric and/or non-parametric techniques. The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner. Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model. In one sense, ensemble learning may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. On the other hand, the alternative is to do a lot more learning with one non-ensemble model. An ensemble may be more efficient at improving overall accuracy for the same increase in compute, storage, or communication resources by using that increase on two or more methods, than would have been improved by increasing resource use for a single method. Fast algorithms such as decision trees are commonly used in ensemble methods (e.g., random forests), although slower algorithms can benefit from ensemble techniques as well. By analogy, ensemble techniques have been used also in unsupervised learning scenarios, for example in consensus clustering or in anomaly detection."
            },
            {
                "text": "Ensemble theory Empirically, ensembles tend to yield better results when there is a significant diversity among the models.Kuncheva, L. and Whitaker, C., Measures of diversity in classifier ensembles, Machine Learning, 51, pp. 181-207, 2003Sollich, P. and Krogh, A., Learning with ensembles: How overfitting can be useful, Advances in Neural Information Processing Systems, volume 8, pp. 190-196, 1996. Many ensemble methods, therefore, seek to promote diversity among the models they combine.Brown, G. and Wyatt, J. and Harris, R. and Yao, X., Diversity creation methods: a survey and categorisation., Information Fusion, 6(1), pp.5-20, 2005. Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).Ho, T., Random Decision Forests, Proceedings of the Third International Conference on Document Analysis and Recognition, pp. 278-282, 1995. Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity."
            },
            {
                "text": "It is possible to increase diversity in the training stage of the model using correlation for regression tasks or using information measures such as cross entropy for classification tasks. 400px|An ensemble of classifiers usually has smaller classification error than base models. Theoretically, one can justify the diversity concept because the lower bound of the error rate of an ensemble system can be decomposed into accuracy, diversity, and the other term.Terufumi Morishita et al, Rethinking Fano’s Inequality in Ensemble Learning, International Conference on Machine Learning, 2022 The geometric framework Ensemble learning, including both regression and classification tasks, can be explained using a geometric framework.Wu, S., Li, J., & Ding, W. (2023) A geometric framework for multiclass ensemble classifiers, Machine Learning, 112(12), pp. 4929-4958. Within this framework, the output of each individual classifier or regressor for the entire dataset can be viewed as a point in a multi-dimensional space. Additionally, the target result is also represented as a point in this space, referred to as the \"ideal point.\""
            },
            {
                "text": "The Euclidean distance is used as the metric to measure both the performance of a single classifier or regressor (the distance between its point and the ideal point) and the dissimilarity between two classifiers or regressors (the distance between their respective points). This perspective transforms ensemble learning into a deterministic problem. For example, within this geometric framework, it can be proved that the averaging of the outputs (scores) of all base classifiers or regressors can lead to equal or better results than the average of all the individual models. It can also be proved that if the optimal weighting scheme is used, then a weighted averaging approach can outperform any of the individual classifiers or regressors that make up the ensemble or as good as the best performer at least. Ensemble size While the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a limited number of studies addressing this problem. A priori determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers."
            },
            {
                "text": "Mostly statistical tests were used for determining the proper number of components. More recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy. It is called \"the law of diminishing returns in ensemble construction.\" Their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy. Common types of ensembles Bayes optimal classifier The Bayes optimal classifier is a classification technique. It is an ensemble of all the hypotheses in the hypothesis space. On average, no other ensemble can outperform it.Tom M. Mitchell, Machine Learning, 1997, pp. 175 The Naive Bayes classifier is a version of this that assumes that the data is conditionally independent on the class and makes the computation more feasible. Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis."
            },
            {
                "text": "The Bayes optimal classifier can be expressed with the following equation: $y=\\underset{c_j \\in C}{\\mathrm{argmax}} \\sum_{h_i \\in H}{P(c_j|h_i)P(T|h_i)P(h_i)}$ where $y$ is the predicted class, $C$ is the set of all possible classes, $H$ is the hypothesis space, $P$ refers to a probability, and $T$ is the training data. As an ensemble, the Bayes optimal classifier represents a hypothesis that is not necessarily in $H$. The hypothesis represented by the Bayes optimal classifier, however, is the optimal hypothesis in ensemble space (the space of all possible ensembles consisting only of hypotheses in $H$). This formula can be restated using Bayes' theorem, which says that the posterior is proportional to the likelihood times the prior: $P(h_i|T) \\propto P(T|h_i)P(h_i)$ hence, $y=\\underset{c_j \\in C}{\\mathrm{argmax}} \\sum_{h_i \\in H}{P(c_j|h_i)P(h_i|T)}$ Bootstrap aggregating (bagging) Three datasets bootstrapped from an original set."
            },
            {
                "text": "Example A occurs twice in set 1 because these are chosen with replacement.Bootstrap aggregation (bagging) involves training an ensemble on bootstrapped data sets. A bootstrapped set is created by selecting from original training data set with replacement. Thus, a bootstrap set may contain a given example zero, one, or multiple times. Ensemble members can also have limits on the features (e.g., nodes of a decision tree), to encourage exploring of diverse features.Salman, R., Alzaatreh, A., Sulieman, H., & Faisal, S. (2021). A Bootstrap Framework for Aggregating within and between Feature Selection Methods. Entropy (Basel, Switzerland), 23(2), 200. The variance of local information in the bootstrap sets and feature considerations promote diversity in the ensemble, and can strengthen the ensemble.Breiman, L., Bagging Predictors, Machine Learning, 24(2), pp.123-140, 1996. To reduce overfitting, a member can be validated using the out-of-bag set (the examples that are not in its bootstrap set).Brodeur, Z. P., Herman, J. D., & Steinschneider, S. (2020)."
            },
            {
                "text": "Bootstrap aggregation and cross-validation methods to reduce overfitting in reservoir control policy search. Water Resources Research, 56, e2020WR027184. Inference is done by voting of predictions of ensemble members, called aggregation. It is illustrated below with an ensemble of four decision trees. The query example is classified by each tree. Because three of the four predict the positive class, the ensemble's overall classification is positive. Random forests like the one shown are a common application of bagging. frameless|center|An example of the aggregation process for an ensemble of decision trees. Individual classifications are aggregated, and an overall classification is derived.|615x615px Boosting Boosting involves training successive models by emphasizing training data mis-classified by previously learned models. Initially, all data (D1) has equal weight and is used to learn a base model M1. The examples mis-classified by M1 are assigned a weight greater than correctly classified examples. This boosted data (D2) is used to train a second base model M2, and so on. Inference is done by voting. In some cases, boosting has yielded better accuracy than bagging, but tends to over-fit more."
            },
            {
                "text": "The most common implementation of boosting is Adaboost, but some newer algorithms are reported to achieve better results. Bayesian model averaging Bayesian model averaging (BMA) makes predictions by averaging the predictions of models weighted by their posterior probabilities given the data.e.g., BMA is known to generally give better answers than a single model, obtained, e.g., via stepwise regression, especially where very different models have nearly identical performance in the training set but may otherwise perform quite differently. The question with any use of Bayes' theorem is the prior, i.e., the probability (perhaps subjective) that each model is the best to use for a given purpose. Conceptually, BMA can be used with any prior. R packages ensembleBMA and BMA. use the prior implied by the Bayesian information criterion, (BIC), following Raftery (1995). R package BAS supports the use of the priors implied by Akaike information criterion (AIC) and other criteria over the alternative models as well as priors over the coefficients.. The difference between BIC and AIC is the strength of preference for parsimony."
            },
            {
                "text": "BIC's penalty for model complexity is $\\ln(n) k$ , while AIC's is $2k$. Large-sample asymptotic theory establishes that if there is a best model, then with increasing sample sizes, BIC is strongly consistent, i.e., will almost certainly find it, while AIC may not, because AIC may continue to place excessive posterior probability on models that are more complicated than they need to be. On the other hand, AIC and AICc are asymptotically \"efficient\" (i.e., minimum mean square prediction error), while BIC is not ., ch. 4. Haussler et al. (1994) showed that when BMA is used for classification, its expected error is at most twice the expected error of the Bayes optimal classifier. Burnham and Anderson (1998, 2002) contributed greatly to introducing a wider audience to the basic ideas of Bayesian model averaging and popularizing the methodology. and . The availability of software, including other free open-source packages for R beyond those mentioned above, helped make the methods accessible to a wider audience.The Wikiversity article on Searching R Packages mentions several ways to find available packages for something like this."
            },
            {
                "text": "For example, \"sos::findFn('{Bayesian model averaging}')\" from within R will search for help files in contributed packages that includes the search term and open two tabs in the default browser. The first will list all the help files found sorted by package. The second summarizes the packages found, sorted by the apparent strength of the match. Bayesian model combination Bayesian model combination (BMC) is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weights drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. BMC has been shown to be better on average (with statistical significance) than BMA and bagging. Use of Bayes' law to compute model weights requires computing the probability of the data given each model."
            },
            {
                "text": "Typically, none of the models in the ensemble are exactly the distribution from which the training data were generated, so all of them correctly receive a value close to zero for this term. This would work well if the ensemble were big enough to sample the entire model-space, but this is rarely possible. Consequently, each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data. It essentially reduces to an unnecessarily complex method for doing model selection. The possible weightings for an ensemble can be visualized as lying on a simplex. At each vertex of the simplex, all of the weight is given to a single model in the ensemble. BMA converges toward the vertex that is closest to the distribution of the training data. By contrast, BMC converges toward the point where this distribution projects onto the simplex. In other words, instead of selecting the one model that is closest to the generating distribution, it seeks the combination of models that is closest to the generating distribution."
            },
            {
                "text": "The results from BMA can often be approximated by using cross-validation to select the best model from a bucket of models. Likewise, the results from BMC may be approximated by using cross-validation to select the best ensemble combination from a random sampling of possible weightings. Bucket of models A \"bucket of models\" is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set. The most common approach used for model-selection is cross-validation selection (sometimes called a \"bake-off contest\"). It is described with the following pseudo-code: For each model m in the bucket: Do c times: (where 'c' is some constant) Randomly divide the training dataset into two sets: A and B Train m with A Test m with B Select the model that obtains the highest average score Cross-Validation Selection can be summed up as: \"try them all with the training set, and pick the one that works best\".Saso Dzeroski, Bernard Zenko, Is Combining Classifiers Better than Selecting the Best One, Machine Learning, 2004, pp."
            },
            {
                "text": "255-273 Gating is a generalization of Cross-Validation Selection. It involves training another learning model to decide which of the models in the bucket is best-suited to solve the problem. Often, a perceptron is used for the gating model. It can be used to pick the \"best\" model, or it can be used to give a linear weight to the predictions from each model in the bucket. When a bucket of models is used with a large set of problems, it may be desirable to avoid training some of the models that take a long time to train. Landmark learning is a meta-learning approach that seeks to solve this problem. It involves training only the fast (but imprecise) algorithms in the bucket, and then using the performance of these algorithms to help determine which slow (but accurate) algorithm is most likely to do best. Amended Cross-Entropy Cost: An Approach for Encouraging Diversity in Classification Ensemble The most common approach for training classifier is using Cross-entropy cost function. However, one would like to train an ensemble of models that have diversity so when we combine them it would provide best results."
            },
            {
                "text": "Assuming we use a simple ensemble of averaging $ K $ classifiers. Then the Amended Cross-Entropy Cost is $ e^k = H(p,q^k)-\\frac{\\lambda}{K}\\sum_{j\\neq k}H(q^j,q^k) $ where $ e^k $ is the cost function of the $ k^{th} $ classifier, $ q^k $ is the probability of the $ k^{ th} $ classifier, $ p $ is the true probability that we need to estimate and $ \\lambda $ is a parameter between 0 and 1 that define the diversity that we would like to establish. When $ \\lambda=0 $ we want each classifier to do its best regardless of the ensemble and when $ \\lambda=1 $ we would like the classifier to be as diverse as possible. Stacking Stacking (sometimes called stacked generalization) involves training a model to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm (final estimator) is trained to make a final prediction using all the predictions of the other algorithms (base estimators) as additional inputs or using cross-validated predictions from the base estimators which can prevent overfitting."
            },
            {
                "text": "If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although, in practice, a logistic regression model is often used as the combiner. Stacking typically yields performance better than any single one of the trained models. It has been successfully used on both supervised learning tasks (regression, classification and distance learning ) and unsupervised learning (density estimation). It has also been used to estimate bagging's error rate. It has been reported to out-perform Bayesian model-averaging.Clarke, B., Bayes model averaging and stacking when model approximation error cannot be ignored, Journal of Machine Learning Research, pp 683-712, 2003 The two top-performers in the Netflix competition utilized blending, which may be considered a form of stacking. Voting Voting is another form of ensembling. See e.g. Weighted majority algorithm (machine learning). Implementations in statistics packages R: at least three packages offer Bayesian model averaging tools, including the (an acronym for Bayesian Model Selection) package, the (an acronym for Bayesian Adaptive Sampling) package, and the package."
            },
            {
                "text": "Python: scikit-learn, a package for machine learning in Python offers packages for ensemble learning including packages for bagging, voting and averaging methods. MATLAB: classification ensembles are implemented in Statistics and Machine Learning Toolbox. Ensemble learning applications In recent years, due to growing computational power, which allows for training in large ensemble learning in a reasonable time frame, the number of ensemble learning applications has grown increasingly. Some of the applications of ensemble classifiers include: Remote sensing Land cover mapping Land cover mapping is one of the major applications of Earth observation satellite sensors, using remote sensing and geospatial data, to identify the materials and objects which are located on the surface of target areas. Generally, the classes of target materials include roads, buildings, rivers, lakes, and vegetation. Some different ensemble learning approaches based on artificial neural networks, kernel principal component analysis (KPCA), decision trees with boosting, random forest and automatic design of multiple classifier systems, are proposed to efficiently identify land cover objects. Change detection Change detection is an image analysis problem, consisting of the identification of places where the land cover has changed over time."
            },
            {
                "text": "Change detection is widely used in fields such as urban growth, forest and vegetation dynamics, land use and disaster monitoring. The earliest applications of ensemble classifiers in change detection are designed with the majority voting,Defined by Bruzzone et al. (2002) as \"The data class that receives the largest number of votes is taken as the class of the input pattern\", this is simple majority, more accurately described as plurality voting. Bayesian model averaging, and the maximum posterior probability. Given the growth of satellite data over time, the past decade sees more use of time series methods for continuous change detection from image stacks. One example is a Bayesian ensemble changepoint detection method called BEAST, with the software available as a package Rbeast in R, Python, and Matlab. Computer security Distributed denial of service Distributed denial of service is one of the most threatening cyber-attacks that may happen to an internet service provider. By combining the output of single classifiers, ensemble classifiers reduce the total error of detecting and discriminating such attacks from legitimate flash crowds. Malware Detection Classification of malware codes such as computer viruses, computer worms, trojans, ransomware and spywares with the usage of machine learning techniques, is inspired by the document categorization problem."
            },
            {
                "text": "Ensemble learning systems have shown a proper efficacy in this area. Intrusion detection An intrusion detection system monitors computer network or computer systems to identify intruder codes like an anomaly detection process. Ensemble learning successfully aids such monitoring systems to reduce their total error. Face recognition Face recognition, which recently has become one of the most popular research areas of pattern recognition, copes with identification or verification of a person by their digital images. Hierarchical ensembles based on Gabor Fisher classifier and independent component analysis preprocessing techniques are some of the earliest ensembles employed in this field. Emotion recognition While speech recognition is mainly based on deep learning because most of the industry players in this field like Google, Microsoft and IBM reveal that the core technology of their speech recognition is based on this approach, speech-based emotion recognition can also have a satisfactory performance with ensemble learning. It is also being successfully used in facial emotion recognition. Fraud detection Fraud detection deals with the identification of bank fraud, such as money laundering, credit card fraud and telecommunication fraud, which have vast domains of research and applications of machine learning."
            },
            {
                "text": "Because ensemble learning improves the robustness of the normal behavior modelling, it has been proposed as an efficient technique to detect such fraudulent cases and activities in banking and credit card systems. Financial decision-making The accuracy of prediction of business failure is a very crucial issue in financial decision-making. Therefore, different ensemble classifiers are proposed to predict financial crises and financial distress. Also, in the trade-based manipulation problem, where traders attempt to manipulate stock prices by buying and selling activities, ensemble classifiers are required to analyze the changes in the stock market data and detect suspicious symptom of stock price manipulation. Medicine Ensemble classifiers have been successfully applied in neuroscience, proteomics and medical diagnosis like in neuro-cognitive disorder (i.e. Alzheimer or myotonic dystrophy) detection based on MRI datasets, cervical cytology classification. Besides, ensembles have been successfully applied in medical segmentation tasks, for example brain tumor and hyperintensities segmentation. See also Ensemble averaging (machine learning) Bayesian structural time series (BSTS) Mixture of experts References Further reading External links The Waffles (machine learning) toolkit contains implementations of Bagging, Boosting, Bayesian Model Averaging, Bayesian Model Combination, Bucket-of-models, and other ensemble techniques"
            }
        ],
        "latex_formulas": [
            "y=\\underset{c_j \\in C}{\\mathrm{argmax}} \\sum_{h_i \\in H}{P(c_j|h_i)P(T|h_i)P(h_i)}",
            "y",
            "C",
            "H",
            "P",
            "T",
            "H",
            "H",
            "P(h_i|T) \\propto P(T|h_i)P(h_i)",
            "y=\\underset{c_j \\in C}{\\mathrm{argmax}} \\sum_{h_i \\in H}{P(c_j|h_i)P(h_i|T)}",
            "\\ln(n) k",
            "2k",
            "K",
            "e^k = H(p,q^k)-\\frac{\\lambda}{K}\\sum_{j\\neq k}H(q^j,q^k)",
            "e^k",
            "k^{th}",
            "q^k",
            "k^{ th}",
            "p",
            "\\lambda",
            "\\lambda=0",
            "\\lambda=1"
        ]
    },
    "Bootstrap_aggregating": {
        "title": "Bootstrap_aggregating",
        "chunks": [
            {
                "text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating) or bootstrapping, is a machine learning (ML) ensemble meta-algorithm designed to improve the stability and accuracy of ML classification and regression algorithms. It also reduces variance and overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the ensemble averaging approach. Description of the technique Given a standard training set $D$ of size $n$, bagging generates $m$ new training sets $D_i$, each of size $n'$, by sampling from $D$ uniformly and with replacement. By sampling with replacement, some observations may be repeated in each $D_i$. If $n'=n$, then for large $n$ the set $D_i$ is expected to have the fraction (1 - 1/e) (~63.2%) of the unique samples of $D$, the rest being duplicates.Aslam, Javed A.; Popa, Raluca A.; and Rivest, Ronald L. (2007); On Estimating the Size and Confidence of a Statistical Audit, Proceedings of the Electronic Voting Technology Workshop (EVT '07), Boston, MA, August 6, 2007."
            },
            {
                "text": "More generally, when drawing with replacement $n'$ values out of a set of $n$ (different and equally likely), the expected number of unique draws is $n(1 - e^{-n'/n})$. This kind of sample is known as a bootstrap sample. Sampling with replacement ensures each bootstrap is independent from its peers, as it does not depend on previous chosen samples when sampling. Then, $m$ models are fitted using the above bootstrap samples and combined by averaging the output (for regression) or voting (for classification). upright=2.0|An illustration for the concept of bootstrap aggregating Bagging leads to \"improvements for unstable procedures\", which include, for example, artificial neural networks, classification and regression trees, and subset selection in linear regression. Bagging was shown to improve preimage learning.Sahu, A., Runger, G., Apley, D., Image denoising with a multi-phase kernel principal component approach and an ensemble version, IEEE Applied Imagery Pattern Recognition Workshop, pp.1-7, 2011.Shinde, Amit, Anshuman Sahu, Daniel Apley, and George Runger."
            },
            {
                "text": "\"Preimages for Variation Patterns from Kernel PCA and Bagging.\" IIE Transactions, Vol.46, Iss.5, 2014 On the other hand, it can mildly degrade the performance of stable methods such as k-nearest neighbors. Process of the algorithm Key Terms There are three types of datasets in bootstrap aggregating. These are the original, bootstrap, and out-of-bag datasets. Each section below will explain how each dataset is made except for the original dataset. The original dataset is whatever information is given. Creating the bootstrap dataset The bootstrap dataset is made by randomly picking objects from the original dataset. Also, it must be the same size as the original dataset. However, the difference is that the bootstrap dataset can have duplicate objects. Here is a simple example to demonstrate how it works along with the illustration below: 672x672px|Bootstrap Example Suppose the original dataset is a group of 12 people. Their names are Emily, Jessie, George, Constantine, Lexi, Theodore, John, James, Rachel, Anthony, Ellie, and Jamal. By randomly picking a group of names, let us say our bootstrap dataset had James, Ellie, Constantine, Lexi, John, Constantine, Theodore, Constantine, Anthony, Lexi, Constantine, and Theodore."
            },
            {
                "text": "In this case, the bootstrap sample contained four duplicates for Constantine, and two duplicates for Lexi, and Theodore. Creating the out-of-bag dataset The out-of-bag dataset represents the remaining people who were not in the bootstrap dataset. It can be calculated by taking the difference between the original and the bootstrap datasets. In this case, the remaining samples who were not selected are Emily, Jessie, George, Rachel, and Jamal. Keep in mind that since both datasets are sets, when taking the difference the duplicate names are ignored in the bootstrap dataset. The illustration below shows how the math is done: 840x840px|Complete Example Application Creating the bootstrap and out-of-bag datasets is crucial since it is used to test the accuracy of ensemble learning algorithms like random forest. For example, a model that produces 50 trees using the bootstrap/out-of-bag datasets will have a better accuracy than if it produced 10 trees. Since the algorithm generates multiple trees and therefore multiple datasets the chance that an object is left out of the bootstrap dataset is low. The next few sections talk about how the random forest algorithm works in more detail."
            },
            {
                "text": "Creation of Decision Trees The next step of the algorithm involves the generation of decision trees from the bootstrapped dataset. To achieve this, the process examines each gene/feature and determines for how many samples the feature's presence or absence yields a positive or negative result. This information is then used to compute a confusion matrix, which lists the true positives, false positives, true negatives, and false negatives of the feature when used as a classifier. These features are then ranked according to various classification metrics based on their confusion matrices. Some common metrics include estimate of positive correctness (calculated by subtracting false positives from true positives), measure of \"goodness\", and information gain. These features are then used to partition the samples into two sets: those that possess the top feature, and those that do not. The diagram below shows a decision tree of depth two being used to classify data. For example, a data point that exhibits Feature 1, but not Feature 2, will be given a \"No\". Another point that does not exhibit Feature 1, but does exhibit Feature 3, will be given a \"Yes\"."
            },
            {
                "text": "Decision Tree Depth 2 This process is repeated recursively for successive levels of the tree until the desired depth is reached. At the very bottom of the tree, samples that test positive for the final feature are generally classified as positive, while those that lack the feature are classified as negative. These trees are then used as predictors to classify new data. Random Forests The next part of the algorithm involves introducing yet another element of variability amongst the bootstrapped trees. In addition to each tree only examining a bootstrapped set of samples, only a small but consistent number of unique features are considered when ranking them as classifiers. This means that each tree only knows about the data pertaining to a small constant number of features, and a variable number of samples that is less than or equal to that of the original dataset. Consequently, the trees are more likely to return a wider array of answers, derived from more diverse knowledge. This results in a random forest, which possesses numerous benefits over a single decision tree generated without randomness. In a random forest, each tree \"votes\" on whether or not to classify a sample as positive based on its features."
            },
            {
                "text": "The sample is then classified based on majority vote. An example of this is given in the diagram below, where the four trees in a random forest vote on whether or not a patient with mutations A, B, F, and G has cancer. Since three out of four trees vote yes, the patient is then classified as cancer positive. center|frameless|1035x1035px Because of their properties, random forests are considered one of the most accurate data mining algorithms, are less likely to overfit their data, and run quickly and efficiently even for large datasets. They are primarily useful for classification as opposed to regression, which attempts to draw observed connections between statistical variables in a dataset. This makes random forests particularly useful in such fields as banking, healthcare, the stock market, and e-commerce where it is important to be able to predict future results based on past data. One of their applications would be as a useful tool for predicting cancer based on genetic factors, as seen in the above example. There are several important factors to consider when designing a random forest."
            },
            {
                "text": "If the trees in the random forests are too deep, overfitting can still occur due to over-specificity. If the forest is too large, the algorithm may become less efficient due to an increased runtime. Random forests also do not generally perform well when given sparse data with little variability. However, they still have numerous advantages over similar data classification algorithms such as neural networks, as they are much easier to interpret and generally require less data for training. As an integral component of random forests, bootstrap aggregating is very important to classification algorithms, and provides a critical element of variability that allows for increased accuracy when analyzing new data, as discussed below. Improving Random Forests and Bagging While the techniques described above utilize random forests and bagging (otherwise known as bootstrapping), there are certain techniques that can be used in order to improve their execution and voting time, their prediction accuracy, and their overall performance. The following are key steps in creating an efficient random forest: Specify the maximum depth of trees: Instead of allowing the random forest to continue until all nodes are pure, it is better to cut it off at a certain point in order to further decrease chances of overfitting."
            },
            {
                "text": "Prune the dataset: Using an extremely large dataset may create results that are less indicative of the data provided than a smaller set that more accurately represents what is being focused on. Continue pruning the data at each node split rather than just in the original bagging process. Decide on accuracy or speed: Depending on the desired results, increasing or decreasing the number of trees within the forest can help. Increasing the number of trees generally provides more accurate results while decreasing the number of trees will provide quicker results. +Pros and Cons of Random Forests and Bagging ProsConsThere are overall less requirements involved for normalization and scaling, making the use of random forests more convenient.The algorithm may change significantly if there is a slight change to the data being bootstrapped and used within the forests. In other words, random forests are incredibly dependent on their datasets, changing these can drastically change the individual trees' structures. Easy data preparation. Data is prepared by creating a bootstrap set and a certain number of decision trees to build a random forest that also utilizes feature selection, as mentioned in ."
            },
            {
                "text": "Random Forests are more complex to implement than lone decision trees or other algorithms. This is because they take extra steps for bagging, as well as the need for recursion in order to produce an entire forest, which complicates implementation. Because of this, it requires much more computational power and computational resources. Consisting of multiple decision trees, forests are able to more accurately make predictions than single trees. Requires much more time to train the data compared to decision trees. Having a large forest can quickly begin to decrease the speed in which one's program operates because it has to traverse much more data even though each tree is using a smaller set of samples and features. Works well with non-linear data. As most tree based algorithms use linear splits, using an ensemble of a set of trees works better than using a single tree on data that has nonlinear properties (i.e. most real world distributions). Working well with non-linear data is a huge advantage because other data mining techniques such as single decision trees do not handle this as well. Much easier to interpret than a random forest."
            },
            {
                "text": "A single tree can be walked by hand (by a human) leading to a somewhat \"explainable\" understanding for the analyst of what the tree is actually doing. As the number of trees and schemes grow for ensembling those trees into predictions, this reviewing becomes much more difficult if not impossible. There is a lower risk of overfitting and runs efficiently on even large datasets. This is the result of the random forest's use of bagging in conjunction with random feature selection. Does not predict beyond the range of the training data. This is a con because while bagging is often effective, not all of the data is being considered, therefore it cannot predict an entire dataset. The random forest classifier operates with a high accuracy and speed. Random forests are much faster than decision trees because of using a smaller dataset. To recreate specific results, it is necessary to keep track of the exact random seed used to generate the bootstrap sets. This may be important when collecting data for research or within a data mining class. Using random seeds is essential to the random forests, but can make it hard to support claims based on forests if there is a failure to record the seeds."
            },
            {
                "text": "Deals with missing data and datasets with many outliers well. They deal with this by using binning, or by grouping values together to avoid values that are terribly far apart. Algorithm (classification) thumb|Flow chart of the bagging algorithm when used for classification For classification, use a training set $D$, Inducer $I$ and the number of bootstrap samples $m$ as input. Generate a classifier $C^*$ as output Create $m$ new training sets $D_i$, from $D$ with replacement Classifier $C_i$ is built from each set $D_i$ using $I$ to determine the classification of set $D_i$ Finally classifier $C^*$ is generated by using the previously created set of classifiers $C_i$ on the original dataset $D$, the classification predicted most often by the sub-classifiers $C_i$ is the final classification for i = 1 to m { D' = bootstrap sample from D (sample with replacement) Ci = I(D') } C*(x) = argmax #{i:Ci(x)=y} (most often predicted label y) y∈Y Example: ozone data To illustrate the basic principles of bagging, below is an analysis on the relationship between ozone and temperature (data from Rousseeuw and Leroy (1986), analysis done in R)."
            },
            {
                "text": "The relationship between temperature and ozone appears to be nonlinear in this dataset, based on the scatter plot. To mathematically describe this relationship, LOESS smoothers (with bandwidth 0.5) are used. Rather than building a single smoother for the complete dataset, 100 bootstrap samples were drawn. Each sample is composed of a random subset of the original data and maintains a semblance of the master set's distribution and variability. For each bootstrap sample, a LOESS smoother was fit. Predictions from these 100 smoothers were then made across the range of the data. The black lines represent these initial predictions. The lines lack agreement in their predictions and tend to overfit their data points: evident by the wobbly flow of the lines. center By taking the average of 100 smoothers, each corresponding to a subset of the original dataset, we arrive at one bagged predictor (red line). The red line's flow is stable and does not overly conform to any data point(s). Advantages and disadvantages Advantages: Many weak learners aggregated typically outperform a single learner over the entire set, and have less overfit Reduces variance in high-variance low-bias weak learner, which can improve efficiency (statistics) Can be performed in parallel, as each separate bootstrap can be processed on its own before aggregation. Disadvantages: For a weak learner with high bias, bagging will also carry high bias into its aggregate Loss of interpretability of a model. Can be computationally expensive depending on the dataset. History The concept of bootstrap aggregating is derived from the concept of bootstrapping which was developed by Bradley Efron."
            },
            {
                "text": "Bootstrap aggregating was proposed by Leo Breiman who also coined the abbreviated term \"bagging\" (bootstrap aggregating). Breiman developed the concept of bagging in 1994 to improve classification by combining classifications of randomly generated training sets. He argued, \"If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy\". See also Boosting (machine learning) Bootstrapping (statistics) Cross-validation (statistics) Out-of-bag error Random forest Random subspace method (attribute bagging) Resampled efficient frontier Predictive analysis: Classification and regression trees References Further reading Category:Ensemble learning Category:Machine learning algorithms Category:Computational statistics"
            }
        ],
        "latex_formulas": [
            "D",
            "n",
            "m",
            "D_i",
            "n'",
            "D",
            "D_i",
            "n'=n",
            "n",
            "D_i",
            "D",
            "n'",
            "n",
            "n(1 - e^{-n'/n})",
            "m",
            "D",
            "I",
            "m",
            "C^*",
            "m",
            "D_i",
            "D",
            "C_i",
            "D_i",
            "I",
            "D_i",
            "C^*",
            "C_i",
            "D",
            "C_i"
        ]
    },
    "Sampling_with_replacement": {
        "title": "Sampling_with_replacement",
        "chunks": [
            {
                "text": "REDIRECT Simple random sample"
            }
        ],
        "latex_formulas": []
    },
    "Random_subspace_method": {
        "title": "Random_subspace_method",
        "chunks": [
            {
                "text": "In machine learning the random subspace method, also called attribute bagging or feature bagging, is an ensemble learning method that attempts to reduce the correlation between estimators in an ensemble by training them on random samples of features instead of the entire feature set. Motivation In ensemble learning one tries to combine the models produced by several learners into an ensemble that performs better than the original learners. One way of combining learners is bootstrap aggregating or bagging, which shows each learner a randomly sampled subset of the training points so that the learners will produce different models that can be sensibly averaged. In bagging, one samples training points with replacement from the full training set. The random subspace method is similar to bagging except that the features (\"attributes\", \"predictors\", \"independent variables\") are randomly sampled, with replacement, for each learner. Informally, this causes individual learners to not over-focus on features that appear highly predictive/descriptive in the training set, but fail to be as predictive for points outside that set. For this reason, random subspaces are an attractive choice for high-dimensional problems where the number of features is much larger than the number of training points, such as learning from fMRI data or gene expression data."
            },
            {
                "text": "The random subspace method has been used for decision trees; when combined with \"ordinary\" bagging of decision trees, the resulting models are called random forests. It has also been applied to linear classifiers, support vector machines, nearest neighbours and other types of classifiers. This method is also applicable to one-class classifiers. The random subspace method has also been applied to the portfolio selection problem showing its superiority to the conventional resampled portfolio essentially based on Bagging. To tackle high-dimensional sparse problems, a framework named Random Subspace Ensemble (RaSE) was developed. RaSE combines weak learners trained in random subspaces with a two-layer structure and iterative process. RaSE has been shown to enjoy appealing theoretical properties and practical performance. Algorithm An ensemble of models employing the random subspace method can be constructed using the following algorithm: Let the number of training points be N and the number of features in the training data be D. Let L be the number of individual models in the ensemble. For each individual model l, choose n (n < N) to be the number of input points for l. It is common to have only one value of n for all the individual models. For each individual model l, create a training set by choosing d features from D with replacement and train the model. Now, to apply the ensemble model to an unseen point, combine the outputs of the L individual models by majority voting or by combining the posterior probabilities. Footnotes References Category:Classification algorithms Category:Ensemble learning"
            }
        ],
        "latex_formulas": []
    },
    "Feature_selection": {
        "title": "Feature_selection",
        "chunks": [
            {
                "text": "In machine learning, feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons: simplification of models to make them easier to interpret, shorter training times, to avoid the curse of dimensionality, improve the compatibility of the data with a certain learning model class, to encode inherent symmetries present in the input space. The central premise when using feature selection is that data sometimes contains features that are redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundancy and irrelevance are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated. Feature extraction creates new features from functions of the original features, whereas feature selection finds a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (data points). Introduction A feature selection algorithm can be seen as the combination of a search technique for proposing new feature subsets, along with an evaluation measure which scores the different feature subsets."
            },
            {
                "text": "The simplest algorithm is to test each possible subset of features finding the one which minimizes the error rate. This is an exhaustive search of the space, and is computationally intractable for all but the smallest of feature sets. The choice of evaluation metric heavily influences the algorithm, and it is these evaluation metrics which distinguish between the three main categories of feature selection algorithms: wrappers, filters and embedded methods. Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model, which is tested on a hold-out set. Counting the number of mistakes made on that hold-out set (the error rate of the model) gives the score for that subset. As wrapper methods train a new model for each subset, they are very computationally intensive, but usually provide the best performing feature set for that particular type of model or typical problem. Filter methods use a proxy measure instead of the error rate to score a feature subset. This measure is chosen to be fast to compute, while still capturing the usefulness of the feature set."
            },
            {
                "text": "Common measures include the mutual information, the pointwise mutual information, Pearson product-moment correlation coefficient, Relief-based algorithms, and inter/intra class distance or the scores of significance tests for each class/feature combinations. Filters are usually less computationally intensive than wrappers, but they produce a feature set which is not tuned to a specific type of predictive model. This lack of tuning means a feature set from a filter is more general than the set from a wrapper, usually giving lower prediction performance than a wrapper. However the feature set doesn't contain the assumptions of a prediction model, and so is more useful for exposing the relationships between the features. Many filters provide a feature ranking rather than an explicit best feature subset, and the cut off point in the ranking is chosen via cross-validation. Filter methods have also been used as a preprocessing step for wrapper methods, allowing a wrapper to be used on larger problems. One other popular approach is the Recursive Feature Elimination algorithm, commonly used with Support Vector Machines to repeatedly construct a model and remove features with low weights. Embedded methods are a catch-all group of techniques which perform feature selection as part of the model construction process."
            },
            {
                "text": "The exemplar of this approach is the LASSO method for constructing a linear model, which penalizes the regression coefficients with an L1 penalty, shrinking many of them to zero. Any features which have non-zero regression coefficients are 'selected' by the LASSO algorithm. Improvements to the LASSO include Bolasso which bootstraps samples; Elastic net regularization, which combines the L1 penalty of LASSO with the L2 penalty of ridge regression; and FeaLect which scores all the features based on combinatorial analysis of regression coefficients. AEFS further extends LASSO to nonlinear scenario with autoencoders. These approaches tend to be between filters and wrappers in terms of computational complexity. In traditional regression analysis, the most popular form of feature selection is stepwise regression, which is a wrapper technique. It is a greedy algorithm that adds the best feature (or deletes the worst feature) at each round. The main control issue is deciding when to stop the algorithm. In machine learning, this is typically done by cross-validation. In statistics, some criteria are optimized. This leads to the inherent problem of nesting. More robust methods have been explored, such as branch and bound and piecewise linear network."
            },
            {
                "text": "Subset selection Subset selection evaluates a subset of features as a group for suitability. Subset selection algorithms can be broken up into wrappers, filters, and embedded methods. Wrappers use a search algorithm to search through the space of possible features and evaluate each subset by running a model on the subset. Wrappers can be computationally expensive and have a risk of over fitting to the model. Filters are similar to wrappers in the search approach, but instead of evaluating against a model, a simpler filter is evaluated. Embedded techniques are embedded in, and specific to, a model. Many popular search approaches use greedy hill climbing, which iteratively evaluates a candidate subset of features, then modifies the subset and evaluates if the new subset is an improvement over the old. Evaluation of the subsets requires a scoring metric that grades a subset of features. Exhaustive search is generally impractical, so at some implementor (or operator) defined stopping point, the subset of features with the highest score discovered up to that point is selected as the satisfactory feature subset. The stopping criterion varies by algorithm; possible criteria include: a subset score exceeds a threshold, a program's maximum allowed run time has been surpassed, etc."
            },
            {
                "text": "Alternative search-based techniques are based on targeted projection pursuit which finds low-dimensional projections of the data that score highly: the features that have the largest projections in the lower-dimensional space are then selected. Search approaches include: Exhaustive Best first Simulated annealing Genetic algorithm Greedy forward selection Greedy backward elimination Particle swarm optimization Targeted projection pursuit Scatter searchF.C. Garcia-Lopez, M. Garcia-Torres, B. Melian, J.A. Moreno-Perez, J.M. Moreno-Vega. Solving feature subset selection problem by a Parallel Scatter Search, European Journal of Operational Research, vol. 169, no. 2, pp. 477–489, 2006. Variable neighborhood searchF.C. Garcia-Lopez, M. Garcia-Torres, B. Melian, J.A. Moreno-Perez, J.M. Moreno-Vega. Solving Feature Subset Selection Problem by a Hybrid Metaheuristic. In First International Workshop on Hybrid Metaheuristics, pp. 59–68, 2004.M. Garcia-Torres, F. Gomez-Vela, B. Melian, J.M. Moreno-Vega. High-dimensional feature selection via feature grouping: A Variable Neighborhood Search approach, Information Sciences, vol. 326, pp. 102-118, 2016. Two popular filter metrics for classification problems are correlation and mutual information, although neither are true metrics or 'distance measures' in the mathematical sense, since they fail to obey the triangle inequality and thus do not compute any actual 'distance' – they should rather be regarded as 'scores'."
            },
            {
                "text": "These scores are computed between a candidate feature (or set of features) and the desired output category. There are, however, true metrics that are a simple function of the mutual information; see here. Other available filter metrics include: Class separability Error probability Inter-class distance Probabilistic distance Entropy Consistency-based feature selection Correlation-based feature selection Optimality criteria The choice of optimality criteria is difficult as there are multiple objectives in a feature selection task. Many common criteria incorporate a measure of accuracy, penalised by the number of features selected. Examples include Akaike information criterion (AIC) and Mallows's Cp, which have a penalty of 2 for each added feature. AIC is based on information theory, and is effectively derived via the maximum entropy principle... Other criteria are Bayesian information criterion (BIC), which uses a penalty of $\\sqrt{\\log{n}}$ for each added feature, minimum description length (MDL) which asymptotically uses $\\sqrt{\\log{n}}$, Bonferroni / RIC which use $\\sqrt{2\\log{p}}$, maximum dependency feature selection, and a variety of new criteria that are motivated by false discovery rate (FDR), which use something close to $\\sqrt{2\\log{\\frac{p}{q}}}$."
            },
            {
                "text": "A maximum entropy rate criterion may also be used to select the most relevant subset of features. Structure learning Filter feature selection is a specific case of a more general paradigm called structure learning. Feature selection finds the relevant feature set for a specific target variable whereas structure learning finds the relationships between all the variables, usually by expressing these relationships as a graph. The most common structure learning algorithms assume the data is generated by a Bayesian Network, and so the structure is a directed graphical model. The optimal solution to the filter feature selection problem is the Markov blanket of the target node, and in a Bayesian Network, there is a unique Markov Blanket for each node. Information Theory Based Feature Selection Mechanisms There are different Feature Selection mechanisms around that utilize mutual information for scoring the different features. They usually use all the same algorithm: Calculate the mutual information as score for between all features ($ f_{i} \\in F $) and the target class () Select the feature with the largest score (e.g. $\\underset{f_{i} \\in F}\\operatorname{argmax}(I(f_{i},c))$) and add it to the set of selected features () Calculate the score which might be derived from the mutual information Select the feature with the largest score and add it to the set of select features (e.g."
            },
            {
                "text": "$\\underset{f_{i} \\in F}\\operatorname{argmax}(I_{derived}(f_{i},c))$) Repeat 3. and 4. until a certain number of features is selected (e.g. $|S|=l$) The simplest approach uses the mutual information as the \"derived\" score. However, there are different approaches, that try to reduce the redundancy between features. Minimum-redundancy-maximum-relevance (mRMR) feature selection Peng et al. Program proposed a feature selection method that can use either mutual information, correlation, or distance/similarity scores to select features. The aim is to penalise a feature's relevancy by its redundancy in the presence of the other selected features. The relevance of a feature set for the class is defined by the average value of all mutual information values between the individual feature $fi$ and the class as follows: $ D(S,c) = \\frac{1}{|S|}\\sum_{f_{i}\\in S}I(f_{i};c) $."
            },
            {
                "text": "The redundancy of all features in the set is the average value of all mutual information values between the feature $fi$ and the feature $fj$: $ R(S) = \\frac{1}{|S|^{2}}\\sum_{f_{i},f_{j}\\in S}I(f_{i};f_{j})$ The mRMR criterion is a combination of two measures given above and is defined as follows: $\\mathrm{mRMR}= \\max_{S} \\left[\\frac{1}{|S|}\\sum_{f_{i}\\in S}I(f_{i};c) - \\frac{1}{|S|^{2}}\\sum_{f_{i},f_{j}\\in S}I(f_{i};f_{j})\\right].$ Suppose that there are full-set features. Let $xi$ be the set membership indicator function for feature $fi$, so that $1=xi=1$ indicates presence and $1=xi=0$ indicates absence of the feature $fi$ in the globally optimal feature set."
            },
            {
                "text": "Let $c_i=I(f_i;c)$ and $a_{ij}=I(f_i;f_j)$. The above may then be written as an optimization problem: $\\mathrm{mRMR}= \\max_{x\\in \\{0,1\\}^{n}} \\left[\\frac{\\sum^{n}_{i=1}c_{i}x_{i}}{\\sum^{n}_{i=1}x_{i}} - \\frac{\\sum^{n}_{i,j=1}a_{ij}x_{i}x_{j}} {(\\sum^{n}_{i=1}x_{i})^{2}}\\right].$ The mRMR algorithm is an approximation of the theoretically optimal maximum-dependency feature selection algorithm that maximizes the mutual information between the joint distribution of the selected features and the classification variable. As mRMR approximates the combinatorial estimation problem with a series of much smaller problems, each of which only involves two variables, it thus uses pairwise joint probabilities which are more robust."
            },
            {
                "text": "In certain situations the algorithm may underestimate the usefulness of features as it has no way to measure interactions between features which can increase relevancy. This can lead to poor performance when the features are individually useless, but are useful when combined (a pathological case is found when the class is a parity function of the features). Overall the algorithm is more efficient (in terms of the amount of data required) than the theoretically optimal max-dependency selection, yet produces a feature set with little pairwise redundancy. mRMR is an instance of a large class of filter methods which trade off between relevancy and redundancy in different ways.Nguyen, H., Franke, K., Petrovic, S. (2010). \"Towards a Generic Feature-Selection Measure for Intrusion Detection\", In Proc. International Conference on Pattern Recognition (ICPR), Istanbul, Turkey. Quadratic programming feature selection mRMR is a typical example of an incremental greedy strategy for feature selection: once a feature has been selected, it cannot be deselected at a later stage. While mRMR could be optimized using floating search to reduce some features, it might also be reformulated as a global quadratic programming optimization problem as follows: $ \\mathrm{QPFS}: \\min_\\mathbf{x} \\left\\{ \\alpha \\mathbf{x}^T H \\mathbf{x} - \\mathbf{x}^T F\\right\\} \\quad \\mbox{s.t.}"
            },
            {
                "text": "\\ \\sum_{i=1}^n x_i=1, x_i\\geq 0 $ where $F_{n\\times1}=[I(f_1;c),\\ldots, I(f_n;c)]^T$ is the vector of feature relevancy assuming there are features in total, $H_{n\\times n}=[I(f_i;f_j)]_{i,j=1\\ldots n}$ is the matrix of feature pairwise redundancy, and $\\mathbf{x}_{n\\times 1}$ represents relative feature weights. QPFS is solved via quadratic programming. It is recently shown that QFPS is biased towards features with smaller entropy, due to its placement of the feature self redundancy term $I(f_i;f_i)$ on the diagonal of . Conditional mutual information Another score derived for the mutual information is based on the conditional relevancy:Nguyen X. Vinh, Jeffrey Chan, Simone Romano and James Bailey, \"Effective Global Approaches for Mutual Information based Feature Selection\". Proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'14), August 24–27, New York City, 2014. \"\""
            },
            {
                "text": "$ \\mathrm{SPEC_{CMI}}: \\max_{\\mathbf{x}} \\left\\{\\mathbf{x}^T Q \\mathbf{x}\\right\\} \\quad \\mbox{s.t. }\\ \\|\\mathbf{x}\\|=1, x_i\\geq 0 $ where $Q_{ii}=I(f_i;c)$ and $Q_{ij}=(I(f_i;c|f_j)+I(f_j;c|f_i))/2, i\\ne j$. An advantage of $SPECCMI$ is that it can be solved simply via finding the dominant eigenvector of , thus is very scalable. $SPECCMI$ also handles second-order feature interaction. Joint mutual information In a study of different scores Brown et al. recommended the joint mutual information as a good score for feature selection. The score tries to find the feature, that adds the most new information to the already selected features, in order to avoid redundancy. The score is formulated as follows: $ \\begin{align} JMI(f_i) &= \\sum_{f_j \\in S} (I(f_i;c) + I(f_i;c|f_j)) \\\\ &= \\sum_{f_j \\in S} \\bigl[ I (f_j;c) + I (f_i;c) - \\bigl(I (f_i;f_j) - I (f_i;f_j|c)\\bigr)\\bigr] \\end{align} $ The score uses the conditional mutual information and the mutual information to estimate the redundancy between the already selected features ($ f_j \\in S $) and the feature under investigation ($f_i$)."
            },
            {
                "text": "Hilbert-Schmidt Independence Criterion Lasso based feature selection For high-dimensional and small sample data (e.g., dimensionality > and the number of samples < ), the Hilbert-Schmidt Independence Criterion Lasso (HSIC Lasso) is useful. HSIC Lasso optimization problem is given as $ \\mathrm{HSIC_{Lasso}}: \\min_{\\mathbf{x}} \\frac{1}{2}\\sum_{k,l = 1}^n x_k x_l {\\mbox{HSIC}}(f_k,f_l) - \\sum_{k = 1}^n x_k {\\mbox{HSIC}}(f_k,c) + \\lambda \\|\\mathbf{x}\\|_1, \\quad \\mbox{s.t.}"
            },
            {
                "text": "\\ x_1,\\ldots, x_n \\geq 0, $ where ${\\mbox{HSIC}}(f_k,c) =\\mbox{tr}(\\bar{\\mathbf{K}}^{(k)} \\bar{\\mathbf{L}})$ is a kernel-based independence measure called the (empirical) Hilbert-Schmidt independence criterion (HSIC), $\\mbox{tr}(\\cdot)$ denotes the trace, $\\lambda$ is the regularization parameter, $\\bar{\\mathbf{K}}^{(k)} = \\mathbf{\\Gamma} \\mathbf{K}^{(k)} \\mathbf{\\Gamma}$ and $\\bar{\\mathbf{L}} = \\mathbf{\\Gamma} \\mathbf{L} \\mathbf{\\Gamma}$ are input and output centered Gram matrices, $K^{(k)}_{i,j} = K(u_{k,i},u_{k,j})$ and $L_{i,j} = L(c_i,c_j)$ are Gram matrices, $K(u,u')$ and $L(c,c')$ are kernel functions, $\\mathbf{\\Gamma} = \\mathbf{I}_m - \\frac{1}{m}\\mathbf{1}_m \\mathbf{1}_m^T$ is the centering matrix, $\\mathbf{I}_m$ is the -dimensional identity matrix (: the number of samples), $\\mathbf{1}_m$ is the -dimensional vector with all ones, and $\\|\\cdot\\|_{1}$ is the $\\ell_1$-norm."
            },
            {
                "text": "HSIC always takes a non-negative value, and is zero if and only if two random variables are statistically independent when a universal reproducing kernel such as the Gaussian kernel is used. The HSIC Lasso can be written as $ \\mathrm{HSIC_{Lasso}}: \\min_{\\mathbf{x}} \\frac{1}{2}\\left\\|\\bar{\\mathbf{L}} - \\sum_{k = 1}^{n} x_k \\bar{\\mathbf{K}}^{(k)} \\right\\|^2_{F} + \\lambda \\|\\mathbf{x}\\|_1, \\quad \\mbox{s.t.} \\ x_1,\\ldots,x_n \\geq 0, $ where $\\|\\cdot\\|_{F}$ is the Frobenius norm. The optimization problem is a Lasso problem, and thus it can be efficiently solved with a state-of-the-art Lasso solver such as the dual augmented Lagrangian method. Correlation feature selection The correlation feature selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: \"Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other\"."
            },
            {
                "text": "The following equation gives the merit of a feature subset S consisting of k features: $ \\mathrm{Merit}_{S_{k}} = \\frac{k\\overline{r_{cf}}}{\\sqrt{k+k(k-1)\\overline{r_{ff}}}}.$ Here, $ \\overline{r_{cf}} $ is the average value of all feature-classification correlations, and $ \\overline{r_{ff}} $ is the average value of all feature-feature correlations. The CFS criterion is defined as follows: $\\mathrm{CFS} = \\max_{S_k} \\left[\\frac{r_{c f_1}+r_{c f_2}+\\cdots+r_{c f_k}} {\\sqrt{k+2(r_{f_1 f_2}+\\cdots+r_{f_i f_j}+ \\cdots + r_{f_k f_{k-1} })}}\\right].$ The $r_{cf_{i}}$ and $r_{f_{i}f_{j}}$ variables are referred to as correlations, but are not necessarily Pearson's correlation coefficient or Spearman's ρ."
            },
            {
                "text": "Hall's dissertation uses neither of these, but uses three different measures of relatedness, minimum description length (MDL), symmetrical uncertainty, and relief. Let xi be the set membership indicator function for feature fi; then the above can be rewritten as an optimization problem: $\\mathrm{CFS} = \\max_{x\\in \\{0,1\\}^{n}} \\left[\\frac{(\\sum^{n}_{i=1}a_{i}x_{i})^{2}} {\\sum^{n}_{i=1}x_i + \\sum_{i\\neq j} 2b_{ij} x_i x_j }\\right].$ The combinatorial problems above are, in fact, mixed 0–1 linear programming problems that can be solved by using branch-and-bound algorithms. Regularized trees The features from a decision tree or a tree ensemble are shown to be redundant. A recent method called regularized treeH. Deng, G. Runger, \"Feature Selection via Regularized Trees\", Proceedings of the 2012 International Joint Conference on Neural Networks (IJCNN), IEEE, 2012 can be used for feature subset selection."
            },
            {
                "text": "Regularized trees penalize using a variable similar to the variables selected at previous tree nodes for splitting the current node. Regularized trees only need build one tree model (or one tree ensemble model) and thus are computationally efficient. Regularized trees naturally handle numerical and categorical features, interactions and nonlinearities. They are invariant to attribute scales (units) and insensitive to outliers, and thus, require little data preprocessing such as normalization. Regularized random forest (RRF)RRF: Regularized Random Forest, R package on CRAN is one type of regularized trees. The guided RRF is an enhanced RRF which is guided by the importance scores from an ordinary random forest. Overview on metaheuristics methods A metaheuristic is a general description of an algorithm dedicated to solve difficult (typically NP-hard problem) optimization problems for which there is no classical solving methods. Generally, a metaheuristic is a stochastic algorithm tending to reach a global optimum. There are many metaheuristics, from a simple local search to a complex global search algorithm. Main principles The feature selection methods are typically presented in three classes based on how they combine the selection algorithm and the model building."
            },
            {
                "text": "Filter method Filter Method for feature selection Filter type methods select variables regardless of the model. They are based only on general features like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables will be part of a classification or a regression model used to classify or to predict data. These methods are particularly effective in computation time and robust to overfitting. Filter methods tend to select redundant variables when they do not consider the relationships between variables. However, more elaborate features try to minimize this problem by removing variables highly correlated to each other, such as the Fast Correlation Based Filter (FCBF) algorithm. Wrapper method Wrapper Method for Feature selection Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions amongst variables.T. M. Phuong, Z. Lin et R. B. Altman. Choosing SNPs using feature selection. Proceedings / IEEE Computational Systems Bioinformatics Conference, CSB. IEEE Computational Systems Bioinformatics Conference, pages 301-309, 2005. . The two main disadvantages of these methods are: The increasing overfitting risk when the number of observations is insufficient."
            },
            {
                "text": "The significant computation time when the number of variables is large. Embedded method Embedded method for Feature selection Embedded methods have been recently proposed that try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously, such as the FRMT algorithm. Application of feature selection metaheuristics This is a survey of the application of feature selection metaheuristics lately used in the literature. This survey was realized by J. Hammon in her 2013 thesis. Application Algorithm Approach Classifier Evaluation Function Reference SNPs Feature Selection using Feature Similarity Filter r2 Phuong 2005 SNPs Genetic algorithm Wrapper Decision Tree Classification accuracy (10-fold) Shah 2004 SNPs Hill climbing Filter + Wrapper Naive Bayesian Predicted residual sum of squares Long 2007 SNPs Simulated annealing Naive bayesian Classification accuracy (5-fold) Ustunkar 2011 Segments parole Ant colony Wrapper Artificial Neural Network MSE Al-ani 2005 Marketing Simulated annealing Wrapper Regression AIC, r2 Meiri 2006 Economics Simulated annealing, genetic algorithm Wrapper Regression BIC Kapetanios 2007 Spectral Mass Genetic algorithm Wrapper Multiple Linear Regression, Partial Least Squares root-mean-square error of prediction Broadhurst et al."
            },
            {
                "text": "1997 Spam Binary PSO + Mutation Wrapper Decision tree weighted cost Zhang 2014 Microarray Tabu search + PSO Wrapper Support Vector Machine, K Nearest Neighbors Euclidean Distance Chuang 2009 Microarray PSO + Genetic algorithm Wrapper Support Vector Machine Classification accuracy (10-fold) Alba 2007E. Alba, J. Garia-Nieto, L. Jourdan et E.-G. Talbi. Gene Selection in Cancer Classification using PSO-SVM and GA-SVM Hybrid Algorithms. Congress on Evolutionary Computation, Singapore: Singapore (2007), 2007 Microarray Genetic algorithm + Iterated Local Search Embedded Support Vector Machine Classification accuracy (10-fold) Duval 2009B. Duval, J.-K. Hao et J. C. Hernandez Hernandez. A memetic algorithm for gene selection and molecular classification of an cancer. In Proceedings of the 11th Annual conference on Genetic and evolutionary computation, GECCO '09, pages 201-208, New York, NY, USA, 2009. ACM. Microarray Iterated local search Wrapper Regression Posterior Probability Hans 2007C. Hans, A. Dobra et M. West. Shotgun stochastic search for 'large p' regression. Journal of the American Statistical Association, 2007."
            },
            {
                "text": "Microarray Genetic algorithm Wrapper K Nearest Neighbors Classification accuracy (Leave-one-out cross-validation) Jirapech-Umpai 2005 Microarray Hybrid genetic algorithm Wrapper K Nearest Neighbors Classification accuracy (Leave-one-out cross-validation) Oh 2004 Microarray Genetic algorithm Wrapper Support Vector Machine Sensitivity and specificity Xuan 2011 Microarray Genetic algorithm Wrapper All paired Support Vector Machine Classification accuracy (Leave-one-out cross-validation) Peng 2003 Microarray Genetic algorithm Embedded Support Vector Machine Classification accuracy (10-fold) Hernandez 2007 Microarray Genetic algorithm Hybrid Support Vector Machine Classification accuracy (Leave-one-out cross-validation) Huerta 2006 Microarray Genetic algorithm Support Vector Machine Classification accuracy (10-fold) Muni 2006 Microarray Genetic algorithm Wrapper Support Vector Machine EH-DIALL, CLUMP Jourdan 2005Alzheimer's disease Welch's t-test Filter Support vector machine Classification accuracy (10-fold) Zhang 2015Computer visionInfinite Feature SelectionFilterIndependentAverage Precision, ROC AUCRoffo 2015MicroarraysEigenvector Centrality FSFilterIndependentAverage Precision, Accuracy, ROC AUCRoffo & Melzi 2016XML Symmetrical Tau (ST) Filter Structural Associative Classification Accuracy, Coverage Shaharanee & Hadzic 2014 Feature selection embedded in learning algorithms Some learning algorithms perform feature selection as part of their overall operation."
            },
            {
                "text": "These include: -regularization techniques, such as sparse regression, LASSO, and -SVM Regularized trees, e.g. regularized random forest implemented in the RRF package Decision treeR. Kohavi and G. John, \"Wrappers for feature subset selection\", Artificial intelligence 97.1-2 (1997): 273-324 Memetic algorithm Random multinomial logit (RMNL) Auto-encoding networks with a bottleneck-layer Submodular feature selectionLiu et al., Submodular feature selection for high-dimensional acoustic score spaces Zheng et al., Submodular Attribute Selection for Action Recognition in Video Local learning based feature selection. Compared with traditional methods, it does not involve any heuristic search, can easily handle multi-class problems, and works for both linear and nonlinear problems. It is also supported by a strong theoretical foundation. Numeric experiments showed that the method can achieve a close-to-optimal solution even when data contains >1M irrelevant features. Recommender system based on feature selection.D.H. Wang, Y.C. Liang, D.Xu, X.Y. Feng, R.C. Guan(2018), \"A content-based recommender system for computer science publications\", Knowledge-Based Systems, 157: 1-9 The feature selection methods are introduced into recommender system research. See also Cluster analysis Data mining Dimensionality reduction Feature extraction Hyperparameter optimization Model selection Relief (feature selection) References Further reading External links Feature Selection Package, Arizona State University (Matlab Code) NIPS challenge 2003 (see also NIPS) Naive Bayes implementation with feature selection in Visual Basic (includes executable and source code) Minimum-redundancy-maximum-relevance (mRMR) feature selection program FEAST (Open source Feature Selection algorithms in C and MATLAB) Category:Model selection Category:Dimension reduction"
            }
        ],
        "latex_formulas": [
            "''f<sub>i</sub>''",
            "''f<sub>i</sub>''",
            "''f<sub>j</sub>''",
            "''x<sub>i</sub>''",
            "''f<sub>i</sub>''",
            "''x<sub>i</sub>''=1",
            "''x<sub>i</sub>''=0",
            "''f<sub>i</sub>''",
            "SPEC<sub>CMI</sub>",
            "SPEC<sub>CMI</sub>",
            "\\sqrt{\\log{n}}",
            "\\sqrt{\\log{n}}",
            "\\sqrt{2\\log{p}}",
            "\\sqrt{2\\log{\\frac{p}{q}}}",
            "f_{i} \\in F",
            "\\underset{f_{i} \\in F}\\operatorname{argmax}(I(f_{i},c))",
            "\\underset{f_{i} \\in F}\\operatorname{argmax}(I_{derived}(f_{i},c))",
            "|S|=l",
            "D(S,c) = \\frac{1}{|S|}\\sum_{f_{i}\\in S}I(f_{i};c)",
            "R(S) = \\frac{1}{|S|^{2}}\\sum_{f_{i},f_{j}\\in S}I(f_{i};f_{j})",
            "\\mathrm{mRMR}= \\max_{S}\n\\left[\\frac{1}{|S|}\\sum_{f_{i}\\in S}I(f_{i};c) - \n\\frac{1}{|S|^{2}}\\sum_{f_{i},f_{j}\\in S}I(f_{i};f_{j})\\right].",
            "c_i=I(f_i;c)",
            "a_{ij}=I(f_i;f_j)",
            "\\mathrm{mRMR}= \\max_{x\\in \\{0,1\\}^{n}} \n\\left[\\frac{\\sum^{n}_{i=1}c_{i}x_{i}}{\\sum^{n}_{i=1}x_{i}} -\n\\frac{\\sum^{n}_{i,j=1}a_{ij}x_{i}x_{j}}\n{(\\sum^{n}_{i=1}x_{i})^{2}}\\right].",
            "\\mathrm{QPFS}: \\min_\\mathbf{x}  \\left\\{ \\alpha \\mathbf{x}^T H \\mathbf{x} -  \\mathbf{x}^T F\\right\\} \\quad \\mbox{s.t.} \\ \\sum_{i=1}^n x_i=1, x_i\\geq 0",
            "F_{n\\times1}=[I(f_1;c),\\ldots, I(f_n;c)]^T",
            "H_{n\\times n}=[I(f_i;f_j)]_{i,j=1\\ldots n}",
            "\\mathbf{x}_{n\\times 1}",
            "I(f_i;f_i)",
            "\\mathrm{SPEC_{CMI}}: \\max_{\\mathbf{x}} \\left\\{\\mathbf{x}^T Q \\mathbf{x}\\right\\} \\quad \\mbox{s.t.}\\ \\|\\mathbf{x}\\|=1, x_i\\geq 0",
            "Q_{ii}=I(f_i;c)",
            "Q_{ij}=(I(f_i;c|f_j)+I(f_j;c|f_i))/2, i\\ne j",
            "\\begin{align}\nJMI(f_i) &= \\sum_{f_j \\in S} (I(f_i;c) + I(f_i;c|f_j)) \\\\\n         &= \\sum_{f_j \\in S} \\bigl[ I (f_j;c) + I (f_i;c) - \\bigl(I (f_i;f_j) - I (f_i;f_j|c)\\bigr)\\bigr]\n\\end{align}",
            "f_j \\in S",
            "f_i",
            "\\mathrm{HSIC_{Lasso}}: \\min_{\\mathbf{x}} \\frac{1}{2}\\sum_{k,l = 1}^n x_k x_l {\\mbox{HSIC}}(f_k,f_l) - \\sum_{k = 1}^n x_k {\\mbox{HSIC}}(f_k,c) +  \\lambda \\|\\mathbf{x}\\|_1, \\quad \\mbox{s.t.} \\ x_1,\\ldots, x_n \\geq 0,",
            "{\\mbox{HSIC}}(f_k,c) =\\mbox{tr}(\\bar{\\mathbf{K}}^{(k)}  \\bar{\\mathbf{L}})",
            "\\mbox{tr}(\\cdot)",
            "\\lambda",
            "\\bar{\\mathbf{K}}^{(k)} = \\mathbf{\\Gamma} \\mathbf{K}^{(k)} \\mathbf{\\Gamma}",
            "\\bar{\\mathbf{L}} = \\mathbf{\\Gamma} \\mathbf{L} \\mathbf{\\Gamma}",
            "K^{(k)}_{i,j} = K(u_{k,i},u_{k,j})",
            "L_{i,j} = L(c_i,c_j)",
            "K(u,u')",
            "L(c,c')",
            "\\mathbf{\\Gamma} = \\mathbf{I}_m - \\frac{1}{m}\\mathbf{1}_m \\mathbf{1}_m^T",
            "\\mathbf{I}_m",
            "\\mathbf{1}_m",
            "\\|\\cdot\\|_{1}",
            "\\ell_1",
            "\\mathrm{HSIC_{Lasso}}: \\min_{\\mathbf{x}} \\frac{1}{2}\\left\\|\\bar{\\mathbf{L}} - \\sum_{k = 1}^{n} x_k \\bar{\\mathbf{K}}^{(k)} \\right\\|^2_{F}  +  \\lambda \\|\\mathbf{x}\\|_1, \\quad \\mbox{s.t.} \\ x_1,\\ldots,x_n \\geq 0,",
            "\\|\\cdot\\|_{F}",
            "\\mathrm{Merit}_{S_{k}} = \\frac{k\\overline{r_{cf}}}{\\sqrt{k+k(k-1)\\overline{r_{ff}}}}.",
            "\\overline{r_{cf}}",
            "\\overline{r_{ff}}",
            "\\mathrm{CFS} = \\max_{S_k}\n\\left[\\frac{r_{c f_1}+r_{c f_2}+\\cdots+r_{c f_k}}\n{\\sqrt{k+2(r_{f_1 f_2}+\\cdots+r_{f_i f_j}+ \\cdots\n+ r_{f_k f_{k-1} })}}\\right].",
            "r_{cf_{i}}",
            "r_{f_{i}f_{j}}",
            "\\mathrm{CFS} = \\max_{x\\in \\{0,1\\}^{n}} \n\\left[\\frac{(\\sum^{n}_{i=1}a_{i}x_{i})^{2}}\n{\\sum^{n}_{i=1}x_i + \\sum_{i\\neq j} 2b_{ij} x_i x_j }\\right]."
        ]
    }
}