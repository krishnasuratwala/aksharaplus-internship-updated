{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GOOGLE COLAB LINK:-\n",
    "#https://colab.research.google.com/drive/1DZLzhJK9ucD2k07WkxbuoJ09h2J29PBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e238c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (11.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.7.1\n",
    "!pip install torchvision==0.22.1\n",
    "!pip install marker-pdf\n",
    "!pip3 install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!marker_single \"sample.pdf\" --output_format json --output_dir output --format_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a531038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final cleaned and flattened JSON saved to: D:/aksharaplus/output/sample\\final_cleaned_output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "def sanitize_id(block_id):\n",
    "    return block_id.replace(\"/\", \"_\").replace(\":\", \"_\").replace(\"\\\\\", \"_\")\n",
    "\n",
    "def decode_and_save_images(images, block_id, images_dir):\n",
    "    saved_paths = {}\n",
    "    for img_id, base64_str in (images or {}).items():\n",
    "        try:\n",
    "            img_filename = f\"{sanitize_id(block_id)}_{sanitize_id(img_id)}.png\"\n",
    "            img_path = os.path.join(images_dir, img_filename)\n",
    "            with open(img_path, \"wb\") as img_file:\n",
    "                img_file.write(base64.b64decode(base64_str))\n",
    "            saved_paths[img_id] = img_path.replace(\"\\\\\", \"/\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error decoding image {img_id} in block {block_id}: {e}\")\n",
    "    return saved_paths\n",
    "\n",
    "def extract_parent_title(path_parts):\n",
    "    for part in reversed(path_parts):\n",
    "        if \"chapter\" in part.lower() or \"section\" in part.lower():\n",
    "            return part.strip()\n",
    "    return \"\"\n",
    "\n",
    "def flatten_blocks(blocks, page_title=\"\", page_num=\"\", context_path=\"\", parent_title=\"\", flat_list=None, images_dir=None):\n",
    "    if flat_list is None:\n",
    "        flat_list = []\n",
    "\n",
    "    last_section = parent_title\n",
    "    para_counter = 1\n",
    "\n",
    "    for idx, block in enumerate(blocks):\n",
    "        block_id = block.get(\"id\", f\"block_{idx}\")\n",
    "        block_type = block.get(\"block_type\", \"Unknown\")\n",
    "        html = block.get(\"html\", \"\")\n",
    "        text = block.get(\"text\", \"\")\n",
    "\n",
    "        # Remove unnecessary keys\n",
    "        block.pop(\"bbox\", None)\n",
    "        block.pop(\"polygon\", None)\n",
    "\n",
    "        # Decode and save image\n",
    "        image_paths = decode_and_save_images(block.get(\"images\"), block_id, images_dir)\n",
    "\n",
    "        cleaned_text = clean_html(html) if html else text.strip()\n",
    "\n",
    "        if block_type == \"SectionHeader\":\n",
    "            last_section = cleaned_text.strip()\n",
    "\n",
    "        # Skip plain text for structured tables\n",
    "        if block_type == \"Table\" and \"structured_table\" in block:\n",
    "            cleaned_text = \"\"\n",
    "\n",
    "        contextual_path = f\"{parent_title} > Page {page_num}\"\n",
    "        if last_section and block_type != \"SectionHeader\":\n",
    "            contextual_path += f\" > {last_section}\"\n",
    "        if block_type == \"Text\":\n",
    "            contextual_path += f\" > para {para_counter}\"\n",
    "            para_counter += 1\n",
    "\n",
    "        flat_block = {\n",
    "            \"id\": block_id,\n",
    "            \"text\": cleaned_text,\n",
    "            \"blockType\": block_type,\n",
    "            \"section_path\": contextual_path,\n",
    "            \"parent_title\": last_section\n",
    "        }\n",
    "\n",
    "        if block_type == \"Picture\" and image_paths:\n",
    "            flat_block[\"image_path\"] = list(image_paths.values())[0]\n",
    "\n",
    "        if \"structured_table\" in block:\n",
    "            flat_block[\"structured_table\"] = block[\"structured_table\"]\n",
    "\n",
    "        flat_list.append(flat_block)\n",
    "\n",
    "        if \"children\" in block and isinstance(block[\"children\"], list):\n",
    "            flatten_blocks(\n",
    "                block[\"children\"],\n",
    "                page_title=page_title,\n",
    "                page_num=page_num,\n",
    "                context_path=contextual_path,\n",
    "                parent_title=last_section,\n",
    "                flat_list=flat_list,\n",
    "                images_dir=images_dir\n",
    "            )\n",
    "    return flat_list\n",
    "\n",
    "def assign_prev_next(flat_blocks):\n",
    "    # Group by full section path (excluding para info)\n",
    "    section_map = {}\n",
    "\n",
    "    for block in flat_blocks:\n",
    "        section_key = \" > \".join(block[\"section_path\"].split(\" > \")[:-1])  # Drop para X\n",
    "        if block[\"blockType\"] == \"Text\":  # Only include Text blocks\n",
    "            section_map.setdefault(section_key, []).append(block)\n",
    "\n",
    "    for section, blocks in section_map.items():\n",
    "        for i, block in enumerate(blocks):\n",
    "            block[\"prev_text\"] = blocks[i - 1][\"text\"] if i > 0 else \"\"\n",
    "            block[\"next_text\"] = blocks[i + 1][\"text\"] if i + 1 < len(blocks) else \"\"\n",
    "\n",
    "def clean_and_flatten_json(input_json_path):\n",
    "    base_dir = os.path.dirname(input_json_path)\n",
    "    images_dir = os.path.join(base_dir, \"images\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    output_json_path = os.path.join(base_dir, \"final_cleaned_output.json\")\n",
    "\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    children = data.get(\"children\", [])\n",
    "    flat_blocks = []\n",
    "\n",
    "    for page in children:\n",
    "        page_title = page.get(\"title\", \"\")\n",
    "        page_num_match = re.search(r'/page/(\\d+)', page.get(\"id\", \"\"))\n",
    "        page_num = page_num_match.group(1) if page_num_match else \"?\"\n",
    "        flat_blocks.extend(flatten_blocks(\n",
    "            page.get(\"children\", []),\n",
    "            page_title=page_title,\n",
    "            page_num=page_num,\n",
    "            parent_title=extract_parent_title([page_title]),\n",
    "            flat_list=[],\n",
    "            images_dir=images_dir\n",
    "        ))\n",
    "\n",
    "    assign_prev_next(flat_blocks)\n",
    "\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(flat_blocks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Final cleaned and flattened JSON saved to: {output_json_path}\")\n",
    "\n",
    "# üîÅ Run this\n",
    "clean_and_flatten_json(\"D:/aksharaplus/output/sample/sample.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4146546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated structured JSON saved to: D:/aksharaplus/output/sample/final_structured_output_with_tables.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def fix_structure_with_tables(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    fixed_data = []\n",
    "    current_parent_title = \"\"\n",
    "    page_number = \"\"\n",
    "    buffer = []\n",
    "    i = 0\n",
    "    total = len(data)\n",
    "\n",
    "    def flush_buffer():\n",
    "        for i, block in enumerate(buffer):\n",
    "            if block[\"blockType\"] in [\"Text\", \"ListItem\"]:\n",
    "                block[\"section_path\"] = f\"> Page {page_number} > {current_parent_title} > para {i+1}\"\n",
    "                block[\"parent_title\"] = current_parent_title\n",
    "                block[\"prev_text\"] = buffer[i-1][\"text\"] if i > 0 else \"\"\n",
    "                block[\"next_text\"] = buffer[i+1][\"text\"] if i < len(buffer) - 1 else \"\"\n",
    "            elif block[\"blockType\"] == \"Table\":\n",
    "                block[\"section_path\"] = f\"> Page {page_number} > {current_parent_title} > table\"\n",
    "                block[\"parent_title\"] = current_parent_title\n",
    "                block[\"prev_text\"] = buffer[i-1][\"text\"] if i > 0 else \"\"\n",
    "                block[\"next_text\"] = buffer[i+1][\"text\"] if i < len(buffer) - 1 else \"\"\n",
    "            fixed_data.append(block)\n",
    "        buffer.clear()\n",
    "\n",
    "    while i < total:\n",
    "        block = data[i]\n",
    "        page_match = re.search(r'/page/(\\d+)', block[\"id\"])\n",
    "        page_number = page_match.group(1) if page_match else page_number\n",
    "\n",
    "        if block[\"blockType\"] == \"SectionHeader\":\n",
    "            flush_buffer()\n",
    "            current_parent_title = block[\"text\"].strip()\n",
    "            block[\"section_path\"] = f\"> Page {page_number}\"\n",
    "            block[\"parent_title\"] = current_parent_title\n",
    "            fixed_data.append(block)\n",
    "\n",
    "        elif block[\"blockType\"] in [\"Text\", \"ListItem\"]:\n",
    "            buffer.append(block)\n",
    "\n",
    "        elif block[\"blockType\"] == \"Table\":\n",
    "            # Extract associated TableCell blocks\n",
    "            table_cells = []\n",
    "            j = i + 1\n",
    "            while j < total and data[j][\"blockType\"] == \"TableCell\":\n",
    "                cell_text = data[j][\"text\"].strip()\n",
    "                if cell_text:\n",
    "                    table_cells.append(cell_text)\n",
    "                j += 1\n",
    "\n",
    "            # Reconstruct the table\n",
    "            num_columns = 0\n",
    "            headers = []\n",
    "            rows = []\n",
    "\n",
    "            # Heuristic: first row of TableCell after Table block is header\n",
    "            headers = table_cells[:3]  # Adjust dynamically if needed\n",
    "            num_columns = len(headers)\n",
    "            cell_data = table_cells[3:]\n",
    "\n",
    "            # Group remaining cells into rows\n",
    "            for k in range(0, len(cell_data), num_columns + 1):  # +1 to skip index\n",
    "                row = cell_data[k+1:k+1+num_columns]\n",
    "                if len(row) == num_columns:\n",
    "                    rows.append(row)\n",
    "\n",
    "            block[\"text\"] = {\n",
    "                \"table\": {\n",
    "                    \"columns\": headers,\n",
    "                    \"rows\": rows\n",
    "                }\n",
    "            }\n",
    "\n",
    "            buffer.append(block)\n",
    "            i = j - 1  # Skip past TableCells\n",
    "\n",
    "        elif block[\"blockType\"] == \"TableCell\":\n",
    "            # Skip all TableCell blocks\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # Other blocks like PageFooter, Image, etc.\n",
    "            block[\"section_path\"] = f\"> Page {page_number} > {current_parent_title}\"\n",
    "            block[\"parent_title\"] = current_parent_title\n",
    "            block[\"prev_text\"] = \"\"\n",
    "            block[\"next_text\"] = \"\"\n",
    "            fixed_data.append(block)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    flush_buffer()\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(fixed_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Updated structured JSON saved to: {output_path}\")\n",
    "\n",
    "# üîÅ Run the fixer\n",
    "fix_structure_with_tables(\n",
    "    input_path=\"D:/aksharaplus/output/sample/final_cleaned_output.json\",\n",
    "    output_path=\"D:/aksharaplus/output/sample/final_structured_output_with_tables.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9321bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned 727 blocks.\n",
      "‚úÖ Saved to: D:/aksharaplus/output/sample/final_cleaned_output_filtered.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the original JSON data (which is a list of blocks)\n",
    "with open(\"D:/aksharaplus/output/sample/final_structured_output_with_tables.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Filter blocks: keep those that are not PageFooter, blockType is str, and either:\n",
    "# (a) text is a non-empty string, OR (b) blockType is \"Picture\" (even if text is empty)\n",
    "cleaned_blocks = [\n",
    "    block for block in data\n",
    "    if isinstance(block.get(\"blockType\"), str)\n",
    "    and block.get(\"blockType\") != \"PageFooter\"\n",
    "    and (\n",
    "        (isinstance(block.get(\"text\"), str) and block[\"text\"].strip() != \"\")\n",
    "        or block.get(\"blockType\") == \"Picture\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Save cleaned data\n",
    "with open(\"D:/aksharaplus/output/sample/final_cleaned_output_filtered.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_blocks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Cleaned {len(data) - len(cleaned_blocks)} blocks.\")\n",
    "print(\"‚úÖ Saved to: D:/aksharaplus/output/sample/final_cleaned_output_filtered.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22eb602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 3153\n",
      "Sample entry: {'id': '/page/14/SectionHeader/0', 'text': 'CHAPTER 1 Introduction', 'blockType': 'SectionHeader', 'section_path': '> Page 14', 'parent_title': 'CHAPTER 1 Introduction'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "input_file = \"D:/aksharaplus/output/sample/final_cleaned_output_filtered.json\"\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Preview the structure\n",
    "print(f\"Total entries: {len(data)}\")\n",
    "print(\"Sample entry:\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa0414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to string if input is a dict\n",
    "    if isinstance(text, dict):\n",
    "        text = json.dumps(text)  # Or extract specific key if known, e.g., text.get(\"value\", \"\")\n",
    "    # Ensure text is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Fix common typographical errors\n",
    "    text = text.replace(\"hand‚Äê coded\", \"hand-coded\").replace(\"‚Äê\", \"-\")\n",
    "    # Normalize whitespace\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "cleaned_data = []\n",
    "for item in data:\n",
    "    cleaned_item = {\n",
    "        \"text\": clean_text(item[\"text\"]),\n",
    "        \"blockType\": item[\"blockType\"],\n",
    "        \"parent_title\": item.get(\"parent_title\", \"\"),  # Safely get parent_title\n",
    "        \"prev_text\": clean_text(item.get(\"prev_text\", \"\")) if item.get(\"prev_text\") else \"\",  # Safely get prev_text\n",
    "        \"next_text\": clean_text(item.get(\"next_text\", \"\")) if item.get(\"next_text\") else \"\"   # Safely get next_text\n",
    "    }\n",
    "    cleaned_data.append(cleaned_item)\n",
    "\n",
    "# Save cleaned data\n",
    "with open(\"D:/aksharaplus/output/sample/cleaned_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb7be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = {}\n",
    "for item in cleaned_data:\n",
    "    if item[\"blockType\"] in [\"Text\", \"ListItem\"]:  # Focus on content, exclude headers\n",
    "        parent = item[\"parent_title\"] or \"General\"\n",
    "        if parent not in grouped_data:\n",
    "            grouped_data[parent] = []\n",
    "        # Add context with prev_text and next_text if relevant\n",
    "        context = f\"{item['prev_text'] + ' ' if item['prev_text'] else ''}{item['text']}{(' ' + item['next_text']) if item['next_text'] else ''}\"\n",
    "        grouped_data[parent].append(context)\n",
    "\n",
    "# Combine texts under each parent_title\n",
    "combined_data = [\n",
    "    {\"parent_title\": parent, \"text\": \" \".join(texts)}\n",
    "    for parent, texts in grouped_data.items()\n",
    "]\n",
    "\n",
    "# Save grouped data\n",
    "with open(\"D:/aksharaplus/output/sample/grouped_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e7dd54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parent titles: ['A', 'A First Application: Classifying Iris Species', 'About the Authors', 'Accessing Attributes in a Grid-Searched Pipeline', 'Accessing Step Attributes', 'Advanced Tokenization, Stemming, and Lemmatization', 'Agglomerative Clustering', 'Anaconda', 'Analyzing KNeighborsClassifier', 'Analyzing KNeighborsRegressor', 'Analyzing decision trees', 'Analyzing the result of cross-validation', 'Applying Bag-of-Words to a Toy Dataset', 'Applying Data Transformations', 'Applying NMF to face images', 'Applying NMF to synthetic data', 'Applying PCA to the cancer dataset for visualization', 'Approaching a Machine Learning Problem', 'Automatic Feature Selection', 'B', 'Bag-of-Words for Movie Reviews', 'Bag-of-Words with More Than One Word (n-Grams)', 'Benefits of Cross-Validation', 'Binning, Discretization, Linear Models, and Trees', 'Building Pipelines', 'Building Your First Model: k-Nearest Neighbors', 'Building Your Own Estimator', 'Building decision trees', 'C', 'CHAPTER 1 Introduction', 'CHAPTER 2 Supervised Learning', 'CHAPTER 3 Unsupervised Learning and Preprocessing', 'CHAPTER 4 Representing Data and Engineering Features', 'CHAPTER 5 Model Evaluation and Improvement', 'CHAPTER 6 Algorithm Chains and Pipelines', 'CHAPTER 7 Working with Text Data', 'CHAPTER 8 Wrapping Up', 'Categorical Variables', 'Challenges in Unsupervised Learning', 'Checking string-encoded categorical data', 'Classification and Regression', 'Clustering', 'Colophon', 'Comparing algorithms on the faces dataset', 'Comparing and Evaluating Clustering Algorithms', 'Conclusion', 'Confusion matrices', 'Controlling complexity of decision trees', 'Convenient Pipeline Creation with make_pipeline', 'Cross-Validation', 'Cross-Validation in scikit-learn', 'Cross-validation with groups', 'D', 'DBSCAN', 'Decision Trees', 'Decision trees', 'Detecting abnormal access patterns to a website', 'Different Kinds of Preprocessing', 'Dimensionality Reduction, Feature Extraction, and Manifold Learning', 'E', 'Eigenfaces for feature extraction', 'Ensembles of Decision Trees', 'Enthought Canopy', 'Essential Libraries and Tools', 'Evaluating clustering with ground truth', 'Evaluating clustering without ground truth', 'Evaluating the Model', 'Evaluation Metrics and Scoring', 'Example Application: Sentiment Analysis of Movie Reviews', 'F', 'Failure cases of k-means', 'Feature importance in trees', 'First Things First: Look at Your Data', 'From Prototype to Production', 'G', 'General', 'Generalization, Overfitting, and Underfitting', 'Gradient boosted decision trees', 'Gradient boosted regression trees (gradient boosting machines)', 'Grid Search', 'Grid Search with Cross-Validation', 'Grid-Searching Preprocessing Steps and Model Parameters', 'Grid-Searching Which Model To Use', 'H', 'Hierarchical clustering and dendrograms', 'Honing Your Skills', 'Humans in the Loop', 'I', 'Identifying topics in a set of blog posts', 'Illustrating Information Leakage', 'Imbalanced datasets', 'In[104]:', 'In[10]:', 'In[111]:', 'In[114]:', 'In[11]:', 'In[12]:', 'In[13]:', 'In[14]:', 'In[15]:', 'In[16]:', 'In[17]:', 'In[18]:', 'In[19]:', 'In[1]:', 'In[20]:', 'In[21]:', 'In[22]:', 'In[23]:', 'In[24]:', 'In[25]:', 'In[26]:', 'In[27]:', 'In[28]:', 'In[29]:', 'In[2]:', 'In[30]:', 'In[31]:', 'In[32]:', 'In[33]:', 'In[34]:', 'In[35]:', 'In[36]:', 'In[37]:', 'In[39]:', 'In[3]:', 'In[41]:', 'In[42]:', 'In[43]:', 'In[44]:', 'In[45]:', 'In[46]:', 'In[47]:', 'In[48]:', 'In[49]:', 'In[4]:', 'In[50]:', 'In[52]:', 'In[53]:', 'In[54]:', 'In[55]:', 'In[56]:', 'In[58]:', 'In[59]:', 'In[5]:', 'In[60]:', 'In[61]:', 'In[62]:', 'In[63]:', 'In[64]:', 'In[65]:', 'In[66]:', 'In[67]:', 'In[68]:', 'In[6]:', 'In[70]:', 'In[71]:', 'In[75]:', 'In[76]:', 'In[78]:', 'In[79]:', 'In[7]:', 'In[80]:', 'In[81]:', 'In[82]:', 'In[84]:', 'In[85]:', 'In[87]:', 'In[8]:', 'In[90]:', 'In[91]:', 'In[92]:', 'In[93]:', 'In[94]:', 'In[95]:', 'In[97]:', 'In[98]:', 'In[9]:', 'Installing scikit-learn', 'Interactions and Polynomials', 'Investigating Model Coefficients', 'Iterative Feature Selection', 'J', 'Jupyter Notebook', 'K', 'Keep the End Goal in Mind', 'Kernelized Support Vector Machines', 'Kinds of errors', 'Knowing Your Task and Knowing Your Data', 'L', 'Lasso', 'Latent Dirichlet Allocation', 'Leave-one-out cross-validation', 'Linear Models', 'Linear models', 'Linear models and nonlinear features', 'Linear models for classification', 'Linear models for multiclass classification', 'Linear models for regression', 'Linear regression (aka ordinary least squares)', 'M', 'Making Predictions', 'Manifold Learning with t-SNE', 'Measuring Success: Training and Testing Data', 'Meet the Data', 'Method Chaining', 'Metrics for Binary Classification', 'Metrics for Multiclass Classification', 'Model-Based Feature Selection', 'More control over cross-validation', 'N', 'Naive Bayes', 'Naive Bayes Classifiers', 'Nearest neighbors', 'Nested cross-validation', 'Neural Networks', 'Neural Networks (Deep Learning)', 'Neural networks', 'Non-Negative Matrix Factorization (NMF)', 'NumPy', 'Numbers Can Encode Categoricals', 'O', 'One-Hot-Encoding (Dummy Variables)', 'Other Machine Learning Frameworks and Packages', 'Out[100]:', 'Out[101]:', 'Out[102]:', 'Out[103]:', 'Out[106]:', 'Out[107]:', 'Out[108]:', 'Out[109]:', 'Out[10]:', 'Out[110]:', 'Out[113]:', 'Out[116]:', 'Out[117]:', 'Out[118]:', 'Out[119]:', 'Out[11]:', 'Out[12]:', 'Out[13]:', 'Out[14]:', 'Out[15]:', 'Out[16]:', 'Out[17]:', 'Out[18]:', 'Out[19]:', 'Out[20]:', 'Out[21]:', 'Out[22]:', 'Out[23]:', 'Out[24]:', 'Out[25]:', 'Out[26]:', 'Out[27]:', 'Out[28]:', 'Out[29]:', 'Out[2]:', 'Out[30]:', 'Out[31]:', 'Out[32]:', 'Out[33]:', 'Out[34]:', 'Out[35]:', 'Out[36]:', 'Out[37]:', 'Out[38]:', 'Out[39]:', 'Out[3]:', 'Out[40]:', 'Out[41]:', 'Out[42]:', 'Out[43]:', 'Out[44]:', 'Out[45]:', 'Out[46]:', 'Out[47]:', 'Out[48]:', 'Out[49]:', 'Out[4]:', 'Out[50]:', 'Out[51]:', 'Out[53]:', 'Out[55]:', 'Out[56]:', 'Out[57]:', 'Out[58]:', 'Out[59]:', 'Out[5]:', 'Out[60]:', 'Out[61]:', 'Out[62]:', 'Out[63]:', 'Out[64]:', 'Out[65]:', 'Out[66]:', 'Out[67]:', 'Out[69]:', 'Out[6]:', 'Out[70]:', 'Out[72]:', 'Out[73]:', 'Out[74]:', 'Out[75]:', 'Out[79]:', 'Out[7]:', 'Out[82]:', 'Out[83]:', 'Out[86]:', 'Out[87]:', 'Out[88]:', 'Out[8]:', 'Out[9]:', 'P', 'Parallelizing cross-validation and grid search', 'Parameter Selection with Preprocessing', 'Precision-recall curves and ROC curves', 'Predicting Probabilities', 'Preprocessing and Scaling', 'Preprocessing data for SVMs', 'Principal Component Analysis (PCA)', 'Probabilistic Modeling, Inference, and Probabilistic Programming', 'Problems Machine Learning Can Solve', 'Python 2 Versus Python 3', 'Python(x,y)', 'R', 'Random forests', 'Ranking, Recommender Systems, and Other Kinds of Learning', 'Receiver operating characteristics (ROC) and AUC', 'Regression Metrics', 'Relation of Model Complexity to Dataset Size', 'Representing Text Data as a Bag of Words', 'Rescaling the Data with tf‚Äìidf', 'Ridge regression', 'S', 'Scaling Training and Test Data the Same Way', 'Scaling to Larger Datasets', 'SciPy', 'Search over spaces that are not grids', 'Segmenting customers into groups with similar preferences', 'Shortcuts and Efficient Alternatives', 'Shue-split cross-validation', 'Simple Grid Search', 'Some Sample Datasets', 'Stopwords', 'Stratified k-Fold Cross-Validation and Other Strategies', 'Strengths, weaknesses, and parameters', 'Summary and Outlook', 'Summary of Clustering Methods', 'Summary of the Estimator Interface', 'Supervised Machine Learning Algorithms', 'Support vector machines', 'T', 'Taking uncertainty into account', 'Testing Production Systems', 'The Danger of Overfitting the Parameters and the Validation Set', 'The Decision Function', 'The Effect of Preprocessing on Supervised Learning', 'The General Pipeline Interface', 'The kernel trick', 'The neural network model', 'Theory', 'Topic Modeling and Document Clustering', 'Tuning SVM parameters', 'Tuning neural networks', 'Types of Data Represented as Strings', 'Types of Unsupervised Learning', 'U', 'Uncertainty Estimates from Classifiers', 'Uncertainty in Multiclass Classification', 'Understanding SVMs', 'Univariate Nonlinear Transformations', 'Univariate Statistics', 'Using Evaluation Metrics in Model Selection', 'Using Pipelines in Grid Searches', 'Using different cross-validation strategies with grid search', 'Utilizing Expert Knowledge', 'V', 'Vector quantization, or seeing k-means as decomposition', 'Versions Used in this Book', 'W', 'Where to Go from Here', 'Why Machine Learning?', 'Why Python?', 'X', 'fit Resets a Model', 'k-Means Clustering', 'k-Nearest Neighbors', 'k-Neighbors classification', 'k-neighbors regression', 'matplotlib', 'mglearn', 'mglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)', 'pandas', 'plot_feature_importances_cancer(forest)', 'scikit-learn']\n",
      "Found 23 index entries (A-Z)\n",
      "Sample index entries: ['A', 'B', 'C']\n",
      "Processing parent_title 'A': A/B testing, 359 accuracy, 22 , 282 acknowledgments, xi adjusted rand index (ARI), 191 agglomerative...\n",
      "Processing parent_title 'B': bag-of-words representation applying to movie reviews, 330 - 334 applying to toy dataset, 329 more t...\n",
      "Processing parent_title 'C': C parameter in SVC, 99 calibration, 288 cancer dataset, 32 categorical features categorical data, de...\n",
      "Processing parent_title 'D': data points, defined, 4 data representation, 211 - 250 (see also feature extraction/feature engineer...\n",
      "Processing parent_title 'E': eigenfaces, 147 embarrassingly parallel, 274 encoding, 328 ensembles defined, 83 gradient boosted re...\n",
      "Processing parent_title 'F': f(x)=y formula, 18 facial recognition, 147 , 157 factor analysis (FA), 163 false positive rate (FPR)...\n",
      "Processing parent_title 'G': gamma parameter, 100 Gaussian kernels of SVC, 97 , 100 GaussianNB, 68 generalization building models...\n",
      "Processing parent_title 'H': handcoded rules, disadvantages of, 1 heat maps, 146 hidden layers, 106 hidden units, 105 hierarchica...\n",
      "Processing parent_title 'I': imbalanced datasets, 277 independent component analysis (ICA), 163 inference, 363 information leakag...\n",
      "Processing parent_title 'J': Jupyter Notebook, 7...\n",
      "Processing parent_title 'K': k-fold cross-validation, 252 k-means clustering applying with scikit-learn, 170 vs. classification, ...\n",
      "Processing parent_title 'L': L1 regularization, 53 L2 regularization, 49 , 60 , 67 Lasso model, 53 Latent Dirichlet Allocation (L...\n",
      "Processing parent_title 'M': machine learning algorithm chains and pipelines, 305- 321 applications for, 1 -5 approach to problem...\n",
      "Processing parent_title 'N': n-grams, 339 naive Bayes classifiers kinds in scikit-learn, 68 parameters, 70 strengths and weakness...\n",
      "Processing parent_title 'O': offline evaluation, 359 one-hot-encoding, 213 - 217 one-out-of-N encoding, 213 -217 one-vs.-rest app...\n",
      "Processing parent_title 'P': pair plots, 19 pandas benefits of, 10 checking string-encoded data, 214 column indexing in, 216 conv...\n",
      "Processing parent_title 'R': R language, 362 radial basis function (RBF) kernel, 97 random forests analyzing, 85 building, 84 dat...\n",
      "Processing parent_title 'S': Safari Books Online, x samples, defined, 4 scaling, 132 - 140 data transformation application, 134 e...\n",
      "Processing parent_title 'T': t-SNE algorithm (see manifold learning algo- rithms) tangens hyperbolicus (tanh), 106 term frequency...\n",
      "Processing parent_title 'U': uncertainty estimates applications for, 119 decision function, 120 in binary classification evaluati...\n",
      "Processing parent_title 'V': value_counts function, 214 vector quantization, 176 vocabulary building, 328 voting, 36 vowpal wabbi...\n",
      "Processing parent_title 'W': wave dataset, 31 weak learners, 88 weights, 47, 106 whitening option, 150 Wisconsin Breast Cancer da...\n",
      "Processing parent_title 'X': xgboost package, 91 xkcd Color Survey, 324...\n",
      "Extracted 729 index terms\n",
      "Sample terms: [{'term': 'A/B', 'pages': '', 'subtopics': [{'subtopic': 'testing,', 'pages': ''}, {'subtopic': '359', 'pages': ''}], 'parent_title': 'A'}, {'term': 'accuracy,', 'pages': '', 'subtopics': [{'subtopic': '22', 'pages': ''}, {'subtopic': ',', 'pages': ''}, {'subtopic': '282', 'pages': ''}], 'parent_title': 'A'}, {'term': 'acknowledgments,', 'pages': '', 'subtopics': [{'subtopic': 'xi', 'pages': ''}], 'parent_title': 'A'}, {'term': 'adjusted', 'pages': '', 'subtopics': [{'subtopic': 'rand', 'pages': ''}, {'subtopic': 'index', 'pages': ''}, {'subtopic': '(ARI),', 'pages': ''}, {'subtopic': '191', 'pages': ''}], 'parent_title': 'A'}, {'term': 'agglomerative', 'pages': '', 'subtopics': [{'subtopic': 'clustering', 'pages': ''}, {'subtopic': 'evaluating', 'pages': ''}], 'parent_title': 'A'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Load grouped_data.json\n",
    "with open(\"D:/aksharaplus/output/sample/grouped_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    grouped_data = json.load(f)\n",
    "\n",
    "# Debugging: List all parent_title values\n",
    "parent_titles = sorted(set(item[\"parent_title\"] for item in grouped_data))\n",
    "print(\"All parent titles:\", parent_titles)\n",
    "\n",
    "# Filter entries where parent_title is a single letter (A-Z)\n",
    "index_entries = [item for item in grouped_data if re.match(r\"^[A-Z]$\", item[\"parent_title\"])]\n",
    "\n",
    "# Debugging: Verify index entries found\n",
    "print(f\"Found {len(index_entries)} index entries (A-Z)\")\n",
    "if index_entries:\n",
    "    print(\"Sample index entries:\", [item[\"parent_title\"] for item in index_entries[:3]])\n",
    "\n",
    "index_terms = []\n",
    "for item in index_entries:\n",
    "    parent_title = item[\"parent_title\"]\n",
    "    text = item[\"text\"]\n",
    "    # Debugging: Print first 100 chars of text\n",
    "    print(f\"Processing parent_title '{parent_title}': {text[:100]}...\")\n",
    "\n",
    "    # Split text into lines for hierarchical parsing\n",
    "    lines = text.split(\" \")\n",
    "    current_main_term = None\n",
    "    current_subtopics = []\n",
    "    current_pages = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # Match main term or subtopic with pages (e.g., \"A/B testing, 359\" or \"evaluating and comparing, 191\")\n",
    "        match = re.match(r\"(.+?)(,\\s*([\\d\\-,\\sxi]+))?$\", line)\n",
    "        if match:\n",
    "            term, _, pages = match.groups()\n",
    "            term = term.strip()\n",
    "            pages = pages.strip() if pages else \"\"\n",
    "            \n",
    "            # Check if this is a main term (starts with parent_title letter, ignoring case)\n",
    "            if term.lower().startswith(parent_title.lower()):\n",
    "                if current_main_term:\n",
    "                    # Save previous main term and its subtopics\n",
    "                    index_terms.append({\n",
    "                        \"term\": current_main_term,\n",
    "                        \"pages\": current_pages,\n",
    "                        \"subtopics\": current_subtopics,\n",
    "                        \"parent_title\": parent_title\n",
    "                    })\n",
    "                current_main_term = term\n",
    "                current_subtopics = []\n",
    "                current_pages = pages\n",
    "            else:\n",
    "                # This is a subtopic under the current main term\n",
    "                if current_main_term:\n",
    "                    current_subtopics.append({\n",
    "                        \"subtopic\": term,\n",
    "                        \"pages\": pages\n",
    "                    })\n",
    "        else:\n",
    "            # Line might be a continuation or malformed; log for debugging\n",
    "            print(f\"Unmatched line in '{parent_title}': {line}\")\n",
    "\n",
    "    # Save the last main term\n",
    "    if current_main_term:\n",
    "        index_terms.append({\n",
    "            \"term\": current_main_term,\n",
    "            \"pages\": current_pages,\n",
    "            \"subtopics\": current_subtopics,\n",
    "            \"parent_title\": parent_title\n",
    "        })\n",
    "\n",
    "# Save index terms\n",
    "with open(\"D:/aksharaplus/output/sample/index_terms.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(index_terms, f, indent=2)\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Extracted {len(index_terms)} index terms\")\n",
    "print(\"Sample terms:\", index_terms[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94065959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 397 JSONL entries\n",
      "Sample entries:\n",
      "{'text': \"Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science and is also known as predictive analytics or statistical learning. The application of machine learning methods has in recent years become ubiquitous in everyday life. From auto- matic recommendations of which movies to watch, to what food to order or which products to buy, to personalized online radio and recognizing your friends in your photos, many modern websites and devices have machine learning algorithms at their core. When you look at a complex website like Facebook, Amazon, or Netflix, it is very likely that every part of the site contains multiple machine learning models. Outside of commercial applications, machine learning has had a tremendous influ- ence on the way data-driven research is done today. The tools introduced in this book have been applied to diverse scientific problems such as understanding stars, finding distant planets, discovering new particles, analyzing DNA sequences, and providing personalized cancer treatments. Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science and is also known as predictive analytics or statistical learning. The application of machine learning methods has in recent years become ubiquitous in everyday life. From auto- matic recommendations of which movies to watch, to what food to order or which products to buy, to personalized online radio and recognizing your friends in your photos, many modern websites and devices have machine learning algorithms at their core. When you look at a complex website like Facebook, Amazon, or Netflix, it is very likely that every part of the site contains multiple machine learning models. Outside of commercial applications, machine learning has had a tremendous influ- ence on the way data-driven research is done today. The tools introduced in this book have been applied to diverse scientific problems such as understanding stars, finding distant planets, discovering new particles, analyzing DNA sequences, and providing personalized cancer treatments. Your application doesn't need to be as large-scale or world-changing as these exam- ples in order to benefit from machine learning, though. In this chapter, we will explain why machine learning has become so popular and discuss what kinds of problems can be solved using machine learning. Then, we will show you how to build your first machine learning model, introducing important concepts along the way. Outside of commercial applications, machine learning has had a tremendous influ- ence on the way data-driven research is done today. The tools introduced in this book have been applied to diverse scientific problems such as understanding stars, finding distant planets, discovering new particles, analyzing DNA sequences, and providing personalized cancer treatments. Your application doesn't need to be as large-scale or world-changing as these exam- ples in order to benefit from machine learning, though. In this chapter, we will explain why machine learning has become so popular and discuss what kinds of problems can be solved using machine learning. Then, we will show you how to build your first machine learning model, introducing important concepts along the way.\"}\n",
      "{'text': 'In the early days of \"intelligent\" applications, many systems used handcoded rules of \"if \" and \"else\" decisions to process data or adjust to user input. Think of a spam filter whose job is to move the appropriate incoming email messages to a spam folder. You could make up a blacklist of words that would result in an email being marked as spam. This would be an example of using an expert-designed rule system to design an \"intelligent\" application. Manually crafting decision rules is feasible for some applica- tions, particularly those in which humans have a good understanding of the process to model. However, using handcoded rules to make decisions has two major disad- vantages: In the early days of \"intelligent\" applications, many systems used handcoded rules of \"if \" and \"else\" decisions to process data or adjust to user input. Think of a spam filter whose job is to move the appropriate incoming email messages to a spam folder. You could make up a blacklist of words that would result in an email being marked as spam. This would be an example of using an expert-designed rule system to design an \"intelligent\" application. Manually crafting decision rules is feasible for some applica- tions, particularly those in which humans have a good understanding of the process to model. However, using handcoded rules to make decisions has two major disad- vantages: The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system. spam. This would be an example of using an expert-designed rule system to design an \"intelligent\" application. Manually crafting decision rules is feasible for some applica- tions, particularly those in which humans have a good understanding of the process to model. However, using handcoded rules to make decisions has two major disad- vantages: The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system. Designing rules requires a deep understanding of how a decision should be made by a human expert. The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system. Designing rules requires a deep understanding of how a decision should be made by a human expert. One example of where this handcoded approach will fail is in detecting faces in images. Today, every smartphone can detect a face in an image. However, face detec- tion was an unsolved problem until as recently as 2001. The main problem is that the way in which pixels (which make up an image in a computer) are \"perceived\" by the computer is very different from how humans perceive a face. This difference in repre- sentation makes it basically impossible for a human to come up with a good set of rules to describe what constitutes a face in a digital image. Designing rules requires a deep understanding of how a decision should be made by a human expert. One example of where this handcoded approach will fail is in detecting faces in images. Today, every smartphone can detect a face in an image. However, face detec- tion was an unsolved problem until as recently as 2001. The main problem is that the way in which pixels (which make up an image in a computer) are \"perceived\" by the computer is very different from how humans perceive a face. This difference in repre- sentation makes it basically impossible for a human to come up with a good set of rules to describe what constitutes a face in a digital image. Using machine learning, however, simply presenting a program with a large collec- tion of images of faces is enough for an algorithm to determine what characteristics are needed to identify a face. One example of where this handcoded approach will fail is in detecting faces in images. Today, every smartphone can detect a face in an image. However, face detec- tion was an unsolved problem until as recently as 2001. The main problem is that the way in which pixels (which make up an image in a computer) are \"perceived\" by the computer is very different from how humans perceive a face. This difference in repre- sentation makes it basically impossible for a human to come up with a good set of rules to describe what constitutes a face in a digital image. Using machine learning, however, simply presenting a program with a large collec- tion of images of faces is enough for an algorithm to determine what characteristics are needed to identify a face.'}\n",
      "{'text': 'The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, which is known as supervised learning , the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired out- put given an input. In particular, the algorithm is able to create an output for an input it has never seen before without any help from a human. Going back to our example of spam classification, using machine learning, the user provides the algorithm with a large number of emails (which are the input), together with information about whether any of these emails are spam (which is the desired output). Given a new email, the algorithm will then produce a prediction as to whether the new email is spam. Machine learning algorithms that learn from input/output pairs are called supervised learning algorithms because a \"teacher\" provides supervision to the algorithms in the form of the desired outputs for each example that they learn from. While creating a dataset of inputs and outputs is often a laborious manual process, supervised learning algorithms are well understood and their performance is easy to measure. If your application can be formulated as a supervised learning problem, and you are able to The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, which is known as supervised learning , the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired out- put given an input. In particular, the algorithm is able to create an output for an input it has never seen before without any help from a human. Going back to our example of spam classification, using machine learning, the user provides the algorithm with a large number of emails (which are the input), together with information about whether any of these emails are spam (which is the desired output). Given a new email, the algorithm will then produce a prediction as to whether the new email is spam. Machine learning algorithms that learn from input/output pairs are called supervised learning algorithms because a \"teacher\" provides supervision to the algorithms in the form of the desired outputs for each example that they learn from. While creating a dataset of inputs and outputs is often a laborious manual process, supervised learning algorithms are well understood and their performance is easy to measure. If your application can be formulated as a supervised learning problem, and you are able to create a dataset that includes the desired outcome, machine learning will likely be able to solve your problem. Machine learning algorithms that learn from input/output pairs are called supervised learning algorithms because a \"teacher\" provides supervision to the algorithms in the form of the desired outputs for each example that they learn from. While creating a dataset of inputs and outputs is often a laborious manual process, supervised learning algorithms are well understood and their performance is easy to measure. If your application can be formulated as a supervised learning problem, and you are able to create a dataset that includes the desired outcome, machine learning will likely be able to solve your problem. Examples of supervised machine learning tasks include: create a dataset that includes the desired outcome, machine learning will likely be able to solve your problem. Examples of supervised machine learning tasks include: Identifying the zip code from handwritten digits on an envelope Examples of supervised machine learning tasks include: Identifying the zip code from handwritten digits on an envelope Here the input is a scan of the handwriting, and the desired output is the actual digits in the zip code. To create a dataset for building a machine learning model, you need to collect many envelopes. Then you can read the zip codes yourself and store the digits as your desired outcomes. Identifying the zip code from handwritten digits on an envelope Here the input is a scan of the handwriting, and the desired output is the actual digits in the zip code. To create a dataset for building a machine learning model, you need to collect many envelopes. Then you can read the zip codes yourself and store the digits as your desired outcomes. Determining whether a tumor is benign based on a medical image Here the input is a scan of the handwriting, and the desired output is the actual digits in the zip code. To create a dataset for building a machine learning model, you need to collect many envelopes. Then you can read the zip codes yourself and store the digits as your desired outcomes. Determining whether a tumor is benign based on a medical image Here the input is the image, and the output is whether the tumor is benign. To create a dataset for building a model, you need a database of medical images. You also need an expert opinion, so a doctor needs to look at all of the images and decide which tumors are benign and which are not. It might even be necessary to do additional diagnosis beyond the content of the image to determine whether the tumor in the image is cancerous or not. Determining whether a tumor is benign based on a medical image Here the input is the image, and the output is whether the tumor is benign. To create a dataset for building a model, you need a database of medical images. You also need an expert opinion, so a doctor needs to look at all of the images and decide which tumors are benign and which are not. It might even be necessary to do additional diagnosis beyond the content of the image to determine whether the tumor in the image is cancerous or not. Detecting fraudulent activity in credit card transactions Here the input is the image, and the output is whether the tumor is benign. To create a dataset for building a model, you need a database of medical images. You also need an expert opinion, so a doctor needs to look at all of the images and decide which tumors are benign and which are not. It might even be necessary to do additional diagnosis beyond the content of the image to determine whether the tumor in the image is cancerous or not. Detecting fraudulent activity in credit card transactions Here the input is a record of the credit card transaction, and the output is whether it is likely to be fraudulent or not. Assuming that you are the entity dis- tributing the credit cards, collecting a dataset means storing all transactions and recording if a user reports any transaction as fraudulent. Detecting fraudulent activity in credit card transactions Here the input is a record of the credit card transaction, and the output is whether it is likely to be fraudulent or not. Assuming that you are the entity dis- tributing the credit cards, collecting a dataset means storing all transactions and recording if a user reports any transaction as fraudulent. An interesting thing to note about these examples is that although the inputs and out- puts look fairly straightforward, the data collection process for these three tasks is vastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining medical imaging and diagnoses, on the other hand, requires not only expensive machinery but also rare and expensive expert knowledge, not to mention the ethical concerns and privacy issues. In the example of detecting credit card fraud, data col- lection is much simpler. Your customers will provide you with the desired output, as they will report fraud. All you have to do to obtain the input/output pairs of fraudu- lent and nonfraudulent activity is wait. Here the input is a record of the credit card transaction, and the output is whether it is likely to be fraudulent or not. Assuming that you are the entity dis- tributing the credit cards, collecting a dataset means storing all transactions and recording if a user reports any transaction as fraudulent. An interesting thing to note about these examples is that although the inputs and out- puts look fairly straightforward, the data collection process for these three tasks is vastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining medical imaging and diagnoses, on the other hand, requires not only expensive machinery but also rare and expensive expert knowledge, not to mention the ethical concerns and privacy issues. In the example of detecting credit card fraud, data col- lection is much simpler. Your customers will provide you with the desired output, as they will report fraud. All you have to do to obtain the input/output pairs of fraudu- lent and nonfraudulent activity is wait. Unsupervised algorithms are the other type of algorithm that we will cover in this book. In unsupervised learning, only the input data is known, and no known output data is given to the algorithm. While there are many successful applications of these methods, they are usually harder to understand and evaluate. An interesting thing to note about these examples is that although the inputs and out- puts look fairly straightforward, the data collection process for these three tasks is vastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining medical imaging and diagnoses, on the other hand, requires not only expensive machinery but also rare and expensive expert knowledge, not to mention the ethical concerns and privacy issues. In the example of detecting credit card fraud, data col- lection is much simpler. Your customers will provide you with the desired output, as they will report fraud. All you have to do to obtain the input/output pairs of fraudu- lent and nonfraudulent activity is wait. Unsupervised algorithms are the other type of algorithm that we will cover in this book. In unsupervised learning, only the input data is known, and no known output data is given to the algorithm. While there are many successful applications of these methods, they are usually harder to understand and evaluate. Examples of unsupervised learning include: Unsupervised algorithms are the other type of algorithm that we will cover in this book. In unsupervised learning, only the input data is known, and no known output data is given to the algorithm. While there are many successful applications of these methods, they are usually harder to understand and evaluate. Examples of unsupervised learning include:'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load grouped_data.json\n",
    "with open(\"D:/aksharaplus/output/sample/grouped_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    grouped_data = json.load(f)\n",
    "\n",
    "# Create fine-tunable JSONL\n",
    "finetune_data = []\n",
    "for item in grouped_data:\n",
    "    # Only include the text field\n",
    "    finetune_data.append({\"text\": item[\"text\"]})\n",
    "\n",
    "# Save as JSONL\n",
    "with open(\"D:/aksharaplus/output/sample/finetune_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in finetune_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# Debugging: Preview first few entries\n",
    "print(f\"Created {len(finetune_data)} JSONL entries\")\n",
    "print(\"Sample entries:\")\n",
    "for entry in finetune_data[:3]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7fac544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 374 JSONL entries\n",
      "Sample entries:\n",
      "{'text': \"Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science and is also known as predictive analytics or statistical learning. The application of machine learning methods has in recent years become ubiquitous in everyday life. From auto- matic recommendations of which movies to watch, to what food to order or which products to buy, to personalized online radio and recognizing your friends in your photos, many modern websites and devices have machine learning algorithms at their core. When you look at a complex website like Facebook, Amazon, or Netflix, it is very likely that every part of the site contains multiple machine learning models. Outside of commercial applications, machine learning has had a tremendous influ- ence on the way data-driven research is done today. The tools introduced in this book have been applied to diverse scientific problems such as understanding stars, finding distant planets, discovering new particles, analyzing DNA sequences, and providing personalized cancer treatments. Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science and is also known as predictive analytics or statistical learning. The application of machine learning methods has in recent years become ubiquitous in everyday life. From auto- matic recommendations of which movies to watch, to what food to order or which products to buy, to personalized online radio and recognizing your friends in your photos, many modern websites and devices have machine learning algorithms at their core. When you look at a complex website like Facebook, Amazon, or Netflix, it is very likely that every part of the site contains multiple machine learning models. Outside of commercial applications, machine learning has had a tremendous influ- ence on the way data-driven research is done today. The tools introduced in this book have been applied to diverse scientific problems such as understanding stars, finding distant planets, discovering new particles, analyzing DNA sequences, and providing personalized cancer treatments. Your application doesn't need to be as large-scale or world-changing as these exam- ples in order to benefit from machine learning, though. In this chapter, we will explain why machine learning has become so popular and discuss what kinds of problems can be solved using machine learning. Then, we will show you how to build your first machine learning model, introducing important concepts along the way. Outside of commercial applications, machine learning has had a tremendous influ- ence on the way data-driven research is done today. The tools introduced in this book have been applied to diverse scientific problems such as understanding stars, finding distant planets, discovering new particles, analyzing DNA sequences, and providing personalized cancer treatments. Your application doesn't need to be as large-scale or world-changing as these exam- ples in order to benefit from machine learning, though. In this chapter, we will explain why machine learning has become so popular and discuss what kinds of problems can be solved using machine learning. Then, we will show you how to build your first machine learning model, introducing important concepts along the way.\"}\n",
      "{'text': 'In the early days of \"intelligent\" applications, many systems used handcoded rules of \"if \" and \"else\" decisions to process data or adjust to user input. Think of a spam filter whose job is to move the appropriate incoming email messages to a spam folder. You could make up a blacklist of words that would result in an email being marked as spam. This would be an example of using an expert-designed rule system to design an \"intelligent\" application. Manually crafting decision rules is feasible for some applica- tions, particularly those in which humans have a good understanding of the process to model. However, using handcoded rules to make decisions has two major disad- vantages: In the early days of \"intelligent\" applications, many systems used handcoded rules of \"if \" and \"else\" decisions to process data or adjust to user input. Think of a spam filter whose job is to move the appropriate incoming email messages to a spam folder. You could make up a blacklist of words that would result in an email being marked as spam. This would be an example of using an expert-designed rule system to design an \"intelligent\" application. Manually crafting decision rules is feasible for some applica- tions, particularly those in which humans have a good understanding of the process to model. However, using handcoded rules to make decisions has two major disad- vantages: The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system. spam. This would be an example of using an expert-designed rule system to design an \"intelligent\" application. Manually crafting decision rules is feasible for some applica- tions, particularly those in which humans have a good understanding of the process to model. However, using handcoded rules to make decisions has two major disad- vantages: The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system. Designing rules requires a deep understanding of how a decision should be made by a human expert. The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system. Designing rules requires a deep understanding of how a decision should be made by a human expert. One example of where this handcoded approach will fail is in detecting faces in images. Today, every smartphone can detect a face in an image. However, face detec- tion was an unsolved problem until as recently as 2001. The main problem is that the way in which pixels (which make up an image in a computer) are \"perceived\" by the computer is very different from how humans perceive a face. This difference in repre- sentation makes it basically impossible for a human to come up with a good set of rules to describe what constitutes a face in a digital image. Designing rules requires a deep understanding of how a decision should be made by a human expert. One example of where this handcoded approach will fail is in detecting faces in images. Today, every smartphone can detect a face in an image. However, face detec- tion was an unsolved problem until as recently as 2001. The main problem is that the way in which pixels (which make up an image in a computer) are \"perceived\" by the computer is very different from how humans perceive a face. This difference in repre- sentation makes it basically impossible for a human to come up with a good set of rules to describe what constitutes a face in a digital image. Using machine learning, however, simply presenting a program with a large collec- tion of images of faces is enough for an algorithm to determine what characteristics are needed to identify a face. One example of where this handcoded approach will fail is in detecting faces in images. Today, every smartphone can detect a face in an image. However, face detec- tion was an unsolved problem until as recently as 2001. The main problem is that the way in which pixels (which make up an image in a computer) are \"perceived\" by the computer is very different from how humans perceive a face. This difference in repre- sentation makes it basically impossible for a human to come up with a good set of rules to describe what constitutes a face in a digital image. Using machine learning, however, simply presenting a program with a large collec- tion of images of faces is enough for an algorithm to determine what characteristics are needed to identify a face.'}\n",
      "{'text': 'The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, which is known as supervised learning , the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired out- put given an input. In particular, the algorithm is able to create an output for an input it has never seen before without any help from a human. Going back to our example of spam classification, using machine learning, the user provides the algorithm with a large number of emails (which are the input), together with information about whether any of these emails are spam (which is the desired output). Given a new email, the algorithm will then produce a prediction as to whether the new email is spam. Machine learning algorithms that learn from input/output pairs are called supervised learning algorithms because a \"teacher\" provides supervision to the algorithms in the form of the desired outputs for each example that they learn from. While creating a dataset of inputs and outputs is often a laborious manual process, supervised learning algorithms are well understood and their performance is easy to measure. If your application can be formulated as a supervised learning problem, and you are able to The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, which is known as supervised learning , the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired out- put given an input. In particular, the algorithm is able to create an output for an input it has never seen before without any help from a human. Going back to our example of spam classification, using machine learning, the user provides the algorithm with a large number of emails (which are the input), together with information about whether any of these emails are spam (which is the desired output). Given a new email, the algorithm will then produce a prediction as to whether the new email is spam. Machine learning algorithms that learn from input/output pairs are called supervised learning algorithms because a \"teacher\" provides supervision to the algorithms in the form of the desired outputs for each example that they learn from. While creating a dataset of inputs and outputs is often a laborious manual process, supervised learning algorithms are well understood and their performance is easy to measure. If your application can be formulated as a supervised learning problem, and you are able to create a dataset that includes the desired outcome, machine learning will likely be able to solve your problem. Machine learning algorithms that learn from input/output pairs are called supervised learning algorithms because a \"teacher\" provides supervision to the algorithms in the form of the desired outputs for each example that they learn from. While creating a dataset of inputs and outputs is often a laborious manual process, supervised learning algorithms are well understood and their performance is easy to measure. If your application can be formulated as a supervised learning problem, and you are able to create a dataset that includes the desired outcome, machine learning will likely be able to solve your problem. Examples of supervised machine learning tasks include: create a dataset that includes the desired outcome, machine learning will likely be able to solve your problem. Examples of supervised machine learning tasks include: Identifying the zip code from handwritten digits on an envelope Examples of supervised machine learning tasks include: Identifying the zip code from handwritten digits on an envelope Here the input is a scan of the handwriting, and the desired output is the actual digits in the zip code. To create a dataset for building a machine learning model, you need to collect many envelopes. Then you can read the zip codes yourself and store the digits as your desired outcomes. Identifying the zip code from handwritten digits on an envelope Here the input is a scan of the handwriting, and the desired output is the actual digits in the zip code. To create a dataset for building a machine learning model, you need to collect many envelopes. Then you can read the zip codes yourself and store the digits as your desired outcomes. Determining whether a tumor is benign based on a medical image Here the input is a scan of the handwriting, and the desired output is the actual digits in the zip code. To create a dataset for building a machine learning model, you need to collect many envelopes. Then you can read the zip codes yourself and store the digits as your desired outcomes. Determining whether a tumor is benign based on a medical image Here the input is the image, and the output is whether the tumor is benign. To create a dataset for building a model, you need a database of medical images. You also need an expert opinion, so a doctor needs to look at all of the images and decide which tumors are benign and which are not. It might even be necessary to do additional diagnosis beyond the content of the image to determine whether the tumor in the image is cancerous or not. Determining whether a tumor is benign based on a medical image Here the input is the image, and the output is whether the tumor is benign. To create a dataset for building a model, you need a database of medical images. You also need an expert opinion, so a doctor needs to look at all of the images and decide which tumors are benign and which are not. It might even be necessary to do additional diagnosis beyond the content of the image to determine whether the tumor in the image is cancerous or not. Detecting fraudulent activity in credit card transactions Here the input is the image, and the output is whether the tumor is benign. To create a dataset for building a model, you need a database of medical images. You also need an expert opinion, so a doctor needs to look at all of the images and decide which tumors are benign and which are not. It might even be necessary to do additional diagnosis beyond the content of the image to determine whether the tumor in the image is cancerous or not. Detecting fraudulent activity in credit card transactions Here the input is a record of the credit card transaction, and the output is whether it is likely to be fraudulent or not. Assuming that you are the entity dis- tributing the credit cards, collecting a dataset means storing all transactions and recording if a user reports any transaction as fraudulent. Detecting fraudulent activity in credit card transactions Here the input is a record of the credit card transaction, and the output is whether it is likely to be fraudulent or not. Assuming that you are the entity dis- tributing the credit cards, collecting a dataset means storing all transactions and recording if a user reports any transaction as fraudulent. An interesting thing to note about these examples is that although the inputs and out- puts look fairly straightforward, the data collection process for these three tasks is vastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining medical imaging and diagnoses, on the other hand, requires not only expensive machinery but also rare and expensive expert knowledge, not to mention the ethical concerns and privacy issues. In the example of detecting credit card fraud, data col- lection is much simpler. Your customers will provide you with the desired output, as they will report fraud. All you have to do to obtain the input/output pairs of fraudu- lent and nonfraudulent activity is wait. Here the input is a record of the credit card transaction, and the output is whether it is likely to be fraudulent or not. Assuming that you are the entity dis- tributing the credit cards, collecting a dataset means storing all transactions and recording if a user reports any transaction as fraudulent. An interesting thing to note about these examples is that although the inputs and out- puts look fairly straightforward, the data collection process for these three tasks is vastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining medical imaging and diagnoses, on the other hand, requires not only expensive machinery but also rare and expensive expert knowledge, not to mention the ethical concerns and privacy issues. In the example of detecting credit card fraud, data col- lection is much simpler. Your customers will provide you with the desired output, as they will report fraud. All you have to do to obtain the input/output pairs of fraudu- lent and nonfraudulent activity is wait. Unsupervised algorithms are the other type of algorithm that we will cover in this book. In unsupervised learning, only the input data is known, and no known output data is given to the algorithm. While there are many successful applications of these methods, they are usually harder to understand and evaluate. An interesting thing to note about these examples is that although the inputs and out- puts look fairly straightforward, the data collection process for these three tasks is vastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining medical imaging and diagnoses, on the other hand, requires not only expensive machinery but also rare and expensive expert knowledge, not to mention the ethical concerns and privacy issues. In the example of detecting credit card fraud, data col- lection is much simpler. Your customers will provide you with the desired output, as they will report fraud. All you have to do to obtain the input/output pairs of fraudu- lent and nonfraudulent activity is wait. Unsupervised algorithms are the other type of algorithm that we will cover in this book. In unsupervised learning, only the input data is known, and no known output data is given to the algorithm. While there are many successful applications of these methods, they are usually harder to understand and evaluate. Examples of unsupervised learning include: Unsupervised algorithms are the other type of algorithm that we will cover in this book. In unsupervised learning, only the input data is known, and no known output data is given to the algorithm. While there are many successful applications of these methods, they are usually harder to understand and evaluate. Examples of unsupervised learning include:'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Load grouped_data.json\n",
    "with open(\"D:/aksharaplus/output/sample/grouped_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    grouped_data = json.load(f)\n",
    "\n",
    "# Create fine-tunable JSONL, excluding index entries\n",
    "finetune_data = []\n",
    "for item in grouped_data:\n",
    "    # Skip entries where parent_title is a single letter (A-Z)\n",
    "    if re.match(r\"^[A-Z]$\", item[\"parent_title\"]):\n",
    "        continue\n",
    "    finetune_data.append({\"text\": item[\"text\"]})\n",
    "\n",
    "# Save as JSONL\n",
    "with open(\"D:/aksharaplus/output/sample/finetune_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in finetune_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# Debugging: Preview results\n",
    "print(f\"Created {len(finetune_data)} JSONL entries\")\n",
    "print(\"Sample entries:\")\n",
    "for entry in finetune_data[:3]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cb82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
