{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ceba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#google colab link:-\n",
    "# https://colab.research.google.com/drive/1K68jDQhKwvy4IL_X48AuBC9HrieOgHDz#scrollTo=PRj1HrUTaUyV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c0683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clean uninstall\n",
    "!pip uninstall -y torch torchvision torchaudio transformers accelerate peft datasets numpy numba -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889311ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Install compatible versions (CUDA 11.8)\n",
    "!pip install --index-url https://download.pytorch.org/whl/cu118 torch==2.1.2+cu118 torchvision==0.16.2+cu118 torchaudio==2.1.2+cu118 -q\n",
    "!pip install transformers==4.38.2 peft==0.7.1 accelerate==0.30.1 datasets==2.18.0 numpy==1.26.4 numba==0.60.0 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install compatible libraries\n",
    "# !pip uninstall -y torch torchaudio torchvision transformers datasets peft accelerate numpy -q\n",
    "# !pip install -q torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 transformers==4.41.2 datasets==2.20.0 peft==0.11.1 accelerate==0.31.1 numpy==1.26.4\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from google.colab import files\n",
    "\n",
    "# --- Precaution 1: Set Output Directory ---\n",
    "OUTPUT_DIR = \"/content/finetuned_qwen\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Precaution 2: Validate Dataset ---\n",
    "DATA_PATH = \"/content/finetune_data.jsonl\"\n",
    "print(\"Validating dataset...\")\n",
    "try:\n",
    "    with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        num_lines = len(lines)\n",
    "        if num_lines < 100:\n",
    "            print(f\"Warning: dataset has only {num_lines} lines. Consider adding more data.\")\n",
    "        index_detected = False\n",
    "        for i, line in enumerate(lines[:10]):\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                if not entry.get(\"text\"):\n",
    "                    print(f\"Error: empty text field in line {i+1}\")\n",
    "                    continue\n",
    "                if any(term in entry[\"text\"].lower() for term in [\"a/b testing, 359\", \"accuracy, 22\"]):\n",
    "                    print(f\"Warning: index data detected in line {i+1}. Filtering now...\")\n",
    "                    index_detected = True\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: malformed JSON in line {i+1}\")\n",
    "        if index_detected:\n",
    "            print(\"Filtering index data...\")\n",
    "            clean_lines = [line for line in lines if not any(term in json.loads(line.strip())[\"text\"].lower() for term in [\"a/b testing\", \"accuracy, 22\"])]\n",
    "            with open(DATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.writelines(clean_lines)\n",
    "            num_lines = len(clean_lines)\n",
    "            print(f\"Cleaned dataset: {num_lines} lines remain.\")\n",
    "    print(f\"Dataset validation complete: {num_lines} lines found.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: finetune_data.jsonl not found. Uploading now...\")\n",
    "    uploaded = files.upload()\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(\"Upload failed. Please upload finetune_data.jsonl\")\n",
    "\n",
    "# --- Precaution 3: Check for Existing Checkpoints ---\n",
    "latest_checkpoint = None\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        print(f\"Found checkpoint: {latest_checkpoint}. Will resume training.\")\n",
    "\n",
    "# --- Precaution 4: Load Dataset ---\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    # Load JSONL manually to avoid caching issues\n",
    "    data = []\n",
    "    with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: skipping malformed JSON line\")\n",
    "    dataset = Dataset.from_list(data)\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"Dataset is empty. Check finetune_data.jsonl content.\")\n",
    "    print(f\"Loaded dataset with {len(dataset)} examples.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Create validation split (10% of data, if possible)\n",
    "if len(dataset) > 10:\n",
    "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "else:\n",
    "    train_dataset = dataset\n",
    "    eval_dataset = None\n",
    "    print(\"Warning: dataset too small for validation split. Skipping early stopping.\")\n",
    "\n",
    "# --- Precaution 5: Load Model and Tokenizer ---\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}. Check internet connection or model name.\")\n",
    "    raise e\n",
    "\n",
    "# --- Precaution 6: Tokenize Dataset ---\n",
    "def tokenize_function(example):\n",
    "    # Tokenize the text\n",
    "    tokenized_output = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # For causal language modeling, labels are the same as input_ids\n",
    "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"].clone()\n",
    "    return tokenized_output\n",
    "\n",
    "try:\n",
    "    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "    if eval_dataset:\n",
    "        tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error tokenizing dataset: {e}. Check dataset content or memory.\")\n",
    "    raise e\n",
    "\n",
    "# Remove non-tensor columns, but keep 'labels'\n",
    "tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
    "if eval_dataset:\n",
    "    tokenized_eval = tokenized_eval.remove_columns([\"text\"])\n",
    "\n",
    "# --- Precaution 7: Configure LoRA ---\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- Precaution 8: Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy=\"epoch\",  # Align with evaluation_strategy\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    logging_dir=\"/content/logs\",\n",
    "    report_to=\"none\",\n",
    "    # Changed 'evaluation_strategy' to 'eval_strategy' for compatibility with transformers==4.38.2\n",
    "    eval_strategy=\"epoch\" if eval_dataset else \"no\",\n",
    "    load_best_model_at_end=True if eval_dataset else False,\n",
    "    metric_for_best_model=\"loss\" if eval_dataset else None,\n",
    "    greater_is_better=False if eval_dataset else None\n",
    ")\n",
    "\n",
    "# --- Precaution 9: Initialize Trainer with Early Stopping ---\n",
    "callbacks = []\n",
    "if eval_dataset:\n",
    "    callbacks.append(EarlyStoppingCallback(\n",
    "        early_stopping_patience=1,\n",
    "        early_stopping_threshold=0.01\n",
    "    ))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval if eval_dataset else None,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# --- Precaution 10: Resume Training ---\n",
    "try:\n",
    "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}. Check logs in /content/logs for details.\")\n",
    "    raise e\n",
    "\n",
    "# --- Precaution 11: Save Final Model ---\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# --- Precaution 12: Save Logs ---\n",
    "with open(f\"{OUTPUT_DIR}/training_log.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "# --- Precaution 13: Zip and Download Model ---\n",
    "!zip -r finetuned_qwen.zip {OUTPUT_DIR}\n",
    "files.download(\"finetuned_qwen.zip\")\n",
    "print(f\"Model saved to {OUTPUT_DIR}. Download finetuned_qwen.zip for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3230b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, \"/content/finetuned_qwen\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/finetuned_qwen\")\n",
    "inputs = tokenizer(\"what is OvO in multiclass classification,do not hellucinate\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=400, do_sample=True, top_p=0.9)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
